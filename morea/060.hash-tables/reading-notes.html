<!DOCTYPE html>
<html>
<head>
  <title> Chapter 6 Notes | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">

  <!--  Load bootswatch-based Morea theme file. -->
  <link rel="stylesheet" href="/ics311s14/css/themes/cerulean/bootstrap.min.css">
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" href="/ics311s14/css/syntax.css">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        
          <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        
        
          <li><a href="/ics311s14/readings/">Readings</a></li>
        
        
          <li><a href="/ics311s14/experiences/">Experiences</a></li>
        
        
        <li><a href="/ics311s14/schedule/">Schedule</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h2 id="outline">Outline</h2>

<ol>
  <li>Motivations and Introduction</li>
  <li>Hash Tables with Chaining </li>
  <li>Hash Functions and Universal Hashing</li>
  <li>Open Addressing Strategies</li>
</ol>

<h2 id="motivations-and-introduction">Motivations and Introduction</h2>

<p>Many applications only need the insert, search and delete operations of a
<a href="http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-04
.html#dynamicsetadt">dynamic set</a>. Example: symbol table in a compiler.</p>

<p>Hash tables are an effective approach. Under reasonable assumptions, they have
O(1) operations, but they can be Θ(n) worst case</p>

<h3 id="direct-addressing">Direct Addressing</h3>

<p>Hash tables generalize arrays. Let’s look at the idea with arrays first. Given
a key <em>k</em> from a universe <em>U</em> of possible keys, a <strong>direct address table</strong>
stores and retrieves the element in position <em>k</em> of the array.</p>

<p><img src="fig/Fig-11-1-direct-address.jpg" alt="" /></p>

<p>Direct addressing is applicable when we can allocate an array with one element
for every key (i.e., of size |<em>U</em>|). It is trivial to implement:</p>

<p><img src="fig/pseudocode-direct-address.jpg" alt="" /></p>

<p>However, often the space of possible keys is much larger than the number of
actual keys we expect, so it would be wasteful of space (and sometimes not
possible) to allocate an array of size |<em>U</em>|.</p>

<h3 id="hash-tables-and-functions">Hash Tables and Functions</h3>

<p><strong>Hash tables</strong> are also arrays, but typically of size proportional to the number of keys expected to be stored (rather than to the number of keys). </p>

<table>
  <tbody>
    <tr>
      <td>If the expected keys K ⊂ U, the Universe of keys, and</td>
      <td>K</td>
      <td>is substantially</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>smaller than</td>
      <td>U</td>
      <td>, then hash tables can reduce storage requirements to Θ(</td>
      <td>K</td>
      <td>).</td>
    </tr>
  </tbody>
</table>

<p>A <strong>hash function</strong> <em>h(k)</em> maps the larger universe U of external keys to
indices into the array. Given a table of size <em>m</em> with zero-based indexing (we
shall see why this is useful):</p>

<ul>
  <li><em>h</em> : U -&gt; {0, 1, …, <em>m</em>-1}.</li>
  <li>We say that <em>k</em> <strong>hashes</strong> to slot <em>h(k)</em>. </li>
</ul>

<h3 id="collisions">Collisions</h3>

<p>The major issue to deal with in designing and implementing hash tables is what
to do when the hash function maps multiple keys to the same table entry.</p>

<p><img src="fig/Fig-11-2-collisions.jpg" alt="" /></p>

<table>
  <tbody>
    <tr>
      <td>Collisions may or may not happen when</td>
      <td>K</td>
      <td>≤ <em>m</em>, but definitely happens when</td>
    </tr>
    <tr>
      <td> </td>
      <td>K</td>
      <td>&gt; <em>m</em>. <em>(Is there any way to avoid this?)</em></td>
    </tr>
  </tbody>
</table>

<p>There are two major approaches: Chaining (the preferred method) and Open
Addressing. We’ll look at these and also hash function design.</p>

<hr />

<h2 id="hash-tables-with-chaining">Hash Tables with Chaining</h2>

<p>A simple resolution: Put elements that hash to the same slot into a linked
list. This is called <em>chaining</em> because we chain elements off the slot of the
hash table.</p>

<ul>
  <li>Slot <em>j</em> points to the head of a list of all stored elements that hash to <em>j</em>, or to NIL if there are no such elements.</li>
  <li>Doubly linked lists may be used when deletions are expected to be frequent.</li>
  <li>Sentinels can also be used to simplify the code.</li>
</ul>

<p><img src="fig/Fig-11-3-chaining.jpg" alt="" /></p>

<h3 id="pseudocode-for-chaining">Pseudocode for Chaining</h3>

<p>Implementation is simple if you already have implemented linked lists:</p>

<p><img src="fig/pseudocode-chained-hashing.jpg" alt="" /></p>

<p><em>What are the running times for these algorithms? Which can we state directly,
and what do we need to know to determine the others?</em></p>

<h3 id="analysis-of-hashing-with-chaining">Analysis of Hashing with Chaining</h3>

<p>How long does it take to find an element with a given key, or to determine
that there is no such element?</p>

<ul>
  <li>Analysis is in terms of the <strong>load factor <em>α = n/m</em></strong>, where 
    <ul>
      <li><em>n</em> = number of elements in the table </li>
      <li><em>m</em> = number of slots in the table = number of (possibly empty) linked lists</li>
    </ul>
  </li>
  <li>The load factor α is the average number of elements per linked list. </li>
  <li>Can have α &lt; 1; α = 1; or α &gt; 1. </li>
  <li>
    <p>Worst case is when all <em>n</em> keys hash to the same slot. <br />
<em>Why? What happens? Θ(</em><em>__</em>?)_</p>
  </li>
  <li>Average case depends on how well the hash function distributes the keys among the slots. </li>
</ul>

<p>Let’s analyze averge-case performance under the assumption of <strong>simple uniform
hashing:</strong> any given element is equally likely to hash into any of the <em>m</em>
slots:</p>

<ul>
  <li>For <em>j</em> = 0, 1, …, <em>m</em>-1, denote the length of list T[<em>j</em>] by <em>nj</em>.</li>
  <li>Then <em>n</em> = <em>n0</em> + <em>n1</em> + … + <em>nm-1</em>. </li>
  <li>Average value of <em>nj</em> is E[<em>nj</em>] = α = <em>n/m</em>. </li>
  <li>Assuming <em>h(k)</em> computed in O(1), so time to search for <em>k</em> depends on length <em>nh(k)</em> of the list T[<em>h(k)</em>]. </li>
</ul>

<p>Consider two cases: Unsuccessful and Successful search. The former analysis is
simpler because you always search to the end, but for successful search it
depends on where in T[<em>h(k)</em>] the element with key <em>k</em> will be found.</p>

<h4 id="unsuccessful-search">Unsuccessful Search</h4>

<p>Simple uniform hashing means that any key not in the table is equally likely
to hash to any of the <em>m</em> slots.</p>

<p>We need to search to end of the list T[<em>h(k)</em>]. It has expected length
E[<em>nh(k)</em>] = α = <em>n/m</em>.</p>

<p>Adding the time to compute the hash function gives <strong>Θ(1 + α)</strong>. (We leave in
the “1” term for the initial computation of <em>h</em> since α can be 0, and we don’t
want to say that the computation takes Θ(0) time).</p>

<h4 id="successful-search">Successful Search</h4>

<p>We assume that the element <em>x</em> being searched for is equally likely to be any
of the <em>n</em> elements stored in the table.</p>

<p>The number of elements examined during a successful search for <em>x</em> is 1 more
than the number of elements that appear before <em>x</em> in <em>x</em>’s list (because we
have to search them, and then examine <em>x</em>).</p>

<p>These are the elements inserted <em>after x</em> was inserted (because we insert at
the head of the list).</p>

<p>Need to find on average, over the <em>n</em> elements <em>x</em> in the table, how many
elements were inserted into <em>x</em>’s list after <em>x</em> was inserted. <em>Lucky we just
studied indicator random variables!</em></p>

<p>For <em>i</em> = 1, 2, …, <em>n</em>, let <em>xi</em> be the <em>i_th element inserted into the
table, and let _ki</em> = <em>key</em>[<em>xi</em>].</p>

<p>For all <em>i</em> and <em>j</em>, define the indicator random variable:</p>

<blockquote>
  <p><em>Xij</em> = I{<em>h(ki)</em> = <em>h(kj)</em>}.     <em>(The event that keys _ki</em> and <em>kj</em> hash
to the same slot.)_</p>
</blockquote>

<p><img src="fig/lemming.jpg" alt="" /></p>

<p>Simple uniform hashing implies that Pr{<em>h(ki)</em> = <em>h(kj)</em>} = 1/<em>m</em> <em>(Why?)</em></p>

<p>Therefore, E[<em>Xij</em>] = 1/<em>m</em> by Lemma 1 (<a href="http://www2.hawaii.edu/~sut
hers/courses/ics311s14/Notes/Topic-05.html#lemma1">Topic #5</a>).</p>

<p>The expected number of elements examined in a successful search is those
elements <em>j</em> that are inserted after the element <em>i</em> of interest <em>and</em> that
end up in the same linked list (<em>Xij</em>):</p>

<p><img src="fig/analysis-chaining-1.jpg" alt="" /></p>

<ul>
  <li>The innermost summation is adding up, for all <em>j</em> inserted after <em>i</em> (<em>j</em>=<em>i</em>+1), those that are in the same hash table (when <em>Xij</em> = 1).</li>
  <li>The outermost summation runs this over all <em>n</em> of the keys inserted (indexed by <em>i</em>), and finds the average by dividing by <em>n</em>.</li>
</ul>

<p>I fill in some of the implicit steps in the rest of the text’s analysis.
First, by linearity of expectation we can move the E in:</p>

<p><img src="fig/analysis-chaining-2.jpg" alt="" /></p>

<p>That is the crucial move: instead of analyzing the probability of complex
events, use indicator random variables to break them down into simple events
that we know the probabilities for. In this case we know E[<em>Xi,j</em>] (if <em>you</em>
don’t know, ask the lemming above):</p>

<p><img src="fig/analysis-chaining-3.jpg" alt="" /></p>

<p>Multiplying 1/<em>n</em> by the terms inside the summation,</p>

<ul>
  <li>For the first term, we get Σ_i<em>=1,_n_1/_n</em>, which is just <em>n</em>/<em>n</em> or 1</li>
  <li>Move 1/<em>m</em> outside the summation of the second term to get 1/<em>nm</em>. This leaves Σ_i<em>=1,_n</em>(Σ_j<em>=_i</em>+1,<em>n_1), which simplifies as shown below (if you added 1 _n</em> times, you would overshoot by <em>i</em>).
<img src="fig/analysis-chaining-4.jpg" alt="" /></li>
</ul>

<p>Splitting the two terms being summed, the first is clearly <em>n_2, and the
second is the familiar sum of the first _n</em> numbers:</p>

<p><img src="fig/analysis-chaining-5.jpg" alt="" />  </p>

<p><img src="fig/analysis-chaining-6.jpg" alt="" /></p>

<p>Distributing the 1/<em>nm</em>, we get 1 + (<em>n_2/_nm</em> - <em>n</em>(<em>n</em>+1)/2_nm_   =   1 +
<em>n</em>/<em>m</em> - (<em>n</em>+1)/2_m_   =   1 + 2_n<em>/2_m</em> - (<em>n</em>+1)/2_m_, and now we can
combine the two fractions:</p>

<p><img src="fig/analysis-chaining-7.jpg" alt="" /></p>

<p>Now we can turn two instances of <em>n</em>/<em>m</em> into α with this preparation: 1 +
(<em>n</em> - 1)/2_m_   =   1 + <em>n</em>/2_m_ - 1/2_m_   =   1 + α/2 - n/2_mn_   =  </p>

<p><img src="fig/analysis-chaining-8.jpg" alt="" /></p>

<p>Adding the time (1) for computing the hash function, the expected total time
for a successful search is:</p>

<blockquote>
  <p>Θ(2 + α/2 - α/2_n_) = <strong>Θ(1 + α).</strong></p>
</blockquote>

<p>since the third term vanishes in significance as <em>n</em> grows, and the constants
2 and 1/2 have Θ(1) growth rate.</p>

<p>Thus, <strong>search is an average of Θ(1 + α) in either case.</strong></p>

<p>If the number of elements stored <em>n</em> is bounded within a constant factor of
the number of slots <em>m</em>, i.e., <em>n</em> = O(<em>m</em>), then α is a constant, and search
is O(1) on average.</p>

<p>Since insertion takes O(1) worst case and deletion takes O(1) worst case when
doubly linked lists are used, all three operations for hash tables are O(1) on
average.</p>

<p><em>(I went through that analysis in detail to show again the utility of
indicator random variables and to demonstrate what is possibly the most
crucial fact of this chapter, but we won’t do the other analyses in detail.
With perserverence you can similarly unpack the other analyses.)</em></p>

<hr />

<h2 id="hash-functions-and-universal-hashing">Hash Functions and Universal Hashing</h2>

<p>Ideally a hash function satisfies the assumptions of simple uniform hashing.</p>

<p>This is not possible in practice, since we don’t know in advance the
probability distribution of the keys, and they may not be drawn independently.</p>

<p>Instead, we use heuristics based on what we know about the domain of the keys
to create a hash function that performs well.</p>

<h3 id="keys-as-natural-numbers">Keys as natural numbers</h3>

<p>Hash functions assume that the keys are natural numbers. When they are not, a
conversion is needed. Some options:</p>

<ul>
  <li>Floating point numbers: If an integer is required, sum the mantissa and exponent, treating them as integers.</li>
  <li>Character string: Sum the ASCII or Unicode values of the characters of the string. </li>
  <li>Character string: Interpret the string as an integer expressed in some radix notation. (This gives very large integers.) </li>
</ul>

<h3 id="division-method">Division method</h3>

<p>A common hash function: <strong><em>h(k)</em> = <em>k</em> mod <em>m</em></strong>.<br />
<em>(Why does this potentially produce all legal values, and only legal values?)</em></p>

<p><em>Advantage:</em> Fast, since just one division operation required.</p>

<p><em>Disadvantage:</em> Need to avoid certain values of <em>m</em>, for example:</p>

<ul>
  <li>
    <p>Powers of 2. If <em>m</em> = 2_p_ for integer <em>p</em> then <em>h(k)</em> is the least significant <em>p</em> bits of <em>k</em>. <br />
(There may be a domain pattern that makes the keys clump together).</p>
  </li>
  <li>
    <p>If character strings are interpreted in radix 2_p_ then <em>m</em> = 2_p_ - 1 is a bad choice: permutations of characters hash the same. </p>
  </li>
</ul>

<p>A prime number not too close to an exact power of 2 is a good choice for <em>m</em>.</p>

<h3 id="multiplication-method">Multiplication method</h3>

<p><strong><em>h(k)</em> = Floor(<em>m</em>(<em>k</em> A mod 1))</strong>, where <em>k</em> A mod 1 = fractional part of _k_A. </p>

<ol>
  <li>Choose a constant A in range 0 &lt; A &lt; 1. </li>
  <li>Multiply <em>k</em> by A</li>
  <li>Extract the fractional part of _k_A</li>
  <li>Multiply the fractional part by <em>m</em></li>
  <li>Take the floor of the result. </li>
</ol>

<p><em>Disadvantage:</em> Slower than division.</p>

<p><em>Advantage:</em> The value of <em>m</em> is not critical.</p>

<p>The book discusses an implementation that we won’t get into …</p>

<p><img src="fig/Fig-11-4-multiplication-hashing.jpg" alt="" /></p>

<h3 id="universal-hashing">Universal Hashing</h3>

<p><img src="fig/badguy.jpg" alt="" /></p>

<p>Our malicious adversary is back! He’s choosing keys that all hash to the same
slot, giving worst case behavior and gumming up our servers! What to do?</p>

<p>Random algorithms to the rescue: randomly choose a different hash function
each time you construct and use a new hash table.</p>

<p>But it has to be a good one. Can we define a family of good candidates?</p>

<p>Consider a finite collection <em>Η</em> of hash functions that map universe U of keys
into {0, 1, …, <em>m</em>-1}.</p>

<p><em>Η</em> is <strong>universal</strong> if for each pair of keys <em>k, l</em> ∈ U, where <em>k ≠ l</em>, the
number of hash functions <em>h ∈ Η</em> for which <em>h(k) = h(l)</em> is less than or equal
to <em>|Η|/m</em> (that’s the size of <em>Η</em> divided by <em>m</em>).</p>

<p>In other words, with a hash function <em>h</em> chosen randomly from <em>Η</em>, the
probability of collision between two different keys is no more than <em>1/m</em>, the
chance of a collision when choosing two slots randomly and independently.</p>

<p>Universal hash functions are good because (proven as Theorem 11.3 in text):</p>

<ul>
  <li>If <em>k</em> is not in the table, the expected length E[<em>nh(k)</em>] of the list that <em>k</em> hashes to is less than or equal to α. </li>
  <li>If <em>k</em> is in the table, the expected length E[<em>nh(k)</em>] of the list that holds <em>k</em> is less than or equal to 1 + α. </li>
</ul>

<p>Therefore, the expected time for search is O(1).</p>

<p>One candidate for a collection <em>Η</em> of hash functions is:</p>

<blockquote>
  <p><em>Η</em> = {<em>hab</em>(<em>k</em>) : <strong><em>hab</em>(<em>k</em>) = ((<em>ak + b</em>) mod <em>p</em>) mod <em>m</em>)},</strong> where
<em>a</em> ∈ {1, 2, …, <em>p</em>-1} and <em>b</em> ∈ {0, 1, …, <em>p</em>-1}, where <em>p</em> is prime and
larger than the largest key.</p>
</blockquote>

<p>Details in text, including proof that this provides a universal set of hash
functions. Java built in hash functions take care of much of this for you:
read the Java documentation for details.</p>

<hr />

<h2 id="open-addressing-strategies">Open Addressing Strategies</h2>

<p>Open Addressing seeks to avoid the extra storage of linked lists by putting
all the keys in the hash table itself.</p>

<p>Of course, we need a way to deal with collisions. If a slot is already
occupied we will apply a systematic strategy for searching for alternative
slots. This same strategy is used in both insertion and search.</p>

<h3 id="probes-and-hki">Probes and <em>h</em>(<em>k</em>,<em>i</em>)</h3>

<p>Examining a slot is called a <strong>probe</strong>. We need to extend the hash function
<em>h</em> to take the probe number as a second argument, so that <em>h</em> can try
something different on subsequent probes. We count probes from 0 to <em>m</em>-1
(you’ll see why later), so the second argument takes on the same values as the
result of the function:</p>

<blockquote>
  <p><strong><em>h</em> : <em>U</em> x {0, 1, … <em>m</em>-1} -&gt; {0, 1, … <em>m</em>-1}</strong>  </p>
</blockquote>

<p>We require that the <strong>probe sequence</strong></p>

<blockquote>
  <p>⟨ <em>h</em>(<em>k</em>,0),   <em>h</em>(<em>k</em>,1)   …   <em>h</em>(<em>k</em>,<em>m</em>-1) ⟩</p>
</blockquote>

<p>be a permutation of ⟨ 0, 1, … <em>m</em>-1 ⟩. Another way to state this requirement
is that all the positions are visited.</p>

<p>There are three possible outcomes to a probe: <em>k</em> is in the slot probed
(successful search); the slot contains NIL (unsuccessful search); or some
other key is in the slot (need to continue search).</p>

<p>The strategy for this continuation is the crux of the problem, but first let’s
look at the general pseudocode.</p>

<h3 id="pseudocode">Pseudocode</h3>

<p><strong>Insertion</strong> returns the index of the slot it put the element in <em>k</em>, or throws an error if the table is full:</p>

<p><img src="fig/pseudocode-open-hash-insert.jpg" alt="" /></p>

<p><strong>Search</strong> returns either the index of the slot containing element of key <em>k</em>, or NIL if the search is unsuccessful:</p>

<p><img src="fig/pseudocode-open-hash-search.jpg" alt="" /></p>

<p><strong>Deletion</strong> is a bit complicated. We can’t just write NIL into the slot we want to delete. <em>(Why?)</em></p>

<p>Instead, we write a special value DELETED. During search, we treat it as if it
were a non-matching key, but insertion treats it as empty and reuses the slot.</p>

<p><em>Problem:</em> the search time is no longer dependent on α. <em>(Why?)</em></p>

<p>The ideal is to have <strong>uniform hashing</strong>, where each key is equally likely to
have any of the <em>m</em>! permutations of ⟨0, 1, … <em>m</em>-1⟩ as its probe sequence.
But this is hard to implement: we try to guarantee that the probe sequence is
<em>some</em> permutation of ⟨0, 1, … <em>m</em>-1⟩.</p>

<p>We will define the hash functions in terms of ** auxiliary hash functions**
that do the initial mapping, and define the primary function in terms of its
<em>i_th iterations, where 0 ≤ _i</em> &lt; <em>m</em>.</p>

<h3 id="linear-probing">Linear Probing</h3>

<p>Given an <strong>auxiliary hash function <em>h’</em></strong>, the probe sequence starts at
<em>h’</em>(<em>k</em>), and continues sequentially through the table:</p>

<blockquote>
  <p><em>h</em>(<em>k</em>,<em>i</em>) = (<em>h’</em>(<em>k</em>) + <em>i</em>) mod <em>m</em></p>
</blockquote>

<p><em>Problem:</em> <strong>primary clustering</strong>: sequences of keys with the same <em>h’</em> value
build up long runs of occupied sequences.</p>

<h3 id="quadratic-probing">Quadratic Probing</h3>

<p>Quadratic probing is attempt to fix this … instead of reprobing linearly, QP
“jumps” around the table according to a quadratic function of the probe, for
example:</p>

<blockquote>
  <p><em>h</em>(<em>k</em>,<em>i</em>) = (<em>h’</em>(<em>k</em>) + <em>c_1_i</em> + <em>c_2_i_2) mod _m</em>,<br />
where _c_1 and _c_2 are constants.</p>
</blockquote>

<p><em>Problem:</em> <strong>secondary clustering</strong>: although primary clusters across
sequential runs of table positions don’t occur, two keys with the same <em>h’</em>
may still have the same probe sequence, creating clusters that are broken
across the same sequence of “jumps”.</p>

<h3 id="double-hashing">Double Hashing</h3>

<p>A better approach: use two auxiliary hash functions <em>h1</em> and <em>h_2, where _h_1
gives the initial probe and _h_2 gives the remaining probes (here you can see
that having _i</em>=0 initially drops out the second hash until it is needed):
<img src="fig/Fig-11-5-double-hashing.jpg" alt="" /></p>

<blockquote>
  <p><em>h</em>(<em>k</em>,<em>i</em>) = (<em>h_1(_k</em>) + <em>ih_2(_k</em>)) mod <em>m</em>.</p>
</blockquote>

<p><em>h_2(_k</em>) must be relatively prime to <em>m</em> (relatively prime means they have no
factors in common other than 1) to guarantee that the probe sequence is a full
permutation of ⟨0, 1, … <em>m</em>-1⟩. Two approaches:</p>

<ul>
  <li>Choose <em>m</em> to be a power of 2 and _h_2 to always produce an odd number &gt; 1.</li>
  <li>Let <em>m</em> be prime and have 1 &lt; <em>h_2(_k</em>) &lt; <em>m</em>. <br />
(The example figure is <em>h_1(_k</em>) = <em>k</em> mod 13, and <em>h_2(_k</em>) = 1 + (<em>k</em> mod
11).)</li>
</ul>

<p>There are Θ(<em>m_2) different probe sequences, since each possible combination
of _h_1(_k</em>) and <em>h_2(_k</em>) gives a different probe sequence. This is an
improvement over linear or quadratic hashing.</p>

<h3 id="analysis-of-open-addressing">Analysis of Open Addressing</h3>

<p>The textbook develops two theorems you will use to compute the expected number
of probes for unsuccessful and successful search. (These theorems require α &lt;
1 because an expression 1/1−α is derived and we don’t want to divide by 0.)</p>

<blockquote>
  <p><strong>Theorem 11.6:</strong> Given an open-address hash table with load factor α =
<em>n</em>/<em>m</em> &lt; 1, the expected number of probes in an <em><strong>unsuccessful</strong></em> search is
at most <strong>1/(1 − α)</strong>, assuming uniform hashing.</p>
</blockquote>

<blockquote>
  <p><strong>Theorem 11.8:</strong> Given an open-address hash table with load factor α =
<em>n</em>/<em>m</em> &lt; 1, the expected number of probes in a <em><strong>successful</strong></em> search is at
most <strong>(1/α) ln (1/(1 − α))</strong>, assuming uniform hashing and assuming that each
key in the table is equally likely to be searched for.</p>
</blockquote>

<p>We leave the proofs for the textbook, but note particularly the “intuitive
interpretation” in the proof of 11.6 of the <strong><em>expected number of probes</em></strong> on
page 275:</p>

<blockquote>
  <p>E[<em>X</em>]   =   1/(1-α)   =   1   +   α   +   α2   +   α3   +   …</p>
</blockquote>

<p>We always make the first probe (1). With probability α &lt; 1, the first probe
finds an occupied slot, so we need to probe a second time (α). With
probability α2, the first two slots are occupied, so we need to make a third
probe …</p>

<hr />

<p>Dan Suthers Last modified: Sun Feb 16 02:14:59 HST 2014<br />
Images are from the instructor’s material for Cormen et al. Introduction to
Algorithms, Third Edition.  </p>


</div>



<div class="footer-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br />
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a href="http://morea-framework.github.io/">Morea Framework</a> (Theme: cerulean)<br>
       Last update on: <span>2014-05-22 15:05:21 -1000</span></p>
    <p style="margin: 0">
      25 modules
      
        | 27 outcomes
      
      
        | 143 readings
      
      
        | 36 experiences
      
      
    </p>
  </div>
</footer>
</div>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
  <title> Chapter 3 Notes | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/3.1.0/cerulean/bootstrap.min.css">

  <!--  Load site-specific customizations after bootstrap. -->
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:normal,italic,bold">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        <li><a href="/ics311s14/readings/">Readings</a></li>
        <li><a href="/ics311s14/experiences/">Experiences</a></li>
        <li><a href="/ics311s14/assessments/">Assessments</a></li>
        <li><a href="/ics311s14/schedule/">Schedule</a></li>
        
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h2>Outline</h2>

<ol>
<li>Intro to Asymptotic Analysis</li>
<li>Big-O</li>
<li>Ω (Omega)</li>
<li>Θ (Theta)</li>
<li>Asymptotic Notation in Equations</li>
<li>Asymptotic Inequality</li>
<li>Properties of Asymptotic Sets</li>
<li>Common Functions</li>
</ol>

<hr>

<h2>Intro to Asymptotic Analysis</h2>

<p>The notations discussed today are ways to describe behaviors of <em>functions,</em>
particularly <em>in the limit</em>, or <em>asymptotic</em> behavior.</p>

<p>The functions need not necessarily be about algorithms, and indeed asymptotic
analysis is used for many other applications.</p>

<p>Asymptotic analysis of algorithms requires:</p>

<ol>
<li>Identifying ** what aspect of an algorithm we care about**, such as:<br></li>
</ol>
<div class="highlight"><pre><code class="text language-text" data-lang="text">* runtime;
* use of space;
* possibly other attributes such as communication bandwidth;
</code></pre></div>
<ol>
<li><p>Identifying *<em>a function that characterizes that aspect; *</em> and </p></li>
<li><p>Identifying <strong>the asymptotic class of functions that this function belongs to</strong>, where classes are defined in terms of bounds on growth rate. </p></li>
</ol>

<p>The different asymptotic bounds we use are analogous to equality and
inequality relations:</p>

<ul>
<li>O   ≈   ≤</li>
<li>Ω   ≈   ≥</li>
<li>Θ   ≈   =</li>
<li>o   ≈   &lt;</li>
<li>ω   ≈   &gt;</li>
</ul>

<p>In practice, most of our analyses will be concerned with run time. Analyses
may examine:</p>

<ul>
<li>Worst case</li>
<li>Best case</li>
<li>Average case (according to some probability distribution across all possible inputs)</li>
</ul>

<hr>

<h2>Big-O (asymptotic ≤)</h2>

<p>Our first question about an algorithm&#39;s run time is often &quot;how bad can it
get?&quot; We want a guarantee that a given algorithm will complete within a
reasonable amount of time for typical n expected. This requires an
<strong>asymptotic upper bound</strong>: the &quot;worst case&quot;.</p>

<p>Big-O is commonly used for worst case analyses, because it gives an upper
bound on growth rate. Its definition in terms of set notation is:</p>

<blockquote>
<p>O(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <em>c</em> and <em>n</em>0 such that 0 ≤
<em>f</em>(<em>n</em>) ≤ <em>c</em><em>g</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0}.</p>
</blockquote>

<p><img src="fig/graph-big-O.jpg" alt=""></p>

<p>This definition means that as <em>n</em> increases, afer a point <em>f</em>(<em>n</em>) grows no
faster than <em>g</em>(<em>n</em>) (as illustrated in the figure): <em>g</em>(<em>n</em>) is an
<em>asymptotic upper bound</em> for <em>f</em>(<em>n</em>).</p>

<p>Since O(<em>g</em>(<em>n</em>)) is a set, it would be natural to write <em>f</em>(<em>n</em>) ∈
O(<em>g</em>(<em>n</em>)) for any given <em>f</em>(<em>n</em>) and <em>g</em>(<em>n</em>) meeting the definition above,
for example, <em>f</em> ∈ O(<em>n</em>2).</p>

<p>But the algorithms literature has adopted the convention of using = instead of
∈, for example, writing <em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)). This &quot;abuse of notation&quot; makes
some manipulations possible that would be more tedious if done strictly in
terms of set notation. (We do <em>not</em> write O(<em>g</em>(<em>n</em>))=<em>f</em>(<em>n</em>); will return to
this point).</p>

<p>Using the = notation, we often see definitions of big-O in in terms of truth
conditions as follows:</p>

<blockquote>
<p><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c</em> and <em>n</em>0 such that 0 ≤
<em>f</em>(<em>n</em>) ≤ <em>c</em><em>g</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0.</p>
</blockquote>

<p>We assume that all functions involved are asymptotically non-negative. Other
authors don&#39;t make this assumption, so may use |<em>f</em>(<em>n</em>)| etc. to cover
negative values. This assumption is reflected in the condition 0 ≤ <em>f</em>(<em>n</em>).</p>

<h3>Examples</h3>

<p>Show that 2<em>n</em>2 is O(<em>n</em>2).</p>

<p>To do this we need to show that there exists some <em>c</em> and <em>n</em>0 such that
(letting 2<em>n</em>2 play the role of <em>f</em>(<em>n</em>) and <em>n</em>2 play the role of <em>g</em>(<em>n</em>) in
the definition):</p>

<blockquote>
<p>0 ≤ 2<em>n</em>2 ≤ <em>c</em><em>n</em>2 for all <em>n</em> ≥ <em>n</em>0.</p>
</blockquote>

<p>It works with <em>c</em> = 2, since this makes the <em>f</em> and <em>g</em> terms equivalent for
all <em>n</em> ≥ <em>n</em>0 = 0. (We&#39;ll do a harder example under Θ.)</p>

<h4>What&#39;s in and what&#39;s out</h4>

<p>These are all O(<em>n</em>2):</p>

<p>These are not:</p>

<ul>
<li><em>n</em>2</li>
<li><em>n</em>2 + 1000<em>n</em></li>
<li>1000<em>n</em>2 + 1000<em>n</em></li>
<li><em>n</em>1.99999</li>
<li><p><em>n</em></p></li>
<li><p><em>n</em>3</p></li>
<li><p><em>n</em>2.00001</p></li>
<li><p><em>n</em>2 lg <em>n</em></p></li>
</ul>

<h4>Insertion Sort Example</h4>

<p>Recall that we did a tedious analysis of the worst case of insertion sort,
ending with this formula:</p>

<p><img src="fig/equation-insertion-worst-3.jpg" alt=""></p>

<p><em>T(n)</em> can be expressed as <em>pn2 + _q</em><em>n</em> - r_ for suitable <em>p, q, r</em> (<em>p</em> =
(<em>c</em>5/2 + <em>c</em>6/2 + <em>c</em>7/2), etc.).</p>

<p>The textbook (page 46) sketches a proof that <strong>f<em>(</em>n_) = _a</strong>n<em>2 + _b</em><em>n</em> +
<em>c</em>_ is Θ(<em>n</em>2), and we&#39;ll see shortly that Θ(<em>n</em>2) -&gt; O(<em>n</em>2). This is
generalized to all polynomials in Problem 3-1. So, any polynomial with highest
order term <em>a</em><em>n</em><em>d</em> (i.e., a polynomial in <em>n</em> of degree <em>d</em>) will be
O(<em>n</em><em>d</em>).</p>

<p>This suggests that the worst case for insertion sort <em>T</em>(<em>n</em>) ∈ O(<em>n</em>2). An
upper bound on the worst case is also an upper bound on all other cases, so we
have already covered those cases.</p>

<p>Notice that the definition of big-O would also work for _<em>g</em>(<em>n</em>) = n3<em>,
_</em>g<em>(</em>n<em>) = 2n</em>, etc., so we can also say that <em>T</em>(<em>n</em>) (the worst case for
insertion sort) is O(<em>n</em>3), O(2<em>n</em>), etc. However, these loose bounds are not
very useful! We&#39;ll deal with this when we get to Θ (Theta).</p>

<hr>

<h2>Ω (Omega, asymptotic ≥)</h2>

<p>We might also want to know what the best we can expect is. In the last lecture
we derived this formula for insertion sort:</p>

<p><img src="fig/equation-insertion-best.jpg" alt=""></p>

<p>We could prove that this best-case version of T(n) is big-O of something, but
that would only tell us that the best case is no worse than that something.
What if we want to know what is &quot;as good as it gets&quot;: a lower bound below
which the algorithm will never be any faster?</p>

<p>We must both pick an appropriate function to measure the property of interest,
and pick an appropriate asymptotic class or comparison to match it to. We&#39;ve
done the former with <em>T</em>(<em>n</em>), but what should it be compared to?</p>

<p>It makes more sense to determine the <strong>asymptotic lower bound</strong> of growth for
a function describing the best case run-time. In other words, what&#39;s the
fastest we can ever expect, in the best case?</p>

<p><img src="fig/graph-Omega.jpg" alt=""></p>

<p><strong>Ω (Omega)</strong> provides what we are looking for. Its set and truth condition definitions are simple revisions of those for big-O:</p>

<blockquote>
<p>Ω(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <em>c</em> and <em>n</em>0 such that 0 ≤
<em>cg</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0}.<br>
<em>[The _f</em>(<em>n</em>) and <em>cg</em>(<em>n</em>) have swapped places.]_</p>

<p><em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c</em> and <em>n</em>0 such that
<em>f</em>(<em>n</em>) ≥ <em>cg</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0.<br>
<em>[≤ has been replaced with ≥.]</em></p>
</blockquote>

<p>The semantics of Ω is: as <em>n</em> increases after a point, <em>f</em>(<em>n</em>) grows no
slower than <em>g</em>(<em>n</em>) (see illustration).</p>

<h3>Examples</h3>

<p>Sqrt(<em>n</em>) is Ω(lg <em>n</em>) with <em>c</em>=1 and <em>n</em>0=16.<br>
<em>(At n=16 the two functions are equal; try at n=64 to see the growth, or graph
it.)</em></p>

<h4>What&#39;s In and What&#39;s Out</h4>

<p>These are all Ω(<em>n</em>2):</p>

<p>These are not:</p>

<ul>
<li><em>n</em>2</li>
<li><p><em>n</em>2 + 1000n   <em>(It&#39;s also O(</em>n<em>2)!)</em>
_</p></li>
<li><p>1000<em>n</em>2 + 1000<em>n</em></p></li>
<li><p>1000<em>n</em>2 - 1000<em>n</em></p></li>
<li><p><em>n</em>3</p></li>
<li><p><em>n</em>2.00001</p>

<hr></li>
<li><p><em>n</em>1.99999</p></li>
<li><p><em>n</em></p></li>
<li><p>lg <em>n</em></p></li>
</ul>

<h4>Insertion Sort Example</h4>

<p>We can show that insertion will take at least Ω(<em>n</em>) time in the best case
(i.e., it won&#39;t get any better than this) using the above formula and
definition.</p>

<p><img src="fig/equation-insertion-best.jpg" alt=""></p>

<p><em>T</em>(<em>n</em>) can be expressed as <em>pn - q</em> for suitable <em>p, q</em> (e.g., <em>q</em> = <em>c</em>2 +
<em>c</em>4 + <em>c</em>5 + <em>c</em>8, etc.). (In this case, <em>p</em> and <em>q</em> are positive.) This
suggests that <em>T</em>(<em>n</em>) ∈ Ω(<em>n</em>), that is, ∃ <em>c, n0</em> s.t. <em>pn - q ≥ cn,</em> ∀<em>n ≥
n0</em>. This follows from the generalized proof for polynomials.</p>

<hr>

<h2>Θ (Theta, asymptotic =)</h2>

<p>We noted that there are _ loose _ bounds, such as <em>f</em>(<em>n</em>) = <em>n</em>2 is O(<em>n</em>3),
etc., but this is an overly pessimistic assessment. It is more useful to have
an <strong>asymptotically tight bound</strong> on the growth of a function. In terms of
algorithms, we would like to be able to say (when it&#39;s true) that a given
characteristic such as run time grows <em>no better and no worse</em> than a given
function. That is, we want to simultaneoulsy bound from above and below.
Combining the definitions for O and Ω:</p>

<p><img src="fig/graph-Theta.jpg" alt=""></p>

<blockquote>
<p>Θ(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <strong><em>c</em>1, <em>c</em>2</strong>, and <em>n</em>0 such
that 0 ≤ <strong><em>c</em>1<em>g</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ≤ <em>c</em>2<em>g</em>(<em>n</em>)</strong>, ∀ <em>n</em> ≥ <em>n</em>0}.</p>
</blockquote>

<p>As illustrated, <em>g</em>(<em>n</em>) is an asymptotically tight bound for <em>f</em>(<em>n</em>): after
a point, <em>f</em>(<em>n</em>) grows no faster and no slower than <em>g</em>(<em>n</em>).</p>

<p>The book suggests the proof of this theorem as an easy exercise (just combine
the two definitions):</p>

<blockquote>
<p><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) iff <em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) ∧ <em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)).</p>
</blockquote>

<p>You may have noticed that some of the functions in the list of examples for
big-O are also in the list for Ω. This indicates that the set Θ is not empty.</p>

<h3>Examples</h3>

<blockquote>
<p><em>Reminder:</em> <em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c</em>1, <em>c</em>2, and
<em>n</em>0 such that 0 ≤ <em>c</em>1<em>g</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ≤ <em>c</em>2<em>g</em>(<em>n</em>)∀ <em>n</em> ≥ <em>n</em>0.</p>
</blockquote>

<p><em>n</em>2 - 2<em>n</em> is Θ(<em>n</em>2),   with <em>c</em>1 = 1/2; <em>c</em>2 = 1, and <em>n</em>0 = 4,   since:</p>

<blockquote>
<p><em>n</em>2/2   ≤   <em>n</em>2 - 2<em>n</em>   ≤   <em>n</em>2   for <em>n</em> ≥ <em>n</em>0 = 4.</p>
</blockquote>

<h4>Find an asymptotically tight bound (Θ) for</h4>

<ul>
<li>4<em>n</em>3</li>
<li>4<em>n</em>3 + 2<em>n</em>. </li>
</ul>

<p>Please try these before you <a href="http://www2.hawaii.edu/%7Esuthers/courses/ics311s14/Notes/Topic-03%0A/Example-Analysis.html">find solutions
here</a>.</p>

<h4>What&#39;s in and what&#39;s out</h4>

<p>These are all Θ(n2):</p>

<p>These are not</p>

<ul>
<li><em>n</em>2</li>
<li><em>n</em>2 + 1000<em>n</em></li>
<li>1000<em>n</em>2 + 1000<em>n</em> + 32,700</li>
<li><p>1000<em>n</em>2 - 1000<em>n</em> - 1,048,315</p></li>
<li><p><em>n</em>3</p></li>
<li><p><em>n</em>2.00001</p></li>
<li><p><em>n</em>1.99999</p></li>
<li><p><em>n</em> lg <em>n</em></p></li>
</ul>

<hr>

<h2>Asymptotic Inequality</h2>

<p>The O and Ω bounds may or may not be asymptotically tight. The next two
notations are for upper bounds that are strictly <em>not</em> asymptotically tight.
There is an <em>analogy</em> to inequality relationships:</p>

<ul>
<li>&quot;≤&quot; is to &quot;&lt;&quot; as big-O (may or may not be tight) is to little-o (strictly not equal) </li>
<li>&quot;≥&quot; is to &quot;&gt;&quot; as Ω (may or may not be tight) is to little-ω (strictly not equal). </li>
</ul>

<h3>o-notation (&quot;little o&quot;, asymptotic &lt;)</h3>

<blockquote>
<p>o(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∀ constants <em>c</em> &gt; 0, ∃ constant <em>n</em>0 &gt; 0 such that
0 ≤ <em>f</em>(<em>n</em>) <strong>&lt;</strong> <em>cg</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0}.</p>
</blockquote>

<p><img src="fig/o-limit-definition.jpg" alt=""></p>

<p>Alternatively, <em>f</em>(<em>n</em>) becomes <em>insignificant</em> relative to <em>g</em>(<em>n</em>) as <em>n</em>
approaches infinity (see box):</p>

<p>We say that <em>f</em>(<em>n</em>) is <strong>asymptotically smaller</strong> than <em>g</em>(<em>n</em>) if <em>f</em>(<em>n</em>) =
o(<em>g</em>(<em>n</em>))</p>

<ul>
<li><em>n</em>1.99999 ∈ o(<em>n</em>2)</li>
<li><em>n</em>2/lg <em>n</em> ∈ o(<em>n</em>2)</li>
<li><em>n</em>2 ∉ o(<em>n</em>2) (similarly, 2 is not less than 2)</li>
<li><em>n</em>2/1000 ∉ o(<em>n</em>2) </li>
</ul>

<h3>ω-notation (&quot;little omega&quot;, asymptotic &gt;)</h3>

<blockquote>
<p>ω(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∀ constants <em>c</em> &gt; 0, ∃ constant <em>n</em>0 &gt; 0 such that
0 ≤ <em>cg</em>(<em>n</em>) <strong>&lt;</strong> <em>f</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n</em>0}.</p>
</blockquote>

<p><img src="fig/omega-limit-definition.jpg" alt=""></p>

<p>Alternatively, <em>f</em>(<em>n</em>) becomes _ arbitrarily large _ relative to <em>g</em>(<em>n</em>) as
<em>n</em> approaches infinity (see box):</p>

<p>We say that <em>f</em>(<em>n</em>) is <strong>asymptotically larger</strong> than <em>g</em>(<em>n</em>) if <em>f</em>(<em>n</em>) =
ω(<em>g</em>(<em>n</em>))</p>

<ul>
<li><em>n</em>2.00001 ∈ ω(<em>n</em>2)</li>
<li><em>n</em>2lg <em>n</em> ∈ ω(<em>n</em>2)</li>
<li><em>n</em>2 ∉ ω(<em>n</em>2)</li>
</ul>

<p>The two are related:   <strong><em>f</em>(<em>n</em>) ∈ ω(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) ∈
o(<em>f</em>(<em>n</em>)).</strong></p>

<hr>

<h2>Asymptotic Notation in Equations</h2>

<p>We already noted that while asymptotic categories such as Θ(<em>n</em>2) are sets, we
usually use &quot;=&quot; instead of &quot;∈&quot; and write (for example) <em>f</em>(<em>n</em>) = Θ(<em>n</em>2) to
indicate that <em>f</em> is in this set.</p>

<p>Putting asymptotic notation in equations lets us do shorthand manipulations
during analysis.</p>

<h3>Asymptotic Notation on Right Hand Side: ∃</h3>

<p>O(<em>g</em>(<em>x</em>)) on the right hand side stands for <em>some</em> anonymous function in the
set O(<em>g</em>(<em>x</em>)).</p>

<blockquote>
<p>2<em>n</em>2 + 3<em>n</em> + 1 = 2<em>n</em>2 + Θ(<em>n</em>)     means:<br>
2<em>n</em>2 + 3<em>n</em> + 1 = 2<em>n</em>2 + <em>f</em>(<em>n</em>)       for <em>some</em> _<em>f</em>(<em>n</em>) ∈ Θ(<em>n</em>)_
(in particular, <em>f</em>(<em>n</em>) = 3<em>n</em> + 1).</p>
</blockquote>

<h3>Asymptotic Notation on Left Hand Side: ∀</h3>

<p>The notation is only used on the left hand side when it is also on the right
hand side.</p>

<p>Semantics: No matter how the anonymous functions are chosen on the left hand
side, there is a way to choose the functions on the right hand side to make
the equation valid.</p>

<blockquote>
<p>2<em>n</em>2 + Θ(<em>n</em>) = Θ(<em>n</em>2)     means   for <em>all</em> <em>f</em>(<em>n</em>) ∈ Θ(<em>n</em>), there _
exists_ <em>g</em>(<em>n</em>) ∈ Θ(<em>n</em>2) such that<br>
2<em>n</em>2 + <em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>).</p>
</blockquote>

<h3>Combining Terms</h3>

<p>We can do basic algebra such as:</p>

<blockquote>
<p>2<em>n</em>2 + 3<em>n</em> + 1   =   2<em>n</em>2 + Θ(<em>n</em>)   =   Θ(<em>n</em>2)</p>
</blockquote>

<hr>

<h2>Properties</h2>

<p>If we keep in mind the analogy to inequality, many of these make sense, but
see the end for a caution concerning this analogy.</p>

<h3>Relational Properties</h3>

<p><strong>Transitivity</strong>:</p>

<ul>
<li><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = Θ(h(n))   ⇒   <em>f</em>(<em>n</em>) = Θ(h(n)).</li>
<li><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = O(h(n))   ⇒   <em>f</em>(<em>n</em>) = O(h(n)).</li>
<li><em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = Ω(h(n))   ⇒   <em>f</em>(<em>n</em>) = Ω(h(n)).</li>
<li><em>f</em>(<em>n</em>) = o(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = o(h(n))   ⇒   <em>f</em>(<em>n</em>) = o(h(n)).</li>
<li><p><em>f</em>(<em>n</em>) = ω(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = ω(h(n))   ⇒   <em>f</em>(<em>n</em>) = ω(h(n)).
<strong>Reflexivity</strong>:</p></li>
<li><p><em>f</em>(<em>n</em>) = Θ(<em>f</em>(<em>n</em>))</p></li>
<li><p><em>f</em>(<em>n</em>) = O(<em>f</em>(<em>n</em>))</p></li>
<li><p><em>f</em>(<em>n</em>) = Ω(<em>f</em>(<em>n</em>))</p></li>
<li><p><em>What about o and ω?</em>
<strong>Symmetry</strong>:</p></li>
<li><p><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = Θ(<em>f</em>(<em>n</em>)) </p></li>
<li><p><em>Should any others be here? Why or why not?</em>
<strong>Transpose Symmetry</strong>:</p></li>
<li><p><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = Ω(<em>f</em>(<em>n</em>)) </p></li>
<li><p><em>f</em>(<em>n</em>) = o(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = ω(<em>f</em>(<em>n</em>)) </p></li>
</ul>

<h3>Incomparability</h3>

<p>Here is where the analogy to numeric (in)equality breaks down: There is no
trichotomy. Unlike with constant numbers, we can&#39;t assume that one of &lt;, =, &gt;
hold. Some functions may be incomparable.</p>

<p>Example: <em>n</em>1 + <em>sin n</em> is incomparable to <em>n</em> since <em>sin n</em> oscillates
between -1 and 1, so 1 + <em>sin n</em> oscillates between 0 and 2. <em>(Try graphing
it.)</em></p>

<hr>

<h2>Common Functions and Useful Facts</h2>

<p>Various classes of functions and their associated notations and identities are
reviewed in the end of the chapter: please review the chapter and refer to ICS
241 if needed. Here we highlight some useful facts:</p>

<h3>Monotonicity</h3>

<ul>
<li><em>f</em>(<em>n</em>) is <strong>monotonically increasing</strong>   if   <em>m</em> ≤ <em>n</em>   ⇒   <em>f</em>(<em>m</em>) ≤ <em>f</em>(<em>n</em>).</li>
<li><em>f</em>(<em>n</em>) is <strong>monotonically decreasing</strong>   if   <em>m</em> ≥ <em>n</em>   ⇒   <em>f</em>(<em>m</em>) ≥ <em>f</em>(<em>n</em>).</li>
<li><em>f</em>(<em>n</em>) is <strong>strictly increasing</strong>   if   <em>m</em> &lt; <em>n</em>   ⇒   <em>f</em>(<em>m</em>) &lt; <em>f</em>(<em>n</em>).</li>
<li><em>f</em>(<em>n</em>) is <strong>strictly decreasing</strong>   if   <em>m</em> &gt; <em>n</em>   ⇒   <em>f</em>(<em>m</em>) &gt; <em>f</em>(<em>n</em>).</li>
</ul>

<h3>Polynomials</h3>

<ul>
<li><em>p</em>(<em>n</em>) = Θ(<em>n</em><em>d</em>), for asymptoptically positive polynomials in <em>n</em> of degree <em>d</em></li>
</ul>

<h3>Exponentials</h3>

<ul>
<li><p><em>n</em><em>b</em> = o(<em>a</em><em>n</em>) for all real constants <em>a</em> and <em>b</em> such that <em>a</em> &gt; 1: <strong><em>Any exponential function with a base greater than 1 grows faster than any polynomial function.</em></strong></p></li>
<li><p>Useful identities: </p>

<ul>
<li><em>a</em>-1 = 1/<em>a</em></li>
<li>(<em>a</em><em>m</em>)<em>n</em> = <em>a</em><em>mn</em></li>
<li><em>a</em><em>m</em><em>a</em><em>n</em> = <em>a</em><em>m</em> + <em>n</em></li>
</ul></li>
</ul>

<h3>Logarithms</h3>

<ul>
<li><p>(lg <em>n</em>)<em>b</em> = lg<em>b</em><em>n</em> = o(<em>n</em><em>a</em>), for a &gt; 0: <strong><em>any positive polynomial function grows faster than any polylogarithmic function.</em></strong></p></li>
<li><p>Useful identities: </p>

<ul>
<li><em>a</em> = <em>b</em>log<em>b</em><em>a</em>   <em>(Definition of logs.)</em></li>
<li>log<em>a</em><em>n</em> = log<em>b</em><em>n</em>/log<em>b</em><em>a</em><br>
<em>(Base change. If _n</em> is variable and <em>a</em> and <em>b</em> are constant, the denominator is constant: this is why asymptotic analysis can ignore the base.)_</li>
<li>log<em>c</em>(<em>ab</em>) = log<em>c</em><em>a</em> + log<em>c</em><em>b</em>   <em>(Ask your slide rule!)</em></li>
<li>log<em>b</em><em>a</em>n__ = <em>n</em> log<em>b</em><em>a</em></li>
<li>log<em>b</em>(1/<em>a</em>) = −log<em>b</em><em>a</em></li>
<li>log<em>b</em><em>a</em> = 1 / log<em>a</em><em>b</em></li>
<li><em>a</em>log<em>b</em><em>c</em> = <em>c</em>log<em>b</em><em>a</em>   <em>(Useful for getting the variable where you want it.)</em></li>
</ul></li>
</ul>

<h3>Factorials</h3>

<ul>
<li><em>n</em>! = ω(2<em>n</em>):   <em><strong>factorials grow faster than exponentials</strong> (but it could be worse):</em></li>
<li><em>n</em>! = o(<em>n</em><em>n</em>)</li>
<li>lg(<em>n</em>!) = Θ(<em>n</em> lg <em>n</em>)</li>
<li>See also the more complex <strong>Stirling&#39;s approximation</strong> from which these are derived.</li>
</ul>

<h3>Iterated Functions</h3>

<ul>
<li>Definition: <em>f</em>(<em>i</em>)(<em>n</em>) is <em>f</em> applied <em>i</em> times to the initial value <em>n</em>. </li>
<li>Iterated Logarithm: lg*<em>n</em> = min{<em>i</em> ≥ 0: lg(<em>i</em>)<em>n</em> ≤ 1}   <em>(The iteration at which lg(</em>i<em>)</em>n_ is less than 1: a very slowly growing function.)_</li>
</ul>

<h3>Fibonacci Numbers</h3>

<ul>
<li>Definition: <em>F</em>0 = 0; <em>F</em>1 = 1; and for <em>i</em> &gt; 1 <em>F</em><em>i</em> = <em>F</em><em>i</em>-1 + <em>F</em><em>i</em>-2. </li>
<li><strong><em>Fibonacci numbers grow exponentially.</em></strong></li>
</ul>

<hr>

<p>Dan Suthers Last modified: Sat Jan 25 03:51:57 HST 2014<br>
Images are from the instructor&#39;s manual for Cormen et al.  </p>

</div>



<div class="dark-blue-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a style="color: white" href="http://morea-framework.github.io/">Morea Framework</a><br>
       Last update on: <span>2014-04-16 14:49:14 -1000</span></p>
  </div>
</footer>
</div>
</body>
</html>

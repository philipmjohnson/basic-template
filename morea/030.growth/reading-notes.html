<!DOCTYPE html>
<html>
<head>
  <title> Chapter 3 Notes | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">

  <!--  Load bootswatch-based Morea theme file. -->
  <link rel="stylesheet" href="/ics311s14/css/themes/simplex/bootstrap.min.css">
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" href="/ics311s14/css/syntax.css">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
	<li class="dropdown">
          <a class="dropdown-toggle" data-toggle="dropdown" href="#">Course Info<b class="caret"></b></a>
            <ul class="dropdown-menu" role="menu">
              <li><a href="/ics311s14/morea/010.introduction/reading-course-info.html">Overview</a></li>
              <li><a href="/ics311s14/morea/010.introduction/reading-policies.html">Policies</a></li>
              <li><a href="/ics311s14/morea/010.introduction/reading-topic-overview.html">Topics</a></li>
            </ul>
          </li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        
          <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        
        
          <li><a href="/ics311s14/readings/">Readings</a></li>
        
        
          <li><a href="/ics311s14/experiences/">Experiences</a></li>
        
        
        <li><a href="/ics311s14/news/">News</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h2 id="outline">Outline</h2>

<ol>
  <li>Intro to Asymptotic Analysis</li>
  <li>Big-O</li>
  <li>Ω (Omega)</li>
  <li>Θ (Theta)</li>
  <li>Asymptotic Notation in Equations</li>
  <li>Asymptotic Inequality</li>
  <li>Properties of Asymptotic Sets</li>
  <li>Common Functions</li>
</ol>

<hr />

<h2 id="intro-to-asymptotic-analysis">Intro to Asymptotic Analysis</h2>

<p>The notations discussed today are ways to describe behaviors of <em>functions,</em>
particularly <em>in the limit</em>, or <em>asymptotic</em> behavior.</p>

<p>The functions need not necessarily be about algorithms, and indeed asymptotic
analysis is used for many other applications.</p>

<p>Asymptotic analysis of algorithms requires:</p>

<ol>
  <li>Identifying ** what aspect of an algorithm we care about**, such as:    </li>
</ol>

<pre><code>* runtime;
* use of space;
* possibly other attributes such as communication bandwidth;
</code></pre>

<ol>
  <li>
    <p>Identifying **a function that characterizes that aspect; ** and </p>
  </li>
  <li>
    <p>Identifying <strong>the asymptotic class of functions that this function belongs to</strong>, where classes are defined in terms of bounds on growth rate. </p>
  </li>
</ol>

<p>The different asymptotic bounds we use are analogous to equality and
inequality relations:</p>

<ul>
  <li>O   ≈   ≤</li>
  <li>Ω   ≈   ≥</li>
  <li>Θ   ≈   =</li>
  <li>o   ≈   &lt;</li>
  <li>ω   ≈   &gt;</li>
</ul>

<p>In practice, most of our analyses will be concerned with run time. Analyses
may examine:</p>

<ul>
  <li>Worst case</li>
  <li>Best case</li>
  <li>Average case (according to some probability distribution across all possible inputs)</li>
</ul>

<hr />

<h2 id="big-o-asymptotic-">Big-O (asymptotic ≤)</h2>

<p>Our first question about an algorithm’s run time is often “how bad can it
get?” We want a guarantee that a given algorithm will complete within a
reasonable amount of time for typical n expected. This requires an
<strong>asymptotic upper bound</strong>: the “worst case”.</p>

<p>Big-O is commonly used for worst case analyses, because it gives an upper
bound on growth rate. Its definition in terms of set notation is:</p>

<blockquote>
  <p>O(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <em>c</em> and <em>n_0 such that 0 ≤
_f</em>(<em>n</em>) ≤ <em>c__g</em>(<em>n</em>) ∀ <em>n</em> ≥ _n_0}.</p>
</blockquote>

<p><img src="fig/graph-big-O.jpg" alt="" /></p>

<p>This definition means that as <em>n</em> increases, afer a point <em>f</em>(<em>n</em>) grows no
faster than <em>g</em>(<em>n</em>) (as illustrated in the figure): <em>g</em>(<em>n</em>) is an
<em>asymptotic upper bound</em> for <em>f</em>(<em>n</em>).</p>

<p>Since O(<em>g</em>(<em>n</em>)) is a set, it would be natural to write <em>f</em>(<em>n</em>) ∈
O(<em>g</em>(<em>n</em>)) for any given <em>f</em>(<em>n</em>) and <em>g</em>(<em>n</em>) meeting the definition above,
for example, <em>f</em> ∈ O(_n_2).</p>

<p>But the algorithms literature has adopted the convention of using = instead of
∈, for example, writing <em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)). This “abuse of notation” makes
some manipulations possible that would be more tedious if done strictly in
terms of set notation. (We do <em>not</em> write O(<em>g</em>(<em>n</em>))=<em>f</em>(<em>n</em>); will return to
this point).</p>

<p>Using the = notation, we often see definitions of big-O in in terms of truth
conditions as follows:</p>

<blockquote>
  <p><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c</em> and <em>n_0 such that 0 ≤
_f</em>(<em>n</em>) ≤ <em>c__g</em>(<em>n</em>) ∀ <em>n</em> ≥ _n_0.</p>
</blockquote>

<p>We assume that all functions involved are asymptotically non-negative. Other
authors don’t make this assumption, so may use |<em>f</em>(<em>n</em>)| etc. to cover
negative values. This assumption is reflected in the condition 0 ≤ <em>f</em>(<em>n</em>).</p>

<h3 id="examples">Examples</h3>

<p>Show that 2_n_2 is O(_n_2).</p>

<p>To do this we need to show that there exists some <em>c</em> and <em>n_0 such that
(letting 2_n_2 play the role of _f</em>(<em>n</em>) and <em>n_2 play the role of _g</em>(<em>n</em>) in
the definition):</p>

<blockquote>
  <p>0 ≤ 2_n_2 ≤ <em>c__n_2 for all _n</em> ≥ _n_0.</p>
</blockquote>

<p>It works with <em>c</em> = 2, since this makes the <em>f</em> and <em>g</em> terms equivalent for
all <em>n</em> ≥ _n_0 = 0. (We’ll do a harder example under Θ.)</p>

<h4 id="whats-in-and-whats-out">What’s in and what’s out</h4>

<p>These are all O(_n_2):</p>

<p>These are not:</p>

<ul>
  <li>_n_2</li>
  <li><em>n_2 + 1000_n</em></li>
  <li>1000_n_2 + 1000_n_</li>
  <li>_n_1.99999</li>
  <li>
    <p><em>n</em></p>
  </li>
  <li>_n_3</li>
  <li>_n_2.00001</li>
  <li><em>n_2 lg _n</em></li>
</ul>

<h4 id="insertion-sort-example">Insertion Sort Example</h4>

<p>Recall that we did a tedious analysis of the worst case of insertion sort,
ending with this formula:</p>

<p><img src="fig/equation-insertion-worst-3.jpg" alt="" /></p>

<p><em>T(n)</em> can be expressed as <em>pn2 + _q__n</em> - r_ for suitable <em>p, q, r</em> (<em>p</em> =
(_c_5/2 + _c_6/2 + _c_7/2), etc.).</p>

<p>The textbook (page 46) sketches a proof that <em>_f</em>(<em>n</em>) = <em>a__n_2 + _b__n</em> +
<em>c__ is Θ(_n_2), and we’ll see shortly that Θ(_n_2) -&gt; O(_n_2). This is
generalized to all polynomials in Problem 3-1. So, any polynomial with highest
order term _a__n__d</em> (i.e., a polynomial in <em>n</em> of degree <em>d</em>) will be
O(<em>n__d</em>).</p>

<p>This suggests that the worst case for insertion sort <em>T</em>(<em>n</em>) ∈ O(_n_2). An
upper bound on the worst case is also an upper bound on all other cases, so we
have already covered those cases.</p>

<p>Notice that the definition of big-O would also work for <em>_g</em>(<em>n</em>) = n3<em>,
__g</em>(<em>n</em>) = 2n<em>, etc., so we can also say that _T</em>(<em>n</em>) (the worst case for
insertion sort) is O(<em>n_3), O(2_n</em>), etc. However, these loose bounds are not
very useful! We’ll deal with this when we get to Θ (Theta).</p>

<hr />

<h2 id="omega-asymptotic-">Ω (Omega, asymptotic ≥)</h2>

<p>We might also want to know what the best we can expect is. In the last lecture
we derived this formula for insertion sort:</p>

<p><img src="fig/equation-insertion-best.jpg" alt="" /></p>

<p>We could prove that this best-case version of T(n) is big-O of something, but
that would only tell us that the best case is no worse than that something.
What if we want to know what is “as good as it gets”: a lower bound below
which the algorithm will never be any faster?</p>

<p>We must both pick an appropriate function to measure the property of interest,
and pick an appropriate asymptotic class or comparison to match it to. We’ve
done the former with <em>T</em>(<em>n</em>), but what should it be compared to?</p>

<p>It makes more sense to determine the <strong>asymptotic lower bound</strong> of growth for
a function describing the best case run-time. In other words, what’s the
fastest we can ever expect, in the best case?</p>

<p><img src="fig/graph-Omega.jpg" alt="" /></p>

<p><strong>Ω (Omega)</strong> provides what we are looking for. Its set and truth condition definitions are simple revisions of those for big-O:</p>

<blockquote>
  <p>Ω(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <em>c</em> and <em>n_0 such that 0 ≤
_cg</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n_0}.<br />
_[The _f</em>(<em>n</em>) and <em>cg</em>(<em>n</em>) have swapped places.]_</p>
</blockquote>

<blockquote>
  <p><em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c</em> and <em>n_0 such that
_f</em>(<em>n</em>) ≥ <em>cg</em>(<em>n</em>) ∀ <em>n</em> ≥ <em>n_0.<br />
_[≤ has been replaced with ≥.]</em></p>
</blockquote>

<p>The semantics of Ω is: as <em>n</em> increases after a point, <em>f</em>(<em>n</em>) grows no
slower than <em>g</em>(<em>n</em>) (see illustration).</p>

<h3 id="examples-1">Examples</h3>

<p>Sqrt(<em>n</em>) is Ω(lg <em>n</em>) with <em>c</em>=1 and <em>n_0=16.<br />
_(At n=16 the two functions are equal; try at n=64 to see the growth, or graph
it.)</em></p>

<h4 id="whats-in-and-whats-out-1">What’s In and What’s Out</h4>

<p>These are all Ω(_n_2):</p>

<p>These are not:</p>

<ul>
  <li>_n_2</li>
  <li>
    <p><em>n_2 + 1000n   _(It’s also O(_n_2)!)</em>
_</p>
  </li>
  <li>1000_n_2 + 1000_n_</li>
  <li>1000_n_2 - 1000_n_</li>
  <li>_n_3</li>
  <li>
    <p>_n_2.00001
__ _</p>
  </li>
  <li>_n_1.99999</li>
  <li><em>n</em></li>
  <li>lg <em>n</em></li>
</ul>

<h4 id="insertion-sort-example-1">Insertion Sort Example</h4>

<p>We can show that insertion will take at least Ω(<em>n</em>) time in the best case
(i.e., it won’t get any better than this) using the above formula and
definition.</p>

<p><img src="fig/equation-insertion-best.jpg" alt="" /></p>

<p><em>T</em>(<em>n</em>) can be expressed as <em>pn - q</em> for suitable <em>p, q</em> (e.g., <em>q</em> = <em>c_2 +
_c_4 + _c_5 + _c_8, etc.). (In this case, _p</em> and <em>q</em> are positive.) This
suggests that <em>T</em>(<em>n</em>) ∈ Ω(<em>n</em>), that is, ∃ <em>c, n0</em> s.t. <em>pn - q ≥ cn,</em> ∀<em>n ≥
n0</em>. This follows from the generalized proof for polynomials.</p>

<hr />

<h2 id="theta-asymptotic-">Θ (Theta, asymptotic =)</h2>

<p>We noted that there are _ loose _ bounds, such as <em>f</em>(<em>n</em>) = <em>n_2 is O(_n_3),
etc., but this is an overly pessimistic assessment. It is more useful to have
an <strong>asymptotically tight bound</strong> on the growth of a function. In terms of
algorithms, we would like to be able to say (when it’s true) that a given
characteristic such as run time grows _no better and no worse</em> than a given
function. That is, we want to simultaneoulsy bound from above and below.
Combining the definitions for O and Ω:</p>

<p><img src="fig/graph-Theta.jpg" alt="" /></p>

<blockquote>
  <p>Θ(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∃ positive constants <strong><em>c_1, _c_2**, and _n_0 such
that 0 ≤ **_c_1_g</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ≤ <em>c_2_g</em>(<em>n</em>)</strong>, ∀ <em>n</em> ≥ _n_0}.</p>
</blockquote>

<p>As illustrated, <em>g</em>(<em>n</em>) is an asymptotically tight bound for <em>f</em>(<em>n</em>): after
a point, <em>f</em>(<em>n</em>) grows no faster and no slower than <em>g</em>(<em>n</em>).</p>

<p>The book suggests the proof of this theorem as an easy exercise (just combine
the two definitions):</p>

<blockquote>
  <p><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) iff <em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) ∧ <em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)).</p>
</blockquote>

<p>You may have noticed that some of the functions in the list of examples for
big-O are also in the list for Ω. This indicates that the set Θ is not empty.</p>

<h3 id="examples-2">Examples</h3>

<blockquote>
  <p><em>Reminder:</em> <em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) iff ∃ positive constants <em>c_1, _c_2, and
_n_0 such that 0 ≤ _c_1_g</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ≤ <em>c_2_g</em>(<em>n</em>)∀ <em>n</em> ≥ _n_0.</p>
</blockquote>

<p><em>n_2 - 2_n</em> is Θ(_n_2),   with _c_1 = 1/2; _c_2 = 1, and _n_0 = 4,   since:</p>

<blockquote>
  <p><em>n_2/2   ≤   _n_2 - 2_n</em>   ≤   <em>n_2   for _n</em> ≥ _n_0 = 4.</p>
</blockquote>

<h4 id="find-an-asymptotically-tight-bound--for">Find an asymptotically tight bound (Θ) for</h4>

<ul>
  <li>4_n_3</li>
  <li>4_n_3 + 2_n_. </li>
</ul>

<p>Please try these before you <a href="http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-03
/Example-Analysis.html">find solutions
here</a>.</p>

<h4 id="whats-in-and-whats-out-2">What’s in and what’s out</h4>

<p>These are all Θ(n2):</p>

<p>These are not</p>

<ul>
  <li>_n_2</li>
  <li><em>n_2 + 1000_n</em></li>
  <li>1000_n_2 + 1000_n_ + 32,700</li>
  <li>
    <p>1000_n_2 - 1000_n_ - 1,048,315</p>
  </li>
  <li>_n_3</li>
  <li>_n_2.00001</li>
  <li>_n_1.99999</li>
  <li><em>n</em> lg <em>n</em></li>
</ul>

<hr />

<h2 id="asymptotic-inequality">Asymptotic Inequality</h2>

<p>The O and Ω bounds may or may not be asymptotically tight. The next two
notations are for upper bounds that are strictly <em>not</em> asymptotically tight.
There is an <em>analogy</em> to inequality relationships:</p>

<ul>
  <li>“≤” is to “&lt;” as big-O (may or may not be tight) is to little-o (strictly not equal) </li>
  <li>“≥” is to “&gt;” as Ω (may or may not be tight) is to little-ω (strictly not equal). </li>
</ul>

<h3 id="o-notation-little-o-asymptotic-">o-notation (“little o”, asymptotic &lt;)</h3>

<blockquote>
  <p>o(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∀ constants <em>c</em> &gt; 0, ∃ constant <em>n_0 &gt; 0 such that
0 ≤ _f</em>(<em>n</em>) <strong>&lt;</strong> <em>cg</em>(<em>n</em>) ∀ <em>n</em> ≥ _n_0}.</p>
</blockquote>

<p><img src="fig/o-limit-definition.jpg" alt="" /></p>

<p>Alternatively, <em>f</em>(<em>n</em>) becomes <em>insignificant</em> relative to <em>g</em>(<em>n</em>) as <em>n</em>
approaches infinity (see box):</p>

<p>We say that <em>f</em>(<em>n</em>) is <strong>asymptotically smaller</strong> than <em>g</em>(<em>n</em>) if <em>f</em>(<em>n</em>) =
o(<em>g</em>(<em>n</em>))</p>

<ul>
  <li>_n_1.99999 ∈ o(_n_2)</li>
  <li><em>n_2/lg _n</em> ∈ o(_n_2)</li>
  <li>_n_2 ∉ o(_n_2) (similarly, 2 is not less than 2)</li>
  <li>_n_2/1000 ∉ o(_n_2) </li>
</ul>

<h3 id="notation-little-omega-asymptotic-">ω-notation (“little omega”, asymptotic &gt;)</h3>

<blockquote>
  <p>ω(<em>g</em>(<em>n</em>)) = {<em>f</em>(<em>n</em>) : ∀ constants <em>c</em> &gt; 0, ∃ constant <em>n_0 &gt; 0 such that
0 ≤ _cg</em>(<em>n</em>) <strong>&lt;</strong> <em>f</em>(<em>n</em>) ∀ <em>n</em> ≥ _n_0}.</p>
</blockquote>

<p><img src="fig/omega-limit-definition.jpg" alt="" /></p>

<p>Alternatively, <em>f</em>(<em>n</em>) becomes _ arbitrarily large _ relative to <em>g</em>(<em>n</em>) as
<em>n</em> approaches infinity (see box):</p>

<p>We say that <em>f</em>(<em>n</em>) is <strong>asymptotically larger</strong> than <em>g</em>(<em>n</em>) if <em>f</em>(<em>n</em>) =
ω(<em>g</em>(<em>n</em>))</p>

<ul>
  <li>_n_2.00001 ∈ ω(_n_2)</li>
  <li><em>n_2lg _n</em> ∈ ω(_n_2)</li>
  <li>_n_2 ∉ ω(_n_2)</li>
</ul>

<p>The two are related:   <strong><em>f</em>(<em>n</em>) ∈ ω(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) ∈
o(<em>f</em>(<em>n</em>)).</strong></p>

<hr />

<h2 id="asymptotic-notation-in-equations">Asymptotic Notation in Equations</h2>

<p>We already noted that while asymptotic categories such as Θ(<em>n_2) are sets, we
usually use “=” instead of “∈” and write (for example) _f</em>(<em>n</em>) = Θ(<em>n_2) to
indicate that _f</em> is in this set.</p>

<p>Putting asymptotic notation in equations lets us do shorthand manipulations
during analysis.</p>

<h3 id="asymptotic-notation-on-right-hand-side-">Asymptotic Notation on Right Hand Side: ∃</h3>

<p>O(<em>g</em>(<em>x</em>)) on the right hand side stands for <em>some</em> anonymous function in the
set O(<em>g</em>(<em>x</em>)).</p>

<blockquote>
  <p>2_n_2 + 3_n_ + 1 = 2_n_2 + Θ(<em>n</em>)     means:<br />
2_n_2 + 3_n_ + 1 = 2_n_2 + <em>f</em>(<em>n</em>)       for <em>some</em> <em>_f</em>(<em>n</em>) ∈ Θ(<em>n</em>)_
(in particular, <em>f</em>(<em>n</em>) = 3_n_ + 1).</p>
</blockquote>

<h3 id="asymptotic-notation-on-left-hand-side-">Asymptotic Notation on Left Hand Side: ∀</h3>

<p>The notation is only used on the left hand side when it is also on the right
hand side.</p>

<p>Semantics: No matter how the anonymous functions are chosen on the left hand
side, there is a way to choose the functions on the right hand side to make
the equation valid.</p>

<blockquote>
  <p>2_n_2 + Θ(<em>n</em>) = Θ(<em>n_2)     means   for _all</em> <em>f</em>(<em>n</em>) ∈ Θ(<em>n</em>), there _
exists_ <em>g</em>(<em>n</em>) ∈ Θ(<em>n_2) such that<br />
2_n_2 + _f</em>(<em>n</em>) = <em>g</em>(<em>n</em>).</p>
</blockquote>

<h3 id="combining-terms">Combining Terms</h3>

<p>We can do basic algebra such as:</p>

<blockquote>
  <p>2_n_2 + 3_n_ + 1   =   2_n_2 + Θ(<em>n</em>)   =   Θ(_n_2)</p>
</blockquote>

<hr />

<h2 id="properties">Properties</h2>

<p>If we keep in mind the analogy to inequality, many of these make sense, but
see the end for a caution concerning this analogy.</p>

<h3 id="relational-properties">Relational Properties</h3>

<p><strong>Transitivity</strong>:</p>

<ul>
  <li><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = Θ(h(n))   ⇒   <em>f</em>(<em>n</em>) = Θ(h(n)).</li>
  <li><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = O(h(n))   ⇒   <em>f</em>(<em>n</em>) = O(h(n)).</li>
  <li><em>f</em>(<em>n</em>) = Ω(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = Ω(h(n))   ⇒   <em>f</em>(<em>n</em>) = Ω(h(n)).</li>
  <li><em>f</em>(<em>n</em>) = o(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = o(h(n))   ⇒   <em>f</em>(<em>n</em>) = o(h(n)).</li>
  <li>
    <p><em>f</em>(<em>n</em>) = ω(<em>g</em>(<em>n</em>)) and <em>g</em>(<em>n</em>) = ω(h(n))   ⇒   <em>f</em>(<em>n</em>) = ω(h(n)).
<strong>Reflexivity</strong>:</p>
  </li>
  <li><em>f</em>(<em>n</em>) = Θ(<em>f</em>(<em>n</em>))</li>
  <li><em>f</em>(<em>n</em>) = O(<em>f</em>(<em>n</em>))</li>
  <li><em>f</em>(<em>n</em>) = Ω(<em>f</em>(<em>n</em>))</li>
  <li>
    <p><em>What about o and ω?</em>
<strong>Symmetry</strong>:</p>
  </li>
  <li><em>f</em>(<em>n</em>) = Θ(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = Θ(<em>f</em>(<em>n</em>)) </li>
  <li>
    <p><em>Should any others be here? Why or why not?</em>
<strong>Transpose Symmetry</strong>:</p>
  </li>
  <li><em>f</em>(<em>n</em>) = O(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = Ω(<em>f</em>(<em>n</em>)) </li>
  <li><em>f</em>(<em>n</em>) = o(<em>g</em>(<em>n</em>))   iff   <em>g</em>(<em>n</em>) = ω(<em>f</em>(<em>n</em>)) </li>
</ul>

<h3 id="incomparability">Incomparability</h3>

<p>Here is where the analogy to numeric (in)equality breaks down: There is no
trichotomy. Unlike with constant numbers, we can’t assume that one of &lt;, =, &gt;
hold. Some functions may be incomparable.</p>

<p>Example: <em>n_1 + _sin n</em> is incomparable to <em>n</em> since <em>sin n</em> oscillates
between -1 and 1, so 1 + <em>sin n</em> oscillates between 0 and 2. <em>(Try graphing
it.)</em></p>

<hr />

<h2 id="common-functions-and-useful-facts">Common Functions and Useful Facts</h2>

<p>Various classes of functions and their associated notations and identities are
reviewed in the end of the chapter: please review the chapter and refer to ICS
241 if needed. Here we highlight some useful facts:</p>

<h3 id="monotonicity">Monotonicity</h3>

<ul>
  <li><em>f</em>(<em>n</em>) is <strong>monotonically increasing</strong>   if   <em>m</em> ≤ <em>n</em>   ⇒   <em>f</em>(<em>m</em>) ≤ <em>f</em>(<em>n</em>).</li>
  <li><em>f</em>(<em>n</em>) is <strong>monotonically decreasing</strong>   if   <em>m</em> ≥ <em>n</em>   ⇒   <em>f</em>(<em>m</em>) ≥ <em>f</em>(<em>n</em>).</li>
  <li><em>f</em>(<em>n</em>) is <strong>strictly increasing</strong>   if   <em>m</em> &lt; <em>n</em>   ⇒   <em>f</em>(<em>m</em>) &lt; <em>f</em>(<em>n</em>).</li>
  <li><em>f</em>(<em>n</em>) is <strong>strictly decreasing</strong>   if   <em>m</em> &gt; <em>n</em>   ⇒   <em>f</em>(<em>m</em>) &gt; <em>f</em>(<em>n</em>).</li>
</ul>

<h3 id="polynomials">Polynomials</h3>

<ul>
  <li><em>p</em>(<em>n</em>) = Θ(<em>n__d</em>), for asymptoptically positive polynomials in <em>n</em> of degree <em>d</em></li>
</ul>

<h3 id="exponentials">Exponentials</h3>

<ul>
  <li>
    <p><em>n__b</em> = o(<em>a__n</em>) for all real constants <em>a</em> and <em>b</em> such that <em>a</em> &gt; 1: <strong><em>Any exponential function with a base greater than 1 grows faster than any polynomial function.</em></strong></p>
  </li>
  <li>
    <p>Useful identities: </p>
    <ul>
      <li><em>a</em>-1 = 1/<em>a</em></li>
      <li>(<em>a__m</em>)<em>n</em> = <em>a__mn</em></li>
      <li><em>a__m__a__n</em> = <em>a__m</em> + <em>n</em></li>
    </ul>
  </li>
</ul>

<h3 id="logarithms">Logarithms</h3>

<ul>
  <li>
    <p>(lg <em>n</em>)<em>b</em> = lg_b__n_ = o(<em>n__a</em>), for a &gt; 0: <strong><em>any positive polynomial function grows faster than any polylogarithmic function.</em></strong></p>
  </li>
  <li>
    <p>Useful identities: </p>
    <ul>
      <li><em>a</em> = <em>b_log_b__a</em>   <em>(Definition of logs.)</em></li>
      <li>log_a__n_ = log_b__n<em>/log_b__a</em>   <br />
<em>(Base change. If _n</em> is variable and <em>a</em> and <em>b</em> are constant, the denominator is constant: this is why asymptotic analysis can ignore the base.)_</li>
      <li>log_c<em>(_ab</em>) = log_c__a_ + log_c__b_   <em>(Ask your slide rule!)</em></li>
      <li>log_b__a_n__ = <em>n</em> log_b__a_</li>
      <li>log_b<em>(1/_a</em>) = −log_b__a_</li>
      <li>log_b__a_ = 1 / log_a__b_</li>
      <li><em>a_log_b__c</em> = <em>c_log_b__a</em>   <em>(Useful for getting the variable where you want it.)</em></li>
    </ul>
  </li>
</ul>

<h3 id="factorials">Factorials</h3>

<ul>
  <li><em>n</em>! = ω(2_n<em>):   _<strong>factorials grow faster than exponentials</strong> (but it could be worse):</em></li>
  <li><em>n</em>! = o(<em>n__n</em>)</li>
  <li>lg(<em>n</em>!) = Θ(<em>n</em> lg <em>n</em>)</li>
  <li>See also the more complex <strong>Stirling’s approximation</strong> from which these are derived.</li>
</ul>

<h3 id="iterated-functions">Iterated Functions</h3>

<ul>
  <li>Definition: <em>f</em>(<em>i</em>)(<em>n</em>) is <em>f</em> applied <em>i</em> times to the initial value <em>n</em>. </li>
  <li>Iterated Logarithm: lg*<em>n</em> = min{<em>i</em> ≥ 0: lg(<em>i</em>)<em>n</em> ≤ 1}   <em>(The iteration at which lg(_i</em>)<em>n</em> is less than 1: a very slowly growing function.)_</li>
</ul>

<h3 id="fibonacci-numbers">Fibonacci Numbers</h3>

<ul>
  <li>Definition: <em>F_0 = 0; _F_1 = 1; and for _i</em> &gt; 1 <em>F__i</em> = <em>F__i</em>-1 + <em>F__i</em>-2. </li>
  <li><strong><em>Fibonacci numbers grow exponentially.</em></strong></li>
</ul>

<hr />

<p>Dan Suthers Last modified: Sat Jan 25 03:51:57 HST 2014<br />
Images are from the instructor’s manual for Cormen et al.  </p>


</div>



<div class="footer-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br />
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a href="http://morea-framework.github.io/">Morea Framework</a> (Theme: simplex)<br>
       Last update on: <span>2014-06-22 06:51:02 -1000</span></p>
    <p style="margin: 0">
      25 modules
      
        | 27 outcomes
      
      
        | 143 readings
      
      
        | 36 experiences
      
      
    </p>
  </div>
</footer>
</div>
</body>
</html>

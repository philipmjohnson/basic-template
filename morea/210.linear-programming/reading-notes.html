<!DOCTYPE html>
<html>
<head>
  <title> Notes on linear programming | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">

  <!--  Load bootswatch-based Morea theme file. -->
  <link rel="stylesheet" href="/ics311s14/css/themes/simplex/bootstrap.min.css">
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" href="/ics311s14/css/syntax.css">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
	<li class="dropdown">
          <a class="dropdown-toggle" data-toggle="dropdown" href="#">Course Info<b class="caret"></b></a>
            <ul class="dropdown-menu" role="menu">
              <li><a href="/ics311s14/morea/010.introduction/reading-course-info.html">Overview</a></li>
              <li><a href="/ics311s14/morea/010.introduction/reading-policies.html">Policies</a></li>
              <li><a href="/ics311s14/morea/010.introduction/reading-topic-overview.html">Topics</a></li>
            </ul>
          </li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        
          <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        
        
          <li><a href="/ics311s14/readings/">Readings</a></li>
        
        
          <li><a href="/ics311s14/experiences/">Experiences</a></li>
        
        
        <li><a href="/ics311s14/news/">News</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h2 id="outline">Outline</h2>

<ol>
  <li>Introduction to Linear Programming</li>
  <li>Formulating Problems as Linear Programs </li>
  <li>Foundations in Gaussian Elimination </li>
  <li>The Simplex Method </li>
</ol>

<h2 id="objectives">Objectives</h2>

<ul>
  <li>Be aware of the range of problems to which linear programming can be applied.</li>
  <li>Understand the Simplex algorithm just enough to understand the format of linear equations used and what is done with them. </li>
  <li>Be able to write a simple linear program for a problem. </li>
</ul>

<h2 id="readings">Readings</h2>

<p>If you have a background in Gaussian Elimination and read and understand
Sections 29.0-29.3 of CLRS, up through the description of Simplex (you need
not read the proofs that follow), these objectives will be met. (The material
of CLRS Sections 29.4-29.5 is excellent, but we don’t need to see all the
proofs concerning Simplex to use it.)</p>

<p>If you don’t have a background in Gaussian Elimination, then reading and
understanding Section 28.1 of CLRS would provide it. However, Section 28.1
provides more detail than is needed to get the gist of Gaussian Elimination
and the Simplex. I found Sedgewick’s (1984), Chapter 5 presentation of
Gaussian Elimination to be clear and sufficient. I also found his presentation
of Linear Programming in Chapter 38 useful for its clear narrative around an
example.</p>

<p>For a full study of linear programming I recommend this reading sequence:</p>

<ul>
  <li><strong>Chapter 5 of Sedgewick (1984) on Gaussian Elimination</strong></li>
  <li><strong>Chapter 38 of Sedgewick (1984) on Linear Programming</strong></li>
  <li><strong>Sections 29.0 through the first half of 29.3 of CLRS</strong></li>
</ul>

<p>If you don’t have time for the full reading:</p>

<ul>
  <li><strong>Read the following web notes</strong> (which summarize the main points from Sedgewick and some material from CLRS 29.0-29.3).</li>
  <li>Then <strong>read 29.0, 29.1 and 29.2 of CLRS before class</strong> (quiz questions and class problems are drawn from those sections). </li>
</ul>

<h2 id="introduction-to-linear-programming">Introduction to Linear Programming</h2>

<p>The following brief conceptual overview of Linear Programming and its roots
in Gaussian Elimination is based largely on Chapters 5 and 38 of: Robert
Sedgewick (1983). Algorithms. Reading, MA: Addison-Wesley. First Edition
(available on Internet), with some comments from CLRS Chapter 29. </p>

<p><strong>Mathematical programming</strong> is the process of modeling a problem as a set of mathematical equations. (The “programming” is in mathematics, not computer code.)</p>

<p><strong>Linear programming</strong> is mathematical programming where the equations are <em>linear equations</em> in a set of variables that model a problem, and include:</p>

<ul>
  <li>a set of <strong>constraints</strong> on the values of the variables (each constraint being expressed as a linear equation), and </li>
  <li>an <strong>objective function</strong> or linear function of these variables that is to be maximized subject to these constraints.</li>
</ul>

<p>A large and diverse set of problems can be expressed as linear programs and
solved. Examples include:</p>

<ul>
  <li><em><strong>Scheduling tasks,</strong></em> such as in business, construction or manufacturing, for example, scheduling flight crews for an airline.</li>
  <li><em><strong>Flows in a network,</strong></em> including flows of multiple types of substances or commodities subject to various constraints (example to be given).</li>
  <li><em>**Maximizing an outcome **</em> given a set of constrained resources, such as deciding where to drill for oil for maximum expected payoff.</li>
</ul>

<h3 id="simplex-algorithm">Simplex Algorithm</h3>

<ul>
  <li>A well established algorithm (actually, family of algorithms) for solving linear programming problems.</li>
  <li>Available in many computer packages.</li>
  <li>Not always the most efficient way to solve a problem (many of the algorithms we have studied are more efficient for their specialized problem), but is often the easiest feasible approach.</li>
  <li>Well studied, but analyzing its asymptotic complexity is still an active area of research, over 50 years after its invention!</li>
  <li>Examples have been given requiring exponential time, but Simplex has been repeatedly shown to have good performance in practice on real problems.</li>
</ul>

<h3 id="examples">Examples</h3>

<p>We begin with examples of problems for which we already have more efficient
algorithms. The point of revisiting them here with less efficient linear
programming solutions is to show you how linear programming works in terms of
familiar problems; and also to reinforce the recurring theme that problems can
be solved with different algorithms if you change problem representation.</p>

<h4 id="linear-program-for-single-pair-shortest-paths">Linear Program for Single-Pair Shortest Paths</h4>

<p>The <a href="http://www2.hawaii.edu/~suthers/courses/ics311s14
/Notes/Topic-18.html#bellmanford">Bellman-Ford algorithm</a> for single-source shortest paths uses the <code>[
Relax</code>](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html#
relax) procedure to find a distance <em>v</em>.<em>d</em>, where for every edge (<em>u</em>, <em>v</em>) ∈
<em>E</em>, <em>v</em>.<em>d</em> ≤ <em>u</em>.<em>d</em> + <em>w</em>(<em>u</em>, <em>v</em>) (since <code>Relax</code> changes <em>v</em>.<em>d</em>
precisely when this is not true). Also, <em>s</em>.<em>d</em> for the source vertex <em>s</em> is
always 0.</p>

<p>We can translate these observations directly into a linear program for the
<strong>single-<em>pair</em> shortest-path</strong> problem from <em>s</em> to <em>t</em>. We will use notation
<em>d__v</em> instead of <em>v</em>.<em>d</em> to be consistent with typical linear programming
notation:</p>

<blockquote>
  <p>Maximize:     <em>d__t</em><br />
Subject to:<br />
                    <em>d__v</em> ≤ <em>d__u</em>+ <em>w</em>(<em>u</em>, <em>v</em>),   ∀ (<em>u</em>, <em>v</em>) ∈ <em>E</em> <br />
                    <em>s</em>.<em>d</em> = 0. </p>
</blockquote>

<p>Why are we <em>maximizing</em> <em>d__t</em> when we seek <em>shortest</em> paths?</p>

<ul>
  <li>If we minimized <em>d__t</em>, then there would be a trivial solution where <em>d__v</em> = 0, ∀ <em>v</em> ∈ <em>V</em>. </li>
  <li>The minimization that finds shortest paths is actually implicit in the first constraint. <br />
Each <em>d__v</em> will be given the maximum value that is yet ≤ the <em>smallest</em>
<em>d__u</em> + <em>w</em>(<em>u</em>, <em>v</em>).</li>
</ul>

<p>(Compare to the fact that we needed to find <em>longest</em> paths when determining
the shortest time in which a set of jobs could finish in the parallel
scheduling problem given in class.)</p>

<p>The extension to <strong>single-source all-destinations</strong> is straightforward:
maximize the <em>sum</em> of the destination distances.</p>

<p>The custom algorithms for <a href="http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html">single-
source</a>
and indeed <a href="http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-19.html">all-
pairs</a>
shortest paths will be more efficient than solving these problems with linear
programming, but this example (and the next) illustrates how linear
programming works in terms of a familiar example.</p>

<h4 id="linear-program-for-max-flow">Linear Program for Max Flow</h4>

<p>Next we show how to model a max-flow problem with linear programming. Instead
of writing <em>f</em>(<em>s</em>,<em>a</em>) to indicate the flow over edge (<em>s</em>,<em>a</em>) (for
example), we follow the conventions of the linear programming literature and
write <em>f__sa</em>. (Sedgewick uses _X_AB.) CLRS present a more general template
for any flow network, whereas here we look at a specific example:</p>

<p><img src="fig/flow-network-example.jpg" alt="" /></p>

<blockquote>
  <p>Maximize:     _f_sa+ _f_sb<br />
Subject to:<br />
                    _f_sa ≤ 8             _f_sb ≤ 2 <br />
                    _f_ac ≤ 6             _f_da ≤ 3 <br />
                    _f_bd≤ 5             _f_cb ≤ 2 <br />
                    _f_ct ≤ 4             _f_dt ≤ 5 <br />
                    _f_sa + _f_da = _f_ac         _f_sb + _f_cb = _f_bd <br />
                    _f_dt + _f_da = _f_bd         _f_cb + _f_ct = _f_ac <br />
                    _f_sa,   _f_sb,   _f_ac,   _f_cb,   _f_ct,   _f_bd,   _f_da,   _f_dt   ≥   0. </p>
</blockquote>

<p>The expression to be maximized,</p>

<blockquote>
  <p><em>fsa</em> + <em>fsb</em></p>
</blockquote>

<p>is the flow over the edges coming out of the source, and hence will be the
flow of the entire network. If the linear program maximizes this, then we have
found the max flow. (If there are edges incoming to <em>s</em> we can subtract these
in the expression to be maximized.)</p>

<p>These inequalities capture edge capacities:</p>

<blockquote>
  <p><em>fsa</em> ≤ 8;   <em>fsb</em> ≤ 2;   <em>fac</em> ≤ 6;   <em>fda</em> ≤ 3;   <em>fbd</em> ≤ 5;   <em>fcb</em> ≤ 2;
<em>fct</em> ≤ 4;   <em>fdt</em> ≤ 5.</p>
</blockquote>

<p>These equalities capture the conservation of flow at vertices (we write the
single edge on the right hand side to give the equations uniform formats):</p>

<blockquote>
  <p><em>fsa</em> + <em>fda</em> = <em>fac</em>     (flow through a)<br />
<em>fsb</em> + <em>fcb</em> = <em>fbd</em>     (flow through b)<br />
<em>fcb</em> + <em>fct</em> = <em>fac</em>     (flow through c)<br />
<em>fdt</em> + <em>fda</em> = <em>fbd</em>     (flow through d)</p>
</blockquote>

<p>The final eight inequalities (written in one line for brevity) express the
constraint that all flows must be positive:</p>

<blockquote>
  <p>_f_sa,   _f_sb,   _f_ac,   _f_cb,   _f_ct,   _f_bd,   _f_da,   _f_dt   ≥
0.</p>
</blockquote>

<p>The Simplex algorithm (discussed later and in the readings), when given a
suitable form of these equations (see section 29.1 CLRS), will return an
assignment of values to variables <em>fsa</em>, … <em>fdt</em> that maximizes the
expression <em>fsa</em> + <em>fsb</em> and hence flow.</p>

<p>The Edmonds-Karp flow algorithm is more efficient than the Simplex algorithm
for solving this version of the max-flow problem. However, Edmonds-Karp is
difficult to modify for problem variations such as multiple commodities or
dealing with cost-benefit tradeoffs. These additional constraints are easy to
add to a linear program.</p>

<p>In general, if a problem can be expressed as a linear program it may be
quicker from a development standpoint to do that rather than to invent a
custom algorithm for it. Linear programming covers a large variety of
problems.</p>

<p>The point here is to introduce linear programming with a familiar example, and
to illustrate its generality, but this also provides another example of
“problem reduction”, a concept that will be at the core of the final topic of
this course on Complexity Theory &amp; NP-Completeness.</p>

<hr />

<h2 id="gaussian-elimination">Gaussian Elimination</h2>

<p>The Simplex algorithm works in a manner similar to (derived from) Gaussian
Elimination for solving a set of linear equations.</p>

<p>Invented by Chinese mathematicians a few thousand years ago, and in Europe by
Newton and revised by Gauss, Gaussian elimination is a two part method for
solving a system of linear equations.</p>

<p>As a simple example, suppose we have the following system:</p>

<blockquote>
  <p>** <em>x</em> + 3_y_ − 4_z_ = 8<br />
<em>x</em> + <em>y</em> − 2_z_ = 2<br />
−<em>x</em> − 2_y_ + 5_z_ = −1 **</p>
</blockquote>

<p>The goal is to find values of <em>x</em>, <em>y</em>, and <em>z</em> that satisfy these equations.
(Recall that there may be zero, one, or an infinite number of solutions, and
you need as many equations as variables to have a unique solution.)</p>

<p>If we think of the variables as subscripted as shown on the left, then we can
rewrite the system of equations as a matrix equation without bothering with
the letters, as shown on the right:</p>

<blockquote>
  <p>_x_1 + 3_x_2 − 4_x_3 = 8<br />
_x_1 + _x_2 − 2_x_3 = 2<br />
−_x_1 − 2_x_2 + 5_x_3 = −1</p>
</blockquote>

<blockquote>
  <p><img src="fig/gauss-example-1c.jpg" alt="" /></p>
</blockquote>

<p>The following operations can be done on systems of linear equations such as
the above. (Later, in the section on linear programing, we’ll drop the
parentheses and put everything in one matrix. Then, the operations below will
be operations on rows and columns of the matrix.)</p>

<ul>
  <li><em><strong>Interchanging equations:</strong></em> Since the order in which we write equations does not matter, we can reorder the rows.</li>
  <li><em><strong>Renaming variables:</strong></em> Swapping entire columns with each other. Swapping columns <em>i</em> and <em>j</em>, what was formerly <em>xi</em> becomes <em>xj</em> and vice-versa.</li>
  <li><em><strong>Multiplying equations by a constant:</strong></em> Accomplished by multiplying all numbers in a row by that constant.</li>
  <li><em><strong>Adding two equations and replacing one by the sum:</strong></em> Since the two sides of an equation are equal, we can add them to the two sides of another equation without affecting equality. </li>
</ul>

<h3 id="the-strategy">The Strategy</h3>

<p>Gaussian elimination is a systematic way of applying these operations to make
the value of one variable obvious (<em>forward elimination</em>), and then
substituting this value back into the other equations to expose their values
(<em>backward substitution</em>).</p>

<h4 id="forward-elimination-triangulation">Forward Elimination (Triangulation)</h4>

<p>Forward elimination turns the matrix into a triangular matrix, where there is
only one variable in the last equation, only that variable plus one more in
the next equation up, etc.</p>

<p>For example, replace the second equation by the difference between the first
two:</p>

<p>Before:     <img src="fig/gauss-example-1c.jpg" alt="" /><br />
After:       <img src="fig/gauss-example-1d.jpg" alt="" /></p>

<p>One term has gone to 0: this means <em>x1</em> has been eliminated from the second
equation. Let’s eliminate <em>x1</em> from the third equation by replacing the third
by the sum of the first and the third:</p>

<p><img src="fig/gauss-example-1e.jpg" alt="" /></p>

<p>Now if we replace the third equation by the difference between the second and
twice the third, we can eliminate <em>x2</em> from the third row, leaving a
<em>triangular</em> matrix. Writing the result as equations:</p>

<blockquote>
  <p>_x_1 + 3_x_2 − 4_x_3 = 8<br />
       2_x_2 − 2_x_3 = 6<br />
              −4_x_3 = −8 </p>
</blockquote>

<p>At the completion of the forward elimination phase, the equations are easy to
solve.</p>

<h4 id="backward-substitution-phase">Backward Substitution Phase</h4>

<p>It is easy to determine from the third equation that <em>x3</em> = 2. Substituting
that into the second equation, we can derive <em>x2</em>:</p>

<blockquote>
  <p>2_x_2 − 4 = 6<br />
        _x_2 = 5 </p>
</blockquote>

<p>Substituting this and <em>x3</em> = 2 into the equation above (rewritten below)
solves for <em>x1</em>:</p>

<blockquote>
  <p>_x_1 + 3_x_2 − 4_x_3 = 8<br />
      _x_1 + 15 − 8 = 8<br />
                    _x_1 = 1 </p>
</blockquote>

<h3 id="the-algorithm">The Algorithm</h3>

<p>So, in general we can solve systems of linear equations as written on the left
by converting them into matrices as written on the right:</p>

<p><img src="fig/gauss-general-example-a.jpg" alt="" />         <img src="fig/gauss-general-example-b.jpg" alt="" /></p>

<p>It is convenient to represent this entire system in one <em>N</em> x (<em>N</em>+1) matrix.</p>

<h4 id="basic-algorithm-for-gaussian-elimination">Basic Algorithm for Gaussian Elimination</h4>

<p>We can eliminate</p>

<ul>
  <li>the <em>first variable</em> from <em>all but the first equation</em> by adding an appropriate multiple of the first equation to each of the second through _N_th equations (the multiple will be different for each equation);</li>
  <li>the <em>second variable</em> from <em>all but the first two equations</em> by adding an appropriate multiple of the second equation to the third through _N_th equations;</li>
  <li>and so on …</li>
</ul>

<p>In general, the algorithm for forward elimination eliminates the <em>i_th
variable in the _j_th equation by multiplying the _i_th equation by _aji</em> /
<em>aii</em> and subtracting it from the <em>j_th equation, for _i</em>+1 ≤ <em>j</em> ≤ <em>N</em>.</p>

<p>We use <em>aji</em> / <em>aii</em> because (<em>aji</em> / <em>aii</em>) * <em>aii</em> = <em>aji</em>, so when we
subtract row <em>i</em> from row <em>j</em> we get <em>aji</em> - <em>aji</em> = 0 in cell <em>j,i</em>.</p>

<p>The essential idea can be expressed in this pseudocode fragment (translated
from Sedgewick’s Pascal):</p>

<pre><code>    for i = 1 to N do
        for j = i + 1 to N do
            for k = N + 1 downto i do
                a[j,k] = a[j,k] − a[i,k] * a[j,i] / a[i,i] 
</code></pre>

<p>There are three nested loops. <em>Trivial Question: How do the loops grow with N?
What’s the complexity?</em></p>

<h4 id="elimination-elaborated">Elimination Elaborated</h4>

<p>This code is too simple: In an actual implementation, various issues must be
dealt with, including:</p>

<ul>
  <li>If <em>aii</em> = 0, cannot divide by 0. Need to swap rows to make <em>aii</em> non-zero in the outer loop. If this is not possible, there is no unique solution.</li>
  <li>If <em>aii</em> is very small, the scaling factor <em>aji</em> / <em>aii</em> could get very large, leading to rounding error in floating point representations used in computers. This is solved by always choosing the row in <em>i</em>+1 to <em>N</em> with the largest absolute value.</li>
</ul>

<p>The process of elimination is also called <strong>pivoting</strong>, a concept that shows
up in the application to linear programming.</p>

<p>Sedgewick presents an improved version as a Pascal procedure. If you want to
understand the algorithm at this level of detail you should read CLRS 28.1.</p>

<hr />

<h2 id="linear-programming">Linear Programming</h2>

<p>Linear programs are systems of linear equations, but with the additional
twists that</p>

<ul>
  <li>The constraint equations may include inequalities.</li>
  <li>There is also a linear expression, the objective function, to be maximized.</li>
</ul>

<p>These two are related:</p>

<ul>
  <li>The constraints being inequalities means there is often no unique solution to the system of constraints.</li>
  <li>Maximizing the objective function helps us choose from among the infinite possible solutions.</li>
</ul>

<p>In fact, these points capture our motivations, in many cases, for using linear
programming for real-world problems! There are many ways to act (i.e., many
solutions), but we want to know which one is the best (i.e., maximized
objective function). The constraints model a set of possible solutions, and
the objective function helps us pick one that maximizes something we care
about. Linear programming is a <em>general</em> way to approach any such situation
that can be modeled with linear equations.</p>

<h3 id="example">Example</h3>

<p>For example, a simple linear program in two variables might look like this:</p>

<p><img src="fig/linear-programming-example-1b.jpg" alt="" /></p>

<blockquote>
  <p>−_x_1 + _x_2 ≤ 5<br />
  _x_1 + 4_x_2 ≤ 45<br />
  2_x_1 + _x_2 ≤ 27<br />
3_x_1 − 4_x_2 ≤ 24  </p>
</blockquote>

<pre><code>  _x_1, _x_2 ≥ 0 
</code></pre>

<h4 id="geometric-interpretation">Geometric Interpretation</h4>

<p>We can graph this example as shown:</p>

<p>Each inequality divides the plane into one half in which a solution cannot lie
and one in which it can.</p>

<p>For example, <em>x1</em> ≥ 0 excludes solutions to the left of the <em>x2</em> axis, and
−<em>x1</em> + <em>x2</em> ≤ 5 means solutions must lie below and to the right of the line
−<em>x1</em> + <em>x2</em> = 5, shown between (0,5) and (5,10).</p>

<h3 id="simplex">Simplex</h3>

<p>Solutions must lie within this feasible region defined by intersecting regions
(half-planes in this example). That region is called the <strong>simplex</strong>.</p>

<p>The simplex is a <strong>convex region:</strong> for any two points in the region, all
points on a line segment between them are also in the region. Convexness can
be used to show an important fact:</p>

<h4 id="fundamental-theorem">Fundamental Theorem</h4>

<p><strong><em>The objective function is always maximized at one of the vertices of the simplex.</em></strong></p>

<p><img src="fig/linear-programming-example-1b-small.jpg" alt="" /></p>

<p>Think of the objective function (here, <em>x1</em> + <em>x2</em>, the dotted line) as a
line of known slope but unknown position. Imagine the line being slid towards
the simplex from infinity. If there is a solution, it will first touch the
simplex at one of the vertices (one solution) or coincide with an edge (many
solutions) that includes a vertex.</p>

<p><em>Where would this line touch the simplex?</em></p>

<p><em>The algorithm does not actually slide a line</em>. Rather, this geometric
interpretation tells us that the algorithm need only need search for a
solution at the vertices of the convex simplex. <em>The simplex method
systematically searches the vertices, moving to new vertices on which the
objective function is no less, and is usually greater than the value for the
previous vertex</em>.</p>

<h4 id="other-issues-exposed-by-the-geometric-interpretation">Other Issues Exposed by the Geometric Interpretation</h4>

<ul>
  <li><strong>Linearity is important</strong>: if either the objective function or the simplex were curved, it would be much harder to tell where they overlap optimally.</li>
  <li>If the intersection of the half-planes is empty, the linear program is <strong>infeasible</strong>.</li>
  <li>A constraint is <strong>redundant</strong> if the simplex defined by the other constraints lies entirely within its half-plane. Not a problem but the code must handle these situations.</li>
  <li>The simplex may be <strong>unbounded</strong>. As a result, the solution may be ill-defined, or even if it is well defined an algorithm may have difficulty with the unbounded portion.</li>
</ul>

<h3 id="multiple-dimensions">Multiple Dimensions</h3>

<p>The geometric interpretation extends to more variables = dimensions.</p>

<p><strong>In three dimensions,</strong></p>

<ul>
  <li>The simplex is a convex 3-D solid defined by the intersection of half-spaces defined by planes rather than lines.</li>
  <li>The objective function is a plane that we can imagine being brought in to intersect with a vertex of the solid.</li>
</ul>

<p><strong>In <em>n</em> dimensions,</strong></p>

<ul>
  <li>(<em>n</em>-1)-dimensional hyperplanes are intersected to define an <em>n</em>-dimensional simplex.</li>
  <li>The objective function is an <em>n</em>-1 dimensional hyper-plane brought from infinity to intersect with the simplex.</li>
</ul>

<p>The anomalous situations get much harder to detect in advance as dimensions
increase, so it is important to handle them well in the code.</p>

<p>As an example, add the inequalities <em>x3</em> ≤ 4 and <em>x3</em> ≥ 0 to our previous
example. The simplex becomes a 3-D solid:</p>

<p><img src="fig/linear-programming-example-2a-nolines.jpg" alt="" /></p>

<blockquote>
  <p>−_x_1 + _x_2 ≤ 5<br />
  _x_1 + 4_x_2 ≤ 45<br />
  2_x_1 + _x_2 ≤ 27<br />
3_x_1 − 4_x_2 ≤ 24<br />
            _x_3 ≤ 4  </p>
</blockquote>

<p>_x_1, _x_2, _x_3 ≥ 0</p>

<p>If the objective function is defined to be <em>x1</em> + <em>x2</em> + <em>x3</em>, this is a
plane perpendicular to the line <em>x1</em>= <em>x2</em> = <em>x3</em>. Imagine this plane being
brought from infinity to the origin: <em>where would it hit the simplex?</em></p>

<p>Again, the algorithm we discuss below does not actually move planes from
infinity; this is just a way of visualizing the fact that an optimal solution
must lie on <em>some</em> vertex of the <em>n</em>-dimensional simplex, so we need only
search these vertices.</p>

<h3 id="the-simplex-method">The Simplex Method</h3>

<p>Now we see how pivoting from Gaussian elimination is used. Pivoting is
analogous to moving between the vertices of the simplex, starting at the
origin. First, we need to prepare the data …</p>

<h4 id="standard-form">Standard Form</h4>

<p><em>(Note: Sedgewick does not distinguish between standard and slack forms; this
discussion is based on CLRS section 29.1, to which the reader is referred for
details.)</em></p>

<p>When equations are written to model a problem in a natural way, they may have
various features that are not suitable for input to the Simplex Method. We
begin by conversion into <strong>standard form</strong>:</p>

<p>Given <strong><em>n</em> real numbers <em>c_1, _c_2, … _c__n</em></strong>     <em>(coefficients on
objective function)</em>,  </p>

<p><strong><em>m</em> real numbers <em>b_1, _b_2, … _b__m</em></strong>             <em>(constants on right hand side of equations)</em>,   </p>

<p>and <strong><em>m__n</em> real numbers <em>a__i__j</em></strong> for   <em>i</em> = 1, 2 … <em>m</em> and <em>j</em> = 1, 2,
… <em>n</em>     <em>(coefficients on variables in equations)</em>,  </p>

<p><strong>find real numbers <em>x_1, _x_2, … _x__n</em></strong>           <em>(the variables)</em></p>

<p><strong>that maximize:   Σ_j<em>=1,_n</em> <em>cj</em> <em>xj</em></strong>     <em>(the objective function)</em>  </p>

<p><strong>subject to:   Σ_j<em>=1,_n</em> <em>aij</em> <em>xj</em> ≤ <em>bj</em>   for <em>i</em> = 1, 2, … <em>m</em></strong>     <em>(regular constraints)</em>   </p>

<p><strong>and   <em>xj</em> ≥ 0,   for <em>j</em> = 1, 2, … <em>n</em></strong>     <em>(nonnegativity constraints)</em></p>

<p>The following conversions may be needed to convert a linear program into
standard form (see CLRS for details and justification):</p>

<ol>
  <li>If the objective function is to be minimized rather than maximized, negate the objective function (i.e., negate its coefficients). </li>
  <li>Replace each variable <em>x</em> that does not have a nonnegativity constraint with <em>x’</em>−<em>x’‘</em>, and introduce the constraints <em>x’</em> ≥ 0 and <em>x’‘</em> ≥ 0. </li>
  <li>Convert equality constraints of form <em>f</em>(<em>x_1, _x_2, … _x__n</em>) = <em>b</em> into two inequality constraints <em>f</em>(<em>x_1, _x_2, … _x__n</em>) ≤ <em>b</em> and <em>f</em>(<em>x_1, _x_2, … _x__n</em>) ≥ <em>b</em>. </li>
  <li>Convert ≥ constraints (except the nonnegativity constraints) into ≤ constraints by multiplying the constraints by -1.</li>
</ol>

<p>Our example above is already in standard form, except that some of the
coefficients <em>aij</em> are equal to 1 and are not written out, and we have not
written terms with 0 coefficents. Making all <em>aij</em> explicit, we would write:</p>

<p>−1_x_1 + 1_x_2 + 0_x_3 ≤ 5<br />
1_x_1 + 4_x_2 + 0_x_3 ≤ 45<br />
2_x_1 + 1_x_2 + 0_x_3 ≤ 27<br />
3_x_1 − 4_x_2 + 0_x_3 ≤ 24<br />
0_x_1 + 0_x_2 + 1_x_3 ≤ 4  </p>

<p>_x_1, _x_2, _x_3 ≥ 0</p>

<h4 id="slack-form">Slack Form</h4>

<p>The Simplex Method is based on methods (akin to Gaussian elimination) for
solving systems of linear equations that require that we work with equalities
rather than inequalities (except for the constraints that the variables are
non-negative).</p>

<p>We can convert standard form into slack form by introducing <strong>slack
variables</strong>, one for each inequality, that “take up the slack” allowed by the
inequality. (These will be allowed to range as needed to do so.)</p>

<p>For example, instead of <em>x_1+ 4_x_2 ≤ 45, we can write _x_1 + 4_x_2 + _y</em> =
45, where <em>y</em> can range over the values needed to “take up the slack” between
inequality and equality.</p>

<p>Applying this idea to the 3-D example above, and using a different <em>yi</em> for
each equation, we can model that example with:</p>

<blockquote>
  <p>Maximize _x_1 + _x_2 + _x_3 subject to the constraints:</p>
</blockquote>

<blockquote>

</blockquote>

<blockquote>
  <blockquote>
    <p>−1_x_1 + 1_x_2 + 0_x_3 + _y_1 = 5<br />
  1_x_1 + 4_x_2 + 0_x_3 + _y_2 = 45<br />
  2_x_1 + 1_x_2 + 0_x_3 + _y_3 = 27<br />
  3_x_1 − 4_x_2 + 0_x_3 + _y_4 = 24<br />
  0_x_1 + 0_x_2 + 1_x_3 + _y_5 =   4  </p>
  </blockquote>
</blockquote>

<p>_x_1, _x_2, _x_3, _y_1, _y_2, _y_3, _y_4, _y_5 ≥ 0</p>

<p>There are <em>m</em> equations in <em>n</em> variables, including up to <em>m</em> slack variables
(one for each inequality). (Note: in using <em>n</em> and <em>m</em>, I am following CLRS.
Sedgewick uses <em>M</em> for number of variables and <em>N</em> for number of equations.)</p>

<ul>
  <li>We assume that <em>n</em> &gt; <em>m</em> (more variables than equations), so there are many solutions possible. (In our example above, <em>n</em> = 8 and <em>m</em> = 5.) </li>
  <li>We assume that the origin ((0, 0, 0) in this example) is a point on the simplex, so we can use it as a starting point for the search for the best solution, which must lie on some vertex. (The assumption that the origin is a solution can be eliminated if needed.) </li>
</ul>

<p>We can now write the slack-form system of equations (e.g., above) as a matrix
(e.g., shown below), where the 0th row contains the negated coefficients of
the objective function. Sedgewick describes how this negation directs the
procedure to select the correct rows and columns for pivoting), and the
(<em>n</em>+1)th column has the numbers on the right hand side of the equation.</p>

<p><img src="fig/linear-programming-example-2c.jpg" alt="" /></p>

<p>We want to perform pivot operations, using the same row and column
manipulations as for Gaussian elimination.</p>

<ul>
  <li>Instead of trying to make a triangular matrix we are trying to get each column corresponding to the non-slack variables _x_1, _x_2, and _x_3 to have exactly one “1” in it and all the rest “0”s.</li>
  <li>This is because the variables with one “1” in it and all the rest “0”s are the <strong>basis variables</strong>: their values give the solution if we set all other variables to 0.</li>
  <li>Then we will be able to read off the values of the variables in the (<em>n</em>+1)th or rightmost column. The value of variable <em>x__i</em> will be found in row <em>i</em> column <em>n</em>+1, or at <em>a__i</em>, <em>n</em>+1.</li>
  <li>We don’t care what the values of the slack variables <em>yi</em> are (they just move the solution around in the feasible inequality areas).</li>
</ul>

<p>As we proceed, the upper right cell will have the current value of the
objective function. We always want to increase this. The question is what
strategy to take.</p>

<p>The most popular strategy is <strong>greatest increment</strong>:</p>

<ul>
  <li>Choose the <em>column q</em> with the smallest value in row 0 (the largest absolute value). The objective function will increase if we use any column with a negative entry in row 0. </li>
  <li>Choose the <em>row p</em> from among those with positive values in the chosen column that has the smallest value when divided into the (<em>n</em>+1)th element in the same row. (Sedgewick discusses how this guarantees that the objective function increases and also that we stay in the simplex.)</li>
  <li>In the case of ties, choose the row that will result in the column of lowest index being removed from the basis (this policy prevents cycling). </li>
</ul>

<p>An alternative strategy is ** steepest descent ** (actually ascent!): evaluate
the alternatives and choose the column that increases the objective function
the most.</p>

<h4 id="example-1">Example</h4>

<p>We’ll solve the example given above and copied below. Keep in mind that row
indices start at 0, but column indices start at 1. (See Sedgewick for
discussion of issues concerning staying in the simplex, detecting unbounded
simplexes, and avoiding circularity; and then CLRS if you want details and
proofs.)</p>

<p><img src="fig/linear-programming-example-2c.jpg" alt="" /></p>

<p>There are three columns with the smallest value (-1) in row 0; we choose to
operate on the lowest indexed column 1. Dividing the last number by the
positive values in this column, 45/1 = 45 (row 2), 27/2 = 13.5 (row 3) and
24/3 = 8 (row 4), so we choose to pivot on row 4, as this has the smallest
result.</p>

<p>Pivot for row <em>p</em>= 4 and column <em>q</em> = 1 by adding an appropriate multiple of
the fourth row to each of the other rows to make the first column 0 except for
a 1 in row 4):</p>

<p><img src="fig/linear-programming-example-2d.jpg" alt="" /></p>

<p>After that pivot, only _x_1 is a basis variable. Setting the others to 0, we
have moved to vertex (8,0,0) on the simplex (see figure), and the objecive
function has value 8.00 (upper right corner of matrix above).</p>

<p><img src="fig/linear-programming-example-2a-small-1.jpg" alt="" /></p>

<p>Now, column 2 has the smallest value. Rows 2 and 3 are candidates: for row 2,
37/5.33 = 6.94; and for row 3, 11/3.67 = 2.99. We choose row 3. Pivoting on
row <em>p</em> = 3 and column <em>q</em> = 2:</p>

<p><img src="fig/linear-programming-example-2e.jpg" alt="" /></p>

<p>After that pivot, _x_1 and _x_2 are basis variables, with values 12 and 3
respectively, so we are at vertex (12,3,0). The objecive function has value
15.00. The figure to the right shows how we are moving through the space.</p>

<p>Now pivot on column <em>q</em> = 3 (it has -1 in row 0) and row <em>p</em> = 5 (it has the
only positive value in column 3).</p>

<p><img src="fig/linear-programming-example-2f.jpg" alt="" /></p>

<p>Now all three <em>x__i</em> are in the basis, and we are at vertex (12,3,4).</p>

<p><img src="fig/linear-programming-example-2a-small-2.jpg" alt="" /></p>

<p>But we are not done: there is still a negative value in row 0 (at column 7),
so we know that we can still increase the objective function. I leave it to
you to do the math to verify that row 2 will be selected. Pivoting on row <em>p</em>
= 2 and column <em>q</em> = 7, we get::</p>

<p><img src="fig/linear-programming-example-2g-solved.jpg" alt="" /></p>

<p>Now row 0 has no negative values, and the columns for the three variables of
interest are in the basis (all 0 except one 1 in each). We can read off the
solution: _x_1 = 9, _x_2 = 9, and _x_3 = 4, with optimum value 22.</p>

<h3 id="sedgewicks-code">Sedgewick’s Code</h3>

<p><em>(Here I briefly explain Sedgewick’s Pascal code, but if you want to
understand the algorithm in detail I recommend going to CLRS for a more
current treatment in pseudocode you are familiar with.)</em></p>

<p>Keep in mind that for Sedgewick there are <em>N</em> equations in <em>M</em> variables.</p>

<p>The main procedure finds values of <em>p</em> and <em>q</em> and calls <code>pivot</code>, repeating
until the optimum is reached (<code>_q_=_M_+1</code>) or the simplex is found to be
unbounded (<code>_p_=_N_+1</code>).</p>

<p><img src="fig/linear-programming-code-main.jpg" alt="" /></p>

<ul>
  <li>The first line finds <em>q</em> by finding the first negative value in the 0th row.</li>
  <li>The second line finds the first positive value in the _q_th column. </li>
  <li>The <code>for</code> loop finds the best row <em>p</em> for pivoting by searching for the smallest ratio with the value in <em>M</em>+1).</li>
  <li>If the conditions for continuation are met, <code>pivot</code> is called.</li>
</ul>

<p>The <code>pivot</code> procedure has similarities to Gaussian elimination. (The <code>for</code>
loops below correspond to the two innermost <code>for</code> loops of Gaussian
elimination, and the outer <code>for</code> loop of Gauss corresponds to the <code>repeat</code>
loop in the main procedure above):</p>

<p><img src="fig/linear-programming-code-pivot.jpg" alt="" /></p>

<p>The innermost line is where one row is scaled and subtracted from another.
Other details are discussed in Sedgewick’s chapter, including the need to
implement cycle avoidance and test whether the matrix has a feasible basis
(absent from the code above).</p>

<hr />

<h3 id="whats-next">What’s Next</h3>

<p>At this point, I highy recommend reading CLRS Sections 29.0 (the introduction
to the chapter) through the middle of 29.3 (where the Simplex algorithm is
introduced: as a “consumer” of the algorithm you don’t need to read the proofs
that follow in the rest of the section).</p>

<hr />

<p>Dan Suthers Last modified: Thu Apr 17 01:58:00 HST 2014<br />
Images are from Sedgewick (1983). Algorithms. Reading, MA: Addison-Wesley.
First Edition.  </p>


</div>



<div class="footer-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br />
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a href="http://morea-framework.github.io/">Morea Framework</a> (Theme: simplex)<br>
       Last update on: <span>2014-06-22 06:51:02 -1000</span></p>
    <p style="margin: 0">
      25 modules
      
        | 27 outcomes
      
      
        | 143 readings
      
      
        | 36 experiences
      
      
    </p>
  </div>
</footer>
</div>
</body>
</html>

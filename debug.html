<!DOCTYPE html>
<html>
<head>
  <title> Debug | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/3.1.0/cerulean/bootstrap.min.css">

  <!--  Load site-specific customizations after bootstrap. -->
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" href="/ics311s14/css/syntax.css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:normal,italic,bold">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        <li><a href="/ics311s14/readings/">Readings</a></li>
        <li><a href="/ics311s14/experiences/">Experiences</a></li>
        <li><a href="/ics311s14/assessments/">Assessments</a></li>
        <li><a href="/ics311s14/schedule/">Schedule</a></li>
        
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h1>Debugging</h1>

<h2>Site</h2>

<pre>Hash
{"source"=>"./master/src",
 "destination"=>"./gh-pages",
 "plugins"=>"_plugins",
 "layouts"=>"_layouts",
 "data_source"=>"_data",
 "keep_files"=>[".git", ".svn"],
 "gems"=>[],
 "timezone"=>nil,
 "encoding"=>nil,
 "safe"=>false,
 "detach"=>false,
 "show_drafts"=>nil,
 "limit_posts"=>0,
 "lsi"=>false,
 "future"=>true,
 "pygments"=>true,
 "relative_permalinks"=>true,
 "markdown"=>"redcarpet",
 "permalink"=>"date",
 "baseurl"=>"/ics311s14",
 "include"=>[".htaccess"],
 "exclude"=>["morea"],
 "paginate_path"=>"/page:num",
 "markdown_ext"=>"markdown,mkd,mkdn,md",
 "textile_ext"=>"textile",
 "port"=>"4000",
 "host"=>"0.0.0.0",
 "excerpt_separator"=>"\n\n",
 "maruku"=>
  {"fenced_code_blocks"=>true,
   "use_tex"=>false,
   "use_divs"=>false,
   "png_engine"=>"blahtex",
   "png_dir"=>"images/latex",
   "png_url"=>"/images/latex"},
 "rdiscount"=>{"extensions"=>[]},
 "redcarpet"=>{"extensions"=>[]},
 "kramdown"=>
  {"auto_ids"=>true,
   "footnote_nr"=>1,
   "entity_output"=>"as_char",
   "toc_levels"=>"1..6",
   "smart_quotes"=>"lsquo,rsquo,ldquo,rdquo",
   "use_coderay"=>false,
   "coderay"=>
    {"coderay_wrap"=>"div",
     "coderay_line_numbers"=>"inline",
     "coderay_line_number_start"=>1,
     "coderay_tab_width"=>4,
     "coderay_bold_every"=>10,
     "coderay_css"=>"style"}},
 "redcloth"=>{"hard_breaks"=>true},
 "name"=>"ICS 311 Spring 2014",
 "morea_debug"=>false,
 "morea_module_pages"=>
  [#Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="module.md"],
 "morea_outcome_pages"=>
  [#Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="outcome-1.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="outcome.md"],
 "morea_reading_pages"=>
  [#Jekyll:Page @name="reading-1.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-4.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-7.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-5.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-2.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-3.md",
   #Jekyll:Page @name="reading-screencast-f.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-6.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-notes.md"],
 "morea_experience_pages"=>
  [#Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="experience-1.md"],
 "morea_assessment_pages"=>[#Jekyll:Page @name="assessment.md"],
 "morea_home_page"=>#Jekyll:Page @name="home.md",
 "morea_footer_page"=>#Jekyll:Page @name="footer.md",
 "morea_page_table"=>
  {"intro"=>#Jekyll:Page @name="module.md",
   "outcome-311"=>#Jekyll:Page @name="outcome-1.md",
   "outcome-algorithm"=>#Jekyll:Page @name="outcome-2.md",
   "reading-algorithms"=>#Jekyll:Page @name="reading-1.md",
   "reading-assessment"=>#Jekyll:Page @name="reading-2.md",
   "reading-assignments"=>#Jekyll:Page @name="reading-3.md",
   "reading-course-info"=>#Jekyll:Page @name="reading-4.md",
   "reading-format"=>#Jekyll:Page @name="reading-5.md",
   "reading-policies"=>#Jekyll:Page @name="reading-6.md",
   "reading-topic-overview"=>#Jekyll:Page @name="reading-7.md",
   "reading-cormen-1"=>#Jekyll:Page @name="reading-cormen.md",
   "experience-1"=>#Jekyll:Page @name="experience-1.md",
   "experience-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-3"=>#Jekyll:Page @name="experience-3.md",
   "examples-insertion-merge-sort"=>#Jekyll:Page @name="module.md",
   "outcome-analysis-style"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-2"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-2"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-2a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-2b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-2c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-2d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-2e"=>#Jekyll:Page @name="reading-screencast-e.md",
   "reading-screencast-mit-1"=>#Jekyll:Page @name="reading-screencast-mit.md",
   "assessment-asymptotic-concepts"=>#Jekyll:Page @name="assessment.md",
   "experience-asymptotic-concepts"=>#Jekyll:Page @name="experience.md",
   "growth"=>#Jekyll:Page @name="module.md",
   "outcome-growth"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-3"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-3"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-3a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-3b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-3c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-3d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-mit-2"=>#Jekyll:Page @name="reading-screencast-mit.md",
   "experience-project-1"=>#Jekyll:Page @name="experience-1.md",
   "experience-asymptotic-basic-data-structures"=>
    #Jekyll:Page @name="experience-2.md",
   "experience-asymptotic-homework"=>#Jekyll:Page @name="experience-3.md",
   "adt"=>#Jekyll:Page @name="module.md",
   "outcome-adt"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-10"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-4"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-4a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-4b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "experience-indicator-random-variables"=>#Jekyll:Page @name="experience.md",
   "probabilistic"=>#Jekyll:Page @name="module.md",
   "outcome-probabilistic"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-5"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-goodrich"=>#Jekyll:Page @name="reading-goodrich.md",
   "reading-notes-5"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-5a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-5b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-5c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-5d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-mit-skip-lists"=>
    #Jekyll:Page @name="reading-screencast-mit.md",
   "experience-data-structures-homework"=>#Jekyll:Page @name="experience-2.md",
   "experience-deletion"=>#Jekyll:Page @name="experience.md",
   "hash-tables"=>#Jekyll:Page @name="module.md",
   "outcome-hash-tables"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-11"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-6"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-6a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-6b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-6c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-6d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-mit-hash-tables-1"=>
    #Jekyll:Page @name="reading-screencast-mit-1.md",
   "reading-screencast-mit-hash-tables-2"=>
    #Jekyll:Page @name="reading-screencast-mit-2.md",
   "experience-master-method"=>#Jekyll:Page @name="experience-2.md",
   "experience-substitution"=>#Jekyll:Page @name="experience.md",
   "divide-conquer"=>#Jekyll:Page @name="module.md",
   "outcome-divide-conquer-recognize"=>#Jekyll:Page @name="outcome-2.md",
   "outcome-divide-conquer-apply"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-4"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-7"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-7a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-7b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-7c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-7d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-mit-divide-conquer"=>
    #Jekyll:Page @name="reading-screencast-mit.md",
   "experience-binary-search-trees-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-binary-search-trees"=>#Jekyll:Page @name="experience.md",
   "binary-search-trees"=>#Jekyll:Page @name="module.md",
   "outcome-binary-search-trees"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-12"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-8"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-8a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-8b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-8c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-8d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "experience-heaps-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-heaps"=>#Jekyll:Page @name="experience.md",
   "heaps"=>#Jekyll:Page @name="module.md",
   "outcome-heaps"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-6"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-9"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-9a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-9b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-9c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-9d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "experience-quicksort-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-quicksort"=>#Jekyll:Page @name="experience.md",
   "quicksort"=>#Jekyll:Page @name="module.md",
   "outcome-quicksort"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-8"=>#Jekyll:Page @name="reading-cormen-1.md",
   "reading-cormen-7"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-10"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-10a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-10b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-10c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "experience-balanced-trees-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-balanced-trees"=>#Jekyll:Page @name="experience.md",
   "balanced-trees"=>#Jekyll:Page @name="module.md",
   "outcome-balanced-trees-algorithm"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-13"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-11"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-11a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-11b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-11c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-11d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-sedgewick-15"=>#Jekyll:Page @name="reading-sedgewick.md",
   "experience-dynamic-programming-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-dynamic-programming"=>#Jekyll:Page @name="experience.md",
   "dynamic-programming"=>#Jekyll:Page @name="module.md",
   "outcome-dynamic-programming"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-15"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-12"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-12a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-12b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-12c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-12d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-sedgewick-37"=>#Jekyll:Page @name="reading-sedgewick.md",
   "experience-greedy-algorithms"=>#Jekyll:Page @name="experience.md",
   "greedy-algorithms"=>#Jekyll:Page @name="module.md",
   "outcome-greedy-algorithms"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-16"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-13"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-13a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-13b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-13c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "experience-graph-bfs-dfs"=>#Jekyll:Page @name="experience-1.md",
   "experience-graph-scc"=>#Jekyll:Page @name="experience-2.md",
   "experience-graph-transpose"=>#Jekyll:Page @name="experience-3.md",
   "graphs"=>#Jekyll:Page @name="module.md",
   "outcome-graphs"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-22"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-goodrich-graphs"=>#Jekyll:Page @name="reading-goodrich.md",
   "reading-notes-14"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-14a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-14b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-14c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-screencast-14d"=>#Jekyll:Page @name="reading-screencast-d.md",
   "reading-screencast-14e"=>#Jekyll:Page @name="reading-screencast-e.md",
   "reading-screencast-14f"=>#Jekyll:Page @name="reading-screencast-f.md",
   "reading-sedgewick-wayne-4"=>#Jekyll:Page @name="reading-sedgewick-2.md",
   "reading-sedgewick-32"=>#Jekyll:Page @name="reading-sedgewick.md",
   "experience-amortized-analysis"=>#Jekyll:Page @name="experience.md",
   "amortized-analysis"=>#Jekyll:Page @name="module.md",
   "outcome-amortized-analysis"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-17"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-15"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-15a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-15b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "experience-disjoint-sets"=>#Jekyll:Page @name="experience.md",
   "disjoint-sets"=>#Jekyll:Page @name="module.md",
   "outcome-disjoint-sets"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-21"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-16"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-16a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "experience-minimum-spanning-tree-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-minimum-spanning-tree"=>#Jekyll:Page @name="experience.md",
   "minimum-spanning-tree"=>#Jekyll:Page @name="module.md",
   "outcome-minimum-spanning-tree"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-23"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-17"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-17a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-17b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-17c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "reading-sedgewick-31"=>#Jekyll:Page @name="reading-sedgewick.md",
   "experience-single-source-shortest-paths-2"=>
    #Jekyll:Page @name="experience-2.md",
   "experience-single-source-shortest-paths"=>
    #Jekyll:Page @name="experience.md",
   "single-source-shortest-paths"=>#Jekyll:Page @name="module.md",
   "outcome-single-source-shortest-paths"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-24"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-18"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-18a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-18b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-18c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "experience-all-pairs-shortest-paths"=>#Jekyll:Page @name="experience.md",
   "all-pairs-shortest-paths"=>#Jekyll:Page @name="module.md",
   "outcome-all-pairs-shortest-paths"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-25"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-19"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-19a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-19b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-19c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "experience-maximum-flow-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-maximum-flow"=>#Jekyll:Page @name="experience.md",
   "maximum-flow"=>#Jekyll:Page @name="module.md",
   "outcome-maximum-flow"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-26"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-20"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-20a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-20b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-20c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "experience-linear-programming"=>#Jekyll:Page @name="experience.md",
   "linear-programming"=>#Jekyll:Page @name="module.md",
   "outcome-linear-programming"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-29"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-21"=>#Jekyll:Page @name="reading-notes.md",
   "reading-sedgewick-38"=>#Jekyll:Page @name="reading-sedgewick-2.md",
   "reading-sedgewick-5"=>#Jekyll:Page @name="reading-sedgewick.md",
   "multithreading"=>#Jekyll:Page @name="module.md",
   "outcome-multithreading"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-27"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-22"=>#Jekyll:Page @name="reading-notes.md",
   "string-matching"=>#Jekyll:Page @name="module.md",
   "outcome-string-matching"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-32"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-23"=>#Jekyll:Page @name="reading-notes.md",
   "np-completeness"=>#Jekyll:Page @name="module.md",
   "outcome-np-completeness"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-34"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-24"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-24a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-24b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "reading-screencast-24c"=>#Jekyll:Page @name="reading-screencast-c.md",
   "approximation algorithms"=>#Jekyll:Page @name="module.md",
   "outcome-approximation-algorithms"=>#Jekyll:Page @name="outcome.md",
   "reading-cormen-35"=>#Jekyll:Page @name="reading-cormen.md",
   "reading-notes-25"=>#Jekyll:Page @name="reading-notes.md",
   "reading-screencast-25a"=>#Jekyll:Page @name="reading-screencast-a.md",
   "reading-screencast-25b"=>#Jekyll:Page @name="reading-screencast-b.md",
   "footer"=>#Jekyll:Page @name="footer.md",
   "home"=>#Jekyll:Page @name="home.md"},
 "morea_fatal_errors"=>false,
 "time"=>2014-04-23 15:59:05 -1000,
 "posts"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
 "pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-1.md",
   #Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="reading-1.md",
   #Jekyll:Page @name="reading-2.md",
   #Jekyll:Page @name="reading-3.md",
   #Jekyll:Page @name="reading-4.md",
   #Jekyll:Page @name="reading-5.md",
   #Jekyll:Page @name="reading-6.md",
   #Jekyll:Page @name="reading-7.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="assessment.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-screencast-f.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "html_pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-1.md",
   #Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="reading-1.md",
   #Jekyll:Page @name="reading-2.md",
   #Jekyll:Page @name="reading-3.md",
   #Jekyll:Page @name="reading-4.md",
   #Jekyll:Page @name="reading-5.md",
   #Jekyll:Page @name="reading-6.md",
   #Jekyll:Page @name="reading-7.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="assessment.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-2.md",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-mit.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-screencast-d.md",
   #Jekyll:Page @name="reading-screencast-e.md",
   #Jekyll:Page @name="reading-screencast-f.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="experience.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-sedgewick-2.md",
   #Jekyll:Page @name="reading-sedgewick.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="reading-screencast-c.md",
   #Jekyll:Page @name="module.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome.md",
   #Jekyll:Page @name="reading-cormen.md",
   #Jekyll:Page @name="reading-notes.md",
   #Jekyll:Page @name="reading-screencast-a.md",
   #Jekyll:Page @name="reading-screencast-b.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "categories"=>
  {"jekyll"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
   "update"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"]},
 "tags"=>{},
 "data"=>{}}
</pre>

<h2>Pages</h2>

<h2>/assessments/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Assessments",
 "url"=>"/assessments/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Assessments <small>in module order</small></h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/intro/index.html\">Introduction</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/examples-insertion-merge-sort/index.html\">Analysis examples</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/growth/index.html\">Growth of functions</a></h2>\n\n    \n\n    \n      \n      <h3>Assessment 1: Ability to recall asymptotic concepts</h3>\n      <p>\n        \n          <span class=\"badge\">Bloom: Remember</span>\n        \n      </p>\n      <p>Assessed ability to remember asymptotic concepts through an in-class multiple choice exam:</p>\n\n<p><link rel=\"stylesheet\" href=\"http://cdn.oesmith.co.uk/morris-0.4.3.min.css\">\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/raphael/2.1.0/raphael-min.js\"></script>\n<script src=\"http://cdn.oesmith.co.uk/morris-0.4.3.min.js\"></script></p>\n\n<div class=\"well\">\n  <div id=\"assessment\" style=\"height: 250px;\"></div>\n</div>\n\n<script>\nMorris.Bar({\n  element: 'assessment',\n  hideHover: false,\n  data: [\n        { y: 'Very satisfactory (%)', num: 15 },\n        { y: 'Satisfactory (%)', num: 55 },\n        { y: 'Unsatisfactory (%)', num: 25 },\n        { y: 'Absent (%)', num: 5 },\n        ],\n  xkey: 'y',\n  ykeys: ['num'],\n  resize: true,\n  labels: ['Students']\n});\n</script>\n\n<p></div>\n</div></p>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/adt/index.html\">Abstract data types</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/probabilistic/index.html\">Probabilistic Analysis</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/hash-tables/index.html\">Hash Tables</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/divide-conquer/index.html\">Divide and conquer</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/binary-search-trees/index.html\">Binary Search Trees</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/heaps/index.html\">Heapsort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/quicksort/index.html\">Quicksort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/balanced-trees/index.html\">Balanced Trees</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/dynamic-programming/index.html\">Dynamic Programming</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/greedy-algorithms/index.html\">Greedy Algorithms</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/graphs/index.html\">Graphs</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/amortized-analysis/index.html\">Amortized analysis</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/disjoint-sets/index.html\">Disjoint sets</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/minimum-spanning-tree/index.html\">Minimum spanning tree</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/single-source-shortest-paths/index.html\">Single source shortest paths</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/all-pairs-shortest-paths/index.html\">All pairs shortest paths</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/maximum-flow/index.html\">Maximum flow</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/linear-programming/index.html\">Linear programming</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/multithreading/index.html\">Multithreading</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/string-matching/index.html\">String matching</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/np-completeness/index.html\">NP-completeness</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/approximation algorithms/index.html\">Approximation algorithms</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n",
 "path"=>"assessments/index.md"}
</pre>

<h2>/debug.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Debug",
 "topdiv"=>"container",
 "url"=>"/debug.html",
 "content"=>
  "Debugging\n=========\n\nSite\n----\n\n{{ site | debug }}\n\nPages\n-----\n\n{% for page in site.pages %}\n{{ page.url }}\n--------------\n\n{{ page | debug }}\n{% endfor %}\n\n\n",
 "path"=>"debug.md"}
</pre>

<h2>/experiences/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Experiences",
 "url"=>"/experiences/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Experiential Learning <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_experiences %}\n      {% assign experience = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n             {{ experience.morea_summary | markdownify }}\n             <p>\n             {% for label in experience.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n       {% if forloop.index == 4 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 8 %}\n         </div><div class=\"row\">\n       {% endif %}\n      {% if forloop.index == 12 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 16 %}\n         </div><div class=\"row\">\n       {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"experiences/index.md"}
</pre>

<h2>/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Home",
 "topdiv"=>"container",
 "url"=>"/index.html",
 "content"=>
  "{% if site.morea_home_page %}\n  {{ site.morea_home_page.content | markdownify }}\n{% else %}\n  No home page content supplied.\n{% endif %}\n\n",
 "path"=>"index.md"}
</pre>

<h2>/modules/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Modules",
 "url"=>"/modules/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Modules <small>Topics covered in this class.</small></h1>\n  <div class=\"row\">\n     {% for module in site.morea_module_pages %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <img src=\"{{ site.baseurl }}{{ module.morea_icon_url }}\" width=\"100\" class=\"img-circle img-responsive\">\n            <div class=\"caption\">\n              <h3 style=\"text-align: center; margin-top: 0\">{{ forloop.index }}. {{ module.title }}</h3>\n              {{ module.content | markdownify }}\n              <p>\n              {% for label in module.morea_labels %}\n                <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n              </p>\n              {% if module.morea_coming_soon %}\n                <p class=\"text-center\"><a href=\"#\" class=\"btn btn-default\" role=\"button\">Coming soon...</a></p>\n              {% else %}\n                <p class=\"text-center\"><a href=\"{{ module.morea_id }}\" class=\"btn btn-primary\" role=\"button\">Learn more...</a></p>\n              {% endif %}\n            </div>\n          </div>\n        </div>\n       {% cycle '', '', '', '</div><div class=\"row\">' %}\n     {% endfor %}\n  </div>\n</div>\n\n\n",
 "path"=>"modules/index.md"}
</pre>

<h2>/outcomes/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Learning Outcomes",
 "url"=>"/outcomes/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Learning Outcomes</h1>\n</div>\n\n{% if site.morea_outcome_pages.size == 0 %}\n<p>No outcomes for this course.</p>\n{% endif %}\n\n\n{% for outcome in site.morea_outcome_pages %}\n\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Outcome:</small> {{ outcome.title }}</h2>\n    <p>\n      {% for label in outcome.morea_labels %}\n         <span class=\"badge\">{{ label }}</span>\n      {% endfor %}\n    </p>\n    {{ outcome.content | markdownify }}\n    <p>\n    <em>Referencing modules:</em>\n    {% for module in outcome.referencing_modules %}\n      <a href=\"../modules/{{ module.morea_id }}\">{{ module.title }}</a>\n    {% endfor %}\n  </div>\n</div>\n\n{% endfor %}\n\n\n",
 "path"=>"outcomes/index.md"}
</pre>

<h2>/readings/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Readings",
 "url"=>"/readings/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Readings and other resources <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_readings %}\n      {% assign reading = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n             {{ reading.morea_summary | markdownify }}\n             <p>\n             {% for label in reading.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n        {% if forloop.index == 4 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 8 %}\n          </div><div class=\"row\">\n        {% endif %}\n       {% if forloop.index == 12 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 16 %}\n          </div><div class=\"row\">\n        {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"readings/index.md"}
</pre>

<h2>/schedule/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Schedule",
 "url"=>"/schedule/index.html",
 "content"=>
  "<!-- Load FullCalendar for schedule page. -->\n<!-- Documentation available at: http://arshaw.com/fullcalendar/docs/google_calendar/ -->\n<link rel=\"stylesheet\" href=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.css\">\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.min.js\"></script>\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/gcal.js\"></script>\n<div class=\"container\">\n  <h1>Schedule</h1>\n  <div id='calendar'></div>\n</div>\n<script>\n  $(document).ready(function () {\n\n    // page is now ready, initialize the calendar. See documentation page above.\n    // In brief: make the calendar public, and paste in the XML feed.\n    $('#calendar').fullCalendar({\n      events: 'http://www.google.com/calendar/feeds/hawaii.edu_h008qigs793hp1llpbindcun50%40group.calendar.google.com/public/basic'\n    })\n  });\n</script>\n",
 "path"=>"schedule/index.html"}
</pre>

<h2>/morea/010.introduction/module.html</h2>

<pre>Hash
{"title"=>"Introduction",
 "published"=>true,
 "morea_id"=>"intro",
 "morea_outcomes"=>["outcome-algorithm", "outcome-311"],
 "morea_readings"=>
  ["reading-course-info",
   "reading-assessment",
   "reading-assignments",
   "reading-format",
   "reading-policies",
   "reading-topic-overview",
   "reading-algorithms",
   "reading-cormen-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/010.introduction/logo.jpg",
 "morea_sort_order"=>10,
 "referencing_modules"=>[],
 "morea_experiences"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/010.introduction/module.html",
 "content"=>
  "Information, assessment, format, assignments, policies, topics, and the role of algorithms in computing.\n\n\n\n",
 "path"=>"morea//010.introduction/module.md"}
</pre>

<h2>/modules/intro/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Introduction",
 "url"=>"/modules/intro/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/intro/index.html"}
</pre>

<h2>/morea/010.introduction/outcome-1.html</h2>

<pre>Hash
{"title"=>"Understand the procedures for ICS 311.",
 "published"=>true,
 "morea_id"=>"outcome-311",
 "morea_type"=>"outcome",
 "morea_sort_order"=>11,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/010.introduction/outcome-1.html",
 "content"=>
  "Understand the policies, course format, assignments, and assessment mechanisms for ICS 311.",
 "path"=>"morea//010.introduction/outcome-1.md"}
</pre>

<h2>/morea/010.introduction/outcome-2.html</h2>

<pre>Hash
{"title"=>"Understand what algorithm analysis is.",
 "published"=>true,
 "morea_id"=>"outcome-algorithm",
 "morea_type"=>"outcome",
 "morea_sort_order"=>10,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/010.introduction/outcome-2.html",
 "content"=>"Understand the formal and informal definitions of \"algorithm.\"",
 "path"=>"morea//010.introduction/outcome-2.md"}
</pre>

<h2>/morea/010.introduction/reading-1.html</h2>

<pre>Hash
{"title"=>"Chapter 1 Notes",
 "published"=>true,
 "morea_id"=>"reading-algorithms",
 "morea_summary"=>
  "Overview of algorithms and why we study them in this course.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-1.html",
 "url"=>"/morea/010.introduction/reading-1.html",
 "content"=>
  "## Algorithms and Programs\n\nBy now you have a working idea of what a \"program\" is because you have written\nmany. Programs are particular instructions that work on specific machines.\n\nIn this course we work with an abstraction of programs: algorithms.\n\n### Algorithms\n\nInformally, an algorithm is a well-defined computational procedure that takes\nsome value(s) as input and produces some value(s) as output.\n\nSomewhat more formally, an algorithm is a finite sequence of instructions\nchosen from a finite, fixed set of instructions, where the sequence of\ninstructions satisfies the following criteria:\n\n  * **Input:** It has zero or more input parameters. \n  * **Output:** It produces at least one output parameter. \n  * **Definiteness:** Each instruction must be clear and unambiguous. \n  * **Finiteness:** For every input, it executes only a finite number of instructions (it eventually halts). \n  * **Effectiveness:** Every instruction must be sufficiently basic so that a machine can execute the instruction.\n\n_Discussion:_ What is the difference between an algorithm and a program?\n\n_Discussion:_ What kinds of algorithms do you think are needed when you use\nyour smartphone (or any mobile phone for that matter?)\n\n* * *\n\n## Algorithm Design & Analysis\n\nHere is how algorithm design is situated within the phases of problem solving\nin software development:\n\nPhase 1: **Formulation of the Problem** (Requirements Specification)\n\nTo understand a real problem, **model it mathematically**, and specify\ninput and output of the problem clearly.\n\n  \nPhase 2: **Design and Analysis of an Algorithm** for the Problem (our focus)\n\n* Step 1: **Specification** of an Algorithm - **what is it?**\n* Step 2: **Verification** of the Algorithm - **is it correct?**\n* Step 3: **Analysis** of the Algorithms - **what is its time and space complexity?**\n\nPhase 3: **Implementation of the Algorithm**\n\nDesign data structures and realize the algorithm as executable code for\na targeted platform (lower level abstraction).\n\nPhase 4: **Performance Evaluation of the Implementation** (Testing)\n\nThe predicted performance of the algorithm can be evaluated/verified by\nempirical testing.\n\nAlgorithms are often specified in **pseudocode**, a mixture of programming\nlanguage control structures, mathematical notation, and natural language\nwritten to be as unambiguous as possible. Examples will be given in the next\nlecture. In this course, we will use mostly the notation used in the book.\n\n* * *\n\n###  Why not just test programs?\n\nWhy not just run experimental studies on programs? We can implement the\nalgorithms of interest, run them on a modern computer on various input sizes,\nand compare the results. Why bother with all this math in the book?\n\nI find the math painful too, but there are three major limitations to\nexperimental studies:\n\n  * To run experiments, you have to implement and run the algorithm. Implementation takes time, and some of these runs may take a long time.\n  * Experiments can only be done on a limited set of test inputs. Are you sure your results generalize to all possible inputs? \n  * It is difficult to compare the efficency of tests run on one hardware and software environment to what will happen on others. Are you sure that your results generalize across platforms? \n\nFormal analysis of algorithms:\n\n  * Can be performed on a high-level description of the algorithm without implementation.\n  * Takes into account all possible inputs. \n  * Allows comparisons of algorithms independently of hardware and software.\n\nSo, there is a good reason ICS 311 is THE central course of the ICS\ncurriculum! Stick with us.\n\n* * *\n\n\n## Computational Complexity\n\n### Input Size\n\nThe computational complexity of an algorithm generally depends on the amount\nof information given as input to the algorithm.\n\nThis amount can be formally defined as the **number of bits** needed to\nrepresent the input information with a reasonable, non-redundant coding\nscheme.\n\nTo simplify things, we often analyze algorithms in terms of larger constant-\nsized **data units** (e.g., signed integer, floating point number, string of\nbounded length, or data record).\n\nThese units are a _constant factor_ larger than a single bit, and are operated\non as a unit, so the result of the analysis is the same.\n\n### Measures of Complexity\n\nThe choice of algorithms and data structures has a critical impact on the\nfollowing, both of which are used as measures of computational complexity:\n\n  * Run **time** to solve a problem of a given input size\n  * Storage **space** for data, including auxiliary structures\n\n###  Example (preview of next lecture)\n\nFor example, suppose you have an input size of n elements, such as n strings\nto be sorted in lexicographic order. Suppose further that you have two\nalgorithms at your disposal (these algorithms will be examined in detail in\nthe next lecture):\n\n**Insertion sort**:\n    \n\n  1. start with an empty list \n  2. take each item to be sorted and insert it in its proper location \n  \n**Merge sort**:\n    \n\n  1. if the list has only one item, return it \n  2. otherwise, split the list in half, sort each half with this procedure, and then merge the results \n\nWe will see that given _n_ items to be sorted (it does not matter what they\nare as long as they are bounded by a constant size and can be compared by an <\noperator),\n\n  * _Insertion sort_ takes time proportional to _c_1_n_2 steps, where _c_1 is a constant depending on the implementation, and requires space proportional to _n_.\n  \n\n  * _Merge sort_ takes _c_2_n_ lg(_n_) steps, where _c_2 is another constant depending on the merge sort implementation, and requires space proportional to 2_n_. \n\n_Exercise:_ This would be a good place for you to pause and do excercise\n1.2-2, page 14:\n\nSuppose we are comparing implementations of insertion sort and merge sort on\nthe same machine, where c1=8 and c2=16. For which values of n does insertion\nsort beat merge sort?\n\nConstants matter for small input sizes, but since constants don't grow we\nignore them when concerned with the time complexity of large inputs: it is the\ngrowth in terms of _n_ that matters.\n\nIn the example above, ignoring the constants and factoring out the common n in\neach term shows that the difference in growth rate is _n_ versus lg(_n_). For\none million items to sort, this would be a time factor of one million for\ninsertion sort, but about 20 for merge sort.\n\n### Models of Computation\n\nRather than bother with determining the constant factors for any given\nimplementation or computer, algorithms for a problem are analyzed by using an\nabstract machine called a **model of computation**.\n\nMany models of computation have been proposed, but they are essentially\nequivalent to each other (Church-Turing Thesis) as long as computation\nexecuted on them are _deterministic_ and _sequential_. Commonly used models\nare _Turing Machines_ and _Random Access Machines_ (see Section 2.2 of the\ntextbook).\n\n### Run Times for Different Complexities\n\nIn general, suppose that you have a computer of speed 107 steps per second.\nThe running time of algorithms of the given complexity (rows) as a function of\n_n_ would be:\n\n    \n    \n      --------------------------------------------------------------------\n      size n    10       20       30       50      100       1000    10000\n      --------------------------------------------------------------------\n      n         0.001ms  0.002ms  0.003ms  0.005ms  0.01ms   0.1ms     1ms\n      n lg n    0.003ms  0.008ms  0.015ms  0.03ms   0.07ms   1ms      13ms\n      n^2       0.01ms   0.04ms   0.09ms   0.25ms   1ms      100ms    10s\n      n^3       0.1ms    0.8ms    2.7ms    12.5ms   100ms    100s     28h\n    \n      ...................................................................\n    \n      2^n       0.1ms    0.1s     100s     3yr      3x10^13c  inf     inf\n      --------------------------------------------------------------------\n    \n\n_Discussion:_ What is the difference between analysis of an algorithm and\nanalysis of an implementation (a program)?\n\n_Discussion:_ What is the relationship between the efficiency of an algorithm\nand the difficulty of the problem to be solved by that algorithm? (We return\nto this in the last two topics of the semester, but see also below.)\n\nConsider the example above: the problem of sorting a list of items. We saw two\nalgorithms for solving the problem, one more efficient than the other. Is it\npossible to make a statement about the time efficiency of _any possible_\nalgorithm for the problem of sorting? (We address this question in Topic #10.)\n\n### Easy vs Hard Problems\n\nTheoretical computer science has made substantial progress on understanding\nthe intrinsic difficulty of _problems_ (across all possible algorithms),\nalthough there are still significant open questions (one in particular).\n\nFirst of all, there are problems that we cannot solve, i.e., problems for\nwhich there does not exist any algorithm. Those problems are called\n**unsolvable** (or **undecidable** or **incomputable**), and include the\n_Halting Problem_ (refer to Section 3.1 pp. 176-177 of the textbook for ICS141\n& 241).\n\nWithin the problems that can be solved, there is a hierarchy of **complexity\nclasses** according to how difficult they are. Difficulty is based on proofs\nof the minimum complexity of _any_ algorithm that solves the problem, and on\nproofs of equivalences between problems (translating one into another). Here\nis a graphic:\n\n![](Complexity-Hierarchy.jpg)\n\n(Although algorithms can be ranked by this hierarchy, the above figure refers\nto problem classes, not algorithms.)\n\nSometimes small differences in a problem specification can make a big\ndifference in complexity.\n\nFor example, suppose you use a graph of vertices representing cities and\nweighted edges between the vertices representing the distance via the best\nroad traveling directly between the cities.\n\n  * The **Single Pair Shortest Paths** problem: what is the shortest path between a single pair of vertices (from one start vertex to one destination vertex) in the weighted graph? \n  * The **Shortest Paths** problem: what is the shortest path from one vertex to all of the other vertices in the weighted graph? \n  * The **All Pairs Shortest Paths** problem: what is the shortest path between every pair of vertices in the weighted graph? \n  * The **Traveling Salesman** problem: what is the shortest path that starts at given vertex in a weighted graph and visits all (or a specified set of) other vertices once before returning to the start vertex?\n\n_Discussion:_ How do these problems differ from each other? Which are easier\nand which are harder? Which are tractable (e.g., can be computed in polynomial\ntime) and which are potentially intractable (e.g, require exponential time)?\n\nComplexity theory will be the topic of our second to last lecture.\n\n* * *\n\n##  Abstract Data Types\n\nAlgorithms and data structures go together. We often study algorithms in the\ncontext of Abstract Data Types (ADTs). But let's start with Data Structures.\n\n### Data Structures\n\nYou are already familiar with Data Structures. They are defined by:\n\n  * **Operations:** Specifications of external appearance of a data structure. \n  * **Storage Structures:** Organizations of data implemented in lower level data structures. (We are almost always building abstractions on layers of abstractions above the actual physical implementation.) \n  * **Algorithms:** Description of how to manipulate information in the storage structures to obtain the results defined for the operations \n\nThe definition of a data structure requires that you specify implementation\ndetails such as storage structures and algorithms. It would be better to hide\nthese details until we are ready to deal with them.\n\nAlso we may want to write a specification in terms of desired behavior\n(Operations) and then compare alternative storage structures and algorithms as\npossible solutions meeting those specifications. Data structures don't work\nbecause they already assume a given solution. Abstract Data Types or ADTs let\nus do this by abstracting the representations and algorithms.\n\n### Definition of Abstract Data Types (ADTs)\n\nAn ADT is a class of instances (i.e., data objects) with a set of the\noperations that can be applied to the data objects.\n\nAn ADT tells us _what to do_ instead of how to do it. This provides the\nspecifications againsts which we can design different algorithms: the _how_\npart.\n\nAn ADT is specified by\n\n  1. **the type(s) of data objects involved **\n  2. **a set of operations that can be applied to those objects**, and \n  3. **a set of properties (called axioms) that all the objects and operations must satisfy**.\n\n### Example: Stack ADT\n\nObjects:\n\n    Stack, and Elements (of arbitrary type) \n  \nOperations (categorized into three types):\n\n\nConstructor\n\n    `new()` creates the empty stack and returns it.\n\nAccessors\n\n     `empty(_s_)` returns whether stack _s_ is empty.  \n`top(_s_)` returns the element of stack _s_ that has been inserted into s\nlast.\n\nMutators (or Modifiers)\n\n     `push(_s_,_e_)` inserts an element _e_ into _s_.  \n`pop(_s_)` deletes the top element from stack _s_.\n\n  \nProperties:\n\n  * `top(push(_s_,_e_)) returns value _e_`\n  * `pop(push(_s_,_e_)) leaves _s_` in the same state \n  * `empty(new()) = true `\n  * `empty(push(_s_,_i_)) = false `\n  * `pop(new()) is an error `\n  * `top(new()) is an error `\n\n### Specification and Implementation\n\nADTs can be specified in different languages:\n\n  * formal languages (axiomatic, algebraic, functional, denotational semantics, etc.)\n  * natural language\n\nImplementation of an ADT requires\n\n  * defining the storage for the data structures\n  * implementing the algorithms for the operations\n\n### Advantages of ADTs\n\nModularity (Encapsulation)\n\nAbstract operations mean a program using an ADT are isolated from (need not know about or be affected by) the implementation of the ADT.\n\n  * Implementation of ADT can be changed without modifying programs using ADT.\n  * Makes a program smaller, simpler, and have less side effects   \n  * Helps to construct correct programs \n  \nHierarchical Specification\n\n     Supports Top-Down Design and Stepwise Refinement\n  \nImplementation\n\n    ADTs map well to Object-Oriented Programming Languages\n\n_Discussion:_ What is the difference between an ADT and a data structure?\n\n* * *\n\n_Some of the material in this page was adopted with permission (and\nsignificant editing) from Kazuo Sugihara's spring 2011 Lecture Notes #02._\n\n",
 "path"=>"morea//010.introduction/reading-1.md"}
</pre>

<h2>/morea/010.introduction/reading-2.html</h2>

<pre>Hash
{"title"=>"Assessment",
 "published"=>true,
 "morea_id"=>"reading-assessment",
 "morea_summary"=>"Grading in ICS 311",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-2.html",
 "url"=>"/morea/010.introduction/reading-2.html",
 "content"=>
  "## Overview\n\nThe approach to assignments, exams, and relative weighting is intended to\nassess multiple aspects of your developing expertise in algorithm design,\nanalysis and implementation; while providing a little flexibility to recognize\nindividual strengths. In summary, the components and their default weights\n(percentages and points, where 10 points = 1% of your grade) include:\n\n  * **Quizzes** (120 points = 12%, 24 quizzes of 5 points each),\n  * **Problem Sets** (305 points = 30.5%, 8 sets of 30 points, 3 sets of 15 points, 1 set of 20 points; of which 105 is _in-class_ work and 200 _homework_)\n  * **Projects** (260 points = 26%, three at 60, 100, and 100 points, respectively)\n  * **Midterm Exams** (240 points = 24%, three at 80 points each)\n  * **Final Exam** (100 points = 10% of grade). \n\nThis sums to 1025 points: you can miss 25 points of work and still earn 1000\npoints.\n\nExtra credit (Variable), may be earned by peer assessment of group\nparticipation in class, by additional work that will be specified in some\nhomework problems and implementation assignments, and by having others use\nyour software in the third assigment. (There will be no separate extra credit\nproblems or projects, as this creates more grading work than we have TA time.)\n\n## Points, Percents and Letter Grades\n\nA \"point\" will be worth 0.1% of your grade. For example, an item worth 100\npoints is 10% of your grade, and a perfect score is 1000 points. At the end of\nthe semester, I add up all your points and divide by 10 to get your percentage\nof points earned, capping it at 100%.\n\nTo determine letter grades, I use a 4-percent spread per grade increment,\ni.e., 100-97=A+, 96-93=A, 92-89=A-, 88-85=B+, 84-81=B, 80-77=B-, 76-73=C+,\n72-69=C, 68-65=C-, 64-61=D+, 60-57=D, 56-53=D-, 52-0=F. I will set it up this\nway in Laulima. If upon inspection of the distribution of grades I feel that\ntoo many students who understand the material are not getting the grades they\ndeserve, I may then make adjustments in favor of students ... but don't rely\non this!\n\n## Components\n\n**Quizzes (12%, 120 points):**\n\nBefore the classes that have lecture material, a brief quiz will be due online. These quizzes will test basic understanding of the chapter on which the day's topic is based, such as whether you can simulate the operation of the data structure or algorithms or get the main point of the analyses of their relative merits. Most quizzes will not involve mathematical analysis or proofs: problems requiring deeper thought will be left for the classwork and homework problems. Quizzes will be given in Laulima, and will be automatically graded. Solutions will be given in class immediately after the quizzes are due, so _quizzes cannot be made up_.\n  \nI am expecting 24 quizzes worth 5 points each. There are 25 topics. We don't\ninclude Topic 1 or the two special topics, which leaves 22 topics with\nquizzes, but Topic 2 and Topic 14 have two quizzes.\n\n  \n**Problems (30.5%, 305 points):**\n\nThese will relatively frequent assignments intended to help assess how well you understand the algorithms and analytic concepts being presented. They will require more thought than the problems given on the quizzes. They will come in two parts: an in-class portion done, turned in, and graded in groups, and an after-class portion turned in individually the following Monday. I attempt to choose in-class problems that help expose conceptual issues in the material and prepare you to work on the take-home portion on your own. Thus, by working in a group in class you help each other understand the problem that you will then solve and turn in individually. See also \"Peer Evaluation of Participation\" under Extra Credit below for how everyone can earn extra credit through participating in the group sessions.\n  \nPoints are allocated as follows. Most topics are allocated 15 points, of which\n5 are allocated to the group work turned in at the end of class, and 10 to the\nindividual work turned in later. In 8 of the weeks, the problem set will\nconsist of two topics, for 30 points each week (10 being groupwork turned in\nover the two days and 20 turned in by the individual). In 3 of the weeks,\nthere is one topic, for 15 points. When we cover Topic 14 on Graphs, there\nwill be two in-class problems and one homework for 20 points. The TA will\ngrade problem sets. We aspire to a one week or less turn-around. If you have\nquestions about solutions after they are due or graded, post them in Laulima\nand I will discuss solutions in class after the exercises are due.\n\n  \n**Projects (Analysis, Implementation, Testing) (26%, 260 points):**\n\nThere will be three projects. These typically involve writing Java implementations of abstract data types and associated algorithms, and testing these on sample data. You will also provide instructions on how to compile and run the program, document your design and implementation (including complexith analysis), and present and discuss test results. The assignments will progressively give you more responsibility. For the first project, you will be told what to implement, and it will be weighted 6% (60 points). For the second assignment you will need to make some implementation choices. The second assignment will be weighted 10% (100 points). The third assignment will require some research and decision making on your part to solve the problem. It will be weighted 10% (100 points). The TA will grade the first two projects, and both the TA and the instructor will grade the third.\n  \n**Midterms (24%, 240 points):**\n\nThere will be three midterm exams taking one class period each. They will include problems similar to those on the quizzes (for the easiest problems), and class and homework problems (for the harder ones), covering both understanding of the algorithms and how to analyze them. They will cover the most recent set of lecture topics, but cumulative \"review\" questions may also be included. Exams are open-book, open-notes on paper, but no electronic devices allowed. Each midterm is 80 points; there are three for 240 points or 24%. The instructor will grade all midterm exams.\n  \n**Final (10%, 100 points):**\n\nThe final exam will take place at the time scheduled by the university and will be longer than a midterm exam. It will cover the final set of lecture topics, but also include review of the entire semester. Since the final is longer and is cumulative as well as covering recent material, it is weighted more (100 points). Grading may be shared between TA and instructor.\n\n## Extra Credit (Variable Points)\n\nYou may earn extra credit several ways. _The extra credit points will be\nrecorded in a separate field in Laulima and allocated where they are needed at\nthe end of the semester. Thus they do not appear in the grade estimate\ncalculated by Laulima during the semester._\n\n**Peer Evaluation of Participation**\n\nEach week in which there is a problem set, each individual in the group may assign points distributed across the other individuals in the group to assess how effectively they collaborated in the group. You should allocate the points according to how well the others worked as team members, including their role in team functioning (e.g., keeping the group focused and organized, or playing another important role), and how much they helped others understand the material (e.g., by explaining what they understood), as well as their contributions to the actual problem solution.\n  \nTo ensure that students in smaller groups are not penalized, we use this\nscheme, which distributes 12 points across each group:  \n\n  * If _three members_ besides yourself were present at some time, you have a total of _3 points_ to allocate across all members (NOT 3 points per member!).\n  * If only _two members_ besides yourself were present, you have a total of _4 points_ to allocate across all members.\n  * If only _one other member_ was present, you have a total of _6 points_ to allocate to that person. \n  * You need not allocate all the points available to you.\n  * _You cannot allocate these points to yourself!_ Points allocated to yourself will not be recorded.\n  * You will allocate these points when you turn in your assignment. To encourage you to do this, _you will be given one extra credit point for each assignment in which you assess your peers_.\n  \nSome example scenarios: if everyone else contributed equally, you might give 1\npoint to each person. If one person in the group was taking the majority of\nthe initiative in a helpful way and the other two were not so engaged, you\nmight assign the helpful person all three points If there was one person who\ndid slightly more, one who helped some, and one slacker, you might allocate 2,\n1, and 0 points. Or if you had to do everything yourself you can allocate 0\npoints to everyone.  \n  \nObviously, a person who is helpful in the groups can earn extra credit this\nway. The maximum possible (very unlikely) is that they get all the points from\neveryone in every group they participate in: 9*12=108, or about a 10% grade\nincrease. If everyone gave their three points equally to all group members,\nthe result would be 3*12=36 points, or about a 3.5% grade increase. So, just\nby participating in class you get a little boost to your grade.\n\n  \n**Classwork, Homework and Project Add-ons**\n\nExtra credit problems will be included in some class sessions, homework problem sets, and projects. For example, an extra problem may be provided in class for those groups who finish early; a more difficult problem may be included in a homework set; or a student may program more challenging graph manipulations for extra credit in the programming projects. Points will be specified when they are given.\n\n**Software Reuse**\n\nIn the third assignment, students will have the option of using other student's implementations of ADTs from prior assignments. If your software is chosen by another student, you will be awarded 5 points for each \"customer\" _in proportion to their grade and use of your software_. For example, if a student uses your Graph ADT from Project 2 in their Project 3 and gets 80%, you will get 4 points. Partial credit is given if only parts of your code are used. The \"customer\" student must acknowledge your code in their project documentation (Readme and Reference manual).\n\n## Flexibility\n\nWe each have our own strengths. If a student performs _significantly_ better\non one area than others, I may elect to put greater weight on the area that\ngives the student a better grade. I am more willing to do this with strong\nexam performance, as exams are not easy and are proctored so I know it's the\nstudent's work. However, excellent programming may be considered as well.\n\n",
 "path"=>"morea//010.introduction/reading-2.md"}
</pre>

<h2>/morea/010.introduction/reading-3.html</h2>

<pre>Hash
{"title"=>"Assignments",
 "published"=>true,
 "morea_id"=>"reading-assignments",
 "morea_summary"=>"Requirements for programming assignments.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-3.html",
 "url"=>"/morea/010.introduction/reading-3.html",
 "content"=>
  "Updated March 8, 2014 to discuss the use of GPL licenses.\n\nThis page discusses general procedures for implementation assignments and\nextra credit projects. See the individual assignment pages for details.\n\n## Implementation Assignments\n\n### Overview\n\nThere will be three implementation assignments. These will involve writing\nJava implementations of abstract data types and associated algorithms, and\ntesting the implementations on sample data. You will also provide instructions\non how to compile and run the program, document your design and implementation\n(including complexith analysis), and present and discuss test results. The\nassignments will progressively give you more responsibility. For the first\nproject, you will be told what to implement, and it will be weighted 6% (60\npoints). For the second assignment you will need to make some implementation\nchoices. The second assignment will be weighted 10% (100 points). The third\nassignment will require some research and decision making on your part to\nsolve the problem. It will be weighted 10% (100 points).\n\n### Software Requirements\n\nThe following requirements have been adopted from Dr. Sugihara:\n\n#### Programming Language\n\nAll software must be written in Java. Other programming languages may not be\nused except where specified by the assignment.\n\n**The software must be compilable on the default version of Java available on uhunix.hawaii.edu** at the time of the submission deadline. The reason for this is to ensure that there is a common reference environment against which we can resolve disputes. We can't grade projects on claims that \"it compiled on my machine\".\n\nUhunix is running [Solaris](http://www.oracle.com/technetwork/server-\nstorage/solaris/overview/index.html). At this writing, the Java version is:\n\n    \n    \n    java version \"1.7.0_51\"\n    Java(TM) SE Runtime Environment (build 1.7.0_51-b13)\n    Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)\n    \n\nProjects submitted with higher versions of Java (if they become available) are\nat your risk. Note that the instructor is presently running research projects\nin Java 1.6 in Snow Leopard on which Java 1.7 is not available. Some features\nof Java 1.7 do not compile in 1.6, so if you could stick to 1.6 features he\nwon't have to go to UH Unix to run your code.\n\n#### Program Type and Interface\n\nThe software should run as a Java application.\n\n  * Command line (console) applications are acceptable.\n  * GUI (graphical user interface versions) are also permissible.\n\n#### Source Code Requirements\n\nThe student shall **submit .java source files** (not class or jar files),\norganized in folders as required for your package structure, along with\ninstructions for compiling the program and other documentation discussed in\nthe next section.\n\n  * The Teaching Assistant will verify that the programs compile on the current version of Java on uhunix, as specified above.\n  * Submissions will only be evaluated for credit if they successfully compile.\n  * This procedure is intended to (1) enable us to verify that the software is based on the student's own source code and (2) provide an objective basis for evaluating whether code works..\n\nSource files should include the BSD License Header based on [ this\ntemplate,](../Resources/bsd_license_header.txt) with \"<year>\", \"<copyright\nholder>\" and \"<organization>\" replaced appropriately.\n\nOther open source licenses (e.g., [Apache](http://www.apache.org/licenses/) or\n[GNU](http://www.gnu.org/licenses/)) may be used with prior permission from\nthe instructor if the student has a specific reason for doing so and\nunderstands the consequences. See the discussion under Other Licenses two\nsections below.\n\nSource should include appropriate in-line comments documenting the software\ndesign. Comments should not document the obvious (e.g., \"this line adds 1 to\nthe index variable\"), but rather document functional intent and constraints\nsuch as loop invariants, explain something that would otherwise be obscure,\nmark places that need improvement, etc. Descriptive use of variable and method\nnames also constitutes internal documentation. See next section for external\ndocumentation requirements.\n\n#### Including Open Source Software\n\nEach assignment will specify where you are allowed to reuse source code of\nopen source software developed by other authors, and where you must write your\nown code for the assignment. Where allowed by the assignment, reuse of open\nsource code is allowed if the following conditions are also met:\n\n  * The software license of the reused open source code is compatible with the [ BSD license](http://www.opensource.org/licenses/bsd-license.php) (see discussion under Other Licenses below).\n  * The license header of reused source code is also inserted into the source files containing the reused source code.\n  * The `Readme.txt ` of your product clearly gives credit to the authors of the reused source code (i.e., including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\nWhen source code of a module is reused, add the name(s) of its original\nauthor(s) to an `@author` tag at the beginning of the reused module. If you\nmodified the source code for more than bug fixes, add your name as an author\nof a derivative from the original source code:\n\n    /**\n     *\n     * @author     Original Author     James Brown\n     * @author     Derivative Author   Robert Smith\n     *\n     */\n\n#### Other Licenses\n\nI am often asked whether one can use code under another license. You may use\nother open source licenses as long as (1) they give you the right to use the\ncode under conditions acceptable to you and (2) you document this as needed.\nAn example is the GPL license. You may use a [GPL compatible\nlicense](https://www.gnu.org/licenses/license-list.html), but you use this\nlicense, all the code you write _must_ also be under GPL. See\n<http://en.wikipedia.org/wiki/Free_software_license> for an overview, and read\nsome blogs about this hotly debated issue.\n\n### Documentation Requirements\n\nSoftware will be submitted with appropriate documentation, including the\nfollowing. (Think of your audience for this documentation as any potential\nusers, including your classmates as well as the TA and instructor.)\n\n**Readme.txt (plaintext file)**\n\nCritical information that a user should know about your product first, including:\n\n  * step-wise instructions for the user to reconstruct an application from your source code (including the version of JDK used),\n  * credits (acknowledgments to authors of open source reused at least in part for this product) including information such as the name of an author, the name of a reused product and a list of source file names of the reused product,\n  * revision history (a log of changes on the product), \n  * bug report (description of known bugs), and \n  * a listing of other documentation available (below).\n\n**Operation Manual (plaintext or PDF)**\n\nConcise, yet sufficient step-wise explanation about how to start and interact with the program, written for an end user who is concerned with using the program in an application domain (not with the code).\n\n**Reference Manuals (plaintext or PDF, and Javadoc HTML)**\n\nRequirements and design specifications; organization of modules; algorithms and data structures used; functionality of each class or method; etc. A reference manual is written for experienced users and/or programmers who perform various maintenance activities for correction, enhancement, adaptation, etc. Javadoc pages may also be included, and should be included if you intend to have others use your code.\n\n**Testing Document (plaintext or PDF)**\n\nTest plan describing objective(s) of testing, method(s) used for testing, assumption(s) of testing, and hardware/software environment in testing; test case specification describing classification of test cases; test data and I/O of test runs; and whatever else is useful to convince other people about the correctness and good features of your program. **For ICS 311 assignments the testing document will include your conclusions related to the purpose of the assignment.**\n\n### Submission Requirements\n\n  1. Place the files and folders required (as discussed above under Software and Documentation Requirements) in a folder titled using the scheme Last-First-A#, for example, Suthers-Dan-A1 for assignment 1, Suthers-Dan-A2 for assignment 2, etc. Extra credit projects should be submitted with extentions -E1, etc.\n  2. Zip (.zip) or gzip (.gz) this folder using commands by those names on uhunix, or appropriate equivalents on your platform.\n  3. It is suggested that you test unzipping, compiling and running the software per the instructions you gave before submitting the assignment.\n  4. Upload the zip file to the Laulima area for the given assignment.\n  5. You should receive email confirmation of your submission at the address registered in Laulima.\n  6. Unlimited resubmissions are allowed up to the assignment deadline. Extra credit for early submission, if any, will be based on the date of the last submission, not the first!\n\n### Evaluation Criteria for Implementation Assignments\n\n### Warning: these will be revised for spring 2014\n\nThese are our default grading criteria. Some adjustments may be made when we\nsee where the greatest effort is required.\n\nProgram: 60%\n\n    \n\n  * 50% if all operations and all ADTs are implemented.\n  * 5% for following instructions for input and output (although many more points will be deducted if the interface is so bad we can't verify that the operations and ADTs are implemented). \n  * 5% for adequate error handling. \nAnalysis and Documentation 40%\n\n    \n\n  * 30% for adequate analysis of the results\n  * 10% for Readme, Operation, Reference manuals\n\n## Use of Software by Other Students\n\nIf others elect to use your software in a subsequent assignment (e.g., using\nyour graph ADT implementation in the third assignment), we will give extra\ncredit. See discussion in [Assessement](Assessment.html) page. Use should be\ncredited in the Readme and Reference Manual.\n\n",
 "path"=>"morea//010.introduction/reading-3.md"}
</pre>

<h2>/morea/010.introduction/reading-4.html</h2>

<pre>Hash
{"title"=>"Course information",
 "published"=>true,
 "morea_id"=>"reading-course-info",
 "morea_summary"=>
  "Student learning outcomes, textbook, instructor information.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-4.html",
 "url"=>"/morea/010.introduction/reading-4.html",
 "content"=>
  "## Catalog Description\n\nICS 311 Algorithms (3 credits) Design and correctness of algorithms, including\ndivide-and-conquer, greedy and dynamic programming methods. Complexity\nanalyses using recurrence relations, probabilistic methods, and NP-\ncompleteness. Applications to order statistics, disjoint sets, B-trees and\nbalanced trees, graphs, network flows, and string matching. Pre: 211 and 241,\nor consent.\n\n## SLOs (Student Learning Outcomes)\n\n  * Students are aware of fundamental algorithms of computer science, and their associated data structures and problem solving techniques. \n  * Students can compose a problem formulation of a real-world problem mathematically.\n  * Students can decide whether given pseudocode is correct for a given problem formulation; construct a counterexample if the given pseudocode is incorrect; and outline a proof for its correctness otherwise.\n  * Students can design a correct algorithm for a given problem and describe the algorithm as pseudocode in a given pseudocode syntax.\n  * Students can analyze the worst-case and best-case space and time complexities of a given algorithm.\n  * Students can create a software program for accurately implementing an algorithm specified in pseudocode. Students can implement software objects meeting Abstract Data Type specifications.\n  * Students can produce a software product including documentation for given requirements & design specifications.\n\n_Comment:_ On the fall 2013 final exam one student wrote about a problem:\n\n> \"This question is too hard! We shouldn't have to know implementations we have not used before!\"\n\nI wrote back to thank the student for concisely expressing (the negation of)\nexactly what this course _ is_ intended to teach! You may not \"know\" an\nimplementation you have not encountered before, but this course should prepare\nyou with the tools to analyze and make informed decisions about new\nimplementations.\n\nDo not approach this course solely as a memorization task, where you can only\ndo algorithms you are trained to do, like a circus animal. We do want you to\nlearn a \"catalog\" of algorithms, but you should be understanding their\nanalyses as examples that enable you to analyze unexpected algorithms in the\nfuture. This is essential for being successful in a fast changing field where\n_you_ are expected to figure out whether a new idea will work, as _ you _ are\nthe computer scientist hired to do this.\n\n## Textbook\n\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein,\n[Introduction to Algorithms, Third\nEdition,](http://mitpress.mit.edu/algorithms/) The MIT Press, 2009.\n\nStudents are advised to purchase the textbook, as this book will serve as a\nlifelong reference (it is the second most cited publication in computer\nscience). Also, the exams will be open book but with no electronics permitted,\nso the PDF version of the book won't help you there.\n\nStudents are also advised to keep their ICS 241 (Discrete Mathematics for\nComputer Science) textbooks for reference.\n\n## Instructor\n\n[Daniel D. Suthers](http://www2.hawaii.edu/~suthers/)  \nProfessor of ICS\n\n  * Office: POST 309B\n  * Office Telephone: (808) 956-3890\n  * Email: [suthers@hawaii.edu](mailto:suthers@hawaii.edu) (Put \"ICS 311\" in the subject line. Use Laulima for questions that may also apply to other students.)\n  * Office Hours: Mondays 3:00-4:00, or by appointment. (It's best to let me know in advance if you plan to come to office hour. Please don't try to talk to me while I am setting up for class. Brief consultations at the end of class are OK unless I indicate that I need to leave.\n\n## Teaching Assistant\n\nRobert Ward  \nM.S./Ph.D. Student in ICS\n\n  * Office: POST 303 cubicle\n  * Email: [rward@hawaii.edu](mailto:rward@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Office Hours: TuTh 1-2:30 or by appointment.\n\n## Assistant Teaching Assistant\n\nAnthony Sarria  \nICS Undergrad and ICS 311 Survivor\n\n  * Lab: POST 318A\n  * Email: [aksarria@hawaii.edu](mailto:aksarria@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Lab Hours: Mondays 3:15-6:15.\n\n## Communications\n\n**Questions about Course Content**:   In general, questions about course content such as concepts, clarifications of assignments, etc. should be posted to the Laulima discussion forum of the week. This is because (1) other students can see our responses there, and thus also benefit; and (2) other students may notice the question and answer before the instructor or TA notices it (though we will check daily). If you email us a question, we will post the reply in Laulima unless personal information is involved.\n\n**Personal Topics**: For topics that are not of interest to other students or are personal, you may email us, or stop by office hours. (Of course you may also use office hours for course content questions.) If using email, put \"ICS 311\" in the subject line.\n\n**Communication with other students (e.g., group members)**: You can send email to other students in the course using the Laulima \"Mailtool\". You don't need to know their real email address to do this.\n\n##  Online Media\n\nWe use the **course website** [www2.hawaii.edu/~suthers/courses/ics311s14/](ww\nw2.hawaii.edu/~suthers/courses/ics311s14/) for posting schedules and notes.\n\nWe use **[Laulima**](https://laulima.hawaii.edu/portal) for all other online\nrequired course functions such as podcasts, quizzes, discussions and\nsubmitting assignments. Please see [this document on everything Laulima users\nshould\nknow](http://www.hawaii.edu/talent/laulima_tab/tabs/laulima_essentials.html).\n\nWe will use **Google Docs** for in-class problem solving, as it supports\nsimultaneous editing.\n\nScreencasts (videos) of lectures are available on **YouTube** and **iTunesU**\nas well as Laulima (your choice). They are linked from the individual Notes\npages (Topic-XX.html).\n\n",
 "path"=>"morea//010.introduction/reading-4.md"}
</pre>

<h2>/morea/010.introduction/reading-5.html</h2>

<pre>Hash
{"title"=>"Format",
 "published"=>true,
 "morea_id"=>"reading-format",
 "morea_summary"=>"Exam cycles, weekly routine, studying, and group work.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-5.html",
 "url"=>"/morea/010.introduction/reading-5.html",
 "content"=>
  "By \"routines\" I mean our periodic patterns of activity: what happens in a\ntypical class, a typical week, and a typical exam and assignment cycle. This\npage tells you what I will do and what you will be expected to do on a\nrecurring basis.\n\n## Exam Cycles\n\nThere are three midterm exams covering the core material. The first two are\ngiven one full week after the last problem set on the exam topics was due, so\nyou have time to get feedback on homework problems. Due to the university\ncutoff for midterm exams, the third is held the week after the relevant topics\nare covered. The first exam is at least half review of 211 and 241: results\nare returned before the withdrawal date so you can assess whether you are\nready for ICS 311.\n\n## Weekly Routine\n\nOn most weeks we cover one book chapter/topic in each of the two classes (two\nchapters per week). The exceptions are the first two weeks when we are getting\nour bearings and covering material that must be understood to comprehend the\nrest of the semester; the weeks we have midterms; and the chapter that\nintroduces graphs (we take a full week for it).\n\nThe pace is intense: set aside time almost every day for ICS 311. The basic\npattern is as follows:\n\n**Saturdays:**\n\n  * Instructor posts any updates to screencasts and notes for the material of the following week by Saturday night, and possibly earlier.\n\n**Sundays:**\n\n  * Projects will be due midnight* on a Sunday night.\n\n**Mondays:**\n\n  * Problem sets from the previous week are due midnight* the Monday after they are given.\n  * Read or review material to understand Tuesday's topic (your choice of my podcasts, my web notes, the CLRS textbook chapter, other readings offered, and/or the MIT lecture videos).\n  * Post any questions you have about the material in the appropriate Laulima discussion forum (instructor will review these after midnight when preparing for Tuesday's class). \n\n**Tuesdays:**\n\n  * The online quiz for Tuesday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n\n**Wednesdays:**\n\n  * Read or review material to understand Thursday's topic.\n  * Post any questions you have about the material in the appropriate Laulima discussion forum.\n\n**Thursdays:**\n\n  * The online quiz for Thursday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n  * Problem sets for the weekend are usually posted Thursday evening.\n\n**Fridays through the weekend:**\n\n  * Use to do problem sets and/or projects in preparation for Sunday deadlines, and to watch screencasts and read book chapters. \n\n  * \"Midnight\": The deadline in Laulima will be set to 23:55 (11:55 pm) on the due date, to avoid any ambiguity of which day \"00:00\" refers to. Upload your solutions before midnight. There will be a 5 minute grace period just in case of network delays. The three projects can be submitted late for 1% off per hour up to 50% off, but the homework assignments cannot be late.\n\nThe Assistant Teaching Assistant will have lab hours on Mondays. This will be\nyour last chance for clarifications before assignments are due. Instructor\noffice hour will be Wednesdays (with some availability Monday by appointment).\nPlan in advance for programming projects due Sundays.\n\n## Class Routine\n\nThe focus of our 75 minute class will be student problem solving in groups,\nwith opportunites to get help. For at least the first part of the semester,\nthe groups will be formed anew _randomly_ each week. Each day you will solve a\nseries of conceptual problems and turn them in as a group for a group grade.\nThese problems prepare you to take on more substantial problems that you turn\nin individually on Sunday. At that time you will also allocate points to group\nmembers. (See [ Assessment](Assessment.html) for explanations of grading.)\n\nHere are typical schedules for 75 minute classes: Adjustments to the class\nroutine will likely be made to meet current needs.\n\n**Tuesdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Icebreaker for new groups (groups rotate weekly)\n  * 5: Solution to previous weekend's homework (if applicable) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n\n**Thursdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Discussion of Tuesday's problem (if needed) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n  * 5: Discussion of homework problems \n\nPlan to **bring your laptop or tablet to classes** held in Webster 101 (but\nnot to exams in BusAd A101). You'll need a VGA connector if you want to be\nable to project your laptop to your working group. Apple iPads can connect via\nApple TV. I'm not aware of another method. Of course, groups can function with\nonly one or two members having a projectable laptop, but it's better for you\nif you can be an active participant. At least one person in each group should\nhave the textbook handy in class as well.\n\n##  Other Comments\n\n### Inverted Classroom\n\nThis class is \"inverted\" in the sense that lectures are recorded and made\navailable outside of class, and classroom time is used for what can only be\ndone in person: collaboration and helping each other.\n\nLectures have their advantages, but they have problems too. For most students\nlistening to lectures is too passive an activity. The temptation to daydream\nor check Facebook may be too great, and it takes effort to keep your mind on\nthe material. Actual problem solving is more effective for learning. Also,\nlectures are a form of \"distance learning\": though we are all in the same room\nwe might as well be at a distance, as there is little interaction. When I ask\nworking professionals what skills they want our students to have, being able\nto collaborate in teams is ALWAYS mentioned on the first breath.\n\nFor these reasons, the inverted classroom puts lectures online so that\nstudents who benefit from them can have them, and even review them repeatedly;\nand uses the classroom time in ways that engage students more actively and\ntakes advantage of the unique opportunity provided by being in the same room.\n\n### Studying Before Class and Quizzes\n\nThe online quizzes before class are of course intended to motivate students to\nreview the material before class. There is another motivation: If you don't\nprepare in advance, you risk looking foolish in front of your peers, who may\nbe annoyed at you for being unprepared to help, and you'll miss a learning\nopportunity. You don't want to get a reputation for being the person who is\nnot prepared. It's a small world: someday your peers may be able to influence\nhiring decisions.\n\n### Groups\n\nMuch has been published by researchers and practitioners on how to organize\ngroups for collaborative learning. My approach is based on this research and\nmy experience with this course.\n\nDuring the first month, students will be assigned randomly to groups, rotating\nto new groups each week to help you get to know each other. (A survey of\nstudents fall 2013 indicated that many liked this format as it was a rare\nopportunity to get to know other ICS students.) Also it helps prevent reliance\non dysfunctional relationships (e.g., freeloading and \"the sucker effect\"): a\nstudent can't plan on being with someone who will do the work for him or her,\nand after a while people figure out who to avoid when forming groups.\n\nAfter the Midterm 1 exam, you may form voluntary groups. Students who form\nvoluntary groups will stay in them until the next midterm, at which time they\nmay elect to continue or disband. Other students will continue to be assigned\nrandomly.\n\nRegardless of whether you are in random or voluntary groups, this is an\nimportant opportunity to develop group collaboration skills and also to\ndevelop a good reputation with your peers. It may affect whether you are\nselected to be part of a good group in ICS 311, and I have seen some students\nafter graduation get hired while others fail to get a job because of the\nreputations they had with their peers.\n\n",
 "path"=>"morea//010.introduction/reading-5.md"}
</pre>

<h2>/morea/010.introduction/reading-6.html</h2>

<pre>Hash
{"title"=>"Policies",
 "published"=>true,
 "morea_id"=>"reading-policies",
 "morea_summary"=>
  "Cheating, reuse of open source, abuse of facilities, makeups, and deadlines",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-6.html",
 "url"=>"/morea/010.introduction/reading-6.html",
 "content"=>
  "_These policies were adopted (with changes) from Kazuo Sugihara._\n\nStudents in this course are subject to all the policies of ICS Department and\nthe University of Hawaii at Manoa, including but not limited to the following:\n\n## Cheating\n\nAny form of cheating (such as plagiarism and unauthorized collaboration on an\nassignment) results in the grade of F and will be reported to ICS Department\nfor further actions. In general, I encourage students to help each other but\nyou should each produce your own final version of the homework assignments to\nturn in, except where use of another student's code is explicitly allowed.\nCheck with me if you are not sure.\n\nIn a technical report, a student may quote a few sentences written by another\nauthor with an explicit citation to its original source. Writing sentences or\nideas of the other author without giving credit to the original author is\nplagiarism. Copying content of online documents is also regarded as\nplagiarism, irrespective of how much the original content is modified, if the\nstudent fails to write an accurate citation to the original source. Avoid so-\ncalled \"patchwork plagiarism.\" Informative suggestions are given in [Avoiding\nPlagiarism](http://emedia.leeward.hawaii.edu/resources/plagiarism/05avoid.htm)\nby Professors Marilyn Bauer and Jacie Moriyama of Leeward Community College.\n\n## Reuse of Open Source\n\nIn a software product for an implementation assignment, a student is allowed\nto reuse open source if ALL of the following are met.\n\n  1. The software does not cover functionality that the assignment specifies that the student will write.\n  2. The software license of the open source reused gives permission for this reuse.\n  3. The license header of reused source code is copied into a source file containing the reused source code.\n  4. Documentation (e.g., Readme, User manual) of the software product clearly gives credits to authors of the reused source code (i.e., acknowledgments to the authors including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\n## Abuse of Facilities\n\nAny form of abuse of computing resources of ICS Department or the University\nof Hawaii will not be tolerated. It results in termination of your account on\ntheir servers any time the abuse is detected, will lead to the grade F, and\nwill be reported to ICS Department for further actions.\n\nPlease keep in your mind that access to and usage of our computing resources\nare your privilege, but not your right.\n\nInappropriate content that may be of an offensive nature to other students\nshould not be displayed on laptops or group workstations during class.\n\n## Makeup\n\nIf a student missed an exam due to illness or injury, a makeup exam will be\ngiven to a student only when the student has a doctor's note dated that day\nand contacts the instructor by email within 3 days after the exam date or the\ndate that the doctor's note suggests the student to recover enough to contact\nthe instructor. However, an ordinary doctor's appointment (scheduled by a\nstudent in advance) is not an acceptable reason for makeup unless it is\ninevitable to conflict with an exam beyond control of the student. The makeup\nexam must be completed before exam solutions are reviewed in class.\n\nFor an exceptional case other than illness or injury, a student must submit an\nofficial document to the instructor providing sufficiently convincing evidence\nof the fact that the cause for missing an exam was beyond control of the\nstudent (e.g., in case of a traffic accident on a way to a class, a police\nrecord of the accident should be furnished).\n\n## Deadlines\n\n**If Laulima or uhunix.hawaii.edu becomes unexpectedly unavailable** in the last several hours before a quiz, homework or project deadline, **_email your solutions to the instructor or TA_** once you are sure that you will not be able to upload it but before the deadline. Please note that some quiz questions use random ordering for multiple choice: specify the content of your response, not just the letter.\n\nThe deadline of every assignment is firm. No late submission will be accepted\n(unless explicitly stated otherwise, possibly with penalties). No extension\nwill be given except the following cases.\n\n  * If Laulima is unexpectedly down for an extended period before the deadline preventing most students from uploading, the deadline will be extended for the downtime. Please notify the instructor of any unexpected downtime when you notice it so that a decision can be made and announced to all students in a timely manner. On the other hand, no extension will be given for scheduled downtime.\n  * A student has a doctor's note dated on the day of a deadline of the assignment and contacts the instructor by email within 3 days after the deadline or the date that the doctor's note suggests that the student has recovered enough to email.\n  * For any other exceptional circumstance other than illness or injury, a student is required to submit to the instructor an official document that is a sufficiently convincing evidence of the fact that the cause for missing a deadline was beyond control of the student.\n\n## Data Backup\n\nEach student is responsible for taking a periodical backup of her/his work and\ncomputer resources needed to meet course requirements. Especially, files for\nassignments should frequently be saved on an external storage device. An\nunexpected failure of the computer is not an acceptable excuse for a late\nsubmission. CS major students should exercise automatic, periodical backup\nprocedures. If you do not practice periodical cloning and backup, start to do\nit from now.\n\nA simple solution is to put all your coursework in a cloud environment such as\nDropbox, but you still need to ensure that you have backup of a working\ncomputer to access this data.\n\nMy own procedure (in OS X) uses two external drives. External drives make it\neasy to move to another computer if yours becomes inoperable. One external\ndrive is about the same size as my internal drive, and I use SuperDuper to\nmake a bootable clone about once a week. This does not save intermediate or\nprevious versions. The other drive is twice the size of the internal drive,\nand I use Time Machine to automatically record previous versions of documents.\nTime Machine does not produce a bootable disk. If my computer failed, I could\nboot off the SuperDuper clone disk and use the Time Machine disk to restore to\nmy most recent versions.\n\nThis is a good procedure for one computer, but the frequency of Time Machine\nruns (once per hour) may be insufficient in time-critical situations. You\nmight manually initiate back up on a more frequent basis. I use four computers\nand a server, so I also use Interarchy to synchronize (via disk mirroring) all\nof my document folders with my server and across the four computers. I run\nsynchronization to my server after each major piece of work.\n\n",
 "path"=>"morea//010.introduction/reading-6.md"}
</pre>

<h2>/morea/010.introduction/reading-7.html</h2>

<pre>Hash
{"title"=>"Topics",
 "published"=>true,
 "morea_id"=>"reading-topic-overview",
 "morea_summary"=>
  "Conceptual overview of how topics are grouped and sequenced.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-7.html",
 "url"=>"/morea/010.introduction/reading-7.html",
 "content"=>
  "## _Part I: Introduction to Analysis_\n\nThis section introduces algorithms as abstractions of programs, and motivates\nwhy we need to do analysis of algorithms rather than just run empirical tests\nof programs. It then introduces some mathematical and conceptual tools for\ndoing analysis. Two useful sorting algorithms are used for illustration; we'll\nreturn to sorting later.\n\n  * **#1** \\- Chapter 1: Introduction to Course Format and to Algorithms \n  * **#2** \\- Chapter 2: Examples of Analysis with Insertion and Merge Sort\n  * **#3** \\- Chapter 3: Growth of Functions and Asymptotic Concepts \n\n## _Part II: Data Structures for Dictionaries & Sets_\n\nIn this section we introduce problem solving and analysis methods (chapters\n3-5), some of which have been covered in ICS 241, and also review review basic\ndata structures and algorithms (chapters 10-12) that were introduced in ICS\n211. The chapters from the text are interleaved and paired up in a manner that\nuses the basic dictionary and set data structures and algorithms to illustrate\nthe problem solving and analysis methods. Hopefully this is mostly review of\nthe two prerequisite courses with some added depth.\n\n  * **#4** \\- Chapter 10: Stacks, Queues, Lists and Trees (Review)\n  * **#5** \\- Chapter 5 and Goodrich & Tamassia section: Probabilistic Analysis and Randomized Algorithms; Skip list example\n  * **#6** \\- Chapter 11: Hash Tables\n  * **#7** \\- Chapter 4: Divide & Conquer and Associated Analysis Methods\n  * **#8** \\- Chapter 12: Binary Search Trees\n\n## _Part III: Sorting and Balanced Trees_\n\nWe continue into more advanced applications of trees (chapters 6 and 13), also\nproviding the basis for another efficient sorting algorithm. We compare\nadditional sorting algorithms to those from Chapter 2. Sorting is one of the\nmost fundamental and common applications of computers, so efficiency is very\nimportant. We consider the broader question of how fast _any_ sort algorithm\ncan be. This is an example of a powerful method of computer science: reasoning\nabout sets of possible algorithms rather than specific algorithms.\n\n  * **#9** \\- Chapter 6: Heapsort and Priority Queues\n  * **#10** \\- Chapters 7 & 8.1: Quicksort, Theoretical Limits, and O(n) sorts \n  * **#11** \\- Chapter 13 & Sedgewick Chapter 15: Balanced Trees \n\n## _Part IV: Problem Solving and Analysis Methods_\n\nThis part introduces two further problem solving methods, _dynamic\nprogramming_ and _greedy algorithms_, with example applications. We then cover\nanother important analytic method, _amortization _, with examples in the\nanalysis of the union-find representation of sets. (Chronologically, #14\ngraphs will be introduced in this sequence to help you get started on a\nprogramming assignment, but conceptually they belong in the next section.)\n\n  * **#12** \\- Chapter 15: Dynamic Programming\n  * **#13** \\- Chapter 16: Greedy Algorithms & Huffman Codes\n  * **#15** \\- Chapter 17 - Amortization\n  * **#16** \\- Chapter 21 - Sets and Union-Find\n\n## _Part V: Graphs_\n\nGraphs are a very flexible data structure for which many applications exist.\nEquipped with various problem solving and analytic tools, we examine the most\nimportant algorithms on graphs -- including some of the most classic work in\ncomputer science -- and their applications.\n\n  * **#14** \\- Chapter 22: Graph Representations and Basic Algorithms _(covered earlier)_\n  * **#17** \\- Chapter 23: Minimum Spanning Trees \n  * **#18** \\- Chapter 24: Single-Source Shortest Paths \n  * **#19** \\- Chapter 25: All Pairs Shortest Paths \n  * **#20** \\- Chapter 26: Maximum Flow\n\n## _Part VI: Selected Topics_\n\nThere are a number of other important or common application areas that have\ntheir own specialized algorithms of interest: multithreading (parallel\nalgorithms), matrix operations, linear programming, operations on polynomials,\nnumber-theory, string matching, and computational geometry. These are covered\nin the text, but we can't fit them all in. Linear Programming will be examined\nbecause it fills out an area not covered well above (numeric algorithms), and\nalso has interesting connections to previous material (e.g., you can solve\nflow problems with a linear program). Two other topics will be covered more\nlightly in class (no homework).\n\n  * **#21** \\- Sedgewick Chapters 5 and 38; Chapter 29: Linear Programming \n  * **#22** \\- Chapter 27: Multithreading \n  * **#23** \\- Chapter 32: String Matching \n\n## _Part VII: Complexity Theory and NP-Completeness_\n\nFinally, abstracting further from algorithms to problems, we deal with the\nimportant question of whether an efficient algorithm is known (or even\npossible) for a given problem, and what to do if none are known. We encounter\nthe most important open problem in theoretical computer science (indeed in all\nof mathematics), which is of _practical_ interest because awareness of\n\"intractable\" problems and approximation algorithms could save you\nconsiderable trouble if you encounter one of these very common problems on the\njob! The seminal book on this topic is the most cited reference in computer\nscience.\n\n  * **#24** \\- Chapter 34 - Complexity Theory & NP-Completeness\n  * **#25** \\- Chapter 35 - Approximation Algorithms \n\n",
 "path"=>"morea//010.introduction/reading-7.md"}
</pre>

<h2>/morea/010.introduction/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 1 - Role of algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-1",
 "morea_summary"=>"The role of algorithms in computing",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/010.introduction/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//010.introduction/reading-cormen.md"}
</pre>

<h2>/morea/020.examples/experience-1.html</h2>

<pre>Hash
{"title"=>"Linear Search",
 "published"=>true,
 "morea_id"=>"experience-1",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze the linear search algorithm",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-1.html",
 "url"=>"/morea/020.examples/experience-1.html",
 "content"=>
  "## In Class: Linear Search\n\n#### 5 points\n\n1. Consider the **searching problem**:\n\nInput:\n\n     A sequence of _n_ numbers _A_ = ⟨ _a_1, _a_2, .... _a__n_ ⟩ (in _no particular order_), and a value _v_.\n\nOutput:\n\n     An index _i_ such that _v_ = _A_[_i_] or the special value NIL if _v_ does not appear in _A_.\n\n**a. (1 pt):** Write pseudocode for `LinearSearch(A,v)`, an algorithm that scans through sequence _A_ looking for _v_, and provides the desired output (_i_ or NIL). Use Java style 0-based indexing, A[_i_] to access elements, and A.length to get _n_. \n\n**b. (2 pts):** Let **_p_** be the number of array positions checked. (_p_ will be replaced with a function of _n_. For example, when the element is not present, _p_ = _n_.) Let _c_1 be the cost of executing line 1, _c_2 the cost of executing line 2, etc. As was done in the lecture notes: \n\n  * Write the expression for the cost to execute each line in terms of _p_ and the _c__i_. \n  * Sum these to get the total cost _T_(_n_). \n  * Then collect the constants and rename them \"a\" and \"b\".\n\n**c. (1 pt):** What is _p_ for the _worst case_: precisely how many elements of the input sequence need to be checked? \n\n  * Rewrite your last expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ(____).) \n\n**d. (1pt):** What is _p_ for the _average case_: precisely how many elements of the input sequence need to be checked on average, assuming that the element being searched for is present and equally likely to be any element in the array? \n\n  * Rewrite your expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ (____).) \n\nJustify all your answers!\n\n",
 "path"=>"morea//020.examples/experience-1.md"}
</pre>

<h2>/morea/020.examples/experience-2.html</h2>

<pre>Hash
{"title"=>"Binary Search",
 "published"=>true,
 "morea_id"=>"experience-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze binary search",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-2.html",
 "url"=>"/morea/020.examples/experience-2.html",
 "content"=>
  "## In Class: Mixing and Matching with `BinarySearch`\n\n####  5 points\n\n**1\\. (1 pt):** The while loop of `InsertionSort` uses a Θ(_n_) linear search to scan backwards through the sorted subarray A[1 .. _j_-1]. Since this subarray is sorted, can we use `BinarySearch` for this search to improve the overall worst case running time of InsertionSort to Θ(_n_ lg _n_)? If yes, argue why the hybrid algorithm would be Θ(_n_ lg _n_). If no, explain why it won't work. \n\n**2\\. (4 pts):** We know from Tuesday's class work that `LinearSearch` is Θ(_n_), so one might think it is always better to use the Θ(lg _n_) `BinarySearch`. However, in order to apply `BinarySearch` we have to sort the data, which (we will soon learn) requires Θ(_n_ lg _n_) time for the best known algorithms (e.g., `MergeSort`). This question explores when it is worth paying this extra cost of sorting the data in order to get fast search.\n\nSuppose you will be **searching a list of _n_ items _m_ times**. When is _m_\nbig enough relative to _n_ to make it worth sorting and using binary search\nrather than just using linear search?\n\nWe have two alternatives. Simply using `LinearSearch` _m_ times on _n_ items\nhas an expected (average) cost of Θ(_mn_). The second alternative is (a)\nbelow, and (b)-(d) explore the tradeoffs.\n\n**a.** What is the expected cost to apply `MergeSort` once to sort _n_ items, and then apply `BinarySearch` _m_ times to _n_ items? \n\n**b.** Suppose _m_ = 1: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**c.** Suppose _n_ = _m_: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**d.** What is the cutoff point in terms of _m_ expressed as a function of _n_ between when it is faster to just apply the linear search and when it is faster to apply MergeSort? (Set up an equation and solve for m.) \n\n(Comment: a more accurate analysis would take into account the different\nconstants associated with each algorithm: `MergeSort` has a higher constant.\nBut it can be hard to know the numerical value of the constant to be used in\nthe analysis. So, if you are really concerned about performance, once the\nanalysis gives you the ballpark tradeoff, empirical studies can be used to\ndecide the best cutoff for a given implementation.)\n\n\n",
 "path"=>"morea//020.examples/experience-2.md"}
</pre>

<h2>/morea/020.examples/experience-3.html</h2>

<pre>Hash
{"title"=>"More on linear and binary search",
 "published"=>true,
 "morea_id"=>"experience-3",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze linear and binary search (homework)",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-3.html",
 "url"=>"/morea/020.examples/experience-3.html",
 "content"=>
  "## #1. Peer Credit Assignment\n\n### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## #2. Correctness of `LinearSearch`\n\n### 5 points\n\n**(a)** Show the pseudocode for `LinearSearch` that you will be analyzing. It should be code that you understand and believe is correct, so you may revise your group's solution if you wish. Give each line a number for reference in your analysis.\n\n**(b)** Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties (page 19).\n\n_(This will be a little tricky because the loop can exit for two reasons.\nRather than complicate your invariant trying to cover the two cases, you might\nwrite a simpler one that gets part of the work done, and reason about the two\nexit conditions when you show how the invariant _helps_ to prove\ncorrectness.)_\n\n## #3. Runtime of `BinarySearch`\n\n### 5 points\n\nThis problem steps you through a recursion tree analysis of BinarySearch (the\nalgorithm for searching a sorted array that was reviewed in class) to show\nthat it is Θ(lg _n_) in the worst case (that is, O(lg _n_) in general). Θ and\n\"big-O\" are concepts introduced in Topic 3: if they are not familiar to you,\njust think of this as meaning the longest possible execution on input of size\n_n_ will take time proportional to lg _n_.\n\n**(a)** Write the recurrence relation for `BinarySearch`, using the formula T(_n_) = _a_T(_n_/_b_) + D(_n_) + C(_n_). (We'll assume T(1) = some constant _c_, and you can use _c_ to represent other constants as well, since we can choose _c_ to be large enough to work as an upper bound everywhere it is used.)\n\n**(b)** Draw the recursion tree for `BinarySearch`, in the style shown in podcast 2E and in Figure 2.5. (Don't just copy the example for `MergeSort`: it will be incorrect. Make use of the recurrence relation you just wrote!)\n\n**(c)** Using a format similar to the counting argument in Figure 2.5 of the text or of podcast 2E, use the tree to show that `BinarySearch` is Θ(lg _n_) in the worst case. Specifically,\n\n  1. show what the row totals are,\n  2. write an expression for the tree height (justifying it), and\n  3. use this information to determine the total computation represented by the tree.\n\nSince this problem involves mathematical expressions and diagrams, you may\nwant to do your work on paper and digitize it (please compress images) and\nsubmit as jpg or pdf. Alternatively, use a drawing program.\n\n\n\n",
 "path"=>"morea//020.examples/experience-3.md"}
</pre>

<h2>/morea/020.examples/module.html</h2>

<pre>Hash
{"title"=>"Analysis examples",
 "published"=>true,
 "morea_id"=>"examples-insertion-merge-sort",
 "morea_outcomes"=>["outcome-analysis-style"],
 "morea_readings"=>
  ["reading-screencast-2a",
   "reading-screencast-2b",
   "reading-screencast-2c",
   "reading-screencast-2d",
   "reading-screencast-2e",
   "reading-cormen-2",
   "reading-screencast-mit-1",
   "reading-notes-2"],
 "morea_experiences"=>["experience-1", "experience-2", "experience-3"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/020.examples/logo.gif",
 "morea_sort_order"=>20,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/020.examples/module.html",
 "content"=>
  "Insertion sort, merge sort, asymptotic analysis, recurrences, loop invariants, linear search, binary search.\n",
 "path"=>"morea//020.examples/module.md"}
</pre>

<h2>/modules/examples-insertion-merge-sort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Analysis examples",
 "url"=>"/modules/examples-insertion-merge-sort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/examples-insertion-merge-sort/index.html"}
</pre>

<h2>/morea/020.examples/outcome.html</h2>

<pre>Hash
{"title"=>"Understand analysis of algorithm styles.",
 "published"=>true,
 "morea_id"=>"outcome-analysis-style",
 "morea_type"=>"outcome",
 "morea_sort_order"=>20,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/020.examples/outcome.html",
 "content"=>
  "By viewing examples, become familiar with the style of analysis used in ICS 311.",
 "path"=>"morea//020.examples/outcome.md"}
</pre>

<h2>/morea/020.examples/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 2 - Getting started",
 "published"=>true,
 "morea_id"=>"reading-cormen-2",
 "morea_summary"=>"Getting started with analysis of algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "26 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-cormen.md"}
</pre>

<h2>/morea/020.examples/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 2 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-2",
 "morea_summary"=>"Modeling a problem, loop invariants, analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/reading-notes.html",
 "url"=>"/morea/020.examples/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. The Sorting Problem\n  2. Insertion Sort: An Incremental Strategy\n  3. Loop Invariants and Correctness of Insertion Sort\n  4. RAM Model; What do we count?\n  5. Analysis of Insertion Sort: Best and Worst Cases\n  6. Worst Case Rate of Growth and Θ (Theta)\n  7. Merge Sort: A Divide & Conquer Strategy\n  8. Brief Comment on Merge Sort Correctness\n  9. Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n## Modeling a Problem: The Sorting Problem\n\n### Problem Formulation\n\nClear and unambiguous definition of what to be solved in terms of:\n\n  * Input of the problem\n  * Output of the problem\n  * Assumptions in the problem\n\nDescriptions in a problem formulation must be declarative (not procedural).\nAll assumptions concerning input and output must be explicit. The problem\nformulation provides the requirements for an algorithm.\n\n### Problem Formulation for Sorting\n\nInput:\n\n    A sequence σ of n real numbers xi (1 ≤ i ≤ n)\nAssumptions:\n\n  1. n is a positive integer.\n  2. The real numbers xi (1 ≤ i ≤ n) are not necessarily distinct.\nOutput:\n\n    A permutation π = x'1 x'2  x'n of the given sequence σ such that x'j ≤ x'j+1 for every j (1 ≤ j < n)\n\nThe numbers are referred to as **keys**.\n\nAdditional information known as **satellite data** may be associated with each\nkey.\n\nSorting is hugely important in most applications of computers. We will cover\nseveral ways to solve this problem in this course.\n\n* * *\n\n## Insertion Sort: An Incremental Strategy\n\n![](fig/sorting-cards.jpg)\n\nInsertion sort takes an **incremental strategy** of problem solving: pick off\none element of the problem at a time and deal with it. Our first example of\nthe text's pseudocode:\n\n![](fig/code-insertion-sort.jpg)\n\nHere's a step by step example:\n\n![](fig/fig-2-2-insertion-sort-example.jpg)\n\n_Is the strategy clear? For fun, see the visualization at\n<http://youtu.be/ROalU379l3U>_\n\n* * *\n\n## Loop Invariants and Correctness of Insertion Sort\n\n### Loop Invariants\n\nA loop invariant is a formal property that is (claimed to be) true at the\nstart of each iteration. We can use loop invariants to prove the correctness\nof iteration in programs, by showing three things about the loop invariant:\n\n**Initialization:**\n    It is true prior to the first iteration.\n**Maintenance:**\n    If it is true prior to a given iteration, then it remains true before the next iteration.\n**Termination:**\n    When the loop terminates, the invariant (and the conditions of termination) gives us a useful property that helps to show that the algorithm is correct.\n\nNotice the similarity to mathematical induction, but here we have a\ntermination condition.\n\n### Correctness of Insertion Sort\n\n![](fig/code-insertion-sort.jpg)\n\n**Loop Invariant:**\n    At the start of each iteration of the outer `for` loop at line 1, the subarray A[1 .. _j_-1] consists of the elements originally in A[1 .. _j_-1] but in sorted order. \n**Initialization:**\n    We start with _j_=2. The subarray A[1 .. _j_-1] is the single element A[1], which is the element originally in A[1] and is trivially sorted.\n**Maintenance:**\n    A precise analysis would state and prove another loop invariant for the `while` loop. For simplicity, we'll note informally that at each iteration the elements A[_j_-1], A[_j_-2], A[_j_-3], etc. are shifted to the right (so they remain in the sequence in proper order) until the proper place for _key_ (the former occupant of A[_j_]) is found. Thus at the next iteration, the subarray A[1 .. _j_] has the same elements but in sorted order.\n**Termination:**\n    The outer `for` loop ends when _j_=_n_+1. Therefore _j_-1=_n_. Plugging _n_ into the loop invariant, the subarray A[1 .. _n_] (which is the entire array) consists of the elements originally in A[1 .. _n_] but in sorted order.\n\n_Convinced? Questions? Could you do it with another problem?_\n\n* * *\n\n## RAM Model: What do we count?\n\nIf we are going to tally up time (and space) requirements, we need to know\nwhat counts as a unit of time (and space). Since computers differ from each\nother in details, it is helpful to have a common abstract model.\n\n### Random Access Machine (RAM) Model\n\nThe RAM model is based on the design of typical von Neumann architecture\ncomputers that are most widely in use. For example:\n\n  * Instructions are executed one after the other (no concurrent operations).\n  * Instructions operate on a small number (one or two) of data \"words\" at a time.\n  * Data words are of a limited, constant size (cannot get arbitrarily large computation done in one operation by putting the data in an arbitrarily large word).\n\n### Categories of Primitive Operations\n\nWe identify the primitive operations that count as \"one step\" of computation.\nThey may differ in actual time taken, but all can be bounded by the same\nconstant, so we can simplify things greatly by counting them as equal.\n\n#### Data Manipulation\n\n  * Arithmetic operation: +, -, *, /, remainder, floor, ceiling, left/right shift\n  * Comparison: <, =, >, ≤, ≥\n  * Logical operation: ∧, ∨, ¬\n\n> _These assume bounded size data objects being manipulated, such as integers\nthat can be represented in a constant number of bits (e.g, a 64-bit word),\nbounded precision floating numbers, or boolean strings that are bounded in\nsize. Arbitrarily large integers, arbitrarily large floating point precision,\nand arbitrarily long strings can lead to nonconstant growth in computation\ntime._\n\n#### Flow Control\n\n  * Branch: case, if, etc.\n  * Loop; while, for   __   <-   ___   to   ___ \n\n> _Here we are stating that the time to execute the machinery of the\nconditional loop controllers are constant time. However, if the language\nallows one to call arbitrary methods as part of the boolean expressions\ninvolved, the overall execution may not be constant time._\n\n#### Miscellaneous\n\n  * Assignment: <-\n  * Subscription: [ ]\n  * Reference\n  * Setting up a procedure or function call (see below)\n  * Setting up an I/O operation (see below) \n\n> _The time to set up a procedure call is constant, but the time to execute\nthe procedure may not be. Count that separately. Similarly, the time to set up\nan I/O operation is constant, but the time to actually read or write the data\nmay be a function of the size of the data. Treat I/O as constant only if you\nknow that the data size is bounded by a constant, e.g., reading one line from\na file with fixed data formats._\n\n###  Input Size\n\nTime taken is a function of input size. How do we measure input size?\n\n  * It is often most convenient to use the number of items in the input, such as the number of numbers being sorted. \n  * For some algorithms we need to measure the size of data, such as the number of bits in two integers being multiplied. \n  * For other algorithms we need more than one number, such as the number of vertices _and_ edges in a graph.\n\n* * *\n\n## Analysis of Insertion Sort: Best and Worst Cases\n\nWe now undertake an exhaustive quantitative analysis of insertion sort. We do\nthis analysis in greater detail than would normally be done, to illustrate why\nthis level of detail is not necessary!!!\n\nFor each line, what does it cost, and how many times is it executed?\n\nWe don't know the actual cost (e.g., in milliseconds) as this varies across\nsoftware and hardware implementations. A useful strategy when you do not know\na quantity is to just give it a name ...\n\n![](fig/analysis-insertion-sort.jpg)\n\nThe _ci_ are the unknown but constant costs for each step. The _tj_ are the\nnumbers of times that line 5 is executed for a given _j_. These quantities\ndepend on the data, so again we just give them names.\n\nLet T(_n_) be the running time of insertion sort. We can compute T(_n_) by\nmultiplying each cost by the number of times it is incurred (on each line) and\nsumming across all of the lines of code:\n\n![](fig/equation-insertion-total-time.jpg)\n\n### Best Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is already sorted, we always find that A[_i_] ≤ _key_ the first\ntime the `while` loop is run; so all _tj_ are 1 and _tj-1_ are 0. Substituting\nthese values into the above:\n\n![](fig/equation-insertion-best.jpg)\n\nAs shown in the second line, this is the same as _a__n_ \\+ _b_ for suitable\nconstants _a_ and _b_. Thus the running time is a **linear function of n.**\n\n### Worst Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is in reverse sorted order, we always find that A[_i_] > _key_\nin the while loop, and will need to compare _key_ to all of the (growing) list\nof elements to the left of _j_. There are _j_-1 elements to compare to, and\none additional test for loop exit. Thus, _tj=j_.\n\n![](fig/equation-insertion-worst-1.jpg) ![](fig\n/equation-insertion-worst-2.jpg)\n\nPlugging those values into our equation:\n\n![](fig/equation-insertion-total-time.jpg)\n\nWe get the worst case running time, which we simplify to gather constants:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _an2 \\+ bn + c_ for some _a, b, c_: _T(n)_ is a\n**quadratic function of n**.\n\nSo we can draw these conclusions purely from mathematical analysis, with _ no\nimplementation or testing needed_: Insertion sort is very quick (linear) on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. But the worst case is slow for reverse sorted data.\n\n* * *\n\n## Worst Case Rate of Growth and Θ (Theta)\n\nFrom the above example we introduce two key ideas and a notation that will be\nelaborated on later.\n\n###  Worst Case Analysis\n\nAbove, both best and worst case scenarios were analyzed. We usually\nconcentrate on the worst-case running times for algorithms, because:\n\n  * This gives us a guaranteed upper bound.\n  * For some algorithms, the worst case occurs often (such as failing to find an item in a search). \n  * The average is often almost as bad as the worst case.\n\n_How long does it take on average to successfully find an item in an unsorted\nlist of n items?  \nHow long does it take in the worst case, when the item is not in the list?  \nWhat is the difference between the two?_\n\n###  Rate of Growth\n\nIn the above example, we kept track of unknown but named constant values for\nthe time required to execute each line once. In the end, we argued that these\nconstants don't matter!\n\n  * Their specific values don't matter because they all add up to summary constants in the equations (e.g., _a_ and _b_).\n  * Even their presence does not matter, because it is the growth of the function of _n_ that dominates the time taken to run the algorithm.\n\nThis is good news, because it means that all of that excruciating detail is\nnot needed!\n\nFurthermore, only the fastest growing term matters. In _an2 \\+ bn + c_, the\ngrowth of _n2_ dominates all the other terms (including _bn_) in its growth.\n\n###  Theta: Θ\n\nWe will use Θ notation to concentrate on the fastest growing term and ignore\nconstants.\n\nIf we conclude that an algorithm requires _an2 \\+ bn + c_ steps to run, we\nwill dispense with the constants and lower order terms and say that its growth\nrate (the growth of how long it takes as _n_ grows) is Θ(_n_2).\n\nIf we see _bn + c_ we will write Θ(_n_).\n\nA simple constant _c_ will be Θ(1), since it grows the same as the constant 1.\n\nWhen we combine Θ terms, we similarly attend only to the dominant term. For\nexample, suppose an analysis shows that the first part of an algorithm\nrequires Θ(_n_2) timeand the second part requires Θ(_n_) time. Since the\nformer term dominates, we need not write Θ(_n_2 \\+ _n_): the overall algorithm\nis Θ(_n_2).\n\nFormal definitions next week!\n\n\n\n* * *\n\n## Merge Sort: A Divide & Conquer Strategy\n\nAnother strategy is to **Divide and Conquer**:\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\nMerge Sort takes this strategy:\n\n**Divide:**\n    Given A[_p .. r_], split the given array into two subarrays A[_p .. q_] and A[_q+1 .. r_] where _q_ is the halfway point of A[_p .. r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure `Merge` that iterates over the subarrays from the smallest element up to copy the next smallest element into a result array.   \n(This is like taking two decks of sorted cards and picking the next smallest\none off to place face-down in a new pile to make one sorted deck.)\n\nThe strategy can be written simply and elegantly in recursive code ...\n\n![](fig/code-merge-sort.jpg)\n\nHere are examples when the input is a power of two, and another example when\nit is not a power of two:\n\n![](fig/example-merge-sort-1.jpg) ![](fig/example-merge-sort-2.jpg)\n\nNow let's look in detail at the merge procedure, implemented using ∞ as\n**sentinels** _(what do lines 1-2 do? lines 3-9 ? lines 10-17?)_:\n\n![](fig/code-merge-procedure.jpg)\n\nHere's an example of how the final pass of `MERGE(9, 12, 16)` happens in an\narray, starting at line 12. Entries with slashes have had their values copied\nto either L or R and have not had a value copied back in yet. Entries in L and\nR with slashes have been copied back into A.\n\n![](fig/example-merge-sort-3.jpg)\n\nWe can also dance this one: <http://youtu.be/XaqR3G_NVoo>\n\n* * *\n\n## Merge Sort Correctness\n\nA loop invariant is used in the book to establish correctness of the Merge\nprocedure. Since the loop is rather straightforward, we will leave it to the\nabove example. Once correctness of Merge is established, induction can be used\nto show that Merge-Sort is correct for any N.\n\n* * *\n\n## Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n![](fig/code-merge-procedure-small.jpg)\n\nMerge Sort provides us with our first example of using recurrence relations\nand recursion trees for analysis. We will go into more detail on these methods\nwhen we cover Chapter 4.\n\n### Analysis of Merge\n\nAnalysis of the Merge procedure is straightforward. The first two `for` loops\n(lines 4 and 6) take Θ(_n1+n2_) = Θ(_n_) time, where _n_1+_n_2 = _n_. The last\n`for` loop (line 12) makes _n_ iterations, each taking constant time, for\nΘ(_n_) time. Thus total time is Θ(_n_).\n\n### Analyzing Divide-and-Conquer Algorithms\n\n**Recurrence equations** are used to describe the run time of Divide & Conquer algorithms. Let _T(n)_ be the running time on a problem of size _n_. \n\n  * If _n_ is below some constant (or often, _n=1_), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each _1/b_ size of the original. Often, as in Merge Sort, _a = b = 2_.\n  * We pay cost **_D(n)_** to divide the problems and **_C(n)_** to combine the solutions. \n  * We also pay cost **_aT(n/b)_** solving subproblems. \n\nThen the total time to solve a problem of size _n_ by dividing into _a_\nproblems of size _n_/_b_can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\n### Recurrence Analysis of Merge Sort\n\n![](fig/code-merge-sort.jpg)\n\nMerge-Sort is called with _p=1_ and _r=n_. For simplicity, assume that _n_ is\na power of 2. (We can always raise a given _n_ to the next power of 2, which\ngives us an upper bound on a tighter Θ analysis.) When _n≥2_, the time\nrequired is:\n\n  * **Divide** (line 2): Θ(1) is required to compute _q_ as the average of _p_ and _r_.\n  * **Conquer** (lines 3 and 4): 2_T_(_n_/2) is required to recursively solve two subproblems, each of size _n/2_.\n  * **Combine** (line 5): Merging an n-element subarray takes Θ(_n_) (this term absorbs the Θ(1) term for Divide). \n![](fig/recurrence-mergesort-theta.jpg)\n\nIn Chapter 4 we'll learn some methods for solving this, such as the Master\nTheorem, by which we can show that it has the solution T(_n_) = Θ(_n_\nlg(_n_)). Thus, Merge Sort is faster than Insertion Sort in proportion to the\ndifference in growth of lg(_n_) versus _n_.\n\n### Recursion Tree Analysis\n\nRecursion trees provide an intuitive understanding of the above result. In\ngeneral, recursion trees can be used to plan out a formal analysis, or even\nconstitute a formal analysis if applied carefully.\n\nLet's choose a constant _c_ that is the largest of all the constant costs in\nthe algorithm (the base case and the divide steps). Then the recurrence can be\nwritten:\n\n![](fig/recurrence-mergesort-c.jpg)\n\nIt costs _cn_ to divide the original problem in half and then to merge the\nresults. We then have to pay cost _T_(_n_/2) twice to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-1.jpg)\n\nFor each of the two subproblems, _n_/2 is playing the role of _n_ in the\nrecurrence. So, it costs _cn_/2 to divide and then merge the _n_/2 elements,\nand _T_(_n_/4) to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-2.jpg)\n\nIf we continue in this manner we eventually bottom out at problems of size 1:\n\n![](fig/recurrence-tree-mergesort-3.jpg)\n\nNotice that if we sum across the rows each level has cost _cn_. So, all we\nhave to do is multiply this by the number of levels. Cool, huh?\n\n_But how many levels are there?_ A little thought (or a more formal inductive\nproof you'll find in the book) shows that there are about (allowing for the\nfact that n may not be a power of 2) lg(_n_)+1 levels of the tree. This is\nbecause you can only divide a power of two in half as many times as that power\nbefore you reach 1, and _n_ = 2lg(_n_). The 1 counts the root note before we\nstart dividing: there is always at least one level.\n\n_Questions? Does it make sense, or is it totally mysterious?_\n\n### One more Animation\n\nRecapitulating our conclusions, we have seen that Insertion sort is quick on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. Due to its simplicity it is a good choice when the sequence to\nsort will always be small. But for large inputs Merge Sort will be faster than\nInsertion Sort, as _n_2 grows much faster than _n_lg(_n_). Each sort algorithm\nhas different strengths and weaknesses, and performance depends on the data.\nSome of these points are made in the following visualizations (also watch for\npatterns that help you understand the strategies):\n\n> <http://www.sorting-algorithms.com/> (set to 50 elements)\n\n* * *\n\n## Next\n\nNext week we cover Chapter 3: Growth of Functions and Asymptotic Concepts.\nProblems will be posted for my students in Laulima.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:09:15 HST 2014\n\nImages are from the instructor's manual for Cormen et al.\n\n",
 "path"=>"morea//020.examples/reading-notes.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Insertion Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2a",
 "morea_summary"=>"Insertion sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-a.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Loop Invariant",
 "published"=>true,
 "morea_id"=>"reading-screencast-2b",
 "morea_summary"=>"Loop invariant example",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://youtu.be/t1ranlQmofQ",
 "morea_labels"=>["Screencast", "Suthers", "6 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-b.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Insertion sort analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2c",
 "morea_summary"=>"Insertion sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://youtu.be/UtEMLcKHcGc",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-c.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Merge Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2d",
 "morea_summary"=>"Merge sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://youtu.be/9BI0Lw1kzkE",
 "morea_labels"=>["Screencast", "Suthers", "11 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-d.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-e.html</h2>

<pre>Hash
{"title"=>"Merge Sort Analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2e",
 "morea_summary"=>"Merge sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://youtu.be/1JbqqmK7e5s",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-e.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-e.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-mit.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 1",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-1",
 "morea_summary"=>
  "Analysis of algorithms-insertion sort, asymptotic analysis, merge sort, recurrences",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec01/",
 "morea_labels"=>["Screencast", "Leiserson", "80 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-mit.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-mit.md"}
</pre>

<h2>/morea/030.growth/assessment.html</h2>

<pre>Hash
{"title"=>"Ability to recall asymptotic concepts",
 "published"=>true,
 "morea_id"=>"assessment-asymptotic-concepts",
 "morea_type"=>"assessment",
 "morea_sort_order"=>1,
 "morea_labels"=>["Bloom: Remember"],
 "referencing_modules"=>[],
 "url"=>"/morea/030.growth/assessment.html",
 "content"=>
  "Assessed ability to remember asymptotic concepts through an in-class multiple choice exam:\n\n<link rel=\"stylesheet\" href=\"http://cdn.oesmith.co.uk/morris-0.4.3.min.css\">\n<script src=\"//cdnjs.cloudflare.com/ajax/libs/raphael/2.1.0/raphael-min.js\"></script>\n<script src=\"http://cdn.oesmith.co.uk/morris-0.4.3.min.js\"></script>\n\n<div class=\"well\">\n  <div id=\"assessment\" style=\"height: 250px;\"></div>\n</div>\n\n<script>\nMorris.Bar({\n  element: 'assessment',\n  hideHover: false,\n  data: [\n        { y: 'Very satisfactory (%)', num: 15 },\n        { y: 'Satisfactory (%)', num: 55 },\n        { y: 'Unsatisfactory (%)', num: 25 },\n        { y: 'Absent (%)', num: 5 },\n        ],\n  xkey: 'y',\n  ykeys: ['num'],\n  resize: true,\n  labels: ['Students']\n});\n</script>",
 "path"=>"morea//030.growth/assessment.md"}
</pre>

<h2>/morea/030.growth/experience.html</h2>

<pre>Hash
{"title"=>"Asymptotic concepts",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-concepts",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of functions with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/030.growth/experience.html",
 "url"=>"/morea/030.growth/experience.html",
 "content"=>
  "## Asymptotic Concepts\n\n#### 5 points\n\n**1\\. (1 pt)** We can extend asymptotic notation to the case of two parameters n and m that can go to infinity independently at different rates. For example, we denote by O(g(n,m)) the set of functions:\n\n> O(_g_(_n_,_m_)) = {_f_(_n_,_m_) : there exists positive constants _c_, _n_0\nand _m_0 such that 0 ≤ _f_(_n_,_m_) ≤ _c__g_(_n_,_m_) for all _n_ ≥ _ _n0 or\n_m_ ≥ _m_0}\n\nGive a corresponding definition for Θ(_g_(_n_,_m_)).\n\n**2\\. (4 pts)** Indicate, for each pair of expressions (_f_(_n_), _g_(_n_)) in the table below, whether _f_(_n_) = ___(_g_(_n_)), where the ___ may be O, o, Ω, ω or Θ. Assume that k ≥ 1, _c_ > 1, and d > 0 are constants and we are analyzing growth rates in terms of the variable _n_. To respond, write \"Yes\" or \"No\" in each box. Grading will be based on these entries first, but if you give your justifications below we can give better feedback and possibly partial credit in case of wrong answers. \n\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\"><i>f</i>(<i>n</i>)</th>\n    <th scope=\"col\"><i>g</i>(<i>n</i>)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">a.</th>\n    <td>n<sup>lg <i>c</i></sup></td>\n    <td>c<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">b.</th>\n    <td>lg<sup><i>k</i></sup><i>n</i></td>\n    <td>n<sup><i>d</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">c.</th>\n    <td>2<sup><i>n</i></sup></td>\n    <td>2<sup><i>n</i>/2</sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">d.</th>\n    <td>lg(<i>n</i>!)</td>\n    <td>lg(<i>n</i><sup><i>n</i></sup>)</td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n",
 "path"=>"morea//030.growth/experience.md"}
</pre>

<h2>/morea/030.growth/module.html</h2>

<pre>Hash
{"title"=>"Growth of functions",
 "published"=>true,
 "morea_id"=>"growth",
 "morea_outcomes"=>["outcome-growth"],
 "morea_readings"=>
  ["reading-screencast-3a",
   "reading-screencast-3b",
   "reading-screencast-3c",
   "reading-screencast-3d",
   "reading-cormen-3",
   "reading-notes-3",
   "reading-screencast-mit-2"],
 "morea_experiences"=>["experience-asymptotic-concepts"],
 "morea_assessments"=>["assessment-asymptotic-concepts"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/030.growth/logo.png",
 "morea_sort_order"=>30,
 "referencing_modules"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/030.growth/module.html",
 "content"=>
  "Asymptotic notations, omega, theta, recurrences, substitution, master method.\n",
 "path"=>"morea//030.growth/module.md"}
</pre>

<h2>/modules/growth/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Growth of functions",
 "url"=>"/modules/growth/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/growth/index.html"}
</pre>

<h2>/morea/030.growth/outcome.html</h2>

<pre>Hash
{"title"=>"Remember concepts of asymptotic growth.",
 "published"=>true,
 "morea_id"=>"outcome-growth",
 "morea_type"=>"outcome",
 "morea_sort_order"=>30,
 "morea_labels"=>["Bloom: Remember"],
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/030.growth/outcome.html",
 "content"=>
  "Learn the concepts and definitions of asymptotic growth and recognize them in context.",
 "path"=>"morea//030.growth/outcome.md"}
</pre>

<h2>/morea/030.growth/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 3 - Growth of functions",
 "published"=>true,
 "morea_id"=>"reading-cormen-3",
 "morea_summary"=>
  "Asymptotic notation, standard notation, and common functions.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-cormen.md"}
</pre>

<h2>/morea/030.growth/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 3 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-3",
 "morea_summary"=>"Introduction to asymptotic analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/030.growth/reading-notes.html",
 "url"=>"/morea/030.growth/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Intro to Asymptotic Analysis\n  2. Big-O\n  3. Ω (Omega)\n  4. Θ (Theta)\n  5. Asymptotic Notation in Equations\n  6. Asymptotic Inequality\n  7. Properties of Asymptotic Sets\n  8. Common Functions\n\n* * *\n\n## Intro to Asymptotic Analysis\n\nThe notations discussed today are ways to describe behaviors of _functions,_\nparticularly _in the limit_, or _asymptotic_ behavior.\n\nThe functions need not necessarily be about algorithms, and indeed asymptotic\nanalysis is used for many other applications.\n\nAsymptotic analysis of algorithms requires:\n\n  1. Identifying ** what aspect of an algorithm we care about**, such as:    \n\n    * runtime;\n    * use of space;\n    * possibly other attributes such as communication bandwidth;\n  \n\n  2. Identifying **a function that characterizes that aspect; ** and \n  \n\n  3. Identifying **the asymptotic class of functions that this function belongs to**, where classes are defined in terms of bounds on growth rate. \n\nThe different asymptotic bounds we use are analogous to equality and\ninequality relations:\n\n  * O   ≈   ≤\n  * Ω   ≈   ≥\n  * Θ   ≈   =\n  * o   ≈   <\n  * ω   ≈   >\n\nIn practice, most of our analyses will be concerned with run time. Analyses\nmay examine:\n\n  * Worst case\n  * Best case\n  * Average case (according to some probability distribution across all possible inputs)\n\n* * *\n\n## Big-O (asymptotic ≤)\n\nOur first question about an algorithm's run time is often \"how bad can it\nget?\" We want a guarantee that a given algorithm will complete within a\nreasonable amount of time for typical n expected. This requires an\n**asymptotic upper bound**: the \"worst case\".\n\nBig-O is commonly used for worst case analyses, because it gives an upper\nbound on growth rate. Its definition in terms of set notation is:\n\n> O(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/graph-big-O.jpg)\n\nThis definition means that as _n_ increases, afer a point _f_(_n_) grows no\nfaster than _g_(_n_) (as illustrated in the figure): _g_(_n_) is an\n_asymptotic upper bound_ for _f_(_n_).\n\nSince O(_g_(_n_)) is a set, it would be natural to write _f_(_n_) ∈\nO(_g_(_n_)) for any given _f_(_n_) and _g_(_n_) meeting the definition above,\nfor example, _f_ ∈ O(_n_2).\n\nBut the algorithms literature has adopted the convention of using = instead of\n∈, for example, writing _f_(_n_) = O(_g_(_n_)). This \"abuse of notation\" makes\nsome manipulations possible that would be more tedious if done strictly in\nterms of set notation. (We do _not_ write O(_g_(_n_))=_f_(_n_); will return to\nthis point).\n\nUsing the = notation, we often see definitions of big-O in in terms of truth\nconditions as follows:\n\n> _f_(_n_) = O(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0.\n\nWe assume that all functions involved are asymptotically non-negative. Other\nauthors don't make this assumption, so may use |_f_(_n_)| etc. to cover\nnegative values. This assumption is reflected in the condition 0 ≤ _f_(_n_).\n\n### Examples\n\nShow that 2_n_2 is O(_n_2).\n\nTo do this we need to show that there exists some _c_ and _n_0 such that\n(letting 2_n_2 play the role of _f_(_n_) and _n_2 play the role of _g_(_n_) in\nthe definition):\n\n> 0 ≤ 2_n_2 ≤ _c__n_2 for all _n_ ≥ _n_0.\n\nIt works with _c_ = 2, since this makes the _f_ and _g_ terms equivalent for\nall _n_ ≥ _n_0 = 0. (We'll do a harder example under Θ.)\n\n#### What's in and what's out\n\nThese are all O(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_\n  * _n_1.99999\n  * _n_\n\n  * _n_3\n  * _n_2.00001\n  * _n_2 lg _n_\n\n#### Insertion Sort Example\n\nRecall that we did a tedious analysis of the worst case of insertion sort,\nending with this formula:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _pn2 \\+ _q__n_ \\- r_ for suitable _p, q, r_ (_p_ =\n(_c_5/2 + _c_6/2 + _c_7/2), etc.).\n\nThe textbook (page 46) sketches a proof that __f_(_n_) = _a__n_2 \\+ _b__n_ \\+\n_c__ is Θ(_n_2), and we'll see shortly that Θ(_n_2) -> O(_n_2). This is\ngeneralized to all polynomials in Problem 3-1. So, any polynomial with highest\norder term _a__n__d_ (i.e., a polynomial in _n_ of degree _d_) will be\nO(_n__d_).\n\nThis suggests that the worst case for insertion sort _T_(_n_) ∈ O(_n_2). An\nupper bound on the worst case is also an upper bound on all other cases, so we\nhave already covered those cases.\n\nNotice that the definition of big-O would also work for __g_(_n_) = n3_,\n__g_(_n_) = 2n_, etc., so we can also say that _T_(_n_) (the worst case for\ninsertion sort) is O(_n_3), O(2_n_), etc. However, these loose bounds are not\nvery useful! We'll deal with this when we get to Θ (Theta).\n\n* * *\n\n## Ω (Omega, asymptotic ≥)\n\nWe might also want to know what the best we can expect is. In the last lecture\nwe derived this formula for insertion sort:\n\n![](fig/equation-insertion-best.jpg)\n\nWe could prove that this best-case version of T(n) is big-O of something, but\nthat would only tell us that the best case is no worse than that something.\nWhat if we want to know what is \"as good as it gets\": a lower bound below\nwhich the algorithm will never be any faster?\n\nWe must both pick an appropriate function to measure the property of interest,\nand pick an appropriate asymptotic class or comparison to match it to. We've\ndone the former with _T_(_n_), but what should it be compared to?\n\nIt makes more sense to determine the **asymptotic lower bound** of growth for\na function describing the best case run-time. In other words, what's the\nfastest we can ever expect, in the best case?\n\n![](fig/graph-Omega.jpg)\n\n**Ω (Omega)** provides what we are looking for. Its set and truth condition definitions are simple revisions of those for big-O:\n\n> Ω(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_cg_(_n_) ≤ _f_(_n_) ∀ _n_ ≥ _n_0}.  \n_[The _f_(_n_) and _cg_(_n_) have swapped places.]_\n\n> _f_(_n_) = Ω(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that\n_f_(_n_) ≥ _cg_(_n_) ∀ _n_ ≥ _n_0.  \n_[≤ has been replaced with ≥.]_\n\nThe semantics of Ω is: as _n_ increases after a point, _f_(_n_) grows no\nslower than _g_(_n_) (see illustration).\n\n### Examples\n\nSqrt(_n_) is Ω(lg _n_) with _c_=1 and _n_0=16.  \n_(At n=16 the two functions are equal; try at n=64 to see the growth, or graph\nit.)_\n\n####  What's In and What's Out\n\nThese are all Ω(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000n   _(It's also O(_n_2)!)_\n_\n\n  * 1000_n_2 \\+ 1000_n_\n  * 1000_n_2 \\- 1000_n_\n  * _n_3\n  * _n_2.00001\n__ _\n\n  * _n_1.99999\n  * _n_\n  * lg _n_\n\n#### Insertion Sort Example\n\nWe can show that insertion will take at least Ω(_n_) time in the best case\n(i.e., it won't get any better than this) using the above formula and\ndefinition.\n\n![](fig/equation-insertion-best.jpg)\n\n_T_(_n_) can be expressed as _pn - q_ for suitable _p, q_ (e.g., _q_ = _c_2 \\+\n_c_4 \\+ _c_5 \\+ _c_8, etc.). (In this case, _p_ and _q_ are positive.) This\nsuggests that _T_(_n_) ∈ Ω(_n_), that is, ∃ _c, n0_ s.t. _pn - q ≥ cn,_ ∀_n ≥\nn0_. This follows from the generalized proof for polynomials.\n\n* * *\n\n## Θ (Theta, asymptotic =)\n\nWe noted that there are _ loose _ bounds, such as _f_(_n_) = _n_2 is O(_n_3),\netc., but this is an overly pessimistic assessment. It is more useful to have\nan **asymptotically tight bound** on the growth of a function. In terms of\nalgorithms, we would like to be able to say (when it's true) that a given\ncharacteristic such as run time grows _no better and no worse_ than a given\nfunction. That is, we want to simultaneoulsy bound from above and below.\nCombining the definitions for O and Ω:\n\n![](fig/graph-Theta.jpg)\n\n> Θ(_g_(_n_)) = {_f_(_n_) : ∃ positive constants **_c_1, _c_2**, and _n_0 such\nthat 0 ≤ **_c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)**, ∀ _n_ ≥ _n_0}.\n\nAs illustrated, _g_(_n_) is an asymptotically tight bound for _f_(_n_): after\na point, _f_(_n_) grows no faster and no slower than _g_(_n_).\n\nThe book suggests the proof of this theorem as an easy exercise (just combine\nthe two definitions):\n\n> _f_(_n_) = Θ(_g_(_n_)) iff _f_(_n_) = Ω(_g_(_n_)) ∧ _f_(_n_) = O(_g_(_n_)).\n\nYou may have noticed that some of the functions in the list of examples for\nbig-O are also in the list for Ω. This indicates that the set Θ is not empty.\n\n### Examples\n\n> _Reminder:_ _f_(_n_) = Θ(_g_(_n_)) iff ∃ positive constants _c_1, _c_2, and\n_n_0 such that 0 ≤ _c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)∀ _n_ ≥ _n_0.\n\n_n_2 \\- 2_n_ is Θ(_n_2),   with _c_1 = 1/2; _c_2 = 1, and _n_0 = 4,   since:\n\n> _n_2/2   ≤   _n_2 \\- 2_n_   ≤   _n_2   for _n_ ≥ _n_0 = 4.\n\n#### Find an asymptotically tight bound (Θ) for\n\n  * 4_n_3\n  * 4_n_3 \\+ 2_n_. \n\nPlease try these before you [find solutions\nhere](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-03\n/Example-Analysis.html).\n\n#### What's in and what's out\n\nThese are all Θ(n2):\n\nThese are not\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_ \\+ 32,700\n  * 1000_n_2 \\- 1000_n_ \\- 1,048,315\n\n  * _n_3\n  * _n_2.00001\n  * _n_1.99999\n  * _n_ lg _n_\n\n* * *\n\n## Asymptotic Inequality\n\nThe O and Ω bounds may or may not be asymptotically tight. The next two\nnotations are for upper bounds that are strictly _not_ asymptotically tight.\nThere is an _analogy_ to inequality relationships:\n\n  * \"≤\" is to \"<\" as big-O (may or may not be tight) is to little-o (strictly not equal) \n  * \"≥\" is to \">\" as Ω (may or may not be tight) is to little-ω (strictly not equal). \n\n### o-notation (\"little o\", asymptotic <)\n\n> o(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _f_(_n_) **<** _cg_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/o-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _insignificant_ relative to _g_(_n_) as _n_\napproaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically smaller** than _g_(_n_) if _f_(_n_) =\no(_g_(_n_))\n\n  * _n_1.99999 ∈ o(_n_2)\n  * _n_2/lg _n_ ∈ o(_n_2)\n  * _n_2 ∉ o(_n_2) (similarly, 2 is not less than 2)\n  * _n_2/1000 ∉ o(_n_2) \n\n### ω-notation (\"little omega\", asymptotic >)\n\n> ω(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _cg_(_n_) **<** _f_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/omega-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _ arbitrarily large _ relative to _g_(_n_) as\n_n_ approaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically larger** than _g_(_n_) if _f_(_n_) =\nω(_g_(_n_))\n\n  * _n_2.00001 ∈ ω(_n_2)\n  * _n_2lg _n_ ∈ ω(_n_2)\n  * _n_2 ∉ ω(_n_2)\n\nThe two are related:   **_f_(_n_) ∈ ω(_g_(_n_))   iff   _g_(_n_) ∈\no(_f_(_n_)).**\n\n* * *\n\n## Asymptotic Notation in Equations\n\nWe already noted that while asymptotic categories such as Θ(_n_2) are sets, we\nusually use \"=\" instead of \"∈\" and write (for example) _f_(_n_) = Θ(_n_2) to\nindicate that _f_ is in this set.\n\nPutting asymptotic notation in equations lets us do shorthand manipulations\nduring analysis.\n\n### Asymptotic Notation on Right Hand Side: ∃\n\nO(_g_(_x_)) on the right hand side stands for _some_ anonymous function in the\nset O(_g_(_x_)).\n\n> 2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ Θ(_n_)     means:  \n2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ _f_(_n_)       for _some_ __f_(_n_) ∈ Θ(_n_)_\n(in particular, _f_(_n_) = 3_n_ \\+ 1).\n\n### Asymptotic Notation on Left Hand Side: ∀\n\nThe notation is only used on the left hand side when it is also on the right\nhand side.\n\nSemantics: No matter how the anonymous functions are chosen on the left hand\nside, there is a way to choose the functions on the right hand side to make\nthe equation valid.\n\n> 2_n_2 \\+ Θ(_n_) = Θ(_n_2)     means   for _all_ _f_(_n_) ∈ Θ(_n_), there _\nexists_ _g_(_n_) ∈ Θ(_n_2) such that  \n2_n_2 \\+ _f_(_n_) = _g_(_n_).\n\n### Combining Terms\n\nWe can do basic algebra such as:\n\n> 2_n_2 \\+ 3_n_ \\+ 1   =   2_n_2 \\+ Θ(_n_)   =   Θ(_n_2)\n\n* * *\n\n## Properties\n\nIf we keep in mind the analogy to inequality, many of these make sense, but\nsee the end for a caution concerning this analogy.\n\n### Relational Properties\n\n**Transitivity**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_)) and _g_(_n_) = Θ(h(n))   ⇒   _f_(_n_) = Θ(h(n)).\n  * _f_(_n_) = O(_g_(_n_)) and _g_(_n_) = O(h(n))   ⇒   _f_(_n_) = O(h(n)).\n  * _f_(_n_) = Ω(_g_(_n_)) and _g_(_n_) = Ω(h(n))   ⇒   _f_(_n_) = Ω(h(n)).\n  * _f_(_n_) = o(_g_(_n_)) and _g_(_n_) = o(h(n))   ⇒   _f_(_n_) = o(h(n)).\n  * _f_(_n_) = ω(_g_(_n_)) and _g_(_n_) = ω(h(n))   ⇒   _f_(_n_) = ω(h(n)).\n**Reflexivity**:\n    \n\n  * _f_(_n_) = Θ(_f_(_n_))\n  * _f_(_n_) = O(_f_(_n_))\n  * _f_(_n_) = Ω(_f_(_n_))\n  * _What about o and ω?_\n**Symmetry**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_))   iff   _g_(_n_) = Θ(_f_(_n_)) \n  * _Should any others be here? Why or why not?_\n**Transpose Symmetry**:\n    \n\n  * _f_(_n_) = O(_g_(_n_))   iff   _g_(_n_) = Ω(_f_(_n_)) \n  * _f_(_n_) = o(_g_(_n_))   iff   _g_(_n_) = ω(_f_(_n_)) \n\n### Incomparability\n\nHere is where the analogy to numeric (in)equality breaks down: There is no\ntrichotomy. Unlike with constant numbers, we can't assume that one of <, =, >\nhold. Some functions may be incomparable.\n\nExample: _n_1 + _sin n_ is incomparable to _n_ since _sin n_ oscillates\nbetween -1 and 1, so 1 + _sin n_ oscillates between 0 and 2. _(Try graphing\nit.)_\n\n* * *\n\n## Common Functions and Useful Facts\n\nVarious classes of functions and their associated notations and identities are\nreviewed in the end of the chapter: please review the chapter and refer to ICS\n241 if needed. Here we highlight some useful facts:\n\n### Monotonicity\n\n  * _f_(_n_) is **monotonically increasing**   if   _m_ ≤ _n_   ⇒   _f_(_m_) ≤ _f_(_n_).\n  * _f_(_n_) is **monotonically decreasing**   if   _m_ ≥ _n_   ⇒   _f_(_m_) ≥ _f_(_n_).\n  * _f_(_n_) is **strictly increasing**   if   _m_ < _n_   ⇒   _f_(_m_) < _f_(_n_).\n  * _f_(_n_) is **strictly decreasing**   if   _m_ > _n_   ⇒   _f_(_m_) > _f_(_n_).\n\n### Polynomials\n\n  * _p_(_n_) = Θ(_n__d_), for asymptoptically positive polynomials in _n_ of degree _d_\n\n### Exponentials\n\n  * _n__b_ = o(_a__n_) for all real constants _a_ and _b_ such that _a_ > 1: **_Any exponential function with a base greater than 1 grows faster than any polynomial function._**\n  \n\n  * Useful identities: \n    * _a_-1 = 1/_a_\n    * (_a__m_)_n_ = _a__mn_\n    * _a__m__a__n_ = _a__m_ \\+ _n_\n\n### Logarithms\n\n  * (lg _n_)_b_ = lg_b__n_ = o(_n__a_), for a > 0: **_any positive polynomial function grows faster than any polylogarithmic function._**\n  \n\n  * Useful identities: \n    * _a_ = _b_log_b__a_   _(Definition of logs.)_\n    * log_a__n_ = log_b__n_/log_b__a_     \n    _(Base change. If _n_ is variable and _a_ and _b_ are constant, the denominator is constant: this is why asymptotic analysis can ignore the base.)_\n    * log_c_(_ab_) = log_c__a_ \\+ log_c__b_   _(Ask your slide rule!)_\n    * log_b__a_n__ = _n_ log_b__a_\n    * log_b_(1/_a_) = −log_b__a_\n    * log_b__a_ = 1 / log_a__b_\n    * _a_log_b__c_ = _c_log_b__a_   _(Useful for getting the variable where you want it.)_\n\n### Factorials\n\n  * _n_! = ω(2_n_):   _**factorials grow faster than exponentials** (but it could be worse):_\n  * _n_! = o(_n__n_)\n  * lg(_n_!) = Θ(_n_ lg _n_)\n  * See also the more complex **Stirling's approximation** from which these are derived.\n\n### Iterated Functions\n\n  * Definition: _f_(_i_)(_n_) is _f_ applied _i_ times to the initial value _n_. \n  * Iterated Logarithm: lg*_n_ = min{_i_ ≥ 0: lg(_i_)_n_ ≤ 1}   _(The iteration at which lg(_i_)_n_ is less than 1: a very slowly growing function.)_\n\n### Fibonacci Numbers\n\n  * Definition: _F_0 = 0; _F_1 = 1; and for _i_ > 1 _F__i_ = _F__i_-1 \\+ _F__i_-2. \n  * **_Fibonacci numbers grow exponentially._**\n\n* * *\n\nDan Suthers Last modified: Sat Jan 25 03:51:57 HST 2014  \nImages are from the instructor's manual for Cormen et al.  \n\n",
 "path"=>"morea//030.growth/reading-notes.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Asymptotic notations",
 "published"=>true,
 "morea_id"=>"reading-screencast-3a",
 "morea_summary"=>"Notations for this analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=y86z2OrIYQQ",
 "morea_labels"=>["Screencast", "Suthers", "12 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-a.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Omega and Theta",
 "published"=>true,
 "morea_id"=>"reading-screencast-3b",
 "morea_summary"=>"The omega and theta notations",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-b.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"little-o and omega",
 "published"=>true,
 "morea_id"=>"reading-screencast-3c",
 "morea_summary"=>"The little guys, properties, and use in equations",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=uaqLI449XQw",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-c.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Common Functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-3d",
 "morea_summary"=>"Common functions and useful identities",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=f2czg61AtQw",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-d.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-mit.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 2",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-2",
 "morea_summary"=>
  "Asymptotic notation, recurrences, substitution, master method (start at 16 min)",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec02/",
 "morea_labels"=>["Screencast", "Demaine", "71 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-mit.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-mit.md"}
</pre>

<h2>/morea/040.adt/experience-1.html</h2>

<pre>Hash
{"title"=>"Battle of the Dynamic Sets",
 "published"=>true,
 "morea_id"=>"experience-project-1",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Provide four implementations of the Dynamic Set ADT and compare their performance.",
 "morea_sort_order"=>1,
 "morea_labels"=>["Programming"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-1.html",
 "url"=>"/morea/040.adt/experience-1.html",
 "content"=>
  "# Battle of the Dynamic Sets\n\nLate submissions with **penalty of 1% per hour** accepted until 2AM Saturday\nMarch 15th, at which time the deduction reaches 50%. (If you used all that\ntime and turned in a perfect \"A+\" project, you would get a \"C\".)\n\n\n### Change Log\n\nMar 10, 2014\n\n    Assignment extended three days. Corrected the late deadline on March 11th.\nMar 1, 2014\n\n    Changed the interface to Robert Ward's version with generics and comparable.\nFeb 7, 2014\n\n    Added delete to operations to be tested. \nFeb 6, 2014\n\n    First released version. This differs from the 2012 and 2013 versions. \n\n## Overview\n\nYou will provide four implementations of the Dynamic Set ADT (see below,\nmodified from [Topic 4 Notes](/morea/040.adt/reading-notes-4.html)). You will then analyze their expected\nperformance (this has been largely done in the lectures and textbook), and\nthen test them empirically to see how your test data compares to expected\nperformance. You will write a report on the results. The project is worth 60\npoints.\n\nYou may either use open source implementations of the following data\nstructures (see the [Assignments Page](/morea/010.introduction/reading-assignments.html) Section \"Including Open Source Software\"\nfor requirements), or implement them yourself:\n\n  * **Sorted Doubly Linked Lists** (Chapter 10 or [Notes/Topic-04](morea/040.adt/reading-notes-4.html)) \n  * **Skip Lists** ([Notes/Topic-05](morea/050.probabilistic/reading-notes-5.html) or ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf) \n  * **Binary Search Trees** (Chapter 12 or [ Notes/Topic-08](morea/080.binary-search-trees/reading-notes-8.html)) \n  * **Red-Black trees** (Chapter 13 or [Notes/Topic-11]morea/110.balanced-trees/reading-notes-11.html)): for this, use Java TreeMap. \n\nThere are several advantages to implementing at least some of these\nyourself:\n\n  * Others' implementations may have incompatible assumptions.\n  * You will understand the data structures and algorithms better\n  * You can then offer your implementation to other students for extra credit in Implementation Project 2.\n\nYou will then write your own code, including:\n\n  * Implementations of Dynamic Sets that use these data structures;\n  * A main method to read in a file of strings, insert these in the dynamic set as keys, and run tests on all four implementations as specified below.\n\nWe will provide you with data in the form of files of keywords (automatically\ngenerated names). There will be ten data files, three _unsorted_ and three\n_sorted_ of size 100, 1000, 10,000, 100,000 and 1,000,000. (See discussion\nbelow concerning the largest file.) We may use other data files in our\ntesting, so make your code robust!\n\nYour accompanying report will provide the **minimum**, **average** and\n**maximum** times in **nanoseconds** required for: `insert`, `retrieve`,\n`successor`, and `predecessor`, and the actual time for `minimum` and\n`maximum`, for each of the input files. You will compare these results to the\ntheoretical analyses for the algorithms. While `delete` is not included in the\ntiming tests, we will test that deletion works.\n\nDetails follow. Questions and requests for clarification are welcome and\nshould be submitted early.\n\n## Dynamic Set ADT\n\nNote that this **ADT has been changed**: it is not the same as the one in the\nlecture notes, and it is not the same as the ones used in my 2012 or 2013\nclasses. Also, it was changed to use generics.\n\n{% highlight java linenos %}\n    package ics311;\n    import java.util.Map.Entry;\n\n    \n    /**\n     * This interface is to be used for all of the Dynamic Sets you create for this assignment.\n     * \n     * @author      Robert Ward\n     * @version     1.0       \n     * @since       2014-02-01\n     */\n    public interface DynamicSet<Key extends Comparable<Key> , Value> {\n        \n       /**\n        * Returns the name of Data Structure this set uses. \n        *           \n        * @return  the name of Data Structure this set uses. \n        */\n        public String setDataStructure();\n\n       /**\n        * Returns the number of key-value mappings in this set. \n        * This method returns zero if the set is empty\n        *           \n        * @return  the number of key-value mappings in this set. \n        */\n        public int size();\n       \n       /**\n        * Associates the specified value with the specified key in this map.\n        * If the map previously contained a mapping for the key, the old value is replaced. \n        * If there is no current mapping for this key return null,\n        * otherwise the previous value associated with key. \n        * \n        * @param  key - key with which the specified value is to be associated\n        * @param  value - value to be associated with the specified key\n        * \n        * @return the previous value associated with key, or null if there was no mapping for key. \n        *  (A null return can also indicate that the map previously associated null with key.)  \n        */\n        public Value insert(Key key, Value value);\n        \n       /**\n        * Removes the mapping for this key from this set if present.\n        *\n        * @param key - key for which mapping should be removed\n        * \n        * @return the previous value associated with key, or null if there was no mapping for key. \n        * (A null return can also indicate that the map previously associated null with key.) \n        */\n        public Value delete(Key key);\n      \n    \n       /**\n        * Returns the value to which the specified key is mapped, or null if this set contains no\n        * mapping for the key. \n        * \n        * @param  key The key under which this data is stored. \n        *   \n        * @return the Value of element stored in the set under the Key key.\n        */\n        public Value retrieve(Key key);\n       \n       \n       /**\n        * Returns a key-value mapping associated with the least key in this map, or null if the set\n        * is empty. \n        * IMPORTANT: This operation only applies when there is a total ordering on the Key \n        * Returns null if the set is empty. If there is not total ordering on the Key returns null.  \n        * \n        * @return an entry with the least key, or null if this map is empty\n        */\n        public Entry<Key, Value>  minimum( );\n       \n    \n       /**\n        * Returns a key-value mapping associated with the greatest key in this map, or null if the\n        * map is empty. \n        * IMPORTANT: This operation only applies when there is a total ordering on the Key \n        * Returns null if the set is empty. If there is not total ordering on the key returns null. \n        * \n        * @return an entry with the greatest key, or null if this map is empty.\n        */\n        public Entry<Key, Value>  maximum( );\n       \n        \n       /**\n        * Returns a key-value mapping associated with the least key strictly greater than the given\n        * key, or null if there is no such key. \n        * IMPORTANT: This operation only applies when there is a total ordering on the key \n        * Returns null if the set is empty or the key is not found. \n        * Returns null if the key is the maximum element. \n        * If there is not total ordering on the key for the set returns null. \n        * @param key - the key\n        * \n        * @return an entry with the greatest key less than key, or null if there is no such key\n        */\n        public Entry<Key, Value>  successor(Key key);\n        \n       /**\n        * Returns a key-value mapping associated with the greatest key strictly less than the given\n        * key, or null if there is no such key. \n        * IMPORTANT: This operation only applies when there is a total ordering on the key \n        * Returns null if the set is empty or the key is not found. \n        * Returns null if the key is the minimum element. \n        * If there is not total ordering on the key for the set returns null. \n        * @param key - the key\n        * \n        * @return an entry with the greatest key less than key, or null if there is no such key\n        */\n        public Entry<Key, Value>  predecessor(Key key);\n    \n    }\n{% endhighlight %}\n\n### Comments on ADT\n\nIf you wish, to simplify things you may change KeyType to String. The data\n`Object d` stored under a key is not important for our testing: just give it\nthe key `k`.\n\nIn a real implementation, returning a key in `minimum`, `maximum`, `successor`\nand `predecessor` would be inefficient if the client then has to use\n`retrieve` to get the data. This could be solved by using the position\nabstraction, or by returning multiple values (key + data). But for our\npurposes we only need the key to test runtime.\n\n### Implementing the ADT\n\nYou will have four classes implementing this interface, named to reflect the\nimplementation:\n\n  * **`DLLDynamicSet`**: Doubly Linked List implementation, using OSS \n  * **`SkipListDynamicSet`**: Skip List implementation, using OSS\n  * **`BSTDynamicSet`**: Binary Search Tree implementation, using OSS\n  * **`RedBlackDynamicSet`**: Red Black Tree implementation, _using Java TreeMap_\n\nTo implement the ADT you need to write the methods that are specified by the\ninterface but map to the underlying implementation. As a simple example, in\nRedBlackDynamicSet the interface method   `retrieve (KeyType k)` will be\nimplemented by calling TreeMap's `get(Object key)`. This is trivial code, but\nit accomplishes an important objective: hiding the details of the\nimplementation behind the interface.\n\nWe suggest that you implement **`RedBlackDynamicSet`** first and get the rest\nof the program running on it, as you already have this implementation. Then\neither write or look for OSS of the other three and add them to the testing.\n\nIn general, you are free to make your own implementation decisions and use the\nbest practices you are capable of. This includes whether or not you use\nComparable or generics, whether you use locator abstractions (such as Goodrich\n& Tamassia's \"position\") rather than returning pointers to data stored, how\nmuch error checking is done, etc. The important thing is that you have\nimplemented the set operations in a general way, and in particular can run the\ntests.\n\nWe suggest that you organize your code in packages, but this is not required.\n\n* * *\n\n## Test Data\n\nFiles including from 100 to 1,000,000 keys, sorted and unsorted, are available\nin [this directory](http://www2.hawaii.edu/~suthers/courses/ics311s14/Projects\n/Project-1/). Each file has one name on each line. There are no quotations or\nother delimiters other than newline. For example:\n    \n    Hugh Moreno \n    Traci Obrien \n    Doug Moore \n    Sammy West \n    Anne Reid \n    Deanna Zimmerman \n    Marcus Waters \n    Clyde Walton \n    Matthew Rios \n    Jacqueline Robertson\n    ...\n    \n\nYour program will take the input file name as an argument at the command line\n(i.e., read into String args). It will read the file into an array until end\nof file is reached. We need to read into an array because we will be randomly\nselecting elements for test runs. You do not need to test the input for any\nproperties: just read each line from the file as if it is a string.\n\n* * *\n\n## Input\n\nYour program will read an argument from the command line: the name of the file\nto be loaded in this test run.\n\nRead the string keys in the specified file into an array. Count the keys as\nyou read them so you know what your _n_ is. You will then use this array to\ndrive a test of each of the Dynamic Set implementations, as discused in the\nnext section.  \n\nScreencasts are available showing how to read in data from a file if you have\nforgotten.\n\n* * *\n\n## Tests\n\nEach `runtest` run will use `System.nanoTime()` (we have found that\n`System.currentTimeMillis()` is not precise enough for O(lg n) algorithms) to\ntime the following operations and report **minimum**, **average** and\n**maximum** times required:\n\n  * `insert(k):` min, avg and max across all keys. (You have all the keys in the array. Insert all of them into the data structure, timing the time it takes for each insertion and keeping track of min, max and sum of times as you go so you can compute average at the end.) \n  \n\n  * `retrieve(k):` Use the `Random` class to produce 10 integers in the range of the data array. Then search for the 10 keys (name strings) indexed by these random numbers, timing each search. Report min, avg and max across these 10 searches.\n  \n\n  * `successor(k):` run 10 trials on the same keys you used in the above retrieval tests. Report min, avg and max. \n  \n\n  * `predecessor(k):` run 10 trials on the same keys you used in the above retrieval tests. Report min, avg and max.\n  \n\n  * `minimum:` Report time to run one call (since there is only one minimum). \n  \n\n  * `maximum:` Report time to run one call (since there is only one maximum). \n  \n\n  * `delete(k):` Last, delete the 10 randomly chosen keys you used above, timing the deletion. Report min, avg and max.\n  \n\n(_Note: _ 10 has been specified as the number of trials because the smallest\ndata set has only 100 elements. 10 trials will give you meaningful results,\nbut in real world testing you'd run many more trials for more reliable and\nconvincing averages. Compared to the cost of programming this test, computer\ntrials are cheap! One approach is to use a percentage, e.g., 10% of the keys\nwould give you 100 trials on the data set of 1000.)\n\n#### Memory:\n\nIf you run out of memory, increase the heap size available to Java using the\n-Xmx argument (or set the corresponding preference in your IDE). Try 1024M, or\nhigher if you have the disk space to spare. (For one of my Java-based research\nprojects, I set this value to 32G, which is _twice_ the physical memory on my\n16G machine: Java will use virtual memory.)\n\nAlso try loading the data directly into one Dynamic Set implementation at a\ntime (skipping the array and the interactive interface), and then destroying\nall references to each dynamic set after you test it so it can be garbage\ncollected before loading into the next one from the file.\n\nIf you are unable to run the test file of one million, turn in up to 100,000\nalong with an explanation.\n\n\n## Output\n\nEach `runtest` run will generate an output table as follows (this layout is\ndesigned to enable you to print the table out incrementally as you go).\n\n    \n    \n    Size: N\n    --------------------------------------------------------------------------------------\n                | Insert   | Retrieve | Pred     | Succ     | Min    | Max    | Delete   |\n    --------------------------------------------------------------------------------------\n    Linked List |          |          |          |          |        |        |          |  \n    --------------------------------------------------------------------------------------\n    Skip List   |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    Binary Tree |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    RB Tree     |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    \n\nThe entry in each cell of this table will be of form\n\n    \n    \n             -------------------\n             | min / avg / max |\n             -------------------\n    \n\n(except `minimum` and `maximum`), each being in nanoseconds. (Resize the table\nas needed to fit.)\n\nYou will need to run the program once for each data set: 100, 1000, 10,000,\n100,0000, and, if possible, 1,000,000; each sorted and unsorted. Thus you will\nhave at least 8 runs (8 tables) if you go up to 100,000, or 10 runs (10\ntables) if you are able to include the 1,000,000. Your reports will be based\non these tests, in comparison to the Θ, big-O, and Ω analyses for the\nalgorithms.\n\nThe TA may use other data sets, which may include reverse sorted items,\nduplicates, etc.\n\n* * *\n\n## Documentation and Report\n\nRequirements stated on the [ Assignments Page](/morea/010.introduction/reading-assignments.html) are included in the requirements\nfor this project. Read that first. Further comments are below.\n\n###  Readme.txt\n\nThe Readme file should be clear and enable the TA to compile and run your\nprogram.\n\n### Operation and Reference Manuals\n\nFor this assignment, the Operation manual is not required _ as long as you put\nthe instructions on how to run the program in your Readme.txt._ The Reference\nmanual can be brief, as the program as a whole is not intended to be\ndistributed for general use. Describe the organization of your program and the\nsource of the implemenatations: anything that you as a programmer maintaining\nthe code would want to know.\n\n### Testing Document (30% of grade)\n\nThe Testing Document will include a summary of your empirical test results and\na discussion of how these results compare to the asymptotic analyses of the\nunderlying data structures.\n\nPut the tables output by your program in the appendix of your testing\ndocument. Then, in the beginning of your testing document discuss whether the\ntimes you got fit the theoretical analyses for the various data structures.\nDid implementations behave as expected as _n_ gets bigger? Does the different\nbetween sorted and unsorted data make sense? How do they compare to each\nother?\n\n**This document is 30% of your grade:** be thorough (showing you have thought carefully about the results in relation to the theoretical analyses and your implementation), yet concise (don't put in a lot of blather to make it look bigger). \n\n* * *\n\n## Grading\n\n### Program: 60%\n\n  * 5% for correct handling of input as specified. This includes reading command line arguments and error handling. \n  * 40% = 10% x 4 for each correct implementation of the Dynamic Set ADT. \n  * 10% for proper organization of the tests (reading data into array; running one ADT at a time and recording values; able to run up to one million). \n  * 5% for providing output table as specified. \n\n### Analysis and Documentation: 40%\n\n  * 30% for adequate analysis of the results (in the Testing Document)\n  * 10% for Readme and Reference manuals\n\n* * *\n\nDan Suthers Last modified: Tue Mar 11 03:27:03 HST 2014\n\n",
 "path"=>"morea//040.adt/experience-1.md"}
</pre>

<h2>/morea/040.adt/experience-2.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis of basic data structures",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-basic-data-structures",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of basic data structures with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-2.html",
 "url"=>"/morea/040.adt/experience-2.html",
 "content"=>
  "## Basic Data Structures\n\n####  5 points\n\n**1\\. (4 pts)** For each of the four types of lists in the following table, what is the _asymptotic worst-case running time_ for each dynamic set operation listed, given a list of length _n_? \n\n  * Assume that __k_ is the key_, _d_ the data associated with the key, and __p_ a position_ in the data structure.\n  * As explained in class, positions abstract and provide encapsulated access to elements of a data structure. We don't have to search for the item if we have a position p for it.\n  * Sorted lists are sorted in _ascending order_.\n  * Predecessor, successor, minimum, and maximum are _with respect to ordering of keys in the set under \"<\"_, not necessarily ordering of the list data structure.\n  * The cells filled in are from the quiz.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">insert(k, d)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(1)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">search(k)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(n)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">delete(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">successor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">minimum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n  \n\n**2\\. (1 pt)** Write a Θ(_n_)-time recursive procedure that, _given an _n_-node binary tree, prints out the key of each node of the tree_ in any order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your recursive procedure takes a `TreeNode` as its argument (the root of the tree or subtree being considered by the recursive call).\n    \n    \n    **\n    printTreeNodes(TreeNode root)\n        if (root != null) {\n    \n    \n    **\n\n\n",
 "path"=>"morea//040.adt/experience-2.md"}
</pre>

<h2>/morea/040.adt/experience-3.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis: Homework",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Practice asymptotic analysis.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-3.html",
 "url"=>"/morea/040.adt/experience-3.html",
 "content"=>
  "## Homework Problems\n\n_There are 6 problems for a total of 20 points. Most of them are easier than\nthey look at first._\n\n_Please consider:_ The homework is really \"worth\" a lot more than 20 points\nbecause exams have similar problems. If you have not practiced with the\nhomeworks, you'll do worse on the exams. So, if you think these problems are\nnot worth the time for the points, think again.\n\n### #1. Peer Credit Assignment\n\n#### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### #2. Proving asymptotic complexity\n\n#### 4 points\n\nUsing the truth-condition definition of big-O, prove that 3_n_2 \\+ 9 =\nO(_n_2).\n\n> The definition is the one that starts with \"_f_(_n_) = O(_g_(_n_)) iff ...\".\nYou will have to choose suitable _c_ and _n_0, plug them into the definition\nin \"...\", and argue that the condition is met.  \nThe 4 points are: (1) identification of _c_ and _n_0 that work; (1) writing\nout the definition, and (2) demonstrating that it is satsified as _n_ grows\nbeyond _n_0 (not just for _n_ = _n_0).\n\n* * *\n\n### #3. Relative growth rates of functions\n\n#### 3 points\n\nContinuing in the style of Tuesday's class exercise, fill in the table for\nthese pairs of functions with \"Yes\" or \"No\" in each empty box. Then, for each\nrow, justify your choice, preferably by showing mathematical relationships\n(e.g., transforming one expression into another, or into expressions that are\nmore easily compared).\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">f(n)</th>\n    <th scope=\"col\">g(n)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">e.</th>\n    <td>4<i>n</i><sup>2</sup></td>\n    <td>4<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">f.</th>\n    <td>2<sup>lg <i>n</i></sup></td>\n    <td>lg<sup>2</sup><i>n</i></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">g.</th>\n    <td>&#8730;<i>n</i></td>\n    <td>n<sup>sin <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n### #4. Complexity of Dynamic Set Operations in List Implementations\n\n#### 2 points\n\nFor each of the four types of lists in the following table, what is the\nasymptotic worst-case running time for `predecessor` and `maximum`?\n\nAssume that _k_ is the key, _d_ the data associated with the key, and _p_ a\nposition in the data structure. This version of the ADT is similar to the\nbook's, but abstracts list elements _x_ to positions (returned by search).\n`predecessor` and `maximum` are with respect to ordering of keys in the set\nunder \"<\", NOT ordering of the data structure. Sorted lists are sorted in\nascending order. This continues your in-class work, which you may want to\nreview for correctness first.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  (continued)\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th colspan=\"5\" scope=\"row\"><i>... others done in class here ... </i></th>\n  </tr>\n  <tr>\n    <th scope=\"row\">predecessor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">maximum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n\n### #5. Tree Traversals\n\n#### 4 points\n\n_In class you wrote a recursive procedure for traversal of a binary tree in\nO(n) time, printing out the keys of the nodes. Here you write two other tree\ntraversal procedures. The first is a variation of what you wrote in class; the\nsecond is on a different kind of tree that you read about pages 248-249 and in\nmy lecture notes and screencast._\n\n**(a)** Write an O(_n_)-time **_non-recursive_** procedure that, given an _n_-node binary tree, prints out the key of each node of the tree in whatever order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your procedure takes a `TreeNode` as its argument (the root of the tree). **_Use a stack as an auxiliary data structure._**\n\n>     printBinaryTreeNodes(TreeNode root) {\n\n\n**(b)** Write an O(_n_)-time procedure that, given an _n_-node rooted tree with **_arbitrary_** number of children using the **_left-child, right-sibling representation_**, prints out the key of each node of the tree in whatever order you wish. Assume that for economy of code we are re-using our `TreeNode` class. The instance variable `left` points to the left child as before, but now `right` points to the right sibling instead of the right child (which is no longer unique). Your procedure takes a `TreeNode` as its argument (the root of the tree). You may choose to use either the recursive or non-recursive approach.\n\n>     printLCRSTreeNodes(TreeNode root) {\n\n\n### #6. A Hybrid Merge/Insertion Sort Algorithm\n\n#### 7 points\n\nAlthough MergeSort runs in Θ(_n_ lg _n_) worst-case time and InsertionSort\nruns in Θ(_n_2) worst-case time, the constant factors in insertion sort\n(including that fact that it can sort in-place) can make it faster in practice\nfor small problem sizes on many machines. Thus, it makes sense to\n**_coarsen_** the leaves of the MergeSort recursion tree by using\nInsertionSort within MergeSort when subproblems become sufficiently small.\n\nConsider a modification to MergeSort in which _n_/_k_ sublists of length _k_\nare sorted using InsertionSort and are then merged using the standard merging\nmechanism, where _k_ is a value to be determined in this problem. In the first\ntwo parts of the problem, we get expressions for the contributions of\nInsertionSort and MergeSort to the total runtime as a function of the input\nsize _n_ and the cutoff point between the algorithms _k_.\n\n**(a - 1pt)** Show that InsertionSort can sort the _n_/_k_ sublists, each of length _k_, in Θ(_nk_) worst-case time. To do this:\n\n  1. write the cost for sorting _k_ items with InsertionSort,\n  2. multiply by how many times you have to do it, and \n  3. show that the expression you get simplifies to Θ(_nk_).\n\n**(b - 3pts)** Show that MergeSort can merge the _n_/_k_ sublists of size _k_ in Θ(_n_ lg (_n_/_k_)) worst-case time. To do this: \n\n  1. draw the recursion tree for the merge (a modification of figure 2.5), \n  2. determine how many elements are merged at each level, \n  3. determine the height of the recursion tree from the _n_/_k_ lists that InsertionSort had already taken care of up to the single list that results at the end, and \n  4. show how you get the final expression Θ(_n_ lg (_n_/_k_)) from these two values. \n\n_**Putting it together:**_ The asymptotic runtime of the hybrid algorithm is\nthe sum of the two expressions above: the cost to sort the _n_/_k_ sublists of\nsize _k_, and the cost to divide and merge them. You have just shown this to\nbe\n\n> Θ(_n__k_ \\+ _n_ lg (_n_/_k_))\n\nIn the second two parts of the question we explore what _k_ can be.\n\n**(c - 2pts)** The bigger we make _k_ the bigger lists InsertionSort has to sort. At some point, its Θ(_n_2) growth will overcome the advantage it has over MergeSort in lower constant overhead. How big can _k_ get before InsertionSort starts slowing things down? Derive a theoretical answer by proving the largest value of _k_ for which the hybrid sort has the same Θ runtime as a standard Θ(_n_ lg _n_) MergeSort. This will be an upper bound on _k_. To do this: \n\n  1. Looking at the expression for the hybrid algorithm runtime Θ(_n__k_ \\+ _n_ lg (_n_/_k_)), identify the upper bound on _k_ expressed as a function of _n_, above which Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) would grow faster than Θ(_n_ lg _n_). _Give the _f_ for _k_ = Θ(_f_(_n_)) and argue for why it is correct._\n  2. Show that this value for _k_ works by substituting it into Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) and showing that the resulting expression simplifies to Θ(_n_ lg _n_). \n\n**(d - 1pt)** Now suppose we have implementations of InsertionSort and MergeSort. How should we choose the optimal value of _k_ to use for these given implementations in practice? \n\n",
 "path"=>"morea//040.adt/experience-3.md"}
</pre>

<h2>/morea/040.adt/module.html</h2>

<pre>Hash
{"title"=>"Abstract data types",
 "published"=>true,
 "morea_id"=>"adt",
 "morea_outcomes"=>["outcome-adt"],
 "morea_readings"=>
  ["reading-screencast-4a",
   "reading-screencast-4b",
   "reading-cormen-10",
   "reading-notes-4"],
 "morea_experiences"=>
  ["experience-asymptotic-basic-data-structures",
   "experience-asymptotic-homework",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/040.adt/logo.png",
 "morea_sort_order"=>40,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/040.adt/module.html",
 "content"=>
  "Stacks, queues, lists, trees, dynamic sets, pointers and objects, rooted trees, asymptotic analysis.\n",
 "path"=>"morea//040.adt/module.md"}
</pre>

<h2>/modules/adt/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Abstract data types",
 "url"=>"/modules/adt/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/adt/index.html"}
</pre>

<h2>/morea/040.adt/outcome.html</h2>

<pre>Hash
{"title"=>
  "Understand definition, implementation, and behavior of abstract data types.",
 "published"=>true,
 "morea_id"=>"outcome-adt",
 "morea_type"=>"outcome",
 "morea_sort_order"=>40,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/040.adt/outcome.html",
 "content"=>
  "Gain further practice in algorithm analysis through examination of stacks, queues, lists, and trees.",
 "path"=>"morea//040.adt/outcome.md"}
</pre>

<h2>/morea/040.adt/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 10 - Elementary Data Structures",
 "published"=>true,
 "morea_id"=>"reading-cormen-10",
 "morea_summary"=>
  "Stacks, queues, linked lists, pointers and objects, rooted trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "21 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-cormen.md"}
</pre>

<h2>/morea/040.adt/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 4 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-4",
 "morea_summary"=>"Introduction to abstract data types",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/reading-notes.html",
 "url"=>"/morea/040.adt/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Stacks \n  2. Queues \n  3. Lists \n  4. First peek at Trees \n  5. Dynamic Set ADT\n\nHere we review some basic Abstract Data Types that organize information in\nuseful ways. This should be review, so will be covered briefly, although some\nnuances of implementation are discussed and we will also do asymptotic\nanalyses of the main operations of implementations.\n\n## Stacks\n\nStacks follow the **Last In, First Out (LIFO)** principle. They are useful\nwhen a problem has goal-subgoal structure, and we need to keep track of higher\nlevel goals or processes when we set them aside to pursue subgoals or sub-\nprocesses (e.g., the run-time stack of a computer operating system, or keeping\ntrack of neighbor vertices yet to be visited when searching a graph).\n\n### Stack ADT\n\nWe start by specifying the desired behavior of stacks before looking at\nimplementations. Here's the Stack ADT written as a simple Java interface:\n\n{% highlight java %}    \n      // ADT that stores and retrieves Objects in a LIFO manner\n      public interface Stack {\n    \n         public Stack( ); \n         // Create an instance of ADT Stack and initialize it to the empty stack.\n    \n         public void push(Object o); \n         // Insert object o at the top of the stack.\n    \n         public Object pop( ); \n         // Remove and return the top (most recently pushed) object on the stack.\n         // Error occurs if the stack is empty. \n     \n         public int size( ); \n         // Return the number of objects in the stack.\n    \n         public boolean isEmpty( ); \n         // Return a boolean indicating whether the stack is empty.\n    \n         public Object top( ); \n         // Return the top (most recently pushed) object on the stack, without \n         // removing it. Error occurs if the stack is empty.\n      }\n\n{% endhighlight %}\n    \n\n**Properties**, given `s` a stack instance:\n\n  1. { `push(_s_,_e_); _s_.top()` } returns value `_e_`\n  2. { `push(_s_,_e_); _s_.pop()` } returns value `_e_` and leaves `_s_` in the same state \n  3. { `_s_ = new(); _s_.isEmpty() } returns true `\n  4. { `push(_s_,_i_); _s_.isEmpty() } returns false `\n  5. if `s.isEmpty()` then `s.top()` is an error, and does not change `s`\n  6. if `s.isEmpty()` then `s.pop()` is an error, and does not change `s`\n  7. if `s.isEmpty()` then `s.size() == 0`\n  8. if `s.size() == _n_` then after ` s.push(o), s.size() == _n_+1`\n  9. if `¬s.isEmpty()` and `s.size() == _n_` then after `s.pop(o), s.size() == _n_-1.`\n\n_What is the relationship of stacks to method execution in the Java Virtual\nMachine?_\n\n_What is the relationship of stacks to recursion?_\n\n### Array Implementation\n\nAssume instance variables (fields) of object array `S` and `int top`. The\nthree essential operations follow. (I am modifying the book's pseudocode\nslightly.)\n\n![](fig/Fig-10-1-a-Stack.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if top == 0 \n      2       return TRUE\n      3     else\n      4       return FALSE\n    \n      void **push**(Object o)\n      1     top = top + 1\n      2     S[top] = o                // what might happen here?\n    \n      Object **pop**( )\n      1     if isEmpty()\n      2       error \"stack underflow\" // or throw new StackException (...) \n      3     else\n      4       top = top - 1\n      5       return S[top+1]         // we comment on this later \n    \n\n_What is the asymptotic complexity of these operations?_\n\nThe potential error in `push` is an implementation concern outside of the\nscope of the _logical_ definition of the stack ADT. How might it be handled?\n\n#### Example\n\nLet's start with this stack:\n\n![](fig/Fig-10-1-a-Stack.jpg)  \n  \n\nPush 17, and then 3:\n\nPop once:\n\n![](fig/Fig-10-1-b-Stack.jpg)\n\n![](fig/Fig-10-1-c-Stack.jpg)\n\n_What is the status of S[top+1] after pop returns? Why might that be a\nproblem?_\n\n####  An Improvement\n\n    \n    \n      Object **pop**( )  // version that dereferences objects for garbage collection\n      1     if isEmpty()\n      2       error \"stack underflow\" \n      3     else\n      4       o = S[top]\n      5       S[top] = null  // don't keep references to objects not really there \n      6       top = top - 1\n      7       return o \n    \n\n* * *\n\n## Queues\n\nQueues operate in a **First In, First Out (FIFO)**, like what the British call\na \"queue\" at the post office or bank. They are also very useful for managing\nprioritization of tasks in computing.\n\n### Queue ADT\n\nAgain, expressed as a simple Java interface:\n\n    \n    \n      public interface **Queue**{\n      // ADT that stores and retrieves Objects in a FIFO manner\n    \n        public **Queue**( ); \n        // Create an instance of ADT Queue and initialize it to the empty queue.\n    \n        public void **enqueue**(Object o); \n        // Insert object o at the rear of the queue.\n    \n        public Object **dequeue**( );\n        // Remove and return the frontmost (least recently queued) object from the queue. \n        // queue. Error occurs if the queue is empty.\n    \n        public int **size**( ); \n        // Return the number of objects in the queue.\n    \n        public boolean **isEmpty**( ); \n        // Return a boolean indicating whether the queue is empty.\n    \n        public Object **front**( ); \n        // Return the front (least recently queued) object in the queue, without \n        // removing it. Error occurs if the queue is empty.\n      }\n    \n\n**Properties** (given `q` a queue instance): are very similar to those for Stack, except for operations where ordering matters (FIFO rather than LIFO). Replace the first two properties for Stack with:\n\n  1. if `q.enqueue(o1) ` occurs before `q.enqueue(o2)` then successive `q.dequeue()` returns `o1` before `o2`\n  2. `q.front() ` returns the least recently enqueued element that has not been dequeued.\n\nThen rewrite the other properties with substitution `{enqueue/push,\ndequeue/pop, front/top}`.\n\n###  Array Implementation\n\nAssume three instance variables (fields): object array `Q`; `int head`\nindexing the next element to dequeue; and `int tail` indexing the next place a\nnew element may be placed.\n\n![](fig/Fig-10-2-a-Queue.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if head == tail\n      2       return TRUE\n      3     else\n      4       return FALSE\n      \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     if tail == length  \n      3       tail = 1           // wrap around\n      4     else\n      5       tail = tail + 1\n      \n      Object **dequeue**( )     \n      1     o = Q[head]\n      2     if head == length\n      3       head = 1\n      4     else \n      5       head = head + 1\n      6     return o\n    \n\nThe queue is full when `head == tail + 1`; an error results if enqueue is\ncalled (again, this is an implementation concern outside the logical\ndefinition of the ADT).\n\n#### Example\n\nBeginning with this Queue:\n\n![](fig/Fig-10-2-a-Queue.jpg)  \n  \n\nEnqueue 17, 3 and 5 (notice wrap-around):\n\nDequeue once:\n\n![](fig/Fig-10-2-b-Queue.jpg)\n\n![](fig/Fig-10-2-c-Queue.jpg)\n\nThe same issue concerning object dereferencing applies.\n\n#### Variation using modular arithmetic\n\nThis version handles dereferencing but does not check for overflow or\nunderflow. It assumes that the array index starts with 0, but can be changed\nfor 1-based indexing.\n\n    \n    \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     tail = (tail + 1) mod length // mod is % in Java \n      \n      Object **dequeue**( )\n      1     o = Q[head]\n      2     Q[head] = null               // allow garbage collection!\n      3     head = (head + 1) mod length \n      4     return o\n    \n\n_What is the asymptotic complexity of these operations?_\n\n### Deques\n\nOne can combine the stack and queue concepts into a double-ended queue (deque)\nthat allows insertion and deletion at both ends. O(1) procedures are possible\nfor all insertion and deletion algorithms.\n\n\n\n* * *\n\n## Lists\n\nLists store objects in linear order. We will assume that list elements have a\n`key` and may have other satellite data.\n\nIn an **unsorted** list, we assume no particular order to the elements (the\norder is arbitrary). In a **sorted** list or set, the elements are ordered by\nkey.\n\nA suitable ADT for lists will be given later, in the form of `DynamicSet`.\n\n### Linked Lists\n\n**Linked lists** use list element objects to hold the data (here in the form of a `key`), and record the linear order using `next` pointers. **Doubly linked lists** also have `prev` pointers.\n\n  * `L.head` points to the first element in the list.\n  * If `x.next == nil` then x is the last element of the list.\n  * If `x.prev == nil` then x is the first element of the list.\n\n_What are the advantages of adding `prev` pointers?_\n\nOur examples will assume List instance variables for `head` and `tail`, and\nListElement instance variables `key`, `next`, and `prev`. (Note: public\ninterfaces for ADTs would probably not expose listElement: see discussion\nunder Dynamic Sets later.)\n\n### Searching\n\nThe procedure for seaching is the same for singly and doubly linked lists:\n\n    \n    \n      ListElement **listSearch**(Key k)\n      1     e = head\n      2     while e ≠ null and e.key ≠ k\n      3       e = e.next \n      4     return e\n    \n\n![](fig/Fig-10-3-a-DLL.jpg)\n\n_What is returned if `k` is not in the list?_\n\n_What is the worst case complexity of this algorithm?_\n\n### Inserting and Deleting\n\nSince you are familiar with singularly linked lists from your previous\nstudies, we'll go direct to doubly linked lists, but recall that with singly\nlinked lists you had to be careful to keep track of the tail end of the list\nthat you had \"snipped off\" during an insertion or deletion. The same applies\nhere, but we also have to manage prev pointers.\n\n    \n    \n      void **listInsert**(ListElement e) // inserts at beginning of list\n      1     e.next = head\n      2     if head ≠ null\n      3       head.prev = e \n      4     head = e\n      5     e.prev = null\n    \n\nInserting 25:  \n![](fig/Fig-10-3-b-DLL.jpg)\n\n    \n    \n      void **listDelete**(ListElement e) // removes from list, wherever it is \n      1     if e.prev ≠ null\n      2       e.prev.next = e.next\n      3     else \n      4       head = e.next \n      5     if e.next ≠ null\n      6       e.next.prev = e.prev\n    \n\nDeleting the element keyed by 4:  \n![](fig/Fig-10-3-c-DLL.jpg)\n\n_What is the worst case complexity of these algorithms?_\n\n_What about garbage collection in listDelete? Same problem as for pop and\ndequeue?_\n\n### Circular DLLs with Sentinels\n\nCLRS discuss adding an extra **sentinel** element that marks the beginning of\nthe list and making the linked list circular so that we don't have to check\nfor null (falling off the end of the list). It also enables us to get to the\nend of the list quickly\n\nSentinels remove the need for a conditional test, but this only speeds up\noperations a small constant, at the cost of an extra listElement object per\nevery list. Their use is more compelling if you often need to go to the end of\nthe list.\n\nFor example, here is the above list as a circular doubly linked list. (`L.nil`\nreferences the sentinel.)\n\n![](fig/Fig-10-4-b-DLL-Sentinel.jpg)\n\n    \n    \n     \n      void **listInsert**(ListElement e) // Sentinel version \n      1     e.next = nil.next           \n      2     nil.next.prev = e \n      3     nil.next = e \n      5     e.prev = nil\n    \n\nInsert 25: ![](fig/Fig-10-4-c-DLL-Sentinel.jpg)\n\n_Let's insert something into the empty list ..._  \n![](fig/Fig-10-4-a-DLL-Sentinel.jpg)\n\n(Left for you to try.)\n\nYou might check your understanding by doing exercises 10.2-1, 10.2-2 and\n10.2-3.\n\n* * *\n\n## Array Representations of Lists\n\nWe generally do not need to be concerned with the topic of this section in\nmodern programming languages, but if you ever have to program in FORTRAN, the\nsection shows how to store objects such as listElement in arrays:\n\n![](fig/Fig-10-5-DLL-Array.jpg)\n\n... and how to manage your own **free list** of available listElements\n(languages like Java and LISP do this automatically, but (cue old fart voice)\n\"when I was your age ...\"). Here is an array with both a DLL and a free list\nembedded in it:\n\n![](fig/Fig-10-7-a-Allocate-Free.jpg)\n\nAfter allocating one free cell to add 7 to the front of the list:\n\nAfter deleting list item 2 at array position 5:\n\n![](fig/Fig-10-7-b-Allocate-Free.jpg)\n\n![](fig/Fig-10-7-c-Allocate-Free.jpg)\n\nOf course, someone has to implement the memory management, and there is a\nlarge literature on methods of **garbage collection**.\n\n* * *\n\n## Binary Trees (A First Look)\n\nTrees in general and binary trees in particular are _hugely_ important data\nstructures in computer science. There are many ways to represent them. A\nlinked represention provides great flexibility and is widely used. In a few\nweeks we'll also see how trees can be embedded in arrays.\n\nAssume that class `BinaryTree` has instance variable `root`, and it consists\nof vertices of class `TreeNode` with instance variables `parent`, `left` and\n`right`, as well as possibly other data.\n\n![](fig/Fig-10-9-Binary-Tree.jpg)\n\nIn a few weeks we will study methods for search, insertion and deletion in\nspecial types of tree, **heaps** and **binary search trees**.\n\n_Do you have any thoughts on what insertion and deletion might involve, in\ngeneral?_\n\n_Exercises:_  \n10.4-2: write an O(n) recursive procedure to visit (e.g., print out) the nodes\nof the tree.  \n10.4-3: write an O(n) non-recursive procedure to visit the nodes of the tree.\nUse a stack.\n\n* * *\n\n## N-ary Trees\n\nWe can represent n-ary trees by providing each node with a fixed number _n_\nchild fields (child1, child2, child3 ... childn). An equivalent approach is\nused for **b-trees,** which are used for efficient disk access.\n\nBut a fixed _n_ is only viable if we can bound the number of children, and can\nbe wasteful of memory if many nodes do not have _n_ children.\n\nAn alternative representation allows each TreeNode to have an arbitrary number\nof children while still using O(n) space.\n\n### Left-Child Right-Sibling Representation\n\nThis implementation has instance variable `root`, but consists of vertices\nthat are instances of a class we'll call LCRSTreeNode with instance variables\n`parent`, `left-child` and `right-sibling`, as well as possibly other data.\n(Alternatively, we can just use TreeNode, but understand `left` to refer to\nthe left-child and `right` to refer to the right sibling.)\n\n![](fig/Fig-10-10-LC-RS-Tree.jpg)\n\nA good practice problem is to write a procedure for visiting (printing out)\nall the nodes of these kinds of trees.\n\n\n\n* * *\n\n## Dynamic Set ADT\n\nAbove we have been reviewing basic data structures for keeping track of\nobjects under specific organizational schemes (e.g., FIFO, LIFO, sequential,\nand hierarchical).\n\nAnother organizational scheme is the **set** or **ordered set**. We often need\nto keep track of a set of objects, query it for membership, and possibly\nmodify the set dynamically. Other operations are also possible if the elements\nof the set are ordered.\n\nThese capabilities can be implemented in different ways. The Dynamic Set ADT\ncaptures the requirements that implementations must meet. Many of the ADTs\n(and their implementations as data structures and algorithms) we will study\ncan be seen as specializations of the Dynamic Set ADT.\n\n### Text's Dynamic Set ADT\n\nThe introduction to Part III of the textbook, page 230, gives this\nspecification:\n\nSEARCH(S; k)\n\n    A query that, given a set S and a key value k, returns a pointer x to an element in S such that x.key = k, or NIL if no such element belongs to S.\n  \nINSERT(S; x)\n\n    A modifying operation that augments the set S with the element pointed to by x. We usually assume that any attributes in element x needed by the set implementation have already been initialized.\n  \nDELETE(S; x)\n\n    A modifying operation that, given a pointer x to an element in the set S, removes x from S. (Note that this operation takes a pointer to an element x, not a key value.)\n  \nMINIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the smallest key.\n  \nMAXIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the largest key.\n  \nSUCCESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next larger element in S, or NIL if x is the maximum element.\n  \nPREDECESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next smaller element in S, or NIL if x is the minimum element.\n\nThere are some issues with this specification, particularly in the use of x.\n\n  * The specification seems to require that the client know about the the internal implementation of the set (\"We usually assume that any attributes in element x needed by the set implementation have already been initialized\").\n  * Alternatively, if the elements are client objects, the set implementation would have to know how to access these to get the key. \n\nA safer specification would give INSERT and DELETE the key k rather than the\nelement x, hiding implementation details and reducing dependencies between\nclient and ADT. This in turn leads to a performance problem, dicussed below,\nbut it can be resolved.\n\n### Encapsulated Dynamic Set ADT\n\nAn encapsulated version of the ADT is given as a Java interface below. It\ncommunicates with clients primarily through keys and associated elements that\nonly the client need understand.\n\n    \n    \n      public interface **DynamicSet** {\n      // ADT that stores and retrieves Objects according to keys of type KeyType\n     \n         public **DynamicSet**( ); \n         // Creates an instance of ADT DynamicSet and initializes it to the empty set.   \n     \n         public void **insert**(KeyType k; Object e); \n         // Inserts element e in the set under key k.\n     \n         public void **delete**(KeyType k); \n         // Given a key k, removes elements indexed by k from the set.\n     \n         public Object **search**(KeyType k); \n         // Finds an Object with key k and returns a pointer to it,\n         // or null if not found. \n     \n         // The following operations apply when there is a total ordering on KeyType   \n     \n         public Object **minimum**( ); \n         // Finds an Object that has the smallest key, and returns a pointer to it,\n         // or null if the set is empty. \n     \n         public Object **maximum**( ); \n         // Finds an Object that has the largest key, and returns a pointer to it,\n         // or null if the set is empty.\n     \n         public Object **successor**(KeyType k); \n         // Finds an Object that has the next larger key in the set above k, \n         // and returns a pointer to it, or null if k is the maximum element.\n     \n         public Object **predecessor**(KeyType k); \n         // Finds an Object that has the next smaller key in the set below k,\n         // and returns a pointer to it, or null if k is the minimum element.\n     }\n    \n\nAs hinted above, we may pay a cost for proper encapsulation. For example,\nsuppose an application must frequently pair `search` and `delete` operations\nto find elements we want to remove. If `search` cannot communicate the\nlocation found in the underlying datastructure to `delete`, then `delete` will\nhave to search again to find what to operate on.\n\nThis inefficiency could be eliminated by abstracting the concept of a\n**position** in a data structure, and passing around position objects that\nhide implementation details. This solution is not discussed here as it is more\nof a software engineering rather than algorithm design and analysis concern:\nsee Goodrich & Tamassia's Algorithms textbook for one approach.\n\n### Alternative Dynamic Set Implementations\n\nLinked lists can be used to support a viable Dynamic Set implementation for\nsmall sets, for example using `listInsert` and `listSearch` to implement\n`insert` and `search`, respectively.\n\nFuture Topics will present Hash Tables, Binary Search Trees, and Red-Black\nTrees as alternative implementations of DynamicSet. You will use some of these\nin your assignments (and often as a working professional), so need to\nunderstand them well.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:30:23 HST 2014  \nImages are from Cormen et al. Introduction to Algorithms, Third Edition.  \n\n",
 "path"=>"morea//040.adt/reading-notes.md"}
</pre>

<h2>/morea/040.adt/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Stacks, queues, and lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-4a",
 "morea_summary"=>"Basic abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=86QY8mBX7Ks",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-screencast-a.md"}
</pre>

<h2>/morea/040.adt/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Trees and dynamic sets",
 "published"=>true,
 "morea_id"=>"reading-screencast-4b",
 "morea_summary"=>"More on abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=eECZ_lKXsHs",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-screencast-b.md"}
</pre>

<h2>/morea/050.probabilistic/experience.html</h2>

<pre>Hash
{"title"=>"Indicator random variables: Homework",
 "published"=>true,
 "morea_id"=>"experience-indicator-random-variables",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about indicator random variables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/050.probabilistic/experience.html",
 "url"=>"/morea/050.probabilistic/experience.html",
 "content"=>
  "# Indicator Random Variable Analysis\n\nUse indicator random variables to compute the expected value of the sum of _n_\nrolls of a fair dice that has _s_ sides. A fair dice can have values from 1 to\n_s_ with equal probability. Do it in these steps, and answer the numbered\nquestions:\n\nDefine the _indicator_ random variable _X__i_ = I {the event of a dice coming\nup with value _i_}, for each _i_ = {1, 2, ... _s_}.\n\n**1.** What is Pr{_X__i_ = 1} for each i? \n\n**2.** What is E[_X__i_]? In other words, the expected value of _X__i_? \n\nDefine the _regular_ random variable _X_ to be the value of a single roll of a\ndice with _s_ sides.\n\n**3.** Write an equation expressing _X_ in terms of _X__i_.   _(Keep in mind that indicator random variables take on values 0 or 1.)_\n\n**4.** Take the expectation of both sides of this equation and solve for E[_X_], the expected value of _X_.   _(Show all steps, like was done in the derivation of the expected number of inversions.)_\n\n**5.** Use the result to write an expression for the expected value of _n_ rolls of an _s_-sided fair dice. \n\n# Additional Activity\n\nIf you finish the above early, this will get you started on future work.\n\nSuppose I assigned the _n_ students in a class randomly to groups, with no\nconstraint on group size, but I decided in advance to have _n_/4 groups.\n\n**6.** Let's pick two students from our class. Call them Michael Jackson and Bruno Mars. What is the probability that Michael and Bruno end up in the same group? Express as a function of _n_. \n\n\n",
 "path"=>"morea//050.probabilistic/experience.md"}
</pre>

<h2>/morea/050.probabilistic/module.html</h2>

<pre>Hash
{"title"=>"Probabilistic Analysis",
 "published"=>true,
 "morea_id"=>"probabilistic",
 "morea_outcomes"=>["outcome-probabilistic"],
 "morea_readings"=>
  ["reading-screencast-5a",
   "reading-screencast-5b",
   "reading-screencast-5c",
   "reading-screencast-5d",
   "reading-screencast-mit-skip-lists",
   "reading-cormen-5",
   "reading-goodrich",
   "reading-notes-5"],
 "morea_experiences"=>
  ["experience-indicator-random-variables", "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/050.probabilistic/logo.png",
 "morea_sort_order"=>50,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/050.probabilistic/module.html",
 "content"=>
  "Indicator random variables, inversions, randomized algorithms, skip lists, the hiring problem.\n",
 "path"=>"morea//050.probabilistic/module.md"}
</pre>

<h2>/modules/probabilistic/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Probabilistic Analysis",
 "url"=>"/modules/probabilistic/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/probabilistic/index.html"}
</pre>

<h2>/morea/050.probabilistic/outcome.html</h2>

<pre>Hash
{"title"=>"Understand probabilistic analysis.",
 "published"=>true,
 "morea_id"=>"outcome-probabilistic",
 "morea_type"=>"outcome",
 "morea_sort_order"=>50,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/050.probabilistic/outcome.html",
 "content"=>
  "Understand when and how to analyze an algorithm based on a distribution of the probability of each case.",
 "path"=>"morea//050.probabilistic/outcome.md"}
</pre>

<h2>/morea/050.probabilistic/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 5 - Probabilistic Analysis and Randomized Algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-5",
 "morea_summary"=>
  "The hiring problem, indicator random variable, randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "16 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-cormen.md"}
</pre>

<h2>/morea/050.probabilistic/reading-goodrich.html</h2>

<pre>Hash
{"title"=>"Skip Lists in Java",
 "published"=>true,
 "morea_id"=>"reading-goodrich",
 "morea_summary"=>
  "Skip lists, from Goodrich and Tamassia's Data Structures and Algorithms in Java",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>
  "https://laulima.hawaii.edu/portal/tool/a8c355d6-b3af-4db8-a856-1713858f8720?panel=Main#",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-goodrich.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-goodrich.md"}
</pre>

<h2>/morea/050.probabilistic/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 5 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-5",
 "morea_summary"=>"Probabilistic analysis and randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/050.probabilistic/reading-notes.html",
 "url"=>"/morea/050.probabilistic/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Probabilistic Analysis\n  2. Randomized Algorithms\n  3. Skip Lists \n\n## Motivations and Preview\n\nInstead of limiting analysis to best case or worst case, analyze all cases\nbased on a distribution of the probability of each case.\n\nWe implicitly used probabilistic analysis when we said that _given random\ninput_ it takes n/2 comparisons _on average_ to find an item in a linked list\nof n items.\n\n### Hiring Problem and Cost\n\nThe book's example is a little strange but illustrates the points well.\nSuppose you are using an employment agency to hire an office assistant.\n\n  * The agency sends you one candidate per day: interview and decide.\n  * Cost to interview is _c__i_ per candidate (fee to agency). \n  * Cost to hire is _c__h_ per candidate (includes firing prior assistant and fee to agency).\n  * _ch_ > _ci_\n  * You always hire the best candidate seen so far.\n    \n    \n      Hire-Assistant(n)\n      1  best = 0                // fictional least qualified candidate\n      2  for i = 1 to n\n      3    interview candidate i // paying cost  _ci_\n      4    if candidate i is better than candidate best\n      5      best = i\n      6      hire candidate i    // paying cost _ch_\n    \n\nWhat is the cost of this strategy?\n\n  * If we interview _n_ candidates and hire _m_ of them, cost is O(_cin_ \\+ _chm_)\n  * We interview all _n_ and _ci_ is small, so we focus on _chm_.\n  * _chm_ varies with each run and depends on interview order\n  * This is a common paradigm: finding the maximum or minimum in a sequence by examining each element, and changing the winner _m_ times.\n\n#### Best Case\n\nIf each candidate is worse than all who came before, we hire one candidate:  \n    O(_cin_ \\+ _ch_) = O(_cin_)\n\n#### Worst Case\n\nIf each candidate is better than all who came before, we hire all _n_ (_m_ =\n_n_):  \n    O(_cin_ \\+ _chn_) = O(_chn_) since _ch_ > _ci_  \nBut this is pessimistic. What happens in the average case?\n\n### Probabilistic Analysis\n\n  * We must know or make assumptions about the distribution of inputs.\n  * The expected cost is over this distribution.\n  * The analysis will give us **_average case_** running time.\n\nWe don't have this information for the Hiring Problem, but suppose we could\nassume that candidates come in random order. Then the analysis can be done by\ncounting permutations:\n\n  * Each ordering of candidates (relative to some reference ordering such as a ranking of the candidates) is equally likely to be any of the n! permutations of the candidates. \n  * In how many do we hire once? twice? three times? ... _n_−1 times? _n_ times?\n  * It depends on how many permutations have zero, one two ... _n_−2 or _n_−1 candidates that come before a better candidate.\n  * This is complicated!\n  * Instead, we can do this analysis with indicator variables (next section)\n\n### Randomized Algorithms\n\nWe might not know the distribution of inputs or be able to model it.\n\nInstead we _randomize_ within the algorithm to _impose_ a distribution on the\ninputs.\n\nAn algorithm is **randomized** if its behavior is determined in parts by\nvalues provided by a random number generator.\n\nThis requires a change in the hiring problem scenario:\n\n  * The employment agency sends us a list of _n_ candidates in advance and lets us choose the interview order.\n  * We choose randomly.\n\nThus we _take control_ of the question of whether the input is randomly\nordered: we _enforce_ random order, so the average case becomes the **_\nexpected value_**.\n\n* * *\n\n## Probabilistic Analysis with Indicator Random Variables\n\nHere we introduce technique for computing the expected value of a random\nvariable, even when there is dependence between variables. Two informal\ndefinitions will get us started:\n\nA **random variable** (e.g., _X_) is a variable that takes on any of a range\nof values according to a probability distribution.\n\nThe **expected value** of a random variable (e.g., E[_X_]) is the average\nvalue we would observe if we sampled the random variable repeatedly.\n\n###  Indicator Random Variables\n\nGiven sample space _S_ and event _A_ in _S_, define the **indicator random\nvariable**\n\n![](fig/indicator-random-variable.jpg)\n\nWe will see that indicator random variables simplify analysis by letting us\nwork with the probability of the values of a random variable separately.\n\n![](fig/lemming.jpg)\n\n#### Lemma 1\n\nFor an event _A_, let _XA_ = I{_A_}. Then the expected value **E[_XA_] =\nPr{_A_}** (the probability of event _A_).\n\n_Proof:_ Let ¬_A_ be the complement of _A_. Then\n\n> E[_XA_] = E[I{_A_}]   (by definition)  \n    = 1*Pr{_A_} + 0*Pr{¬_A_}   (definition of expected value)  \n    = Pr{_A_}. \n\n### Simple Example\n\nWhat is the expected number of heads when flipping a fair coin once?\n\n  * Sample space _S_ is {H, T}\n  * Pr{H} = Pr{T} = 1/2\n  * Define indicator random variable _X_H= I{H}, which counts the number of heads in one flip.\n  * Since Pr{H} = 1/2, Lemma 1 says that E[_X_H] = 1/2. \n\n### Less Simple Example\n\nWhat is the expected number of heads when we flip a fair coin _n_ times?\n\nLet _X_ be a random variable for the number of heads in _n_ flips.\n\nWe could compute E[_X_] = ∑_i_=0,_n__i_ Pr{_X_=_i_} \\-- that is, compute and\nadd the probability of there being 0 heads total, 1 head total, 2 heads total\n... n heads total, as is done in C.37 in the appendix and in my screencast\nlecture [5A](http://youtu.be/MgnvWTZgqcA) \\-- but it's messy!\n\nInstead use indicator random variables to count something we _do_ know the\nprobability for: the probability of getting heads when flipping the coin once:\n\n  * For _i = 1, 2, ... n_ define _Xi_ = I{the _i_th flip results in event H}.\n  * Then _X_ = ∑_i_=1,_n__Xi_.   _ (That is, count the flips individually and add them up.)_\n  * Lemma 1 says that E[_Xi_] = Pr{H} = 1/2 for _i = 1, 2, ... n_.\n  * Expected number of heads is E[_X_] = E[∑_i_=1,_n__Xi_]\n  * _Problem:_ We don't have ∑_i_=1,_n__Xi_; we only have E[_X_1], E[_X_2], ... E[_Xn_].\n  * _Solution:_ **Linearity of expectation** (appendix C): _**expectation of sum equals sum of expectations.**_ Therefore:   \n![](fig/expected-value-n-flips.jpg)\n\nThe key idea: if it's hard to count one way, use indicator random variables to\ncount an easier way!\n\n### Hiring Problem Revisited\n\nAssume that the candidates arrive in random order.\n\nLet _X_ be the random variable for the number of times we hire a new office\nassistant.\n\nDefine indicator random variables _X_1, _X_2, ... _Xn_ where _Xi_ =\nI{candidate _i_ is hired}.\n\nWe will rely on these properties:\n\n  * _X_ = _X_1 \\+ _X_2 \\+ ... + _Xn_   _(The total number of hires is the sum of whether we did each individual hire (1) or not (0).)_\n  * Lemma 1 implies that E[_Xi_] = Pr{candidate _i_ is hired}.\n\nWe need to compute Pr{candidate _i_ is hired}:\n\n  * Candidate _i_ is hired iff candidate _i_ is better than candidates 1, 2, ..., _i_−1\n  * Assumption of random order of arrival means any of the first _i_ candidates are equally likely to be the best one so far. \n  * Thus, Pr{candidate _i_ is the best so far} = 1/i.   \n_(Intuitively, as you add more candidates each candidate is less and less\nlikely to be better than all the ones prior.)_\n\nBy Lemma 1, E[Xi] = _1/i_, a fact that lets us compute E[X]:  \n![](fig/expected-value-hiring-problem.jpg)\n\nThe sum is a harmonic series. From formula A7 in appendix A, the _n_th\n**harmonic number** is:  \n![](fig/A7-harmonic-number.jpg)\n\nThus, the expected hiring cost is O(_ch_ ln _n_), much better than worst case\nO(_chn_)! (ln is the natural log. Formula 3.15 of the text can be used to show\nthat ln _n_ = O(lg _n_.)\n\nWe will see this kind of analysis repeatedly. Its strengths are that it lets\nus count in ways for which we have probabilities (compare to C.37), and that\nit works even when there are dependencies between variables.\n\n### Expected Number of Inversions\n\nThis is Exercise 5.2-5 page 122, for which there is a publicly posted\nsolution. This example shows the great utility of random variables.\n\nLet A[1.. _n_] be an array of _n_ distinct numbers. If _i < j_ and A[_i_] >\nA[_j_], then the pair (_i_, _j_) is called an **inversion** of A (they are\n\"out of order\" with respect to each other). Suppose that the elements of A\nform a uniform random permutation of ⟨1, 2, ... _n_⟩.\n\nWe want to find the expected number of inversions. This has obvious\napplications to analysis of sorting algorithms, as it is a measure of how much\na sequence is \"out of order\". In fact, each iteration of the `while` loop in\ninsertion sort corresponds to the elimination of one inversion (see the posted\nsolution to problem 2-4c).\n\n_If we had to count in terms of whole permutations, figuring out how many\npermutations had 0 inversions, how many had 1, ... etc. (sound familiar? :),\nthat would be a real pain, as there are _n_! permutations of n items. Can\nindicator random variables save us this pain by letting us count something\neasier? _\n\nWe will count the number of inversions directly, without worrying about what\npermutations they occur in:\n\nLet _Xij_, _i < j_, be an indicator random variable for the event where A[_i_] > A[_j_] (they are inverted).\n\nMore precisely, define: X_ij_= I{A[_i_] > A[_j_]} for 1 ≤ _i_ < _j_ ≤ _n_.\n\nPr{X_ij_ = 1} = 1/2 because given two distinct random numbers the probability\nthat the first is bigger than the second is 1/2. _(We don't care where they\nare in a permutation; just that we can easily identify the probabililty that\nthey are out of order. Brilliant in its simplicity!)_\n\nBy Lemma 1, E[X_ij_] = 1/2, and now we are ready to count.\n\nLet X be the random variable denoting the total number of inverted pairs in\nthe array. X is the sum of all X_ij_ that meet the constraint 1 ≤ _i_ < _j_ ≤\n_n_:  \n![](fig/inversions-random-var.jpg)\n\nWe want the expected number of inverted pairs, so take the expectation of both\nsides:  \n![](fig/inversions-expected.jpg)\n\nUsing linearity of expectation, we can simplify this far:  \n![](fig/inversions-solution-a.jpg)\n\nThe fact that our nested summation is choosing 2 things out of _n_ lets us\nwrite this as:  \n![](fig/inversions-solution-b.jpg)\n\nWe can use formula C.2 from the appendix:  \n![](fig/C2-n-choose-k.jpg)\n\nIn screencast [5A](http://youtu.be/MgnvWTZgqcA) I show how to simplify this to\n(_n_(_n_−1))/2, resulting in:\n\n![](fig/inversions-solution-c.jpg)\n\nTherefore the expected number of inverted pairs is _n_(_n_ − 1)/4, or O(_n_2).\n\n* * *\n\n## Randomized Algorithms\n\n![](fig/badguy.jpg)\n\nAbove, we had to _assume_ a distribution of inputs, but we may not have\ncontrol over inputs.\n\nAn \"adversary\" can always mess up our assumptions by giving us worst case\ninputs. (This can be a fictional adversary in making analytic arguments, or it\ncan be a real one ...)\n\nRandomized algorithms foil the adversary by _imposing_ a distribution of\ninputs.\n\nThe modifiation to HIRE-ASSISTANT is trivial: add a line at the beginning that\nrandomizes the list of candidates.\n\n  * The randomization is now in the algorithm, not the input distribution. \n  * Whereas before the algorithm was deterministic, and we could predict the hiring cost for a given input, now we can no longer say what the hiring cost will be.\n  * But our payoff is that no particular input elicits worst-case behavior, even what was worst-case for the deterministic version!\n  * Bad behavior occurs only if we get \"unlucky\" numbers. \n\nHaving done so, the above analysis applies to give us _expected value_ rather\nthan average case.\n\n_Discuss:_ Summarize the difference between probabilistic analysis and\nrandomized algorithms.\n\n####  Randomization Strategies\n\nThere are different ways to randomize algorithms. One way is to randomize the\nordering of the input before we apply the original algorithm (as was suggested\nfor HIRE-ASSISTANT above). A procedure for randomizing an array:\n\n    \n    \n      Randomize-In-Place(A)\n      1  _n_ = A.length\n      2  for _i_ = 1 to _n_\n      3      swap A[_i_] with A[Random(_i_,_n_)]  \n    \n\nThe text offers a proof that this produces a uniform random permutation. It is\nobviously O(_n_).\n\nAnother approach to randomization is to randomize choices made within the\nalgorithm. This is the approach taken by Skip Lists ...\n\n\n\n* * *\n\n## Skip Lists\n\nThis is additional material, not found in your textbook. I introduce Skip\nLists here for three reasons:\n\n  1. They are a natural extension of the linked list implementation of Dynamic Sets, which we covered recently.\n  2. They are a good example of a randomized algorithm, where randomization is used to _improve_ asymptotic behavior from O(_n_) to O(lg _n_).\n  3. They are one candidate implementation to be tested in your homework, the Battle of the Dynamic Sets!\n\nMotivation: Why do we have to search the entire linked list one item at a\ntime? Can't we be more efficient by diving into the middle somewhere?\n\nSkip lists were first described by William Pugh. 1990. Skip lists: a\nprobabilistic alternative to balanced trees. Commun. ACM 33, 6 (June 1990),\n668-676. DOI=10.1145/78973.78977 <http://doi.acm.org/10.1145/78973.78977> or\n<ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf> (actually he had a\nconference paper the year before, but the CACM verion is more accessible).\n\nMy discussion below follows Goodrich & Tamassia (1998), _Data Structures and\nAlgorithms in Java_, first edition, and uses images from their slides. Some\ndetails differ from the edition 4 version of the text.\n\nAn animated applet may be found at\n<http://iamwww.unibe.ch/~wenger/DA/SkipList/>.\n\n### Definition of Skip List\n\nGiven a set _S_ of items with distinct keys, a **skip list** is a series of\nlists _S_0, _S_1, ... _Sh_ (as we shall see, _h_ is the height) such that:\n\n  * Each _S__i_ contains the special keys −∞ and +∞\n  * List _S__h_ contains only −∞ and +∞\n  * List _S_0 contains all of the keys of _S_ in nondecreasing order. \n  * Each list is a subsequence of the previous one: _S_0 ⊇ _S_1 ⊇ ... ⊇ _Sh_. \n![](fig/skip-list.jpg)\n\nWe can implement skip lists with nodes that have `above` and `below` fields as\nwell as the more familiar `prev` and `next`:\n\n![](fig/skip-list-node.jpg)\n\n### Searching a Skip List\n\nAn algorithm for searching for a key _k_ in a skip list as follows:\n\n    \n    \n     SkipSearch(k)\n       Input: search key k\n       Output: Position p in S such that the item at p has the largest key ≤ k.\n       Let p be the topmost-left position of S // _which has at least -∞ and +∞_\n       while below(p) ≠ null do\n           p = below(p)                       // _drop down_\n           while key (next(p)) ≤ k do\n               p = next(p)                    // _scan forward _\n       return p. \n    \n\nExample: Search for 78:\n\n![](fig/skip-list-search.jpg)\n\n### Insertion and Randomization\n\nConstruction of a skip list is randomized:\n\n  * Begin by inserting the new item where it belongs in S0\n  * After inserting an item at level Si, flip a coin to decide whether to also insert it at Si+1.\n  * If Si+1 does not exist, the height of the Skip lists can be increased.   \n_(Alternatively, some policy can be used to limit growth as a function of n,\nbut the probability of a run of \"heads\" diminishes greatly as the number of\nflips increases.)._\n\nThe psuedocode provided by Goodrich & Tamassia uses a helper procedure\n`InsertAfterAbove(p1, p2, k, d)` (left as exercise), which inserts key `k` and\ndata `d` after `p1` and above `p2`. (The following omits code for returning\n\"elements\" not relevant here.)\n\n    \n    \n     SkipInsert(k,d)\n       Input: search key k and data d\n       Instance Variables: s is the start node of the skip list,\n         h is the height of the skip list, and n the number of entries \n       Output: None (list is modified to store d under k)\n       p = SkipSearch(k)\n       q = InsertAfterAbove(p, null, k, d)    // _we are at the bottom level_\n       l = 0                                  // _keeps track of level we are at_ \n       while random(0,1) ≤ 1/2 do\n           l = l + 1\n           if l ≥ h then                      // need to add a level\n               h = h + 1\n               t = next(s)\n               s = insertAfterAbove(null, s, −∞, null)\n               insertAfterAbove(s, t +∞, null) \n           while above(p) == null do\n               p = prev(p)                    // _scan backwards to find tower_\n           p = above(p)                       // _jump higher_\n           q = insertAfterAbove(p, q, k, d)   // _add new item to top of tower_\n       n = n + 1.\n    \n\nFor example, inserting key 15, when the randomization gave two \"heads\",\nforcing growth of _h_ (for simplicity the figure does not include the above\nand below pointers):\n\n![](fig/skip-list-insert.jpg)\n\nDeletion requires finding and removing all occurrences, and removing all but\none empty list if needed. Example for removing key 34:\n\n![](fig/skip-list-delete.jpg)\n\n### Analysis\n\nThe **worst case** performance of skip lists is very bad, but highly unlikely.\nSuppose `random(0,1)` is always less than 1/2. If there were no bound on the\nheight of the data structure, `SkipInsert` would never exit! But this is as\nlikely as an unending sequence of \"heads\" when flipping a fair coin.\n\nIf we do impose a bound _h_ on the height of the list (_h_ can be a function\nof _n_), the worst case is that every item is inserted at every level. Then\nsearching, insertion and deletion is O(_n+h_): you not only have to search a\nlist S0 of _n_ items, as with conventional linked lists; you also have to go\ndown _h_ levels.\n\nBut the probabilistic analysis shows that the expected time is much better.\nThis requires that we find the expected value of the height _h_:\n\n  * Probability that item is stored at level _i_ is the probability of getting _i_ consecutive heads: 1/2_i_.\n  * Probability P_i_ that level _i_ has at least one item: P_i_ ≤ n/2_i_   _(We had n tries at getting i consecutive heads.)_\n  * Probablity that _h_ is larger than _i_ is no more than P_i_.\n  * G&T show that given a constant _c_ > 1, the probability that _h_ is larger than _c_ lg _n_ is at most 1/_n__c_−1 (also worked out in screencast [5A](http://youtu.be/MgnvWTZgqcA)).\n  * For example, for _c_ = 3, the probability that _h_ is larger than 3 lg _n_ is at most 1/_n_2, which gets very small as n grows (e.g., p = .000001 = 1/1000000 for a list of length 1000).\n  * They conclude that the height _h_ is O(lg _n_).\n\nThe search time is proportional to the number of drop-down steps plus the\nnumber of scan-forward steps. The number of drop-down steps is the same as _h_\nor O(lg _n_). So, we need the number of scan-forward steps.\n\nIn their textbook (1998), G&T provide this argument: Let _Xi_ be the number of\nkeys examined scanning forward at level _i_.\n\n![](fig/code-SkipSearch.jpg)\n\n  * After the starting position, each key examined at level _i_ cannot also belong to level _i+1_. _(Why?)_\n  * Thus the probability that any key is counted in _Xi_ is 1/2. _(Why??)_\n  * Therefore the expected value of _Xi_ is the expected number of times we must flip a coin before it comes up heads: 2.\n  * Hence the expected amount of time scanning forward at each level is O(1). _(Wow!)_\n  * Since there are O(lg _n_) levels, the expected search time is O(lg _n_). \n\nIn their slides (2002), they provide this alternative analysis of the number\nof scan-forwards needed. The reasoning is very similar, but based on the odds\nof the list we encounter being constructed:\n\n  * When we scan forward in a list, the destination key does not belong to a higher list.\n  * Therefore, a scan forward is associated with a former coin toss that gave tails (otherwise it would be in the higher list).\n  * The expected number of coin tosses in order to get tails is 2.\n  * Therefore the expected number of scan-forward steps at each level is 2.\n  * Thus the total number of expected scan forward steps (summing across all _h_ or O(lg _n_) levels) is O(lg _n_). \n\nA similar analysis can be applied to insertion and deletion. Thus, skip lists\nare far superior to linked lists in performance.\n\nG&T also show that the expected space requirement is O(n). They leave as an\nexercise the elimination of `above` and `prev` fields: if random(0,1) is\ncalled up to _h_ times in advance of the insertion search, then one can insert\nthe item \"on the way down\" as specified by the results.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:40:33 HST 2014  \nImages of mathematical expressions are from the instructor's material for\nCormen et al. Introduction to Algorithms, Third Edition. Images of skip lists\nare from lecture slides provided by M. Goodrich & R. Tamassia.  \n\n",
 "path"=>"morea//050.probabilistic/reading-notes.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Indicator random variables",
 "published"=>true,
 "morea_id"=>"reading-screencast-5a",
 "morea_summary"=>"Indicator random variables.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=MgnvWTZgqcA",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-a.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Example analysis: inversions",
 "published"=>true,
 "morea_id"=>"reading-screencast-5b",
 "morea_summary"=>"Inversions",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=k-jusEhrRik",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-b.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Randomized algorithms and skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5c",
 "morea_summary"=>"Randomized algorithms and skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=iaKu6jaKPFw",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-c.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Analysis of skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5d",
 "morea_summary"=>"Analysis of skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=oW2VnviRh5M",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-d.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-mit.html</h2>

<pre>Hash
{"title"=>"Skip Lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-skip-lists",
 "morea_summary"=>"Skip Lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec12/",
 "morea_labels"=>["Screencast", "Demaine", "85 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-mit.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-mit.md"}
</pre>

<h2>/morea/060.hash-tables/experience-2.html</h2>

<pre>Hash
{"title"=>"Analysis of data structures",
 "published"=>true,
 "morea_id"=>"experience-data-structures-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Consolidate your understanding of data structures",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/060.hash-tables/experience-2.html",
 "url"=>"/morea/060.hash-tables/experience-2.html",
 "content"=>
  "# Analysis of data structures\n\nThis week's problems focus on ensuring you understand the operations of the\nmain data structures. They are not conceptually difficult but require\ndilligence in execution. Don't be careless or just go on intuition: you should\nactually follow the algorithms and hash functions precisely, or you will go\nwrong.\n\n## Peer Credit Assignment\n\n**(1)** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate across all members (_NOT_ 3 points per member!).\n  * If two members besides yourself were present, you have a total of 4 points to allocate across all members.\n  * If only one other member was present, you have a total of 6 points to allocate across all members.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## Skip Lists\n\n#### 11 points\n\nPlease read carefully; this has multiple parts. Answer the lettered parts in\nboldface.\n\nHere is a skip list, including instance variables **s** (the starting\nposition), **h** (the height: we assume that `h` starts counting from 0), and\n**n** (the number of keys currently stored in the skip list). We won't bother\nto show the data associated with the keys. The double lines in the graphic are\nmeant to remind you that these are doubly linked lists in both the horizontal\nand vertical directions, but you need not draw double lines in your responses.\n\n![](fig/starting-skip-list.jpg)\n\n**(2)** _Trace the path that `SkipSearch(36)` takes, by circling every node that p is assigned to as the `SkipSearch` algorithm executes, starting with s. _\n\nIn the remaining questions, you will how what the skip list shown above looks\nlike after the cumulative operations indicated below, using the pseudocode for\n`SkipInsert` and `SkipSearch` in the lecture notes, and your understanding of\nhow `SkipDelete` works from the class activity.\n\nSince this is a random algorithm and we want everyone to have the same answer\nto facilitate grading, I also give you sequences of random numbers (not all of\nwhich will be used, as I am testing your understanding of when and how the\nrandom numbers are used). Note that the insertion code says ` while\nrandom(0,1) ≤ 1/2 do`...\n\nThe operations are **cumulative:** each step builds on the result of the\nprevious one. Redraw the entire data structure after each operation, and also\nupdate instance variables **s**, **h** and **n** as needed.\n\n**(3)** _Redraw after `SkipInsert(19,data)`_ where `random(0,1)` returns .70, .94, .14, .11, .89, ... \n\n**(4)** _Redraw after `SkipInsert(53,data)`_ where `random(0,1)` returns .14, .51, .22, .68, .45, ... \n\n**(5)** _Redraw after `SkipInsert(32,data)`_ where `random(0,1)` returns .25, .39, .18, .97, .02, ... \n\n**(6)** _ Redraw after `SkipDelete(SkipSearch(15))`. _\n\n_Something to think about (but not graded): What should the list look like if\nwe now deleted 32? There is a choice to be made here that we have not\ndiscussed! _\n\n**(7)** _Now draw what an _empty_ skip list would look like, including s, h and n._\n\n## Hashing\n\n#### 9 points\n\n###  Hashing with Chaining\n\n**(8)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **chaining** and **_h_(_k_) = _k_ mod 11.**_ _Draw this one with a vertical table indexed from 0 to 10, and linked lists going off to the right, as shown._\n\n![](fig/hash-chaining-template.jpg)\n\n###  Open Addressing with Linear Probing\n\n**(9)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **linear probing**_ and\n\n> h'(k) = k mod 11  \nh(k,i) = (h'(k) + i) mod 11\n\n_Draw this and the next result as horizontal arrays indexed from 0 to 10 as\nshown below. Show your work to justify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(10)** _How many re-hashes after collision are required for this set of keys?_ _ Show your work here so we can give partial credit or feedback if warranted._\n\n### Open Addressing with Double Hashing\n\n**(11)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size _m_ = 11 with **double hashing**_ and\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _i__h_2(_k_)) mod 11  \n_h_1(_k_) = _k_ mod 11  \n_h_2(_k_) = 1 + (_k_ mod 7)\n\n_Refer to the code in the book for how i is incremented. Show your work to\njustify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(12)** _How many re-hashes after collision are required for this set of keys?_ _Show your work here so we can give partial credit or feedback if warranted._\n\n**(13)** Open addressing insertion is like an unsuccessful search, as you need to find an empty cell, i.e., to _not_ find the key you are looking for! If the open addressing hash functions above were uniform hashing, _ what is the expected number of probes at the time that the last key (40) was inserted?_ _Hints: At that point, 7 keys are in the table. Use the theorem for unsuccessful search in open addressing._ _Answer with a specific number, not O or Theta._\n\n    \n",
 "path"=>"morea//060.hash-tables/experience-2.md"}
</pre>

<h2>/morea/060.hash-tables/experience.html</h2>

<pre>Hash
{"title"=>"Hash tables: understanding deletion",
 "published"=>true,
 "morea_id"=>"experience-deletion",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about deletion in hash tables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/060.hash-tables/experience.html",
 "url"=>"/morea/060.hash-tables/experience.html",
 "content"=>
  "# Deletion under Open Addressing\n\nFollowing the directions below, write pseudocode for `HASH-DELETE` to delete\nby writing a special `DELETED` value, and modify `HASH-INSERT` to handle the\n`DELETED` value. You will write your response in the exact same form as the\nbook's pseudocode (shown).\n\n**1.** Write `HASH-DELETE` by renaming `HASH-SEARCH` and adding or changing ONE line in the body.\n    \n    \n    Hash-Search (T,k)  // rename to Hash-Delete and remove this line \n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == k\n               return j\n           i = i + 1\n       until T[j] == NIL or i == m\n      return NIL\n    \n\n**2.** Write the new `HASH-INSERT` by changing only ONE line in the following. \n    \n    \n    Hash-Insert (T,k)\n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == NIL\n               T[j] = k\n               return j\n           else i = i + 1\n       until i == m\n       error \"hash table overflow\" \n    \n\n**3.** What is the Θ runtime complexity of the worst case for the modified `HASH-INSERT` and `HASH-DELETE` in terms of _n_ (number of elements stored) and _m_ (table size)? (_ Describe the worst possible situation. Express its runtime with Θ. _) \n\n### Deletion from Skip Lists\n\n![](fig/SgkipList-Small.jpg)\n\n`SkipSearch(k)` returns a pointer `p` to the bottom most element of the tower\nyou want to delete. Suppose this were passed to a method `SkipDelete(p)` −\nnotice it takes `p` as argument, not `k`. In this problem you analyze the\ncomplexity of `SkipDelete`. Before you start the analysis, you should discuss\nhow it works! Then, if you have time, you can write pseudocode for it for\nextra credit.\n\n  * Assume that a skip list node has fields `p.next`, `p.prev`, `p.above` and `p.below`.\n  * Assume that `SkipInsert(k,d)` has built the skip list using `random(0,1)` with cutoff of 0.5. \n  * The delete procedure climbs the tower of linked lists above `p`, doing repeated deletion from each doubly linked list that the element occurs in. \n\n**4.** Assuming a uniform distribution of keys stored in random order, what is the Θ expected case performance of `SkipDelete` in terms of _n_, the number of keys stored in the skiplist?\n\n**5.** What is the probability that a given call to `SkipDelete` would have to _delete at least _k_ nodes_? (_Hint: Think of the probability that SkipInsert builds a \"tower\" of _k_ nodes for a given key._) \n\n#### Extra Credit\n\n**6.** If you finish early, write the recursive pseudocode for `SkipDelete(p)`. \n\nHint:\n    \n    SkipDelete(p)\n    if p ≠ null {\n    // splice out of this doubly linked list\n    // recurse to splice out of list above\n    }\n    \n",
 "path"=>"morea//060.hash-tables/experience.md"}
</pre>

<h2>/morea/060.hash-tables/module.html</h2>

<pre>Hash
{"title"=>"Hash Tables",
 "published"=>true,
 "morea_id"=>"hash-tables",
 "morea_outcomes"=>["outcome-hash-tables"],
 "morea_readings"=>
  ["reading-screencast-6a",
   "reading-screencast-6b",
   "reading-screencast-6c",
   "reading-screencast-6d",
   "reading-cormen-11",
   "reading-notes-6",
   "reading-screencast-mit-hash-tables-1",
   "reading-screencast-mit-hash-tables-2"],
 "morea_experiences"=>
  ["experience-deletion", "experience-data-structures-homework"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/060.hash-tables/logo.png",
 "morea_sort_order"=>60,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/060.hash-tables/module.html",
 "content"=>
  "Analysis of chaining, universal chaining, open addressing, direct address tables, hash functions.\n",
 "path"=>"morea//060.hash-tables/module.md"}
</pre>

<h2>/modules/hash-tables/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Hash Tables",
 "url"=>"/modules/hash-tables/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/hash-tables/index.html"}
</pre>

<h2>/morea/060.hash-tables/outcome.html</h2>

<pre>Hash
{"title"=>"Understand hash tables.",
 "published"=>true,
 "morea_id"=>"outcome-hash-tables",
 "morea_type"=>"outcome",
 "morea_sort_order"=>60,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/060.hash-tables/outcome.html",
 "content"=>
  "Understand the design and run-time characteristics of hash tables and how they compare to related data structures. ",
 "path"=>"morea//060.hash-tables/outcome.md"}
</pre>

<h2>/morea/060.hash-tables/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 11 - Hash tables",
 "published"=>true,
 "morea_id"=>"reading-cormen-11",
 "morea_summary"=>
  "Direct address tables, hash tables, hash functions, and open addressing",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-cormen.md"}
</pre>

<h2>/morea/060.hash-tables/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 6 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-6",
 "morea_summary"=>"Notes on hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/060.hash-tables/reading-notes.html",
 "url"=>"/morea/060.hash-tables/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Motivations and Introduction\n  2. Hash Tables with Chaining \n  3. Hash Functions and Universal Hashing\n  4. Open Addressing Strategies\n\n## Motivations and Introduction\n\nMany applications only need the insert, search and delete operations of a\n[dynamic set](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-04\n.html#dynamicsetadt). Example: symbol table in a compiler.\n\nHash tables are an effective approach. Under reasonable assumptions, they have\nO(1) operations, but they can be Θ(n) worst case\n\n### Direct Addressing\n\nHash tables generalize arrays. Let's look at the idea with arrays first. Given\na key _k_ from a universe _U_ of possible keys, a **direct address table**\nstores and retrieves the element in position _k_ of the array.\n\n![](fig/Fig-11-1-direct-address.jpg)\n\nDirect addressing is applicable when we can allocate an array with one element\nfor every key (i.e., of size |_U_|). It is trivial to implement:\n\n![](fig/pseudocode-direct-address.jpg)\n\nHowever, often the space of possible keys is much larger than the number of\nactual keys we expect, so it would be wasteful of space (and sometimes not\npossible) to allocate an array of size |_U_|.\n\n### Hash Tables and Functions\n\n**Hash tables** are also arrays, but typically of size proportional to the number of keys expected to be stored (rather than to the number of keys). \n\nIf the expected keys K ⊂ U, the Universe of keys, and |K| is substantially\nsmaller than |U|, then hash tables can reduce storage requirements to Θ(|K|).\n\nA **hash function** _h(k)_ maps the larger universe U of external keys to\nindices into the array. Given a table of size _m_ with zero-based indexing (we\nshall see why this is useful):\n\n  * _h_ : U -> {0, 1, ..., _m_-1}.\n  * We say that _k_ **hashes** to slot _h(k)_. \n\n###  Collisions\n\nThe major issue to deal with in designing and implementing hash tables is what\nto do when the hash function maps multiple keys to the same table entry.\n\n![](fig/Fig-11-2-collisions.jpg)\n\nCollisions may or may not happen when |K| ≤ _m_, but definitely happens when\n|K| > _m_. _(Is there any way to avoid this?)_\n\nThere are two major approaches: Chaining (the preferred method) and Open\nAddressing. We'll look at these and also hash function design.\n\n* * *\n\n## Hash Tables with Chaining\n\nA simple resolution: Put elements that hash to the same slot into a linked\nlist. This is called _chaining_ because we chain elements off the slot of the\nhash table.\n\n  * Slot _j_ points to the head of a list of all stored elements that hash to _j_, or to NIL if there are no such elements.\n  * Doubly linked lists may be used when deletions are expected to be frequent.\n  * Sentinels can also be used to simplify the code.\n\n![](fig/Fig-11-3-chaining.jpg)\n\n### Pseudocode for Chaining\n\nImplementation is simple if you already have implemented linked lists:\n\n![](fig/pseudocode-chained-hashing.jpg)\n\n_What are the running times for these algorithms? Which can we state directly,\nand what do we need to know to determine the others?_\n\n### Analysis of Hashing with Chaining\n\nHow long does it take to find an element with a given key, or to determine\nthat there is no such element?\n\n  * Analysis is in terms of the **load factor _α = n/m_**, where \n    * _n_ = number of elements in the table \n    * _m_ = number of slots in the table = number of (possibly empty) linked lists\n  * The load factor α is the average number of elements per linked list. \n  * Can have α < 1; α = 1; or α > 1\\. \n  * Worst case is when all _n_ keys hash to the same slot.   \n_Why? What happens? Θ(_____?)_\n\n  * Average case depends on how well the hash function distributes the keys among the slots. \n\nLet's analyze averge-case performance under the assumption of **simple uniform\nhashing:** any given element is equally likely to hash into any of the _m_\nslots:\n\n  * For _j_ = 0, 1, ..., _m_-1, denote the length of list T[_j_] by _nj_.\n  * Then _n_ = _n0_ \\+ _n1_ \\+ ... + _nm-1_. \n  * Average value of _nj_ is E[_nj_] = α = _n/m_. \n  * Assuming _h(k)_ computed in O(1), so time to search for _k_ depends on length _nh(k)_ of the list T[_h(k)_]. \n\nConsider two cases: Unsuccessful and Successful search. The former analysis is\nsimpler because you always search to the end, but for successful search it\ndepends on where in T[_h(k)_] the element with key _k_ will be found.\n\n#### Unsuccessful Search\n\nSimple uniform hashing means that any key not in the table is equally likely\nto hash to any of the _m_ slots.\n\nWe need to search to end of the list T[_h(k)_]. It has expected length\nE[_nh(k)_] = α = _n/m_.\n\nAdding the time to compute the hash function gives **Θ(1 + α)**. (We leave in\nthe \"1\" term for the initial computation of _h_ since α can be 0, and we don't\nwant to say that the computation takes Θ(0) time).\n\n#### Successful Search\n\nWe assume that the element _x_ being searched for is equally likely to be any\nof the _n_ elements stored in the table.\n\nThe number of elements examined during a successful search for _x_ is 1 more\nthan the number of elements that appear before _x_ in _x_'s list (because we\nhave to search them, and then examine _x_).\n\nThese are the elements inserted _after x_ was inserted (because we insert at\nthe head of the list).\n\nNeed to find on average, over the _n_ elements _x_ in the table, how many\nelements were inserted into _x_'s list after _x_ was inserted. _Lucky we just\nstudied indicator random variables!_\n\nFor _i_ = 1, 2, ..., _n_, let _xi_ be the _i_th element inserted into the\ntable, and let _ki_ = _key_[_xi_].\n\nFor all _i_ and _j_, define the indicator random variable:\n\n> _Xij_ = I{_h(ki)_ = _h(kj)_}.     _(The event that keys _ki_ and _kj_ hash\nto the same slot.)_\n\n![](fig/lemming.jpg)\n\nSimple uniform hashing implies that Pr{_h(ki)_ = _h(kj)_} = 1/_m_ _(Why?)_\n\nTherefore, E[_Xij_] = 1/_m_ by Lemma 1 ([Topic #5](http://www2.hawaii.edu/~sut\nhers/courses/ics311s14/Notes/Topic-05.html#lemma1)).\n\nThe expected number of elements examined in a successful search is those\nelements _j_ that are inserted after the element _i_ of interest _and_ that\nend up in the same linked list (_Xij_):\n\n![](fig/analysis-chaining-1.jpg)\n\n  * The innermost summation is adding up, for all _j_ inserted after _i_ (_j_=_i_+1), those that are in the same hash table (when _Xij_ = 1).\n  * The outermost summation runs this over all _n_ of the keys inserted (indexed by _i_), and finds the average by dividing by _n_.\n\nI fill in some of the implicit steps in the rest of the text's analysis.\nFirst, by linearity of expectation we can move the E in:\n\n![](fig/analysis-chaining-2.jpg)\n\nThat is the crucial move: instead of analyzing the probability of complex\nevents, use indicator random variables to break them down into simple events\nthat we know the probabilities for. In this case we know E[_Xi,j_] (if _you_\ndon't know, ask the lemming above):\n\n![](fig/analysis-chaining-3.jpg)\n\nMultiplying 1/_n_ by the terms inside the summation,\n\n  * For the first term, we get Σ_i_=1,_n_1/_n_, which is just _n_/_n_ or 1\n  * Move 1/_m_ outside the summation of the second term to get 1/_nm_. This leaves Σ_i_=1,_n_(Σ_j_=_i_+1,_n_1), which simplifies as shown below (if you added 1 _n_ times, you would overshoot by _i_).\n![](fig/analysis-chaining-4.jpg)\n\nSplitting the two terms being summed, the first is clearly _n_2, and the\nsecond is the familiar sum of the first _n_ numbers:\n\n![](fig/analysis-chaining-5.jpg)  \n\n![](fig/analysis-chaining-6.jpg)\n\nDistributing the 1/_nm_, we get 1 + (_n_2/_nm_ \\- _n_(_n_+1)/2_nm_   =   1 +\n_n_/_m_ \\- (_n_+1)/2_m_   =   1 + 2_n_/2_m_ \\- (_n_+1)/2_m_, and now we can\ncombine the two fractions:\n\n![](fig/analysis-chaining-7.jpg)\n\nNow we can turn two instances of _n_/_m_ into α with this preparation: 1 +\n(_n_ \\- 1)/2_m_   =   1 + _n_/2_m_ \\- 1/2_m_   =   1 + α/2 - n/2_mn_   =  \n\n![](fig/analysis-chaining-8.jpg)\n\nAdding the time (1) for computing the hash function, the expected total time\nfor a successful search is:\n\n> Θ(2 + α/2 - α/2_n_) = **Θ(1 + α).**\n\nsince the third term vanishes in significance as _n_ grows, and the constants\n2 and 1/2 have Θ(1) growth rate.\n\nThus, **search is an average of Θ(1 + α) in either case.**\n\nIf the number of elements stored _n_ is bounded within a constant factor of\nthe number of slots _m_, i.e., _n_ = O(_m_), then α is a constant, and search\nis O(1) on average.\n\nSince insertion takes O(1) worst case and deletion takes O(1) worst case when\ndoubly linked lists are used, all three operations for hash tables are O(1) on\naverage.\n\n_(I went through that analysis in detail to show again the utility of\nindicator random variables and to demonstrate what is possibly the most\ncrucial fact of this chapter, but we won't do the other analyses in detail.\nWith perserverence you can similarly unpack the other analyses.)_\n\n* * *\n\n## Hash Functions and Universal Hashing\n\nIdeally a hash function satisfies the assumptions of simple uniform hashing.\n\nThis is not possible in practice, since we don't know in advance the\nprobability distribution of the keys, and they may not be drawn independently.\n\nInstead, we use heuristics based on what we know about the domain of the keys\nto create a hash function that performs well.\n\n### Keys as natural numbers\n\nHash functions assume that the keys are natural numbers. When they are not, a\nconversion is needed. Some options:\n\n  * Floating point numbers: If an integer is required, sum the mantissa and exponent, treating them as integers.\n  * Character string: Sum the ASCII or Unicode values of the characters of the string. \n  * Character string: Interpret the string as an integer expressed in some radix notation. (This gives very large integers.) \n\n### Division method\n\nA common hash function: **_h(k)_ = _k_ mod _m_**.  \n_(Why does this potentially produce all legal values, and only legal values?)_\n\n_Advantage:_ Fast, since just one division operation required.\n\n_Disadvantage:_ Need to avoid certain values of _m_, for example:\n\n  * Powers of 2. If _m_ = 2_p_ for integer _p_ then _h(k)_ is the least significant _p_ bits of _k_.   \n(There may be a domain pattern that makes the keys clump together).\n\n  * If character strings are interpreted in radix 2_p_ then _m_ = 2_p_ \\- 1 is a bad choice: permutations of characters hash the same. \n\nA prime number not too close to an exact power of 2 is a good choice for _m_.\n\n### Multiplication method\n\n**_h(k)_ = Floor(_m_(_k_ A mod 1))**, where _k_ A mod 1 = fractional part of _k_A. \n\n  1. Choose a constant A in range 0 < A < 1\\. \n  2. Multiply _k_ by A\n  3. Extract the fractional part of _k_A\n  4. Multiply the fractional part by _m_\n  5. Take the floor of the result. \n\n_Disadvantage:_ Slower than division.\n\n_Advantage:_ The value of _m_ is not critical.\n\nThe book discusses an implementation that we won't get into ...\n\n![](fig/Fig-11-4-multiplication-hashing.jpg)\n\n### Universal Hashing \n\n![](fig/badguy.jpg)\n\nOur malicious adversary is back! He's choosing keys that all hash to the same\nslot, giving worst case behavior and gumming up our servers! What to do?\n\nRandom algorithms to the rescue: randomly choose a different hash function\neach time you construct and use a new hash table.\n\nBut it has to be a good one. Can we define a family of good candidates?\n\nConsider a finite collection _Η_ of hash functions that map universe U of keys\ninto {0, 1, ..., _m_-1}.\n\n_Η_ is **universal** if for each pair of keys _k, l_ ∈ U, where _k ≠ l_, the\nnumber of hash functions _h ∈ Η_ for which _h(k) = h(l)_ is less than or equal\nto _|Η|/m_ (that's the size of _Η_ divided by _m_).\n\nIn other words, with a hash function _h_ chosen randomly from _Η_, the\nprobability of collision between two different keys is no more than _1/m_, the\nchance of a collision when choosing two slots randomly and independently.\n\nUniversal hash functions are good because (proven as Theorem 11.3 in text):\n\n  * If _k_ is not in the table, the expected length E[_nh(k)_] of the list that _k_ hashes to is less than or equal to α. \n  * If _k_ is in the table, the expected length E[_nh(k)_] of the list that holds _k_ is less than or equal to 1 + α. \n\nTherefore, the expected time for search is O(1).\n\nOne candidate for a collection _Η_ of hash functions is:\n\n> _Η_ = {_hab_(_k_) : **_hab_(_k_) = ((_ak + b_) mod _p_) mod _m_)},** where\n_a_ ∈ {1, 2, ..., _p_-1} and _b_ ∈ {0, 1, ..., _p_-1}, where _p_ is prime and\nlarger than the largest key.\n\nDetails in text, including proof that this provides a universal set of hash\nfunctions. Java built in hash functions take care of much of this for you:\nread the Java documentation for details.\n\n* * *\n\n## Open Addressing Strategies\n\nOpen Addressing seeks to avoid the extra storage of linked lists by putting\nall the keys in the hash table itself.\n\nOf course, we need a way to deal with collisions. If a slot is already\noccupied we will apply a systematic strategy for searching for alternative\nslots. This same strategy is used in both insertion and search.\n\n###  Probes and _h_(_k_,_i_)\n\nExamining a slot is called a **probe**. We need to extend the hash function\n_h_ to take the probe number as a second argument, so that _h_ can try\nsomething different on subsequent probes. We count probes from 0 to _m_-1\n(you'll see why later), so the second argument takes on the same values as the\nresult of the function:\n\n> **_h_ : _U_ x {0, 1, ... _m_-1} -> {0, 1, ... _m_-1}**  \n\nWe require that the **probe sequence**\n\n> ⟨ _h_(_k_,0),   _h_(_k_,1)   ...   _h_(_k_,_m_-1) ⟩\n\nbe a permutation of ⟨ 0, 1, ... _m_-1 ⟩. Another way to state this requirement\nis that all the positions are visited.\n\nThere are three possible outcomes to a probe: _k_ is in the slot probed\n(successful search); the slot contains NIL (unsuccessful search); or some\nother key is in the slot (need to continue search).\n\nThe strategy for this continuation is the crux of the problem, but first let's\nlook at the general pseudocode.\n\n### Pseudocode\n\n**Insertion** returns the index of the slot it put the element in _k_, or throws an error if the table is full:\n\n![](fig/pseudocode-open-hash-insert.jpg)\n\n**Search** returns either the index of the slot containing element of key _k_, or NIL if the search is unsuccessful:\n\n![](fig/pseudocode-open-hash-search.jpg)\n\n**Deletion** is a bit complicated. We can't just write NIL into the slot we want to delete. _(Why?)_\n\nInstead, we write a special value DELETED. During search, we treat it as if it\nwere a non-matching key, but insertion treats it as empty and reuses the slot.\n\n_Problem:_ the search time is no longer dependent on α. _(Why?)_\n\nThe ideal is to have **uniform hashing**, where each key is equally likely to\nhave any of the _m_! permutations of ⟨0, 1, ... _m_-1⟩ as its probe sequence.\nBut this is hard to implement: we try to guarantee that the probe sequence is\n_some_ permutation of ⟨0, 1, ... _m_-1⟩.\n\nWe will define the hash functions in terms of ** auxiliary hash functions**\nthat do the initial mapping, and define the primary function in terms of its\n_i_th iterations, where 0 ≤ _i_ < _m_.\n\n### Linear Probing\n\nGiven an **auxiliary hash function _h'_**, the probe sequence starts at\n_h'_(_k_), and continues sequentially through the table:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _i_) mod _m_\n\n_Problem:_ **primary clustering**: sequences of keys with the same _h'_ value\nbuild up long runs of occupied sequences.\n\n### Quadratic Probing\n\nQuadratic probing is attempt to fix this ... instead of reprobing linearly, QP\n\"jumps\" around the table according to a quadratic function of the probe, for\nexample:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _c_1_i_ \\+ _c_2_i_2) mod _m_,  \nwhere _c_1 and _c_2 are constants.\n\n_Problem:_ **secondary clustering**: although primary clusters across\nsequential runs of table positions don't occur, two keys with the same _h'_\nmay still have the same probe sequence, creating clusters that are broken\nacross the same sequence of \"jumps\".\n\n### Double Hashing\n\nA better approach: use two auxiliary hash functions _h1_ and _h_2, where _h_1\ngives the initial probe and _h_2 gives the remaining probes (here you can see\nthat having _i_=0 initially drops out the second hash until it is needed):\n![](fig/Fig-11-5-double-hashing.jpg)\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _ih_2(_k_)) mod _m_.\n\n_h_2(_k_) must be relatively prime to _m_ (relatively prime means they have no\nfactors in common other than 1) to guarantee that the probe sequence is a full\npermutation of ⟨0, 1, ... _m_-1⟩. Two approaches:\n\n  * Choose _m_ to be a power of 2 and _h_2 to always produce an odd number > 1.\n  * Let _m_ be prime and have 1 < _h_2(_k_) < _m_.   \n(The example figure is _h_1(_k_) = _k_ mod 13, and _h_2(_k_) = 1 + (_k_ mod\n11).)\n\nThere are Θ(_m_2) different probe sequences, since each possible combination\nof _h_1(_k_) and _h_2(_k_) gives a different probe sequence. This is an\nimprovement over linear or quadratic hashing.\n\n### Analysis of Open Addressing\n\nThe textbook develops two theorems you will use to compute the expected number\nof probes for unsuccessful and successful search. (These theorems require α <\n1 because an expression 1/1−α is derived and we don't want to divide by 0.)\n\n> **Theorem 11.6:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in an _**unsuccessful**_ search is\nat most **1/(1 − α)**, assuming uniform hashing.\n\n> **Theorem 11.8:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in a _**successful**_ search is at\nmost **(1/α) ln (1/(1 − α))**, assuming uniform hashing and assuming that each\nkey in the table is equally likely to be searched for.\n\nWe leave the proofs for the textbook, but note particularly the \"intuitive\ninterpretation\" in the proof of 11.6 of the **_expected number of probes_** on\npage 275:\n\n> E[_X_]   =   1/(1-α)   =   1   \\+   α   \\+   α2   \\+   α3   \\+   ...\n\nWe always make the first probe (1). With probability α < 1, the first probe\nfinds an occupied slot, so we need to probe a second time (α). With\nprobability α2, the first two slots are occupied, so we need to make a third\nprobe ...\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:14:59 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//060.hash-tables/reading-notes.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Hash tables: introduction and chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6a",
 "morea_summary"=>"Introduction to hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=NMm1BKomO_Y",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-a.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Hash tables: analysis of chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6b",
 "morea_summary"=>"Analysis of chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=ei7T9Y97u0M",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-b.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Hash tables: Hash functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-6c",
 "morea_summary"=>"Examples of hash functions and universal chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=jW4wCfz3DwE",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-c.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Hash table: open addressing",
 "published"=>true,
 "morea_id"=>"reading-screencast-6d",
 "morea_summary"=>
  "Using open addressing to avoid the overhead of linked lists.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=SGGP_HJNUts",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-d.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-mit-1.html</h2>

<pre>Hash
{"title"=>"Hash tables I",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-1",
 "morea_summary"=>"Hash tables and the symbol table problem",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec07/",
 "morea_labels"=>["Screencast", "Leiserson", "77 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-mit-1.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-mit-1.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-mit-2.html</h2>

<pre>Hash
{"title"=>"Hash tables II",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-2",
 "morea_summary"=>"Universal hashing",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec08/",
 "morea_labels"=>["Screencast", "Leiserson", "79 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-mit-2.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-mit-2.md"}
</pre>

<h2>/morea/070.divide-conquer/experience-2.html</h2>

<pre>Hash
{"title"=>"Understanding master method and substitution",
 "published"=>true,
 "morea_id"=>"experience-master-method",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Apply your knowledge of the master method and substitution.",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/070.divide-conquer/experience-2.html",
 "url"=>"/morea/070.divide-conquer/experience-2.html",
 "content"=>
  "### Peer Credit Assignment\n\n**1.** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### Master Method Practice\n\n**2\\. ** (6 pts) Use the Master Method to give tight Θ bounds for the following recurrence relations. Show _a_, _b_, and _f_(_n_). Then explain why it fits one of the cases, if any. If it fits a case, write and _ simplify _ the final Θ result \n\n**a.**   _T_(_n_) = 2_T_(_n_/4) + √_n_  \n\n**b.**   _T_(_n_) = 2_T_(_n_/4) + _n_  \n\n**c.**   _T_(_n_) = 4_T_(_n_/3) + _n_\n\n\n\n### Substitution Method\n\n**3.** (7 pts) Use substitution _as directed below_ to solve \n\n> _T_(_n_) = 4_T_(_n_/3) + _n_\n\nIt is strongly recommended that you read page 85-86 \"Subtleties\" before trying\nthis!\n\n**a.**   First, use the result from the Master Method in 2c as your \"guess\" and inductive assumption. We will do this without Θ and _c_: just use the algebraic portion. Take the proof up to where it fails and say where and why it fails. (See steps below.) \n\n**b.**   Redo the proof, but subtracting _d__n_ from the guess to construct a new guess. This time it should succeed. \n\nAs a reminder, to do a proof by substitution you:\n\n  1. Write the definition _T_(_n_) = 4_T_(_n_/3) + _n_\n  2. Replace the _T_(_n_/3) with your \"guess\" instantiated for _n_/3 (you can do that by the inductive hypothesis because it's smaller than _n_). \n  3. Operating _only_ on the right hand side of the equation, transform that side into the _exact_ form of your \"guess\".\n  4. Determine any constraints on the constants involved. \n  5. Show the base case holds. \n\n\n",
 "path"=>"morea//070.divide-conquer/experience-2.md"}
</pre>

<h2>/morea/070.divide-conquer/experience.html</h2>

<pre>Hash
{"title"=>"Understanding substitution and induction",
 "published"=>true,
 "morea_id"=>"experience-substitution",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about substitution and induction.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/070.divide-conquer/experience.html",
 "url"=>"/morea/070.divide-conquer/experience.html",
 "content"=>
  "## Solving _T_(_n_) = _T_(_n_ − 1) + _n_ with Substitution\n\n### 5 points\n\nUsing substitution and induction, show that the solution of _T_(_n_) = _T_(_n_\n− 1) + _n_ is O(_n_2). In the terminology of CLRS, this is our \"guess\" at the\nsolution to the recurrence relation.\n\n**a.**   Convert the \"guess\" to an equivalent algebraic inequality according to the definition of Big-O (removing the Big-O and adding the implied constant _c_): \n\n> _T_(_n_) =\n\nMake the inductive assumption that what you wrote in (a) holds for all _m_ <\n_n_.\n\nNow you need to use induction and substitution to show that the definition\n_T_(_n_) = _T_(_n_ − 1) + _n_ implies the inequality that you wrote in (a). In\nthe process you will determine the constraints on _c_. We'll do the base case\nlast for your chosen _c_.\n\n**b.**   Write out the definition of T(_n_), and operating _ only_ on the right hand side, substitute in the inductive assumption where appropriate and simplify to isolate the expression (from a) to be proven from the lower order terms:\n\n> _T_(_n_) =\n\n**c.** Use ≤ to get rid of the lower order terms (effectively claiming that what you had above is ≤ _c__n_2), and determine the values of _c_ and _n_ for which the inequality is true:  \n\n> The above is true for all _c_ ≥ ____ and _n_ ≥ ____ because ...\n\n**d.** For the base case, assuming that _T_(0) = 0, show that _T_(1) = _c__n_2 for your choice of _c_:  \n\n> _T_(1) =\n\n### If you finish early:###\n\nTry T(_n_) = 4T(_n_/3) + _n_\n    \n",
 "path"=>"morea//070.divide-conquer/experience.md"}
</pre>

<h2>/morea/070.divide-conquer/module.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"divide-conquer",
 "morea_outcomes"=>
  ["outcome-divide-conquer-recognize", "outcome-divide-conquer-apply"],
 "morea_readings"=>
  ["reading-screencast-7a",
   "reading-screencast-7b",
   "reading-screencast-7c",
   "reading-screencast-7d",
   "reading-cormen-4",
   "reading-notes-7",
   "reading-screencast-mit-divide-conquer"],
 "morea_experiences"=>["experience-substitution", "experience-master-method"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/070.divide-conquer/logo.gif",
 "morea_sort_order"=>70,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/070.divide-conquer/module.html",
 "content"=>
  "Substitution, master method, recurrence relations, induction, maximum subarray problem, Strassen's algorithm. \n",
 "path"=>"morea//070.divide-conquer/module.md"}
</pre>

<h2>/modules/divide-conquer/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Divide and conquer",
 "url"=>"/modules/divide-conquer/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/divide-conquer/index.html"}
</pre>

<h2>/morea/070.divide-conquer/outcome-2.html</h2>

<pre>Hash
{"title"=>"Recognize when divide and conquer is appropriate",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-recognize",
 "morea_type"=>"outcome",
 "morea_sort_order"=>70,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/070.divide-conquer/outcome-2.html",
 "content"=>
  "Be able to recognize when the divide and conquer algorithm is an appropriate algorithm to apply to a programming problem.\n",
 "path"=>"morea//070.divide-conquer/outcome-2.md"}
</pre>

<h2>/morea/070.divide-conquer/outcome.html</h2>

<pre>Hash
{"title"=>"Design divide and conquer algorithms",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-apply",
 "morea_type"=>"outcome",
 "morea_sort_order"=>71,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/070.divide-conquer/outcome.html",
 "content"=>
  "Successfully design and implement divide and conquer algorithms to solve specific programming problems.\n",
 "path"=>"morea//070.divide-conquer/outcome.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 4 - Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-cormen-4",
 "morea_summary"=>
  "The maximum subarray problem, strassen's algorithm for matrix multiplication, substitution method, recursion tree method, and the master method.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "30 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-cormen.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 7 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-7",
 "morea_summary"=>"Notes on divide and conquer",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/070.divide-conquer/reading-notes.html",
 "url"=>"/morea/070.divide-conquer/reading-notes.html",
 "content"=>
  "# Outline\n\n  1. Divide & Conquer and Recurrences\n  2. Substitution Method\n  3. Recursion Trees\n  4. Master Theorem & Method\n\n## Divide & Conquer Strategy\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\n## Recurrences\n\nThe recursive nature of D&C leads to _recurrences_, or functions defined in\nterms of:\n\n  * one or more base cases, and \n  * itself, with smaller arguments.\n\nReviewing from [Topic #2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort), a common (but not the only) form of recurrence\nis as follows. Let _T_(_n_) be the running time on a problem of size _n_.\n\n  * If _n_ is below some constant (often, _n_=1), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each 1/_b_ size of the original. \n  * We pay cost _D_(_n_) to divide the problems and _C_(_n_) to combine the solutions. \n  * We also pay cost _aT_(_n_/_b_) solving subproblems. \n\nThen the total time to solve a problem of size _n_ can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\nSome technical points should be made:\n\n  * Subproblems are not constrained to being a constant fraction of the original problem size, for example, you can have T(_n_) = T(_n-1_) + Θ(1).   _(What's an example algorithm that this describes?)_\n  * There can be other forms, such as multiple ways of dividing the problem. The book gives an example page 91 that divides the problem into 1/3 and 2/3 parts, requiring terms for T(_n/3_) and T(_2n/3_)\n  * Floors and ceilings can easily be removed and don't affect the solution to the recurrence.\n  * Boundary conditions (the smaller order terms that result from base cases) are usually Θ(1) and are omitted from asymptotic analyses, though they do matter for exact solutions.\n  * Recurrences can be inequalities. We use Big-O or Ω as appropriate. \n\nToday we cover three approaches to solving such relations: substitution,\nrecursion tree, and the master method. But first, we look at two examples, one\nof which we have already seen ...\n\n### Merge Sort\n\nSort an array A[_p_ .. _r_] of comparable elements recursivly by divide and\nconquer:\n\n**Divide:**\n    Given A[_p_ .. _r_], split the given array into two subarrays A[_p_ .. _q_] and A[_q_+1 .. _r_] where _q_ is the halfway point of A[_p_ .. _r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure Merge ... \n![](fig/code-merge-sort.jpg)\n\nWe have seen in [Topic 2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort) that this has the following recurrence (please\nreview Topic 2 if you don't see why):\n\n![](fig/recurrence-merge-subarray.jpg)\n\n### Recursive Solution to Maximum Subarray\n\nSuppose you have an array of numbers and need to find the subarray with the\nmaximum sum of elements in the subarray. (The problem is trival unless there\nare negative numbers involved.)\n\n![](fig/Fig-4-3-Maximum-Subarray.jpg)\n\nThe book provides a not very convincing application: there are applications to\ngraphics (2D version: finding the brightest spot in an image).\n\nThe following algorithm is not the fastest known (a linear solution exists),\nbut it illustrates divide and conquer. The solution strategy, given an array\nA[_low_ .. _high_], is:\n\n**Divide**\n     the subarray into two subarrays of equal size as possible by finding the midpoint _mid_ of the subarrays. \n**Conquer**\n    by finding a maximum subarray of A[_low_ .. _mid_] and A[_mid_+1 .. _high_].\n**Combine**\n    by also finding a maximum subarray that crosses the midpoint, and using the best solution of the three (the subarray crossing the midpoint and the best of the solutions in the Conquer step).\n\nThe strategy works because any subarray must lie in one of these three\npositions:\n\n![](fig/Fig-4-4-a-Subarrays.jpg)\n\n####  Pseudocode\n\nRecursion will handle the lower and upper halves. The algorithm relies on a\nhelper to find the crossing subarray. Any maximum subarray crossing the\nmidpoint must include arrays ending at A[_mid_] and starting at A[_mid_+1]:\n\n![](fig/Fig-4-4-b-Crossing.jpg)\n\nTherefore the pseudocode finds the maximum array on each side and adds them\nup:\n\n![](fig/find-max-crossing-subarray.jpg)\n\nIt should be clear that the above is Θ(n). The recursive solution follows.\n\n![](fig/find-maximum-subarray.jpg)\n\n_Check your understanding: Where is the work done? What adds up the values in\nthe left and right subarrays?_\n\n#### Analysis\n\nThe analysis relies on the simplifying assumption that the problem size is a\npower of 2 (the same assumption for merge sort). Let T(_n_) denote the running\ntime of FIND-MAXIMUM-SUBARRAY on a subarray of _n_ elements.\n\n**Base case:**\n    Occurs when _high_ equals _low_, so that _n=1_: it just returns in Θ(1) time. \n  \n**Recursive Case** (when _n_>1):\n    \n\n  * Dividing takes Θ(1) time. \n  * Conquering solves two subproblems, each on an array of n/2 elements: 2T(_n_/2). \n  * Combining calls FIND-MAX-CROSSING-SUBARRAY, which takes Θ(_n_), and some constant tests: Θ(_n_) + Θ(1). \nT(_n_)   =   Θ(1) + 2T(_n_/2) + Θ(_n_) + Θ(1)   =   2T(_n_/2) + Θ(_n_).\n\nThe resulting recurrence is the same as for merge sort:\n\n![](fig/recurrence-merge-subarray.jpg)\n\nSo how do we solve these? We have three methods: Substitution, Recursion\nTrees, and the Master Method.\n\n* * *\n\n##  Substitution Method\n\nDon't you love it when a \"solution method\" starts with ...\n\n  1. Guess the solution!\n  2. Use induction to find any unspecified constants and show that the solution works.\n\nRecursion trees (next section) are one way to guess solutions. Experience\nhelps too. For example, if a problem is divided in half we may expect to see\nlg _n_ behavior.\n\nAs an example, let's solve the recurrence for merge sort and maximum subarray.\nWe'll start with an exact rather than asymptotic version:\n\n![](fig/recurrence-merge-subarray-exact.jpg)\n\n  1. **Guess:**   T(_n_) = _n_ lg _n_ \\+ _n_.  _(Why this guess?)_\n  \n\n  2. **Induction:**\n\n**_Basis:_**\n    _n_ = 1   ⇒   _n_ lg _n_ \\+ _n_   =   1 lg 1 + 1   =   1   =   T(_n_). \n  \n**_Inductive Step:_**\n    Inductive hypothesis is that T(_k_) = _k_ lg _k_ \\+ _k_ for all _k < n_. We'll use _k = n/2_, and show that this implies that T(_n_) = _n_ lg _n_ \\+ _n_. First we start with the definition of T(_n_); then we substitute ...   \n![](fig/proof-merge-subarray-exact.jpg)\n\nInduction would require that we show our solution holds for the boundary\nconditions. This is discussed in the textbook.\n\nNormally we use asymptotic notation rather than exact forms:\n\n  * writing T(_n_) = 2T(_n/2_) + O(_n_),\n  * assuming T(_n_) = O(1) for sufficiently small _n_,\n  * not worrying about boundary or base cases, and\n  * writing solutions in asymptotic notation, e.g., T(_n_) = O(_n_ lg _n_).\n\nIf we want Θ, sometimes we can prove big-O and Ω separately \"squeezing\" the Θ\nresult.\n\nBut be careful when using asymptotic notation. For example, suppose you have\nthe case where _a_=4 and _b_=4 and want to prove T(_n_) = O(_n_) by guessing\nthat T(_n_) ≤ _cn_ and writing:\n\n![](fig/false-proof.jpg)\n\nOne must prove the _exact form_ of the inductive hypothesis, T(_n_) ≤ _cn_.\n\nSee the text for other strategies and pitfalls.\n\nProblems 4.3-1 and 4.3-2 are good practice problems.\n\n* * *\n\n##  Recursion Trees\n\nAlthough recursion trees can be considered a proof format, they are normally\nused to generate guesses that are verified by substitution.\n\n  * Each node represents the cost of a single subproblem in the set of recursive invocations\n  * Sum the costs with each level of the tree to obtain per-level costs\n  * Sum the costs across levels for the total cost.\n\n### A Familiar Example\n\nWe have already seen recursion trees when analyzing the recurrence relations\nfor Merge Sort:\n\n![](fig/recurrence-mergesort-c.jpg)  \n![](fig/recurrence-tree-mergesort-3.jpg)\n\nThe subproblems are of size _n_/20, _n_/21, _n_/22, .... The tree ends when\n_n_/2_p_ = _n_/_n_ = 1, the trivial subproblem of size 1.\n\nThus the height of the tree is the power _p_ to which we have to raise 2\nbefore it becomes _n_, i.e., _p_ = lg _n_. Since we start at 20 there are lg\n_n_ \\+ 1 levels. Multiplying by the work _cn_ at each level, we get _cn_ lg\n_n_ \\+ _cn_ for the total time.\n\n###  A More Complex Example\n\nA more complex example is developed in the textbook for\n\n> T(_n_) = 3T(_n_/4) + Θ(_n_2)\n\nwhich is rewritten (making the implied constant explicit) as\n\n> T(_n_) = 3T(_n_/4)+ _cn_2\n\n![](fig/Fig-4-5-Recursion-Tree-a.jpg) node, T(_n_) = 3T(_n_/4)\n+_cn_2.\n\nWe can develop the recursion tree in steps, as follows. First, we begin the\ntree with its root ![](fig/Fig-4-5-Recursion-Tree-b.jpg)\n\nNow let's branch the tree for the three recursive terms 3T(_n_/4). There are\nthree children nodes with T(_n_/4) as their cost, and we leave the cost _cn_2\nbehind at the root node.\n\nWe repeat this for the subtrees rooted at each of the nodes for T(_n/4_):\nSince each of these costs 3T((_n_/4)/4) +_c_(_n_/4)2, we make three branches,\neach costing T((_n_/4)/4) = T(_n_/16), and leave the _c_(_n_/4)2 terms behind\nat their roots.\n\n![](fig/Fig-4-5-Recursion-Tree-c.jpg)\n\nContinuing this way until we reach the leaf nodes where the recursion ends at\ntrivial subproblems T(1), the tree looks like this:\n\n![](fig/Fig-4-5-Recursion-Tree-d.jpg)\n\nSubproblem size for a node at depth _i_ is _n_/4_i_, so the subproblem size\nreaches _n_ = 1 when (assuming _n_ a power of 4) _n_/4_i_ = 1, or when _i_ =\nlog4_n_.  \nIncluding _i_ = 0, there are log4_n_ \\+ 1 levels. Each level has 3_i_ nodes.  \nSubstituting _i_ = log4_n_ into 3_i_, there are 3log4_n_ nodes in the bottom\nlevel.  \nUsing alogbc = clogba, there are _n_log43 in the bottom level (_not_ _n_, as\nin the previous problem).\n\nAdding up the levels, we get:  \n![](fig/solution-recursion-tree-1.jpg)\n\nIt is easier to solve this summation if we change the equation to an\ninequality and let the summation go to infinity (the terms are decreasing\ngeometrically), allowing us to apply equation A.6 (∑_k_=0,∞_xk_ = 1/1-_x_):  \n![](fig/gsolution-recursion-tree-2.jpg)\n\nAdditional observation: since the root contributes _cn2_, the root dominates\nthe cost of the tree, and the recurrence must also be Ω(_n_2), so we have\nΘ(_n_2).\n\nPlease see the text for an example involving unequal subtrees. For practice,\nexercises 4.4-6 and 4.4-9 have solutions posted on the book's web site.\n\n* * *\n\n##  Master Theorem & Method\n\nIf we have a divide and conquer recurrence of the form\n\n> T(_n_) = _a_T(_n/b_) + _f(n)_  \n  \nwhere _a ≥ 1_, _b > 1_, and _f(n) > 0_ is asymptotically positive,\n\nthen we can apply the **master method**, which is based on the **master\ntheorem**. We compare _f(n)_ to _nlogba_ under asymptotic (in)equality:\n\n**Case 1: _f(n)_ = O(_nlogba - ε_)** for some constant _ε_ > 0.  \n    (That is, _f(n)_ is polynomially smaller than _nlogba_.)  \n    **_Solution:_** T(_n_) = **Θ(_nlogba_).**   \n    Intuitively: the cost is dominated by the leaves.\n\n**Case 2: _f(n)_ = Θ(_nlogba_)**, or more generally (exercise 4.6-2): _f(n)_ = Θ(_nlogba_lg_k__n_), where _k_ ≥ 0.  \n    (That is, _f(n)_ is within a polylog factor of _nlogba_, but not smaller.)  \n    _**Solution:**_ T(_n_) = **Θ(_nlogba_lg_n_),** or T(_n_) = Θ(_nlogba_lg_k+1__n_) in the more general case.  \n    Intuitively: the cost is _nlogba_lg_k_ at each level and there are Θ(lg_n_) levels.\n\n**Case 3: _f(n)_= Ω(_nlogba + ε_)** for some constant _ε_ > 0, and _f(n)_ satisfies the regularity condition _af(n/b) ≤ cf(n)_ for some constant _c<1_ and all sufficiently large _n_.   \n    (That is, _f(n)_ is polynomially greater than _nlogba_.)  \n    _**Solution:**_ T(_n_) = **Θ(_f(n)_)**,  \n    Intuitively: the cost is dominated by the root.\n\nImportant: there are functions that fall between the cases!\n\n### Examples\n\n**T(_n_) = 5T(_n_/2) + Θ(_n_2)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_2\n  * Compare   _n_2   to   _n_log_b__a_ = _n_log25. \n  * log25 - ε = 2 for some constant ε > 0\\. \n  * Case 1: T(_n_) = Θ(_n_lg 5). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 lg _n_\n  * Compare   _n_3 lg _n_   to   _n_log327 = _n_3\n  * Case 2 with _k_ = 1: T(_n_) = Θ(_n_3 lg2 _n_). \n\n**T(_n_) = 5T(_n_/2) + Θ(_n_3)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_3\n  * Compare   _n_3   to   _n_log25\n  * log25 + ε = 3 for some constant ε > 0\\. \n  * Check regularity condition (not necessary since _f_(_n_) is polynomial:  \n_a__f_(_n_/_b_) = 5(_n_/2)3 = 5_n_3/8 ≤ _cn_3 for _c_ = 5/8 < 1\\.\n\n  * Case 3: T(_n_) = Θ(_n_3). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 / lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 / lg _n_\n  * Compare   _n_3/lg _n_   to   _n_log327 = _n_3\n  * Cases 1 and 3 won't work as no ε can adjust the exponent of 3 to account for the 1/lg_n_ = lg−1_n_ factor. Only hope is Case 2. \n  * But _n_3/lg _n_ = _n_3 lg−1_n_ ≠ Θ(_n_3 lg_k_ _n_) for any _k_ ≥ 0\\. \n  * Cannot use master method. \n  * Could try substitution, which requires a guess. Drawing the full recursion tree would be tedious, but perhaps visualizing its general form would help with the guess. \n\n* * *\n\n## Next\n\nChapter 12, Binary Search Trees (entire chapter), to which we can apply divide\n& conquer and use recurrence relations.\n\n* * *\n\nDan Suthers Last modified: Sat Feb 8 02:42:12 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//070.divide-conquer/reading-notes.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Divide and conquer and recurrence relations",
 "published"=>true,
 "morea_id"=>"reading-screencast-7a",
 "morea_summary"=>"Introduction to the divide and conquer algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=W7rChliGE5M",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-a.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: substitution",
 "published"=>true,
 "morea_id"=>"reading-screencast-7b",
 "morea_summary"=>"How to perform substitution.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=X2D80jsS3sY",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-b.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: recursion trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-7c",
 "morea_summary"=>
  "How to generate a guess for the form of the solution to the recurrence.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8F2OvQIlGiU",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-c.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: master method",
 "published"=>true,
 "morea_id"=>"reading-screencast-7d",
 "morea_summary"=>
  "Find solutions to recurrence relations of form T(n) = aT(n/b) + h(n), where a and b are constants, a ≥ 1 and b > 1",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=h4Avr0byu1g",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-d.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-mit.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-divide-conquer",
 "morea_summary"=>
  "Divide and conquer: binary search, powering a number, fibonacci numbers, matrix multiplication",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec03/",
 "morea_labels"=>["Screencast", "Demaine", "68 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-mit.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-mit.md"}
</pre>

<h2>/morea/080.binary-search-trees/experience-2.html</h2>

<pre>Hash
{"title"=>"More reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees some more.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/080.binary-search-trees/experience-2.html",
 "url"=>"/morea/080.binary-search-trees/experience-2.html",
 "content"=>
  "### Binary Search Trees\n\n**4.** (4 pts) Suppose you have some data keys sorted in an array and you want to construct a _**balanced binary search tree**_ from them. Assume a tree node representation `TreeNode` that includes instance variables `key`, `left`, and `right`.\n\n**a.**  Write pseudocode (or Java if you wish) for an algorithm that constructs the tree and returns the root node. (We won't worry about making the enclosing `BinaryTree` class instance.) You will need to use methods for making a new `TreeNode`, and for setting its left and right children.\n\n_Hints: First, identify the array location of the key that would have to be\nthe root of the balanced BST. Now think about how BinarySearch works on the\narray. Which item does it access first in any given subarray it is called\nwith? Using a similar strategy a simple recursive algorithm is possible._\n\n**b.**   What is the Θ cost to construct the tree? How does the expected runtime of BinarySearch on the array compare to the expected runtime of search in the tree you just constructed? \n\n\n\n**5.** (3 pts) In `Tree-Delete` (page 298 or as shown in the web notes), when node _z_ has two children, we arbitrarily decide to replace it with its successor. We could just as well replace it with its predecessor. \n\n**a.**   Rewrite `Tree-Delete` to use the predecessor rather than the successor. Modify this code just as you need to.\n    \n    \n      TREE-DELETE(T, z)\n      1  if z.left == NIL\n      2      TRANSPLANT(T, z, z.right)\n      3  elseif z.right == NIL\n      4     TRANSPLANT(T, z, z.left)\n      5  else y = TREE-MINIMUM(z.right)  // successor\n      6      if y.p != z\n      7          TRANSPLANT(T, y, y.right) \n      8          y.right = z.right\n      9          y.right.p = y\n      10      TRANSPLANT(T, z, y)\n      11      y.left = z.left\n      12      y.left.p = y\n    \n\n**b.**   Some computer scientists have argued that if equal priority were given to replacing the successor and the predecessor to not skew deletions on one side, better performance might result. How might `Tree-Delete` be modified to implement such a strategy? (_Hint:_ think about last week's topics.)\n",
 "path"=>"morea//080.binary-search-trees/experience-2.md"}
</pre>

<h2>/morea/080.binary-search-trees/experience.html</h2>

<pre>Hash
{"title"=>"Reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/080.binary-search-trees/experience.html",
 "url"=>"/morea/080.binary-search-trees/experience.html",
 "content"=>
  "# Reasoning about binary search trees\n\n**1.** Show that if a node in a binary search tree has two children, then its successor Y has no left child and its predecessor has no right child. (_The proofs are symmetric. Hints: Rule out where the successor cannot be to narrow down to where it must be. Draw Pictures!!!_) \n\n> **(a)** Prove by contradiction that the successor Y cannot be an ancestor of\nX, so Y must be in a subtree.  \n**(b)** Identify and prove the subtree of X that successor Y must be in.   \n**(c)** Show by contradiction that successor Y cannot have a left child.  \n**(d)** Indicate how this proof would be changed for predecessor. \n\n**2\\. ** Delete the nodes with keys 10 and 27 from this Binary Search Tree, indicating for each case what \"if/elseif\" block is executed. (_You will need to apply the cases carefully to get this right: refer to the text or web notes. \"Eyeballing\" it may lead to a legal tree that would not result from the code._) \n\n![](fig/BST-for-Class-Problem-small.jpg)\n\n> **(a)** Lines executed in deletion of 10:  \n**(b)** Lines executed in deletion of 27: \n\n![](fig/pseudocode-tree-delete.jpg)",
 "path"=>"morea//080.binary-search-trees/experience.md"}
</pre>

<h2>/morea/080.binary-search-trees/module.html</h2>

<pre>Hash
{"title"=>"Binary Search Trees",
 "published"=>true,
 "morea_id"=>"binary-search-trees",
 "morea_outcomes"=>["outcome-binary-search-trees"],
 "morea_readings"=>
  ["reading-screencast-8a",
   "reading-screencast-8b",
   "reading-screencast-8c",
   "reading-screencast-8d",
   "reading-cormen-12",
   "reading-notes-8"],
 "morea_experiences"=>
  ["experience-binary-search-trees",
   "experience-binary-search-trees-2",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/080.binary-search-trees/logo.svg",
 "morea_sort_order"=>80,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/080.binary-search-trees/module.html",
 "content"=>
  "Queries, insertion, deletion, modification, height, performance. \n",
 "path"=>"morea//080.binary-search-trees/module.md"}
</pre>

<h2>/modules/binary-search-trees/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Binary Search Trees",
 "url"=>"/modules/binary-search-trees/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/binary-search-trees/index.html"}
</pre>

<h2>/morea/080.binary-search-trees/outcome.html</h2>

<pre>Hash
{"title"=>"Understand binary search trees",
 "published"=>true,
 "morea_id"=>"outcome-binary-search-trees",
 "morea_type"=>"outcome",
 "morea_sort_order"=>80,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/080.binary-search-trees/outcome.html",
 "content"=>
  "Understand the properties of binary search trees and how to apply them. \n",
 "path"=>"morea//080.binary-search-trees/outcome.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 12 - Binary search trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-12",
 "morea_summary"=>
  "What is a binary search tree; querying, insertion, and deletion.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "13 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-cormen.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 8 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-8",
 "morea_summary"=>"Notes on binary search trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/080.binary-search-trees/reading-notes.html",
 "url"=>"/morea/080.binary-search-trees/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Trees, Binary Trees, Binary Search Trees \n  2. Querying BSTs\n  3. Modifying BSTs (Insertion and Deletion)\n  4. Performance of BSTs \n\n##  Trees, Binary Trees, Binary Search Trees\n\nFirst, a preliminary look at trees. (This should be review. Some of this\nmaterial is taken from Thomas Standish Data Structure Techniques (1980) and\nGoodrich & Tamassia (1998) as well as the Cormen appendix, but is widely\npublished.)\n\n###  Fundamental Theorem of Free Trees\n\nIf _G_=(_V_,_E_) is a finite graph with _v > 1_ vertices, the following\nproperties are equivalent definitions of a generalized or **free tree**:\n\n  1. _G_ is connected and has no simple cycles. \n  2. _G_ has no simple cycles and has _v-1_ edges (|_E_| = |_V_| - 1)\n  3. _G_ is connected and has _v-1_ edges.\n  4. _G_ is acyclic, and if an edge is added that joins two nonadjacent vertices, exactly one cycle is formed.\n  5. _G_ is connected, but if an edge is deleted, _G_ becomes disconnected. \n  6. Every pair of vertices is connected by exactly one path. \n\nAlthough this is a definition, the theorem is that these definitions are\nequivalent. A classic exercise in basic graph theory is to prove each of these\nstatements using the one before it, and #1 from #6.\n\n#### Comments\n\nWhen we use the term \"tree\" without qualification, we will assume that we mean\na free tree unless the context makes it clear otherwise (e.g., when we are\ndiscussing binary trees).\n\nIn some contexts, _G_=({},{}) and _G_=({_v_},{}) are also treated as trees.\nThese are obvious base cases for recursive algorithms.\n\nA **forest** is a (possibly disconnected) graph, each of whose connected\ncomponents is a tree.\n\nAn **oriented tree** is a directed graph having a designated vertex _r_,\ncalled the **root**, and having exacly one oriented path between the root and\nany vertex _v_ distinct from the root, in which _r_ is the origin of the path\nand _v_ the terminus.\n\nIn some fields (such as social network analysis), the word \"node\" is used\ninterchangeably with \"vertex\". I use \"vertex\" in these notes but may slip into\n\"node\" in my recorded lectures or in class.\n\n![](fig/diagram-tree-heights-tall.jpg)\n\n### Binary Trees\n\nA **binary tree** is a finite set of vertices that is either empty or consists\nof a vertex called the root, together with two binary subtrees that are\ndisjoint from each other and from the root and are called the **left** and\n**right subtrees**.\n\nA **full binary tree ** is a binary tree in which each vertex either is a leaf\nor has exactly two nonempty descendants. In a full binary tree of height _h_:\n\n  1. number of leaves = (number internal vertices) + 1.\n  2. number leaves is at least _h_+1 _(first example figure)_ and at most 2_h_ _(second example figure)_.\n\n![](fig/diagram-tree-heights-wide.jpg)\n\n  3. number internal vertices is at least _h_ _(first example)_ and at most 2_h_-1 _(second example)_.\n  4. Total number of vertices (summing the last two results) is at least 2_h_+1 _(first example)_ and at most 2_h+1_-1 _(second example)_.\n  5. Height _h_ is at least lg(_n_+1)-1 _(second example)_ and at most (_n_-1)/2 _(first example)_\n\nA **complete binary tree ** is full binary tree in which all leaves have the\nsame depth and all internal vertices have degree 2 _(e.g., second example\nabove)_.\n\n(_Note:_ some earlier texts allow the last level of a \"complete\" tree to be\nincomplete! They are defined as binary trees with leaves on at most two\nadjacent levels _l-1_ and _l_ and in which the leaves at the bottommost level\n_l_ lie in the leftmost positions of _l_.)\n\n### Binary Search Trees (BSTs)\n\nA **binary search tree** (BST) is a binary tree that satisfies the **binary\nsearch tree property:**\n\n  * if _y_ is in the left subtree of _x_ then _y.key ≤ x.key_. \n  * if _y_ is in the right subtree of _x_ then _y.key ≥ x.key_. \n\nBSTs provide a useful implementation of the Dynamic Set ADT, as they support\nmost of the operations efficiently (as will be seen).\n\nTwo examples on the same data:  \n![](fig/Fig-12-1a-balanced.jpg) ![](fig/Fig-12-1b-\nunbalanced.jpg)\n\n_Could we just just say \"if y is the **left child** of x then y.key ≤ x.key,\netc., and rely on transitivity? What would go wrong?_\n\nImplementations of BSTs include a _root_ instance variable. Implementations of\nBST vertices usually include fields for the _key_, _left_ and _right_\nchildren, and the _parent_.\n\n* * *\n\n## Querying Binary Search Trees\n\nNote that all of the algorithms described here are given a tree vertex as a\nstarting point. Thus, they can be applied to any subtree of the tree as well\nas the full tree.\n\n### Traversing Trees\n\nTraversals of the tree \"visit\" (e.g., print or otherwise operate on) each\nvertex of the tree exactly once, in some systematic order. This order can be\n**Inorder**, **Preorder**, or **Postorder**, according to when a vertex is\nvisited relative to its children. Here is the code for inorder:\n\n![](fig/pseudocode-inorder-tree-walk.jpg)\n\n_Quick exercise: Do INORDER-TREE-WALK on this tree ... in what order are the\nkeys printed?_\n\n![](fig/example-BST-simple.jpg)\n\n_Quick exercise: How would you define Preorder traversal? Postorder\ntraversal?_\n\nTraversals can be done on any tree, not just binary search trees. For example,\ntraversal of an expression tree will produce preorder, inorder or postorder\nversions of the expressions.\n\n#### Time to Traverse a BST\n\n**Time:** Traversals (INORDER-TREE-WALK and its preorder and postorder variations) take _T_(_n_) = Θ(_n_) time for a tree with _n_ vertices, because we visit and print each vertex once, with constant cost associated with moving between vertices and printing them. More formally, we can prove as follows:\n\n_T_(_n_) = Ω(_n_) since these traversals must visit all _n_ vertices of the\ntree.\n\n_T_(_n_) = O(_n_) can be shown by substitution. First the base case of the\nrecurrence relation captures the work done for the test _x_ ≠ NIL:\n\n> _T_(0) = _c_ for some constant c > 0\n\nTo obtain the recurrence relation for _n_ > 0, suppose the traversal is called\non a vertex _x_ with _k_ vertices in the left subtree and _n_−_k_−1 vertices\nin the right subtree, and that it takes constant time _d_ > 0 to execute the\nbody of the traversal exclusive of recursive calls. Then the time is bounded\nby\n\n> _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_.\n\nWe now need to \"guess\" the inductive hypothesis to prove. The \"guess\" that\nCLRS use is _T_(_n_) ≤ (_c_ \\+ _d_)_n_ \\+ _c_, which is clearly O(_n_). It's\nless clear how they got this guess. As discussed in Chapter 4, section 4\n(especially subsection \"Subtleties\" page 85-86), one must prove the exact form\nof the inductive hypothesis, and sometimes you can get a better guess by\nobserving how your original attempt at the proof fails. Perhaps this is what\nthey did. We'll skip the failure part and go directly to proving their\nhypothesis by substitution (showing two steps skipped over in the book):\n\n> **_Inductive hypothesis:_** Suppose that _T_(_m_) ≤ (_c_ \\+ _d_)_m_ \\+ _c_\nfor all _m_ < _n_  \n  \n**_Base Case:_** (_c_ \\+ _d_)0 + _c_ = _c_ = _T_(0) as defined above.  \n  \n**_Inductive Proof:_**  \n   _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_\n_by definition_  \n           = ((_c_ \\+ _d_)_k_ \\+ _c_) + ((_c_ \\+ _d_)(_n_−_k_−1) + _c_) + _d_    _substiting inductive hypothesis for values < n_   \n            = ((_c_ \\+ _d_)(_k_ \\+ _n_ − _k_ − 1) + _c_ \\+ _c_ \\+ _d_             _collecting factors _   \n            = ((_c_ \\+ _d_)(_n_ − 1) + _c_ \\+ _c_ \\+ _d_                         _simplifying _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_ − (_c_ \\+ _d_) + _c_ \\+ _d_                   _multiplying out _n_−1 and rearranging _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_.                                            _the last terms cancel._\n\n### Searching for an Element in a BST\n\nHere are two implementations of the dynamic set operation `search`:\n\n![](fig/pseudocode-recursive-tree-search.jpg) ![](fig\n/pseudocode-iterative-tree-search.jpg)\n\n_Quick exercise: Do TREE-SEARCH for D and C on this tree ... _\n\n![](fig/example-BST-simple.jpg)\n\nFor now, we will characterize the run time of the remaining algorithms in\nterms of _h_, the height of the tree. Then we will consider what _h_ can be as\na function of _n_, the number of vertices in the tree.\n\n**Time:** Both of the algorithms visit vertices on a downwards path from the root to the vertex sought. In the worst case, the leaf with the longest path from the root is reached, examining _h_+1 vertices (_h_ is the height of the tree, so traversing the longest path must traverse _h_ edges, and _h_ edges connect _h_+1 vertices). Comparisons and movements to the chosen child vertex are O(1), so the algorithm is O(_h_). (_Why don't we say Θ?_) \n\n### Finding the Minimum and Maximum Element\n\nThe BST property guarantees that:\n\n  * The minimum key of a BST is located at the leftmost vertex.\n  * The maximum key of a BST is located at the rightmost vertex.\n\n_(Why?)_ This leads to simple implementations:\n\n![](fig/pseudocode-tree-min-max.jpg) ![](fig/example-\nBST-simple.jpg)\n\n**Time:** Both procedures visit vertices on a path from the root to a leaf. Visits are O(1), so again this algorithm is O(_h_).\n\n###  Finding the Successor or Predecessor of an Element\n\nAssuming that all keys are distinct, the successor of a vertex _x_ is the\nvertex _y_ such that _y.key_ is the smallest _key_ > _x.key_. If _x_ has the\nlargest key in the BST, we define the successor to be NIL.\n\nWe can find _x_'s successor based entirely on the tree structure (no key\ncomparison is needed). There are two cases:\n\n  1. **If vertex _x_ has a non-empty right subtree, then _x_'s successor is the minimum in its right subtree.** _(Why?)_\n  2. **If vertex _x_ has an empty right subtree, then _y_ is the lowest ancestor of _x_ whose left child is also an ancestor of _x_.**   _To see this, consider these facts: _\n    * If _y_ is the successor of _x_ then _x_ is the predecessor of _y_, so _x_ is the maximum in _y_'s left subtree _(flip the reasoning of your answer to the last question)_.\n    * Moving from _x_ to the left up the tree (up through right children) reaches vertices with smaller keys, which must also be in this left subtree. \n![](fig/pseudocode-tree-successor.jpg)\n\n_Exercise: Write the pseudocode for TREE-PREDECESSOR_\n\nLet's trace the min, max, successor (15, 13, 6, 4), and predecessor (6)\noperations:\n\n![](fig/Fig-12-2-example-BST.jpg)\n\n**Time:** The algorithms visit notes on a path down or up the tree, with O(1) operations at each visit and a maximum of _h+1_ visitations. Thus these algorithms are O(_h_). \n\n_Exercise: Show that if a vertex in a BST has two children, then its succesor\nhas no left child and its predecessor has no right child._\n\n* * *\n\n##  Modifying Binary Search Trees\n\nThe key point is that the BST property must be sustained. This is more\nstraightforward with insertion (as we can add a vertex at a leaf position)\nthan with deletion (where an internal vertex may be deleted).\n\n###  Insertion\n\nThe algorithm assumes that the vertex _z_ to be inserted has been initialized\nwith _z.key_ = _v_ and _z.left_ = _z.right_ = NIL.\n\nThe strategy is to conduct a search (as in tree search) with pointer _x_, but\nto sustain a **trailing pointer** _y_ to keep track of the parent of _x_. When\n_x_ drops off the bottom of the tree (becomes NIL), it will be appropriate to\ninsert _z_ as a child of _y_.\n\nComment on variable naming: I would have preferred that they call _x_\nsomething like `leading` and _y_ `trailing`.\n\n![](fig/pseudocode-tree-insert.jpg)\n\nTry `TREE-INSERT(T,C)`:\n\n![](fig/example-BST-simple.jpg)\n\n**Time:** The same as TREE-SEARCH, as there are just a few additional lines of O(1) pointer manipulation at the end.\n\n_Discuss: How would you use TREE-INSERT and INORDER-TREE-WALK to sort a set of\nnumbers?_  \n_Think about at home: How would you prove its time complexity?_\n\n###  Deletion\n\nDeletion is more complex, as the vertex _z_ to be deleted may or may not have\nchildren. We can think of this in terms of three cases:\n\n  1. If _z_ has no children, we can just remove it (by setting _z_'s parent's pointer to NIL). \n  2. If _z_ has just one child _c_, then make _c_ take _z_'s position in the tree, updating _z_'s parent to point to _c_ and \"dragging\" _c_'s subtree along.\n  3. If _z_ has two children, find _z_'s successor _y_ and replace _z_ by _y_ in the tree (noting that _y_ has no left child): \n    * If _y_ is _z_'s right child, then replace _z_ by _y_ (including updating _z_'s parent to point to _y_, and _y_ to point to _z_'s left child) and we are done. \n    * Otherwise _y_ is further down in _z_'s right subtree (and again has no left child): \n      1. Replace _y_ with its own right child. \n      2. The rest of _z_'s right subtree becomes _y_'s new right subtree.\n      3. _z_'s left subtree becomes _y_'s new left subtree.\n      4. Make _z_'s parent point to _y_.\n\nThe code organizes the cases differently to simplify testing and make use of a\ncommon procedure for moving subtrees around. This procedure replaces the\nsubtree rooted at _u_ with the subtree rooted at _v_.\n\n  * It makes _u_'s parent become _v_'s parent (lines 6-7), unless _u_ is the root, in which case it makes _v_ the root (lines 1-2).\n  * _v_ replaces _u_ as _u_'s parent's left or right child (lines 3-5).\n  * It does not update _v.left_ or _v.right_, leaving that up to the caller. \n![](fig/pseudocode-transplant.jpg)\n\n_(If we have time, draw a few examples.)_\n\nHere are the four actual cases used in the main algorithm TREE-DELETE(T,_z_):\n\n![](fig/Fig-12-4-a-no-left-child.jpg)\n\n#### No left child (and possibly no children):\n\nIf _z_ has no left child, replace _z_ by its right child (which may or may not\nbe NIL). This handles case 1 and half of case 2 in the conceptual breakdown\nabove. (Lines 1-2 of final algorithm.)\n\n![](fig/Fig-12-4-b-no-right-child.jpg)\n\n#### No right child (and has left child):\n\nIf _z_ has just one child, and that is its left child, then replace _z_ by its\nleft child. This handles the rest of case 2 in the conceptual breakdown above.\n(Lines 3-4.)\n\nNow we just have to deal with the case where both children are present. Find\n_z_'s successor (line 5), which must lie in _z_'s right subtree and have no\nleft child (_why?_). Handling depends on whether or not the successor is\nimmediately referenced by _z_:\n\n![](fig/Fig-12-4-c-successor-is-child.jpg)\n\n#### Successor is child:\n\nIf successor _y_ is _z_'s right child (line 6), replace _z_ by _y_, \"pulling\nup\" _y_'s right subtree. The left subtree of _y_ is empty so we can make _z_'s\nformer left subtree _l_ be _y_'s new left subtree. (Lines 10-12.)\n\n#### Successor is not child:\n\nOtherwise, _y_ is within _z_'s right subtree rooted at _r _but is not the root\nof this subtree (_y≠r_).\n\n  1. Replace _y_ by its own right child _x_. (Line 7.)\n  2. Set _y_ to be _r_'s parent. (Line 8-9.)\n  3. Then let _y_ take _z_'s place with respect to _z_'s parent __ and left child _l_. (Lines 10-12.)\n![](fig/Fig-12-4-d-successor-not-child.jpg)\n\nNow we are ready for the full algorithm:\n\n![](fig/pseudocode-tree-delete.jpg)\n\nThe last three lines excecute whenever _z_ has two children (the last two\ncases above).\n\nLet's try `TREE-DELETE(T,_x_)` on _x=_ I, G, K, and B:\n\n![](fig/example-BST-to-delete.jpg)\n\n**Time:** Everything is O(1) except for a call to TREE-MINIMUM, which is O(_h_), so TREE-DELETE is O(_h_) on a tree of height _h_. \n\nThe above algorithm fixes a problem with some published algorithms, including\nthe first two editions of the book. Those versions copy data from one vertex\nto another to avoid a tree manipulation. If other program components maintain\npointers to tree vertices (or their positions in Goodrich & Tamassia's\napproach), this could invalidate their pointers. The present version\nguarantees that a call to TREE-DELETE(T, _z_) deletes exactly and only vertex\n_z_.\n\nAn animation is available at\n<http://www.csc.liv.ac.uk/~ullrich/COMP102/applets/bstree/> (The code shown\nprobably has the flaw discussed above.)\n\n* * *\n\n##  Performance of Binary Search Trees\n\nWe have been saying that the asympotic runtime of the various BST operations\n(except traversal) are all O(lg _h_), where _h_ is the height of the tree. But\n_h_ is usually hidden from the user of the ADT implementation and we are more\nconcerned with the runtime as a function of _n_, our input size. So, what is\n_h_ as a function of _n_?\n\nWe know that in the worst case, _h_ = O(_n_) (when the tree degenerates to a\nlinear chain). Is this the expected case? Can we do anything to guarantee\nbetter performance? These two questions are addressed below.\n\n###  Expected height of randomly built binary search trees\n\nThe textbook has a proof in section 12.4 that **the expected height of a\nrandomly build binary search tree on _n_ distinct keys is O(lg _n_).**\n\nWe are not covering the proof (and you are not expected to know it), but I\nrecommend reading it, as the proof elegantly combines many of the ideas we\nhave been developing, including indicator random variables and recurrences.\n(They take a huge step at the end: can you figure out how the log of the last\npolynomial expression simplifies to O(lg _n_)?)\n\nAn alternative proof provided by Knuth (Art of Computer Programming Vol. III,\n1973, p 247), and also summarized by Standish, is based on average path\nlengths in the tree. It shows that about 1.386 lg _n_ comparisons are needed:\n**the average tree is about 38.6% worse than the best possible tree in number\nof comparisons required for average search**.\n\nSurprisingly, _analysts have not yet been able to get clear results when\nrandom deletions are also included_.\n\n### Balanced Trees\n\nGiven the full set of keys in advance, it is possible to build an optimally\nbalanced BST for those keys (guaranteed to be lg _n_ height). See section 15.5\nof the Cormen et al. text.\n\nIf we don't know the keys in advance, many clever methods exist to keep trees\nbalanced, or balanced within a constant factor of optimal, by performing\nmanipulations to re-balance after insertions (AVL trees, Red-Black Trees), or\nafter all operations (in the case of splay trees). We cover Red-Black Trees in\ntwo weeks ([Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html)),\nafter a diversion to heaps (which have tree-like structure) and sorting.\n\n* * *\n\n## Next\n\nIn [Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html) we\nlook at how a special kind of tree, a Heap, can be embedded in an array and\nused to implement a sorting algorithm and priority queues.\n\nAfter a brief diversion to look at other sorting algorithms, we will return to\nother kinds of trees, in particular special kinds of binary search trees that\nare kept balanced to guarantee O(lg _n_) performance, in [Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:15:30 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//080.binary-search-trees/reading-notes.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to binary search trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-8a",
 "morea_summary"=>"Basic qualitative facts about BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=bAxzRuu3Uy4",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-a.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"BST Queries",
 "published"=>true,
 "morea_id"=>"reading-screencast-8b",
 "morea_summary"=>"How to perform queries on BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=LDncFcNOr_I",
 "morea_labels"=>["Screencast", "Suthers", "22 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-b.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Modifying BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8c",
 "morea_summary"=>"How to modify binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=qJ7TyaKSf_0",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-c.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Analyzing BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8d",
 "morea_summary"=>"Determine the height of binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=vx0uWYHIRes",
 "morea_labels"=>["Screencast", "Suthers", "5 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-d.md"}
</pre>

<h2>/morea/090.heaps/experience-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps (again)",
 "published"=>true,
 "morea_id"=>"experience-heaps-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps (at home).",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/experience-2.html",
 "url"=>"/morea/090.heaps/experience-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### Heaps\n\n#### 8 points\n\n**2\\. ** Illustrate `Build-Max-Heap` on this data in a 1-based indexing array:\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5],\n    \n\nRewrite the array as it exists after each execution of line 3 (the call to\n`Max-Heapify`). A template is provided below. Please use a plain text editor\nwith fixed width font and replace each underscore character \"_\" with the\ncorrect value.\n\n    \n    \n      index        1  2  3  4  5  6  7  8  9\n      Start:  A = [1, 6, 2, 8, 3, 9, 4, 7, 5]\n    \n      i = 4:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 3:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 2:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 1:  A = [_, _, _, _, _, _, _, _, _]\n    \n\nYou may want to draw the heap in tree form and do the operations on the tree.\nYou are encouraged to include these trees in your response to help Robert\n\"debug\" any problems, but grading will initially be done on the above\ntemplate.\n\n**3\\. ** Illustrate `Heap-Extract-Max` on the heap you constructed above. Show the array representation:\n\n**(a)** After line 5 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**(b)** After line 6 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**4\\. ** Consider now min-heaps rather than max-heaps. Write pseudocode for `Min-Heapify` and `Heap-Decrease-Key`, by copying the textbook's code for the max versions and changing only what you need to change. To make grading easier, please highlight, boldface or circle your changes.\n    \n    \n      MAX-HEAPIFY (A, i) // Change to **MIN**-HEAPIFY \n      1   l = LEFT(i)\n      2   r = RIGHT(i)\n      3   if l <= A.heap-size and A[l] > A[i]\n      4       largest = l\n      5   else largest = i\n      6   if r <= A.heap-size and A[r] > A[largest]\n      7       largest = r\n      8   if largest != i\n      9       exchange A[i] with A[largest]\n      10      MAX-HEAPIFY (A, largest)\n    \n      HEAP-INCREASE-KEY (A, i, key) // Change to HEAP-**DECREASE**-KEY\n      1  if key < A[i]\n      2      error \"new key is smaller than current key\"\n      3  A[i] = key\n      4  while i > 1 and A[PARENT(i)] < A[i]\n      5      exchange A[i] with A[PARENT(i)]\n      6      i = PARENT(i)\n    \n\n\n",
 "path"=>"morea//090.heaps/experience-2.md"}
</pre>

<h2>/morea/090.heaps/experience.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps",
 "published"=>true,
 "morea_id"=>"experience-heaps",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/experience.html",
 "url"=>"/morea/090.heaps/experience.html",
 "content"=>
  "### 1\\. Heap-Delete(A, i)\n\n#### 2 points\n\nProcedure `Heap-Delete(_A_, _i_)` deletes the node at index _i_ in heap _A_\n(represented as an array). Give an implementation of `Heap-Delete` that runs\nin O(lg _n_) time for a heap of size _n_ = `A.heapSize`. You may use instance\nvariable `A.heapSize` and any of the other procedures already defined in the\ntext.\n\n_This is very similar to an existing procedure._\n\n>\n\n>     Heap-Delete(A,i)\n\n>  \n\n### 2\\. Heapsort on Sorted Data\n\n#### 3 points\n\n**(a)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_increasing_** order_?\n\n**(b)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_decreasing_** order_?\n\n**(c)** For which of these cases would `Heapsort` make _more swaps of elements in the array_, or are they the same?\n\n_Give your reasoning to help grading feedback. You might start working an\nexample for each case, but don't get bogged down in details: return to high\nlevel asymptotic reasoning as soon as you see what is going on. _Refer to line\nnumbers in code _ when discussing your analyses. _\n\n\n",
 "path"=>"morea//090.heaps/experience.md"}
</pre>

<h2>/morea/090.heaps/module.html</h2>

<pre>Hash
{"title"=>"Heapsort",
 "published"=>true,
 "morea_id"=>"heaps",
 "morea_outcomes"=>["outcome-heaps"],
 "morea_readings"=>
  ["reading-screencast-9a",
   "reading-screencast-9b",
   "reading-screencast-9c",
   "reading-screencast-9d",
   "reading-cormen-6",
   "reading-notes-9"],
 "morea_experiences"=>["experience-heaps", "experience-heaps-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/090.heaps/logo.jpg",
 "morea_sort_order"=>90,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/090.heaps/module.html",
 "content"=>
  "Heaps, correctness, run-time analysis, priority queues, application to sorting.\n",
 "path"=>"morea//090.heaps/module.md"}
</pre>

<h2>/modules/heaps/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Heapsort",
 "url"=>"/modules/heaps/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/heaps/index.html"}
</pre>

<h2>/morea/090.heaps/outcome.html</h2>

<pre>Hash
{"title"=>"Understand heaps, heapsort, and priority queues",
 "published"=>true,
 "morea_id"=>"outcome-heaps",
 "morea_type"=>"outcome",
 "morea_sort_order"=>90,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/090.heaps/outcome.html",
 "content"=>"Understand how to manipulate heaps and their benefits. \n",
 "path"=>"morea//090.heaps/outcome.md"}
</pre>

<h2>/morea/090.heaps/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 6 - Heapsort",
 "published"=>true,
 "morea_id"=>"reading-cormen-6",
 "morea_summary"=>
  "Heapsort, heaps, maintaining the heap property, building a heap, priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "19 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-cormen.md"}
</pre>

<h2>/morea/090.heaps/reading-notes.html</h2>

<pre>Hash
{"title"=>"Chapter 9 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-9",
 "morea_summary"=>"Notes on heaps and heapsort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/reading-notes.html",
 "url"=>"/morea/090.heaps/reading-notes.html",
 "content"=>
  "# Heaps, Heapsort, and Priority Queues\n\n## Outline\n\n  1. Heaps and their Properties\n  2. Building and Maintaining Heaps\n  3. Application to Sorting\n  4. Application to Priority Queues\n\n##  Heaps and their Properties\n\nHeaps are a useful data structure with applications to sorting and priority\nqueues.\n\nThey are _nearly complete binary trees_ that satisfy a _heap property_ that\norganizes data under a partial ordering of their keys, enabling access to\nelements with maximum (or minimum) keys without having to pay the cost of\nfully sorting the keys.\n\nHeaps are not to be confused with garbage collected storage (a heap of\ngarbage)!\n\n### Heaps as Nearly Complete Binary Trees\n\nConceptually, heaps are **nearly complete binary trees**: they have leaves on\nat most two adjacent levels _l-1_ and _l_ and in which the leaves at the\nbottommost level _l_ lie in the leftmost positions of _l_:\n\n![](fig/Fig-6-1-max-heap-tree.jpg)\n\nThese quantitative properties concerning full and nearly complete binary trees\nwill be useful:\n\n#### Number of elements in nearly complete binary trees of height _h_ (6.1-1)\n\n![](fig/diagram-tree-heights-wide.jpg)\n\nAs discussed in [Topic 8](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-08.html#binarytrees), a **complete binary tree** has at most 2_h_+1\n− 1 nodes (vertices). We can see this by adding up the number of elements at\neach level: 20 \\+ 21 \\+ ... + 2h for a complete binary tree of height _h_.\nThen apply formula A.5 with _x_=2 and _n_=_h_:\n\n![](fig/formula-A-5.jpg)\n\nYou get (2_h_+1 − 1) / (2 − 1) = 2_h_+1 − 1\\.\n\nSo, a _nearly_ complete binary tree has _at most_ 2_h_+1 − 1 elements (if it\nis complete, as analyzed above). The _fewest_ number of elements it can have\nat height _h_ is when the last level has just 1 element and the level before\nit is complete. So do the math for a complete binary tree of height _h_−1:\nthere are exactly 2_h_ − 1 elements in levels 1 to _l_−1 and one more element\nin the _l_th level, for a total of 2_h_ elements.\n\n#### Height of an _n_-element nearly complete binary tree (6.1-2)\n\nGiven an _n_-element nearly complete binary tree of height _h_, from 6.1-1:\n\n> 2_h_   ≤   _n_   ≤   2_h+1_ − 1   <   2_h+1_\n\nTaking the log of the first, second and last terms,\n\n> _h_   ≤   lg _n_   <   _h_ \\+ 1\n\nSince _h_ is an integer, _h_ = ⌊lg _n_⌋     _(Notice the \"floor\" notation.)_\n\n#### Number of leaves\n\nAn _n_-element nearly complete binary tree has ⌈n/2⌉ leaves.     _(Notice the\n\"ceiling\" notation. Left as exercise.)_\n\n####  Nodes of height _h_ in a nearly complete binary tree (6.3-3)\n\nThere are at most ⌈n/2h+1⌉ nodes of height _h_ in a nearly complete binary\ntree. (A proof by contradiction is possible.)\n\n### The Heap Property\n\nDepending on whether it is a _max heap_ or a _min heap_, to be a heap the\nbinary tree must also satisfy a heap property:\n\n**Max Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≥ key(_i_).  \n  \nBy induction and transitivity of ≥, the max heap property guarantees that the\nmaximum element of a max-heap is at the root.\n\n  \n**Min Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≤ key(_i_).  \n  \nBy induction and transitivity of ≤, the min heap property guarantees that the\nminimum element of a min heap is at the root.\n\n### Array Representation\n\nHeaps are usually represented using arrays, following the mapping shown by the\nindices in the tree:\n\n![](fig/Fig-6-1-max-heap-tree-array-indices.jpg)\n![](fig/Fig-6-1-max-heap-array.jpg)  \n\nThe fact that we can see a heap both as a binary tree and as an array is an\nexample of a powerful idea in computer science: mapping between an\nimplementation representation that has efficient computational properties and\na conceptual representation that fits how we think about a problem.\n\n![](fig/code-parent-children.jpg)\n\nIf a heap is stored in array `A`, then movement in the tree is easy:\n\n  * Root of the tree is `A[1]`\n  * Parent of `A[_i_]` is `A[⌊_i_/2⌋]`     _(Notice we are taking the floor of _i_/2)_.\n  * Left Child of `A[_i_]` is `A[_2i_]`\n  * Right Child of `A[_i_]` is `A[_2i+1_]`\n  * Index operations are fast in binary (left and right shifts and setting the low order bit).\n\n#### Indices of leaves (6.1-7)\n\nBy the number of leaves fact, when an _n_-element heap is stored in the array\nrepresentation, the leaves are the nodes indexed by ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2,\n..., _n_. (Left as exercise.)\n\nThis fact will be used in algorithms that only need to process either the\nleaves or the internal nodes of a heap.\n\n* * *\n\n##  Building and Maintaining Heaps\n\n### Maintaining the Heap Property\n\nMAX-HEAPIFY is used to maintain the max-heap property by addressing a possible\nviolation at node `A[_i_]`:\n\n  * MAX-HEAPIFY assumes that the left and right subtrees of _i_ are max-heaps.\n  * When called, `A[_i_]` may (or may not) be smaller than its children, violating the max-heap property if it is.\n  * After MAX-HEAPIFY, the subtree rooted at _i_ will be a heap. \n![](fig/code-max-heapify.jpg)\n\nIt works by comparing `A[_i_]` with its left and right children (lines 3-7),\nand if necessary swapping `A[_i_]` with the larger of the two children to\npreserve the heap property (lines 8-9). _Tail recursion_ after the swap\npropagates this change until the subtree is a heap (line 10).\n\n#### Example\n\nMax-Heapify from the node at index 2 (containing 4):\n\n![](fig/Fig-6-2-max-heapify.jpg)\n\n#### Analysis\n\nIt is easy to see that the body of each call before recursion is O(1), and the\nrecursion repeats this for at most O(lg _n_) nodes on the path from the root\nto the leaves.\n\nMore formally, the worst case is when the bottom level is exactly half full,\nand in this case, the _children's subtrees_ can have at most 2_n_/3 nodes. So,\nadding the cost to recurse on these subtrees plus Θ(1) cost for comparisons at\na given node, we get the recurrence relation:\n\n> _T_(_n_)   ≤   _T_(2_n_/3) + Θ(1).\n\nThis fits case 2 of the Master Theorem (_a_ = 1, _b_ = 3/2 since 1/(3/2) =\n2/3, and _f_(_n_) = 1 = O(_n_log3/21) = O(_n_0)), giving Θ(lg _n_).\n\n### Building a Heap\n\nSuppose we load some keys into an array in arbitrary order from left to right,\ncreating an almost complete binary tree that may not satisfy the heap\nproperty.\n\nEach leaf of the corresponding tree is trivially a heap. If we call MAX-\nHEAPIFY on the parents of the leaves, the assumption that the right and left\nsubtrees are heaps is met. Once MAX-HEAPIFY returns, the parents are roots of\nheaps too, so we call it on _their_ parents.\n\nUsing the previously established result that the leaves begin in the array at\nindex ⌊n/2⌋ \\+ 1, so the last non-leaf node is at ⌊n/2⌋, the implementation is\ntrivial:\n\n![](fig/code-build-max-heap.jpg)\n\n#### Example\n\n![](fig/Fig-6-3-build-max-heap-array.jpg)\n\nLet's trace this on an array of size 10, for _i_ = 5 downto 1:\n\n![](fig/Fig-6-3-build-max-heap-ab.jpg)\n\n(a) The heap rooted at vertex or array index 5 is already a max heap: no\nchange is made.\n\n(b) The heap rooted at index 4 is not a max heap: the value 2 is smaller than\nits children. We restore the max heap property by swapping 2 with the larger\nchild key, 14 (see next figure for result). If we had swapped with 8, it would\nnot be a max heap: this is why we always swap with the larger child.\n\n![](fig/Fig-6-3-build-max-heap-cd.jpg)\n\n(c) Decrementing _i_ to 3, there is another violation of the max heap\nproperty, and we swap value 3 at index 3 with value 10 at index 7 (the larger\nchild).\n\n(d) The heap at index 2 violates the max heap property: we must propagate the\nvalue 1 down by swapping with 16, and then with 7 in a recursive call to Max-\nHeapify (see next figure).\n\n![](fig/Fig-6-3-build-max-heap-ef.jpg)\n\n(e) Finally, checking the value at index 1 (value 4) against its children, we\nfind we need to swap it with value 16 at index 2, and then with value 14 at\nindex 4 and value 8 at index 9 in two recursive calls to Max-Heapify. (f)\nshows the resulting max heap.\n\n#### Correctness\n\n![](fig/code-build-max-heap.jpg)\n\n_**Loop Invariant:**_\n\n    At the start of every iteration of the `for` loop, each node _i_+1, _i_+2, ..., _n_ is a root of a max heap.\n  \n_**Initialization:**_\n\n    By Exercise 6.1-7, each node ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2, ..., _n_ is a leaf, which is the root of a trivial max-heap. Since _i_ = ⌊n/2⌋ before the first iteration of the `for` loop, the invariant is initially true. \n  \n_**Maintenance:**_\n\n    Children of node _i_ are indexed higher than _i_, so by the loop invariant, they are both roots of max-heaps. Thus the assumption of MAX-HEAPIFY is met, enabling it to make node _i_ a max-heap root. Decrementing _i_ reestablishes the loop invariant at each iteration.\n  \n_**Termination:**_\n\n    The loop terminates when _i_ = 0. By the loop invariant, each node including the root indexed by 1 is the root of a max-heap.\n\n#### Analysis\n\nSometimes a good approach is to prove an easy bound and then tighten it.\n\nIt is easy to see that there are O(_n_) (about _n_/2) calls to MAX-HEAPIFY,\nand we already know that MAX-HEAPIFY on a tree of height O(lg _n_) is O(lg\n_n_). Therefore an upper bound is O(_n_ lg _n_).\n\nHowever, only the root node and those near it are at height O(lg _n_). Many\nnodes are close to the leaves and we don't even process half of them. So let's\ntry for a tighter bound ...\n\nThere are no more than ⌈n/2h+1⌉ nodes of height _h_ (Exercise 6.3-3), and the\nheap is ⌊lg_n_⌋ high (Exercise 6.1-2). MAX-HEAPIFY called on a node of height\n_h_ is O(_h_), so we need to sum this cost times the number of nodes at each\n_h_ for all relevant _h_:\n\n![](fig/analysis-build-max-heap-1.jpg)\n\nWe can simplify this as follows:\n\n  1. Wrap big-O around the whole thing, leaving h behind.\n  2. Remove the ceiling (which does not affect big-O analysis).\n  3. Rewrite the resulting _nh_/2_h_+1 as (_n_/2)(h/2_h_).\n  4. Move _n_/2 out of the summation, as it does not involve _h_.\n  5. Eliminate the constant 1/2, as we are inside the magical world of big-O!\n\nTricky, huh? Now maybe you can see why the text authors write that as:\n\n![](fig/analysis-build-max-heap-2.jpg)\n\nThe above summation runs up to ⌊lg_n_⌋, but we would like to use a convenient\nformula A-8, shown below, which runs up to ∞:\n\n![](fig/formula-A-8.jpg)\n\nSince big-O implies an inequality (upper bound), we can go ahead and run the\nsummation to ∞ instead of ⌊lg_n_⌋, because all of the additional terms are\npositive (and also very small), so the inequality will be maintained. Then, if\nwe let _x_ = 1/2 (since _h_/2_h_ = _h_(1_h_/2_h_) can be written as h(1/2)h),\nwe get:\n\n![](fig/analysis-build-max-heap-3.jpg)\n\nThus our big-O expression simplifies to O(_n_*2) = O(_n_), which is a tighter\nbound than O(_n_ lg _n_). The same analysis appliles to the min-heap version.\n\n(You might wonder why we can build a heap in O(_n_) time when sorting takes\nO(_n_ lg _n_), as will be proven in [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html).\nThis is because a heap is only a partial order, so less work needs to be done\nto guarantee the heap property.)\n\n* * *\n\n##  Application to Sorting\n\nSuppose `A[1.._n_]` contains keys to be sorted. If we call BUILD-MAX-HEAP on\nthis array, the maximum element will be at `A[1]`. We can swap it with the\nitem at `A[_n_]`, then repeat on `A[1.._n_-1]` (reducing the size of the heap\nby 1 each iteration) until this reaches size 1.\n\n![](fig/code-heapsort.jpg)\n\n#### Analysis:\n\nBUILD-MAX-HEAP is O(_n_) (by analysis above). The `for` loop executes _n_-1\ntimes, with O(1) exchange each iteration and a call to O(lg _n_) MAX-HEAPIFY.\nThus heapsort is O(_n_ lg _n_).\n\n#### Example:\n\nSuppose we have an array A with five integers. First, BUILD-MAX-HEAP is called\non it, resulting in the array A = [7, 4, 3, 1, 2] shown as the tree in (a)\nbelow.\n\n![](fig/Fig-6-4-heapsort-alt-ab.jpg)\n\nThen the loop of HEAPSORT successively takes out the maximum from the first\nindex by swapping it with the last element in the heap, and calls MAX-HEAPIFY.\nSo, 7 is swapped with 2, and then the heap (now one smaller) is reconstructed,\nresulting in the heap shown in (b): A = [4, 2, 3, 1, 7], with the first four\nelements being the heap.\n\n![](fig/Fig-6-4-heapsort-alt-cd.jpg)\n\nThe maximum element 4 (from b) was swapped with the minimum element 1\n(removing 4 from the heap) and the heap restored, resulting in (c) A = [3, 2,\n1, 4, 7] with the first three elements being the heap. Then in (d), the max\nelement 3 was swapped with 1 and the heap restored by percolating 1 down: A =\n[2, 1, 3, 4, 7] with the heap being the first two elements.\n\n![](fig/Fig-6-4-heapsort-alt-e.jpg)\n\n(e) Finally, the maximum element 2 is removed by swapping with the only\nremaining element 1, resulting in the sorted array shown.\n\nHere is a [playing card\ndemonstration](http://www.youtube.com/watch?v=WYII2Oau_VY) of heap sort, in\ncase it helps. This demonstration is using a _min-heap_ to sort the cards with\nthe card of _maximum_ value ending up at the top of the stack of cards.\n\n* * *\n\n##  Application to Priority Queues\n\nAn important application of heaps is implementing **priority queues**. There\nare _min_ and _max_ versions.\n\nA **max-priority queue** is an ADT with the following operations:\n\nINSERT(S,_x_)\n\n    S <- S ∪ {_x_}\n  \nMAXIMUM(S)\n\n    Returns the element of S with the largest key.\n  \nEXTRACT-MAX(S)\n\n    Removes and returns the element of S with the largest key.\n  \nINCREASE-KEY(S,_x_,_k_)\n\n    Increases the value of _x_'s key to the new value _k_ ≥ current key(_x_).\n\nA **min-priority queue** has corresponding operations MINIMUM, EXTRACT-MIN,\nand DECREASE-KEY.\n\nMax-priority queues can be used in job scheduling: the highest priority job is\nalways run next, but job priority can be increased as a job ages in the queue,\nor for other reasons.\n\nMin-priority queues will be very important in graph algorithms we cover later\nin the semester: efficient implementations of EXTRACT-MIN and DECREASE-KEY\nwill be especially important.\n\nMin-priority queues also used in event-driven simulations, where an event may\ngenerate future events, and we need to simulate the events in chronological\norder.\n\n#### Accessing Maximums\n\nIn the array representation, MAXIMUM is trival to implement in O(1) by\nreturning the first element of the array. However, if we EXTRACT-MAX we need\nto restore the heap property afterwards.\n\nHEAP-EXTRACT-MAX takes the root out, replaces it with the last element in the\nheap _(stop and think: why this element?)_, and then uses MAX-HEAPIFY to\npropagate that element (which probably has a small key) down to its proper\nplace:\n\n![](fig/code-heap-extract-max.jpg)\n\nHEAP-EXTRACT-MAX is O(lg _n_) since there is only constant work added to the\nO(lg _n_) MAX-HEAPIFY.\n\n#### Increasing keys\n\nAn increase to the key may require propagating the element _up_ the tree (the\nopposite direction as compared to MAX-HEAPIFY):\n\n![](fig/code-heap-increase-key.jpg)\n\nThis is clearly O(lg _n_) due to following a simple path up the tree. Let's\nwork this example, where the element at _i_ has its key increased from 4 to\n15, and then it is propagated up:\n\n![](fig/Fig-6-5-heap-increase-key.jpg)\n\nThis propagation follows the \"Peter Principle\": the claim that persons in a\nhierarchical organization are promoted through the ranks of management until\nthey reach their level of incompetency!!!\n\n#### Inserting New Elements\n\nWhen inserting, we are going to have to make the heap bigger, so let's add the\nelement at the end and propagate it up to where it belongs.\n\nHEAP-INCREASE-KEY already has the code for this propagation, so if we set the\nkey to the smallest possible value and then try to increase it with HEAP-\nINCREASE-KEY, it will end up in the right place:\n\n![](fig/code-max-heap-insert.jpg)\n\nAgain, this is O(lg _n_).\n\n* * *\n\n## Next\n\nIn [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html) we\nwrap up our examination of sort algorithms with Quicksort, a practical sort\nthat performs well in practice and also illustrates the value of probabilistic\nanalysis and random algorithms.\n\nWe will return to other kinds of trees, in particular special kinds of binary\nsearch trees that are kept balanced to guarantee O(lg _n_) performance, in\n[Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sat Feb 15 16:37:46 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//090.heaps/reading-notes.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9a",
 "morea_summary"=>"Basic ideas about heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=0zh4IiKaVN0",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-a.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Building heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9b",
 "morea_summary"=>"Understanding how to build heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=oAfSx7aRkZM",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-b.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Analyzing heap building",
 "published"=>true,
 "morea_id"=>"reading-screencast-9c",
 "morea_summary"=>"Correctness and run time analysis of heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gMwtzAPDupI",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-c.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Applications of heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9d",
 "morea_summary"=>"Heapsort and priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8O5iBigvDIw",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-d.md"}
</pre>

<h2>/morea/100.quicksort/experience-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort (again)",
 "published"=>true,
 "morea_id"=>"experience-quicksort-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort (at home).",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/experience-2.html",
 "url"=>"/morea/100.quicksort/experience-2.html",
 "content"=>
  "### Quicksort\n\n#### 12 points\n\nShow the operation of Partition (not randomized) on this 1-based array:\n\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5], p=1, q=9 \n    \n\nand the two sub-partitions that result as directed below. In other words, you\nwill trace the three calls to Partition that are highest in the recursion\ntree. (They are _not_ the first three calls: #1 is the first call and #2 is\nthe second call, but the call marked as #3 below takes place after all the\nrecursive calls breaking down #2.)\n\nIn order to make the desired response format clear and to make it easy for the\nTA to grade, I am providing a template for your response. You are to fill in\nwherever the underscore character \"_\" appears. Use a plain text editor with\n_fixed-width font_. Be sure to fill in all fields marked with underscore: use\nsearch to make sure you get them all. I start you off with the first few\nlines: continue in the same pattern.\n\n    \n    \n     \n    **(#1) Call to Partition (A, 1, 9) made in Line 2 of the initial call to Quicksort:**\n    \n      Initially: \n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=0, j=1, pivot = A[r] = A[9] = 5 \n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=1, exchanged A[1] with A[1]\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=2, no exchange \n    \n      ... you fill in the rest until the loop exits ... \n    \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=3, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=4, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=5, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=6, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=7, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=8, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does Partition(A, 1, 9) return? __\n    \n    Continuing execution of the top level call to Quicksort, identify the two\n    partitions that will be handled by the recursive calls to Quicksort at\n    this level: \n    (#2) On what subarray will Quicksort in line 3 be called? A[_, _]\n    (#3) On what subarray will Quicksort in line 4 be called? A[_, _]\n    \n    Now trace these two calls in a manner similar to above. \n    \n    **(#2) Call to Partition(A, _, _) handled in the first call to Quicksort line 3: **\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this second call to Partition return? __\n    \n    **(#3) Call to Partition(A, _, _) handled in the first call to Quicksort line 4:**\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this third call to Partition return? __\n    \n\nNot graded, but you might think about:\n\n  * What pattern do you see in the second call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What pattern do you see in the third call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What do these observations tell us about the runtime of Quicksort on data organized as in these partitions? \n\n* * *\n\nDan Suthers Last modified: Wed Mar 19 23:22:39 HST 2014\n\n",
 "path"=>"morea//100.quicksort/experience-2.md"}
</pre>

<h2>/morea/100.quicksort/experience.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort",
 "published"=>true,
 "morea_id"=>"experience-quicksort",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/experience.html",
 "url"=>"/morea/100.quicksort/experience.html",
 "content"=>
  "### 1\\. Finding _i_th largest element with Partition\n\n#### 1-2 points TBD\n\nHow would you _use Quicksort's `Partition` procedure_ to write an algorithm\nfor _finding the _i_th smallest element of an unsorted array_? _Hint: what\ndoes the returned value of Partition tell you about the rank ordering of the\npivot?_\n\n  * Describe the strategy in English\n  * Then if you have time after finishing the next question, write pseudocode for an algorithm.\n\n### 2\\. Calls to Partition in Worst and Best Case\n\n#### 3-4 points TBD\n\nWhen we measure runtime efficiency in terms of _number of comparisons_ to be\nmade, Quicksort is Θ(_n_2) in the worst case (when the pivot is always chosen\nto be the smallest or largest element), and Θ(_n_ lg _n_) in the best case\n(when the pivot is always the median key). But we might also try to _measure\nefficiency in terms of number of calls to `Partition`_, since all the work is\ndone in there.\n\n**(a)**   Asymptotically, _how many calls to `Partition` are made in the **worst case runtime**_ as defined above (when the pivot is always chosen to be the smallest or largest element)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\n**(b)**   Asymptotically, _how many calls to `Partition` are made in the **best case runtime**_ as defined above (when the pivot is always the median key)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\nArgue for your conclusions!! _(One approach is to write and solve recurrence\nrelations. Another approach is to notice that the Quicksort recursion trees\nare binary, and use quantitative facts about binary trees. For a big-O rather\nthan Θ reply, a simple counting argument based on the pseudocode is possible,\nbut not as rigorous.)_\n\n![](fig/pseudocode-quicksort.jpg) ![](fig/pseudocode-quicksort-partition.jpg)\n\n\n",
 "path"=>"morea//100.quicksort/experience.md"}
</pre>

<h2>/morea/100.quicksort/module.html</h2>

<pre>Hash
{"title"=>"Quicksort",
 "published"=>true,
 "morea_id"=>"quicksort",
 "morea_outcomes"=>["outcome-quicksort"],
 "morea_readings"=>
  ["reading-screencast-10a",
   "reading-screencast-10b",
   "reading-screencast-10c",
   "reading-cormen-7",
   "reading-cormen-8",
   "reading-notes-10"],
 "morea_experiences"=>["experience-quicksort", "experience-quicksort-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/100.quicksort/logo.png",
 "morea_sort_order"=>100,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/100.quicksort/module.html",
 "content"=>
  "Randomizing, lower bounds on comparison sorts, counting sort, radix sort, bucket sort.\n",
 "path"=>"morea//100.quicksort/module.md"}
</pre>

<h2>/modules/quicksort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Quicksort",
 "url"=>"/modules/quicksort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/quicksort/index.html"}
</pre>

<h2>/morea/100.quicksort/outcome.html</h2>

<pre>Hash
{"title"=>"Understand quicksort",
 "published"=>true,
 "morea_id"=>"outcome-quicksort",
 "morea_type"=>"outcome",
 "morea_sort_order"=>100,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/100.quicksort/outcome.html",
 "content"=>
  "Understand the quicksort algorithm and how it differs from mergesort.\n",
 "path"=>"morea//100.quicksort/outcome.md"}
</pre>

<h2>/morea/100.quicksort/reading-cormen-1.html</h2>

<pre>Hash
{"title"=>"CLRS 8 - Sorting in linear time",
 "published"=>true,
 "morea_id"=>"reading-cormen-8",
 "morea_summary"=>
  "Lower bounds for sorting, counting sort, radix sort, bucket sort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-cormen-1.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-cormen-1.md"}
</pre>

<h2>/morea/100.quicksort/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 7 - Quicksort",
 "published"=>true,
 "morea_id"=>"reading-cormen-7",
 "morea_summary"=>
  "Description and performance of quicksort, a randomized version, and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "20 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-cormen.md"}
</pre>

<h2>/morea/100.quicksort/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on Quicksort",
 "published"=>true,
 "morea_id"=>"reading-notes-10",
 "morea_summary"=>"Notes on quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>10,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/reading-notes.html",
 "url"=>"/morea/100.quicksort/reading-notes.html",
 "content"=>
  "## Notes on quicksort\n\n  1. Quicksort \n  2. Analysis of Quicksort \n  3. Lower Bound for Comparison Sorts \n  4. O(n) Sorts (briefly)\n\n### Motivations\n\nQuicksort, like Mergesort, takes a divide and conquer approach, but on a\ndifferent basis.\n\nIf we have done two comparisons among three keys and find that _x_ < _p_ and\n_p_ < _y_, do we ever need to compare _x_ to _y_? Where do the three belong\nrelative to each other in the sorted array?\n\nQuicksort uses this idea to partition the set of keys to be sorted into those\nless than the pivot _p_ and those greater than the pivot. (It can be\ngeneralized to allow keys equal to the pivot.) It then recurses on the two\npartitions.\n\n![](fig/quicksort-recursion.jpg)\n\nCompare this to Mergesort.\n\n  * Both take a recursive divide-and-conquer approach.\n  * Mergesort does its work on the way back up the recursion tree (merging), while Quicksort does its work on the way down the recursion tree (partitioning).\n  * Mergesort always partitions in half; for Quicksort the size of the partitions depends on the pivot (this results in Θ(_n_2) worst case behavior, but expected case remains Θ(_n_ lg _n_).\n  * Mergesort requires axillary arrays to copy the data; while as we shall see Quicksort can operate entirely within the given array: it is an **in-place sort**.\n\nQuicksort performs well in practice, and is one of the most widely used sorts\ntoday.\n\n### The Quicksort Algorithm\n\nTo sort any subarray A[_p_ .. _r_],   _p_ < _r_:\n\n**_Divide:_**\n    Partition A[_p_ .. _r_] into two (possibly empty) subarrays \n\n  * A[_p_ .. _q-1_], where every element is ≤ A[_q_]\n  * A[_q + 1_ .. _r_], where A[_q_] ≤ every element\n**_Conquer:_**\n    Sort the two subarrays by recursive calls\n**_Combine:_**\n    No work is needed to combine: all subarrays (including the entire array) are sorted as soon as recursion ends.\n\nAn array is sorted with a call to `QUICKSORT(A, 1, A.length)`:\n\n![](fig/pseudocode-quicksort.jpg)\n\nThe work is done in the PARTITION procedure. A[_r_] will be the pivot. (Note\nthat the _end_ element of the array is taken as the pivot. Given random data,\nthe choice of the position of the pivot is arbitrary; working with an end\nelement simplifies the code):\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nPARTITION maintains four regions.\n\n![](fig/Fig-7-2-partition-regions.jpg)\n\nThree of these are described by the following loop invariants, and the fourth\n(A[_j_ .. _r_-1]) consists of elements that not yet been examined:\n\n> **Loop Invariant:**\n\n>\n\n>   1. All entries in A[_p_ .. _i_] are ≤ pivot.\n\n>   2. All entries in A[_i_+1 .. _j_-1] are > pivot.\n\n>   3. A[_r_] = pivot.\n\n### Example Trace\n\nIt is worth taking some time to trace through and explain each step of this\nexample of the PARTITION procedure, paying particular attention to the\nmovement of the dark lines representing partition boundaries.\n\n![](fig/pseudocode-quicksort-partition.jpg) \n![](fig/quicksort-trace-1.jpg)\n\nContinuing ...\n\n![](fig/quicksort-trace-2.jpg)\n\nHere is the [Hungarian Dance version of\nquicksort](http://www.youtube.com/watch?v=kDgvnbUIqT4), in case that helps to\nmake sense of it!\n\n### Correctness\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nHere use the loop invariant to show correctness:\n\n  1. All entries in A[_p_ .. _i_] are ≤ pivot.\n  2. All entries in A[_i_+1 .. _j_ −1] are > pivot.\n  3. A[_r_] = pivot. \n\n**_Initialization:_**\n    Before the loop starts, _x_ is assigned the pivot A[_r_] (satisfying condition 3), and the subarrays a[_p_ .. _i_] and A[_i_+1 .. _j_−1] are empty (trivially satisfying conditions 1 and 2). \n**_Maintenance:_**\n    While the loop is running, \n\n  * if A[_j_] ≤ pivot, then _i_ is incremented, A[_j_] and A[_i_] are swapped, and _j_ is incremented. Because of the swap, A[_i_] ≤ _x_ for condition 1. The item swapped into A[_j_-1] > _x_ by the loop invariant, for condition 2.\n  * If A[_j_] > pivot, then _j_ is incremented, sustaining condition 2 (the others are unchanged), as the element added was larger\n**_Termination:_**\n    The loop terminates when _j_=_r_, so all elements in A are partitioned into one of three cases: A[_p_ .. _i_] ≤ pivot, A[_i_+1 .. _r_-1] > pivot, and A[_r_] = pivot. The last two lines fix the placement of A[_r_] by moving it between the two subarrays.\n\n* * *\n\n##  Informal Analysis\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nThe formal analysis will be done on a randomized version of Quicksort. This\ninformal analysis helps to motivate that randomization.\n\nFirst, PARTITION is Θ(_n_): We can easily see that its only component that\ngrows with _n_ is the `for` loop that iterates proportional to the number of\nelements in the subarray).\n\nThe runtime depends on the partitioning of the subarrays:\n\n### Worst Case\n\nThe worst case occurs when the subarrays are completely unbalanced, i.e.,\nthere are 0 elements in one subarray and _n_-1 elements in the other subarray\n(the single pivot is not processed in recursive calls). This gives a familiar\nrecurrence (compare to that for insertion sort):\n\n![](fig/analysis-quicksort-worst-recurrence.jpg)\n\nOne example of data that leads to this behavior is when the data is already\nsorted: the pivot is always the maximum element, so we get partitions of size\n_n_−1 and 0 each time. Thus, _quicksort is O(_n_2) on sorted data_. Insertion\nsort actually does better on a sorted array! (O(_n_))\n\n### Best Case\n\nThe best case occurs when the subarrays are completely balanced (the pivot is\nthe median value): subarrays have about _n_/2 elements. The reucurrence is\nalso familiar (compare to that for merge sort):\n\n![](fig/analysis-quicksort-best-recurrence.jpg)\n\n### Effect of Unbalanced Partitioning\n\nIt turns out that expected behavior is closer to the best case than the worst\ncase. Two examples suggest why expected case won't be that bad.\n\n#### Example: 1-to-9 split\n\nSuppose each call splits the data into 1/10 and 9/10. This is highly\nunbalanced: won't it result in horrible performance?\n\n![](fig/Fig-7-4-quicksort-1-9-recursion-tree.jpg)\n\nWe have log10_n_ full levels and log10/9_n_ levels that are nonempty.\n\nAs long as it's constant, the base of the log does not affect asymptotic\nresults. Any split of constant proportionality will yield a recursion tree of\ndepth Θ(lg _n_). In particular (using ≈ to indicate truncation of low order\ndigits),\n\n> log10/9_n_ = (log2_n_) / (log210/9)     _by formula 3.15_  \n            ≈ (log2_n_) / 0.152   \n            = 1/0.152 (log2_n_)  \n            ≈ 6.5788 (log2_n_)  \n            = Θ(lg _n_), where _c_ = 6.5788. \n\nSo the recurrence and its solution is:\n\n![](fig/analysis-quicksort-9-1-recurrence.jpg)\n\nA general lesson that might be taken from this: sometimes, even very\nunbalanced divide and conquer can be useful.\n\n#### Example: extreme cases cancel out\n\nWith random data there will usually be a mix of good and bad splits throughout\nthe recursion tree.\n\nA mixture of worst case and best case splits is asymptotically the same as\nbest case:\n\n![](fig/Fig-7-5-quicksort-unbalanced-splits.jpg)\n\nBoth these trees have the same two leaves. The extra level on the left hand\nside only increases the height by a factor of 2, and this constant disappears\nin the Θ analysis.\n\nBoth result in O(_n_ lg _n_), though with a larger constant for the left.\n\n* * *\n\n##  Randomized Quicksort\n\n![](fig/no-badguy.jpg)\n\nWe expect good average case behavior if all input permutations are equally\nlikely, but what if it is not?\n\nTo get better performance on sorted or nearly sorted data -- and to foil our\nadversary! -- we can randomize the algorithm to get the same effect as if the\ninput data were random.\n\nInstead of explicitly permuting the input data (which is expensive),\nrandomization can be accomplished trivially by **random sampling** of one of\nthe array elements as the pivot.\n\nIf we swap the selected item with the last element, the existing PARTITION\nprocedure applies:\n\n![](fig/pseudocode-randomized-quicksort.jpg)  \n![](fig/pseudocode-randomized-partition.jpg)\n\nNow, even an already sorted array will give us average behavior.\n\n_Curses! Foiled again!_\n\n* * *\n\n##  Quicksort Analysis\n\nThe analysis assumes that all elements are unique, but with some work can be\ngeneralized to remove this assumption (Problem 7-2 in the text).\n\n### Worst Case\n\nThe previous analysis was pretty convincing, but was based on an assumption\nabout the worst case. This analysis proves that our selection of the worst\ncase was correct, and also shows something interesting: we can solve a\nrecurrence relation with a \"max\" term in it!\n\nPARTITION produces two subproblems, totaling size _n_-1. Suppose the partition\ntakes place at index _q_. The recurrence for the worst case always selects the\nmaximum cost among all possible ways of splitting the array (i.e., it always\npicks the worst possible _q_):\n\n![](fig/analysis-quicksort-worst-1.jpg)\n\nBased on the informal analysis, we guess T(_n_) ≤ _cn_2 for some _c_.\nSubstitute this guess into the recurrence:\n\n![](fig/analysis-quicksort-worst-2.jpg)\n\nThe maximum value of _q_2 \\+ (_n_ \\- _q_ \\- 1)2 occurs when _q_ is either 0 or\n_n_-1 (the second derivative is positive), and has value (_n_ \\- 1)2 in either\ncase:\n\n![](fig/analysis-quicksort-worst-3.jpg)\n\nSubstituting this back into the reucrrence:\n\n![](fig/analysis-quicksort-worst-4.jpg)\n\nWe can pick _c_ so that _c_(2_n_ \\- 1) dominates Θ(_n_). Therefore, the worst\ncase running time is O(_n_2).\n\nOne can also show that the recurrence is Ω(_n_2), so worst case is Θ(_n_2).\n\n### Average (Expected) Case\n\nWith a randomized algorithm, expected case analysis is much more informative\nthan worst-case analysis.\n_[Why?](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10/why-\nexpected.txt)_\n\nThis analysis nicely demonstrates the use of indicator variables and two\nuseful strategies.\n\n#### Setup\n\nThe dominant cost of the algorithm is partitioning. PARTITION removes the\npivot element from future consideration, so is called at most _n_ times.\n\nQUICKSORT recurses on the partitions. The amount of work in each call is a\nconstant plus the work done in the `for` loop. We can count the number of\nexecutions of the `for` loop by counting the number of comparisons performed\nin the loop.\n\nRather than counting the number of comparisons in each call to QUICKSORT, it\nis easier to derive a bound on the number of comparisons across the entire\nexecution.\n\nThis is an example of a strategy that is often useful: **if it is hard to\ncount one way** (e.g., \"locally\"), **then count another way** (e.g.,\n\"globally\").\n\nLet _X_ be the total number of comparisons in all calls to PARTITION. The\ntotal work done over the entire execution is O(_n_ \\+ _X_), since QUICKSORT\ndoes constant work setting up _n_ calls to PARTITION, and the work in\nPARTITION is proportional to _X_. But what is _X_?\n\n#### Counting comparisons\n\nFor ease of analysis,\n\n  * Call the elements of A _z_1, _z_2, ... _z__n_, with _z__i_ being the _i_th smallest element. \n  * Define the set Z_ij_ = {_z__i_, _z__i_ \\+ 1, ... _z__j_} to be the set of elements between _z__i_ and _z__j_ inclusive. \n\nWe want to count the number of comparisons. Each pair of elements is compared\nat most once, because elements are compared only to the pivot element and then\nthe pivot element is never in any later call to PARTITION.\n\nIndicator variables can be used to count the comparisons. (Recall that we are\ncounting across all calls, not just during one partition.)\n\n> Let _Xij_ = I{ _zi_ is compared to _zj_ }\n\nSince each pair is compared at most once, the total number of comparisons is:\n\n![](fig/analysis-quicksort-expected-1.jpg)\n\nTaking the expectation of both sides, using linearity of expectation, and\napplying Lemma 5.1 (which relates expected values to probabilities):\n\n![](fig/lemming.jpg) ![](fig/analysis-quicksort-expected-2.jpg)\n\n#### Probability of comparisons\n\nWhat's the probability of comparing _z_i to _z_j?\n\nHere we apply another useful strategy: **if it's hard to determine when\nsomething happens, think about when it does _ not_ happen**.\n\nElements (keys) in separate partitions will not be compared. If we have done\ntwo comparisons among three elements and find that _zi_ < _x_ <_zj_, we do not\nneed to compare _zi_ to _zj_ (no further information is gained), and QUICKSORT\nmakes sure we do not by putting _zi_ and _zj_ in different partitions.\n\nOn the other hand, if either _zi_ or _zj_ is chosen as the pivot before any\nother element in Z_ij_, then that element (as the pivot) will be compared to\n_all_ of the elements of Z_ij_ except itself.\n\n  * The probability that _zi_ is compared to _zj_ is the probability that either is the first element chosen.\n  * Since there are _j_ \\- _i_ \\+ 1 elements in Z_ij_, and pivots are chosen randomly and independently, the probability that any one of them is chosen first is 1/(_j_ \\- _i_ \\+ 1). \n\nTherefore (using the fact that these are mutually exclusive events):\n\n![](fig/analysis-quicksort-expected-3.jpg)\n\nWe can now substitute this probability into the analyis of E[_X_] above and\ncontinue it:\n\n![](fig/analysis-quicksort-expected-4.jpg)\n\nThis is solved by applying equation A.7 for harmonic series, which we can\nmatch by substituting _k_ = _j_ \\- _i_ and shifting the summation indices down\n_i_:\n\n![](fig/analysis-quicksort-expected-5.jpg)\n\nWe can get rid of that pesky \"+ 1\" in the denominator by dropping it and\nswitching to inequality (after all, this is an upper bound analysis), and now\nA7 (shown in box) applies:\n\n![](fig/A7-Harmonic-Series.jpg) ![](fig/analysis-quicksort-expected-6.jpg)\n\nAbove we used the fact that logs of different bases (e.g., ln _n_ and lg _n_)\ngrow the same asymptotically.\n\nTo recap, we started by noting that the total cost is O(_n_ \\+ _X_) where _X_\nis the number of comparisons, and we have just shown that _X_ = O(_n_ lg _n_).\n\nTherefore, the _average running time of QUICKSORT on uniformly distributed\npermutations (random data)_ and the _expected running time of randomized\nQUICKSORT_ are both O(_n_ \\+ _n_ lg _n_) = **O(_n_ lg _n_)**.\n\nThis is the same growth rate as merge sort and heap sort. _Empirical studies\nshow quicksort to be a very efficient sort in practice (better than the other\n_n_ lg _n_ sorts) whenever data is not already ordered._ (When it is nearly\nordered, such as only one item being out of order, insertion sort is a good\nchoice.)\n\n* * *\n\n##  Lower Bound for Comparison Sorts\n\nWe have been studying sorts in which the only operation that is used to gain\ninformation is pairwise comparisons between elements. So far, we have not\nfound a sort faster than O(_n_ lg _n_).\n\nIt turns out it is not possible to give a better guarantee than O(_n_ lg _n_)\nin a comparison sort.\n\nThe proof is an example of a different level of analysis: of all _possible_\nalgorithms of a given type for a problem, rather than particular algorithms\n... pretty powerful.\n\n### Decision Tree Model\n\nA decision tree abstracts the structure of a comparison sort. A given tree\nrepresents the comparisons made by a specific sorting algorithm on inputs of a\ngiven size. Everything else is abstracted, and we count only comparisons.\n\n#### Example Decision Tree\n\nFor example, here is a decision tree for insertion sort on 3 elements.\n\n![](fig/decision-tree-insertion-sort.jpg)\n\nEach internal node represents a branch in the algorithm based on the\ninformation it determines by comparing between elements indexed by their\noriginal positions. For example, at the nodes labeled \"2:3\" we are comparing\nthe item that was originally at position 2 with the item originally at\nposition 3, although they may now be in different positions.\n\nLeaves represent permutations that result. For example, \"⟨2,3,1⟩\" is the\npermutation where the first element in the input was the largest and the third\nelement was the second largest.\n\nThis is just an example of one tree for one sort algorithm on 3 elements. Any\ngiven comparison sort has one tree for each _n_. The tree models all possible\nexecution traces for that algorithm on that input size: a path from the root\nto a leaf is one computation.\n\n#### Reasoning over All Possible Decision Trees\n\nWe don't have to know the specific structure of the trees to do the following\nproof. We don't even have to specify the algorithm(s): the proof works for any\nalgorithm that sorts by comparing pairs of keys. We don't need to know what\nthese comparisons are. Here is why:\n\n  * The root of the tree represents the unpermuted input data.\n  * The leaves of the tree represent the possible permuted (sorted) results.\n  * The branch at each internal node of the tree represents the outcome of a comparision that changes the state of the computation. \n  * The paths from the root to the leaves represent possible courses that the computation can take: to get from the unsorted data at the root to the sorted result at a leaf, the algorithm must traverse a path from the root to the correct leaf by making a series of comparisons (and permuting the elements as needed) \n  * The length of this path is the runtime of the algorithm on the given data.\n  * Therefore, if we can derive a lower bound on the height of _any_ such tree, we have a lower bound on the running time _any_ comparison sort algorithm. \n\n### Proof of Lower Bound\n\nWe get our result by showing that the number of leaves for a tree of input\nsize _n_ implies that the tree must have minimum height O(_n_ lg _n_). This\nwill be a lower bound on the running time of _any_ comparison sort algorithm.\n\n  * There are at least _n_! leaves because every permutation appears at least once (the algorithm must correctly sort every possible permutation): _l_ ≥ _n_! \n  * Any binary tree of height _h_ has _l_ ≤ 2_h_ leaves ([Notes #8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html))\n  * Putting these facts together:   _n_! ≤ _l_ ≤ 2_h_   or   2_h_ ≥ _n_!\n  * Taking logs:   _h_ ≥ lg(_n_!) \n  * Using Sterling's approximation (formula 3.17):   _n_! > (_n_/_e_)_n_\n  * Substituting into the inequality: \n\n> _h_   ≥   lg(_n_/_e_)_n_  \n    =   _n_ lg(_n_/_e_)  \n    =   _n_ lg _n_ \\- _n_ lg _e_   \n    =   Ω (_n_ lg _n_). \n\nThus, the height of a decision tree that permutes _n_ elements to all possible\npermutations cannot be less than _n_ lg _n_.\n\nA path from the leaf to the root in the decision tree corresponds to a\nsequence of comparisons, so there will always be some input that requires at\nleast O(_n_ lg _n_) comparisions in _any_ comparision based sort.\n\nThere may be some specific paths from the root to a leaf that are shorter. For\nexample, when insertion sort is given sorted data it follows an O(_n_) path.\nBut to give an o(_n_ lg _n_) guarantee (i.e, strictly better than O(_n_ lg\n_n_)), one must show that _ all_ paths are shorter than O(_n_ lg _n_), or that\nthe tree height is o(_n_ lg _n_) and we have just shown that this is\nimpossible since it is Ω(_n_ lg _n_).\n\n* * *\n\n##  O(n) Sorts\n\nUnder some conditions it is possible to sort data without comparing two\nelements to each other. If we know something about the structure of the data\nwe can sometimes achieve O(n) sorting. Typically these algorithms work by\nusing information about the keys themselves to put them \"in their place\"\nwithout comparisons. We only introduce these algorithms very briefly so you\nare aware that they exist.\n\n### Counting Sort\n\nAssumes (requires) that keys to be sorted are integers in {0, 1, ... _k_}.\n\nFor each element in the input, determines how many elements are less than that\ninput.\n\nThen we can place the element directly in a position that leaves room for the\nelements below it.\n\n![](fig/pseudocode-counting-sort.jpg)\n\nAn example ...\n\n![](fig/Fig-8-2-counting-sort-trace.jpg)\n\nCounting sort is a **stable sort**, meaning that two elements that are equal\nunder their key will stay in the same order as they were in the original\nsequence. This is a useful property ...\n\nCounting sort requires Θ(_n_ \\+ _k_). Since _k_ is constant in practice, this\nis Θ(_n_).\n\n### Radix Sort\n\n![](fig/320px-Punch_card_sorter.JPG)\n\nUsing a stable sort like counting sort, we can sort from least to most\nsignificant digit:\n\n![](fig/Fig-8-3-radix-sort-trace.jpg)\n\nThis is how punched card sorters used to work. _ (When I was an undergraduate\nstudent my University still had punched cards, and we had to do an assignment\nusing them mainly so that we would appreciate not having to use them!)_\n\nThe code is trivial, but requires a stable sort and only works on _n_ _d_-\ndigit numbers in which each digit can take up to _k_ possible values:\n\n![](fig/pseudocode-radix-sort.jpg)\n\nIf the stable sort used is Θ(_n_ \\+ _k_) time (like counting sort) then RADIX-\nSORT is Θ(_d_(_n_ \\+ _k_)) time.\n\n### Bucket Sort\n\nThis one is reminiscent of hashing with chaining.\n\nIt maps the keys to the interval [0, 1), placing each of the _n_ input\nelements into one of _n_-1 buckets. If there are collisions, chaining (linked\nlists) are used.\n\nThen it sorts the chains before concatenating them.\n\nIt assumes that the input is from a random distribution, so that the chains\nare expected to be short (bounded by constant length).\n\n![](fig/pseudocode-bucket-sort.jpg)\n\n#### Example:\n\nThe numbers in the input array A are thrown into the buckets in B according to\ntheir magnitude. For example, 0.78 is put into bucket 7, which is for keys 0.7\n≤ _k_ < 0.8. Later on, 0.72 maps to the same bucket: like chaining in hash\ntables, we \"push\" it onto the beginning of the linked list.\n\n![](fig/Fig-8-4-bucket-sort-trace.jpg)\n\nAt the end, we sort the lists (B shows the lists after they are sorted;\notherwise we would have 0.23, 0.21, 0.26) and then copy the values from the\nlists back into an array.\n\nBut sorting linked lists is awkward, and I am not sure why CLRS's pseudocode\nand figure imply that one does this. In an alternate implementation, steps 7-9\ncan be done simultaneously: scan each linked list in order, inserting the\nvalues into the array and keeping track of the next free position. Insert the\nnext value at this position and then scan back to find where it belongs,\nswapping if needed as in insertion sort.\n\nSince the values are already partially sorted, an insertion procedure won't\nhave to scan back very far. For example, suppose 0.78 had been inserted after\n0.72. The insertion would only have to scan over one item to put 0.78 in its\nplace, as all values in lists 0..6 are smaller.\n\n* * *\n\n## Comparing the Sorts\n\n![](fig/comparing-sorts.jpg)\n\nYou can also compare some of the sorts with these animations (set to 50\nelements): <http://www.sorting-algorithms.com/>. Do the algorithms make more\nsense now?\n\n* * *\n\n## Next\n\nWe return to the study of trees, with balanced trees.\n\n* * *\n\nDan Suthers Last modified: Wed Feb 19 02:14:38 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition, and from Wikipedia commons.  \n\n",
 "path"=>"morea//100.quicksort/reading-notes.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to quicksort",
 "published"=>true,
 "morea_id"=>"reading-screencast-10a",
 "morea_summary"=>"Basic ideas about quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=v1ghdc_hwMI",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-a.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Quicksort: Randomization and analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-10b",
 "morea_summary"=>"Randomizing and analyzing quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=qS9oMz4_kTU",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-b.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Bounds on sorting",
 "published"=>true,
 "morea_id"=>"reading-screencast-10c",
 "morea_summary"=>"Lower bounds on comparison sorts, and O(n) sorts",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gZmEYyqHefk",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-c.md"}
</pre>

<h2>/morea/110.balanced-trees/experience-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of red-black trees",
 "published"=>true,
 "morea_id"=>"experience-balanced-trees-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about balanced trees (at home).",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/110.balanced-trees/experience-2.html",
 "url"=>"/morea/110.balanced-trees/experience-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### RBT and (2,4) Deletion\n\n**2\\. (6 pts)** Delete the keys **8, 6, 5, 3, 2, 1** in that order from the red/black tree shown below with its (2,4) representation. Following the format we used in class (see above), for each key:\n   \n(a) Show the RBT after the BST-style deletion but before RB-Delete-Fixup  \n(b) Identify (in writing) whether there is a **double black (identifying the node)**, corresponding to underflow. \n(c) Identify (in writing) the situation (the color of the sibling and its\nchildren) and its remedy (adjustment, recolor, and/or or restructure?), using\nthe cases in the web notes.  \n(d) Show the RBT after RB-Delete-Fixup  \n(e) Show the (2,4) tree representation that results.\n\nThe solution to Thursday's sequence of insertions is shown below, to be sure\nyou start with a correct tree. You may use a drawing program or just do it on\npaper in dark ink (or soft dark pencil that reproduces well) and scan or\nphotograph your work.\n\n![](fig/Problem-Set-6-Class-2014-Final.jpg)\n\n### Cormen et al. RBT Code\n\nThe lecture notes were based on Goodrich & Tamassia's textbook, because they\nshow the correspondence of RBTs to 2-4 trees, which makes the former easier to\nunderstand as balanced trees.\n\nThe CLRS version differs somewhat. Because of CLRS's reputation, I would tend\nto trust their version for an actual implementation, even if their\npresentation is more difficult to understand. Below are a few questions to\nhelp you understand the CLRS version.\n\nThe cases for insertion are similar between G&T and CLRS, but the terminology\ndiffers (e.g., what the letters w, x, y, and z refer to). The cases for\ndeletion differ: G&T have 3 while CLRS have 4! Be careful because there are\nmirror images of every situation (e.g., is the double black node a left child\nor a right child?): G&T and CLRS may be describing the same situation with\nmirror image graphs.\n\nThe top level methods in CLRS for RB-INSERT (p. 315) and RB-DELETE (p. 324)\nessentially do legal binary search three (BST) insertion and deletion, and\nthen call \"FIXUP\" methods to fix the red-black properties. Thus they are very\nsimilar to the BST methods TREE-INSERT (p. 294) and TREE-DELETE (p. 298). The\nreal work specific to RBTs is in these fixup methods, so we will focus on them\nin these questions, but you should also study the top level methods to\nunderstand them as BST methods.\n\n#### In RB-INSERT-FIXUP (p. 316):\n\n**3\\. (1 pt)** Which lines of the code handle incorrect representation of the 2-4 node (G&T case 1 in the web notes)? \n\n**4\\. (1 pt)** Which lines of the code handle overflow of the 2-4 node (G&T case 2 in the web notes)? \n\n#### In RB-DELETE-FIXUP (p. 326):\n\n**5\\. (1 pt)** _During the while loop of RB-DELETE-FIXUP, which line(s) of code remove double-black from node x?_ Note: do NOT answer \"line 23\" as this is outside the while loop: I am asking how double black moves up the tree inside the while loop. You will need to read the text: the code doesn't make it obvious. But once you have understood this, the cases in figure 13.7 will be easier to understand for the next question. \n\n**6\\. (1 pt)** G&T deletion has three cases (see web notes) while CLRS deletion has four (see figure 13.7 and explanation in the text). Two of the G&T cases correspond to the CLRS cases. Which G&T deletion cases map directly to which CLRS deletion cases? (Give two pairs). Explain why. \n\nThe correspondence between the other cases is harder to understand in a simple\nway. If you see it, you are welcome to try to explain it.\n\n* * *\n\nDan Suthers Last modified: Sat Mar 1 03:35:53 HST 2014\n\n",
 "path"=>"morea//110.balanced-trees/experience-2.md"}
</pre>

<h2>/morea/110.balanced-trees/experience.html</h2>

<pre>Hash
{"title"=>"Gaining insight into Red-Black tree operations",
 "published"=>true,
 "morea_id"=>"experience-balanced-trees",
 "morea_type"=>"experience",
 "morea_summary"=>"Play with insertion and deletion",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/110.balanced-trees/experience.html",
 "url"=>"/morea/110.balanced-trees/experience.html",
 "content"=>
  "## In Class Thursday\n\nInsert the keys **6, 5, 4, 2, 3, 1** in that order into a Red-Black Tree\nrepresentation of a (2,4) balanced tree. For each key:  \n\n(a) Show the RBT after the insertion but before RB-Insert-Fixup  \n\n(b) Identify (in writing) the RBT property that is violated and whether that\nviolation corresponds to overflow in or incorrect representation of the\ncorresponding (2,4) tree (see next item to help you decide this).  \n\n(c) Identify (in writing) the situation (red uncle/sibling or black\nuncle/sibling?) and its remedy (recolor or restructure?).  \n\n(d) Show the RBT after RB-Insert-Fixup  \n\n(e) Show the (2,4) tree representation that results.  \n\nThe first three keys have been inserted to get you started.\n\n![](fig/Problem-Set-6-Class-2014-Start-a.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-b.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-c.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-d.jpg)  \n\n",
 "path"=>"morea//110.balanced-trees/experience.md"}
</pre>

<h2>/morea/110.balanced-trees/module.html</h2>

<pre>Hash
{"title"=>"Balanced Trees",
 "published"=>true,
 "morea_id"=>"balanced-trees",
 "morea_outcomes"=>["outcome-balanced-trees-algorithm"],
 "morea_readings"=>
  ["reading-screencast-11a",
   "reading-screencast-11b",
   "reading-screencast-11c",
   "reading-screencast-11d",
   "reading-cormen-13",
   "reading-sedgewick-15",
   "reading-notes-11"],
 "morea_experiences"=>
  ["experience-balanced-trees",
   "experience-balanced-trees-2",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/110.balanced-trees/logo.png",
 "morea_sort_order"=>110,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/110.balanced-trees/module.html",
 "content"=>
  "(2,4) trees, red-black trees, insertion, deletion, rotations, comparison of dictionary implementations.\n",
 "path"=>"morea//110.balanced-trees/module.md"}
</pre>

<h2>/modules/balanced-trees/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Balanced Trees",
 "url"=>"/modules/balanced-trees/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/balanced-trees/index.html"}
</pre>

<h2>/morea/110.balanced-trees/outcome.html</h2>

<pre>Hash
{"title"=>"Understand the balanced tree algorithm",
 "published"=>true,
 "morea_id"=>"outcome-balanced-trees-algorithm",
 "morea_type"=>"outcome",
 "morea_sort_order"=>110,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/110.balanced-trees/outcome.html",
 "content"=>
  "Be able to step through insertion and deletion procedures for red-black trees. \n",
 "path"=>"morea//110.balanced-trees/outcome.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 13 - Red-Back Trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-13",
 "morea_summary"=>"Properties, rotations, insertion, and deletion",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "31 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-cormen.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on balanced trees",
 "published"=>true,
 "morea_id"=>"reading-notes-11",
 "morea_summary"=>"Balanced trees and operations on them",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/110.balanced-trees/reading-notes.html",
 "url"=>"/morea/110.balanced-trees/reading-notes.html",
 "content"=>
  "## Outline\n\n  * Balanced and Multi-Way Trees \n  * 2-3-4 or (2,4) Trees \n  * Red-Black Trees \n    * as a binary representation of (2,4) trees\n    * as binary search trees\n  * Insertion in Red-Black Trees\n  * Deletion in Red-Black Trees\n  * Comparison of Dictionary Implementations\n\n_(Much of this material is derived from Goodrich & Tamassia's slides widely\navailable on the Web.)_\n\n* * *\n\n##  Multi-Way Trees\n\nA **multi-way search tree** is an ordered tree such that\n\n  * Each internal node has at least two children and stores _d_-1 key-element items (_ki_, _oi_), where _d_ is the number of children\n  * For a node with children _v1_ _v2_ ... _vd_ storing keys _k1_ _k2_ ... _kd-1_\n    * keys in the subtree of _v1_ are less than _k1_\n    * keys in the subtree of _vi_ are between _ki-1_ and _ki_ (_i_ = 2, ..., _d_ \\- 1)\n    * keys in the subtree of _vd_ are greater than _kd-1_\n  * The leaves store no items and serve as placeholders\n\n![](fig/GT-multiway-search-tree.jpg)\n\n**Multi-way inorder traversal** can be defined by extension of BST inorder traversal to visit the keys in increasing order:\n\n> Visit item (_ki_, _oi_) of node _v_ between the recursive traversals of the\nsubtrees of _v_ rooted at children _vi_ and _vi+1_.\n\n![](fig/GT-multiway-inorder-traversal.jpg)\n\n**Searching** can similarly be extended to multi-way trees by searching within each node as well as down the tree:\n\n  * At each internal node with children _v1_ _v2_ ... _vd_ and keys _k1_ _k2_ ... _kd-1_: \n    * _k_ = _ki_ (_i_ = 1, ... , _d_ \\- 1): the search terminates with success\n    * _k_ < _k1_: we continue the search in child _v1_\n    * _ki-1_ < _k_ < _ki_ (_i_ = 2, ... , _d_ \\- 1): we continue the search in child _vi_\n    * _k_ > _kd-1_: we continue the search in child _vd_\n  * Reaching an external node terminates the search unsuccessfully\n\nFor example, searching for key 30:\n\n![](fig/GT-multiway-searching.jpg)\n\n* * *\n\n## (2,4), 2-4 or 2-3-4 Trees\n\nThese are multi-way trees restricted in two ways:\n\n  * **Node Size Property**: every internal node has at least two children (one key) and at most four children (three keys).\n  * **Depth Property**: all of the external nodes have the same depth. (The tree is balanced.)\n\nThe internal nodes are called 2-nodes, 3-nodes or 4-nodes, depending on the\nnumber of children they have.\n\n![](fig/GT-2-4-trees.jpg)\n\n### Height of (2,4) Trees and Searching\n\n**_Theorem:_ A (2,4) tree storing n items has height Θ(log n).**\n\nProof:\n\nLet _h_ be the height of a (2,4) tree with _n_ items. The tallest possible\ntree (worst case) for a fixed _n_ is when all internal nodes are 2-nodes\n(i.e., the tree is equivalent to a binary tree), so we restrict consideration\nto this case. Due to the depth property, the tree at depth _h_-1 is filled, so\nit is a complete binary tree.\n\n![](fig/GT-2-4-tree-height.jpg)\n\nThe figure illustrates the number of nodes in each level of a complete binary\ntree. Since there are at least 2_i_ items at depth _i_ = 0, ... , _h_-1 and no\nitems at depth _h_ (the leaves store no items):  \n    _n_ ≥ 1 + 2 + 4 + ... + 2_h_-1  \n(we use ≥ because there could be more items in internal 3-nodes or 4-nodes,\nleading to \"better cases\" where _n_ increases without a penalty in _h_).\n\n![](fig/formula-A-5.jpg)\n\nApplying formula A5 (shown) for geometric series with _n_ = _h_-1 and _x_ = 2,  \n1 + 2 + 4 + ... + 2_h_-1 = Σ_k_=0,_h_-12_k_ = (2(h-1) + 1 \\- 1)/(2 - 1) = 2_h_\n\\- 1, so  \n    _n_ ≥ = 2_h_ \\- 1   or   n + 1 ≥ 2_h_\n\nTaking the log of both sides:   lg (_n_ \\+ 1) ≥ _h_.  \nThus, _h_ = Θ(lg _n_).\n\n(See also similar facts concerning full binary trees in [Topic\n8](http://www2.hawaii.edu/~suthers/courses/ics311f12/Notes/Topic-08.html).)\n\nSince searching in a (2,4) tree with _n_ items requires time proportional to a\npath from root to leaves, searching is **O(lg _n_)** time.\n\n### (2,4) Tree Insertion\n\nWe will examine insertion and deletion briefly to understand the conceptual\ncases.\n\nInsert a new item keyed by _k_ into _(not below)_ the parent of the leaf\nreached by searching for _k_. (_In this respect, (2,4) trees differ from\nbinary search trees._)\n\nThis preserves depth but may cause **overflow** (a node may become a 5-node).\n\n_Example:_ Inserting 30, we find its position between 27 and 32. However\ninserting here causes overflow:\n\n![](fig/GT-2-4-tree-insertion.jpg)\n\nOverflow is handled with a **split operation**, as illustrated below with a\nsimpler tree:\n\n  * The 5-node containing keys _k1_, _k2_, _k3_, _k4_ is split into a 3-node with keys _k1_, _k2_ and a 2-node with key _k4_.\n  * Key _k3_ is inserted into the parent node (as would be the case with the tree above).\n  * Overflow may propagate to the parent node.\n  * A new root may be created if the root overflows.\n\n![](fig/GT-2-4-tree-overflow-split.jpg)\n\n_(Note: Sedgewick splits 4-nodes on the way down while searching for the\ninsertion position, guaranteeing that there will be no overflow. Both Goodrich\n& Tamassia and Cormen et al. take the other approach, propagating splits\nupwards only as needed. The asymptotic time complexity remains the same.)_\n\n#### Time Complexity of (2,4) Insertion\n\nA tree with _n_ items has Θ(lg _n_) height. The algorithm first searches for\nthe insertion location, which may require visiting _h_ = Θ(lg _n_) nodes (Θ,\nnot O, because we must go to the leaves in all cases). The insertion takes\nΘ(1) time. If there is overflow, splits (taking Θ(1) time each) may be\npropagated upwards to as many as O(lg _n_) nodes. Since the Θ(lg _n_)\noverrides the possibility of slower growing functions in O(lg _n_), insertion\nis **Θ(lg _n_)**.\n\n### (2,4) Tree Deletion\n\nIf the entry to be deleted is in a node that has internal nodes as children,\nwe replace the entry to be deleted with its inorder successor and delete the\nlatter entry. Example: to delete key 24, we replace it with 27 (inorder\nsuccessor):\n\n![](fig/GT-2-4-tree-deletion.jpg)\n\nThis reduces deletion of an entry to the case where the item is at the node\nwith leaf children.\n\nDeletion of an entry from a node _v_ may cause **underflow,** where node _v_\nbecomes a 1-node with one child and no keys. Underflow at node _v_ with parent\n_u_ is handled in two cases.\n\n**_Case 1_**: An adjacent sibling of _v_ is a 2-node. Perform a **fusion operation**, merging _v_ with the adjacent 2-node sibling _w_ and moving an entry from _u_ to the merged node _v'_.\n\n![](fig/GT-2-4-tree-underflow-fusion.jpg)\n\nAfter a fusion, the underflow may propagate to the parent u, for at most O(lg\n_n_) adjustments up the tree.\n\n**_Case 2_**: An adjacent sibling _w_ of _v_ is a 3-node or a 4-node. Perform a **transfer operation:** move a child of _w_ to _v_; an item from _u_ to _v_; and an item from _w_ to _u_. \n\n![](fig/GT-2-4-tree-underflow-transfer.jpg)\n\nA transfer eliminates underflow.\n\n#### Time Complexity of (2,4) Deletion\n\nThe algorithm first searches for the item to delete, which requires visiting\n_h_ = Θ(lg _n_) nodes on the way down the tree, either to find a bottom level\nkey to delete, or to find the successor of a key in an internal node to\ndelete. Underflow is handled with up to O(lg _n_) fusions and transfers, each\ntaking Θ(1) time. Thus deletion is **Θ(lg _n_)**.\n\n* * *\n\n##  Red-Black Trees\n\n### Red-Black Tree Properties\n\n![](fig/Simple-Red-Black-Tree.jpg)\n\nA red-black tree (RBT) is a binary search tree with the following additional\nproperties:\n\n  1. **Color property**: Every node is either red or black. _(We can indicate this either by coloring the node or by coloring its parent link.)_\n  2. **Root property**: The root is black\n  3. **External property**: Every leaf is black.\n  4. **Internal property**: If a node is red, then both of its children are black. _(Hence, no two reds in a row are allowed on a simple path from the root to a leaf.)_\n  5. **Depth property**: For each node, all the paths from the node to descendant leaves contain the same number of black nodes (the **black height** of the node).\n\nThese properties seem rather arbitrary until we consider the correspondence\nwith (2,4) trees shortly, but first let's see how the properties hold in an\nexample ...\n\n![](fig/Fig-13-1-RBT-Representation-a.jpg)\n\n### Red-Black Tree Representation\n\nA single extra bit is required on each node to mark it as \"red\" or \"black\".\n\nTo save space, we can represent the leaf nodes _and_ the parent with a single\nnode, T.nil:\n\n![](fig/Fig-13-1-RBT-Representation-b.jpg)\n\nThis also simplifies the code, as we can follow pointers without having to\ncheck for null pointers.\n\nWe usually don't draw T.nil:\n\n![](fig/Fig-13-1-RBT-Representation-c.jpg)\n\n### RBTs as a Binary Representation of (2,4) Trees\n\nIt would be rather complex to implement and manipulate 2-nodes, 3-nodes and\n4-nodes. One motivation for red-black trees is that they provide a binary tree\nrepresentation of (2,4) trees, enabling us to manipulate only one kind of\nnode. The mapping is as follows (__you should make sure you understand this\nwell before going on!__):\n\n![](fig/GT-From-2-4-to-RBT.jpg)\n\n**Red nodes (and the links from their parents) capture the _internal structure of a (2,3) node_;**\n\n**Black nodes (and the links from their parents) capture the _structure of the (2,3) tree_ itself.**\n\n### RBTs as Binary Search Trees\n\nAt the same time as they represent (2,4) trees, _**RBTs are also Binary Search\nTrees**_: they satisfy the Binary Search Tree property. For example, here is a\nRBT: we can search for keys or enumerate elements in order as usual:\n\n![](fig/GT-RBTs.jpg)\n\nIn order to maintain the Red-Black-Tree properties, it will be necessary to do\nstructural rotations. These rotations are designed to not disrupt the BST\nproperty. For example, this rotation does not disturb the BST ordering of keys\n9, 11, 12, 14, 17, 18, 19:\n\n![](fig/Fig-13-3-Rotation-BST.jpg)\n\n#### Height of Red-Black Trees and Searching\n\nTheorem: A red-black tree storing n items has height Θ(lg _n_).  \nProof:\n\n  * Let _h_ be the height of a red-black tree with _n_ items\n  * By property 4, there cannot be more red nodes (and links) on a simple path from the root to a leaf than there are black nodes (and links). \n  * Therefore the black height of the root of the tree is between _h_ and _h_/2. \n  * The black height of the root of the red-black tree corresponds to the height _h'_ of the (2,4) tree that the red-black tree represents (since red nodes/links in the RBT represent the internal structure of the nodes in the (2,4) tree). \n  * From the theorem concerning the height of (2,4) trees, _h'_ is Θ(lg _n_). \n  * Since _h_ is no more than twice _h'_, _h_ is also Θ(lg _n_).\n\n(See Cormen et al. for a proof not relying on (2,4) trees.)\n\nTherefore, searching in a red-black tree with _n_ items takes **O(lg _n_)**\ntime (O rather than Θ as we may find the key in an internal node).\n\nWe now consider insertion and deletion. Please see the textbook for the many\ndetails of implementation in pseudocode, etc.: here we will concentrate on\nseeing how the RBT operations correspond to (2,4) tree operations.\n\n###  Insertion in Red-Black Trees\n\nTo insert an element with key _k_, perform the insertion for binary search\ntrees (except that conceptually we insert _k_ in an internal node with null\nchildren, not at a leaf node), and color the newly inserted node _z_ red,\nunless it is the root.\n\nThis preserves the color, root, external, and depth properties. _(You should\ncheck this in the example below.)_\n\nIf the parent _v_ of _z_ is black, this also preserves the internal property\nand we are done.\n\nElse (_v_ is red), we have a **double red** (i.e., a violation of the internal\nproperty), which requires a reorganization of the tree. For example, insert 4:\n\n![](fig/GT-RBT-insertion.jpg)\n\nA double red with child _z_ and parent _v_ is dealt with in two cases. Let _w_\nbe the sibling of _v_ (and hence the uncle of _z_).\n\n**_Case 1:_** _w_ is black. The double red is an _**incorrect representation**_ of a 4-node. (We will fix this with restructuring). For example, the RBT on the left is an incorrect representation of the (2,4) tree on the right:\n\n![](fig/GT-RBT-double-red-case-1.jpg)\n\n**_Case 2:_** w is red. The double red corresponds to an _**overflow**_ in the (2,4) tree. (We will fix this with recoloring, which is the equivalent of a (2,4) split.) For example:\n\n![](fig/GT-RBT-double-red-case-2.jpg)\n\n#### Restructuring\n\n**Restructuring** remedies a child-parent double red when the parent red node has a black sibling. It restores the correct representation (internal property) of a 4-node, leaving other RBT and BST properties intact: \n\n![](fig/GT-RBT-restructuring.jpg)\n\nThere are four restructuring configurations depending on whether the double\nred nodes are left or right children. They all lead to the same end\nconfiguration of a black with two red children:\n\n![](fig/GT-RBT-restructuring-configurations.jpg)\n\nAfter a restructuring, the double red has been remedied without violating any\nof the other properties _(you should verify this)_: there is no need to\npropagate changes upwards.\n\nNotice that the height of the subtree tree has been reduced by one. **_This is\nthe operation that keeps the trees balanced to within a constant factor of\nlg(_n_) height_**, by ensuring that the height of the RBT is no more than\ntwice that of the (necessarily balanced) 2-4 tree it represents. _Do you see\nwhy?_\n\n#### Recoloring\n\n**Recoloring** remedies a child-parent double red when the parent red node has a red sibling. The parent _v_ and its sibling _w_ become black and the grandparent _u_ becomes red, unless it is the root.\n\nIt is equivalent to performing a split on a 5-node in a (2,4) tree. (When\nthere is a double red and yet another red in the parent's sibling, we are\ntrying to collect too many keys under the grandparent.) For example, the RBT\nrecoloring on the top corresponds to the (2,4) transformation on the bottom:\n\n![](fig/GT-RBT-recoloring.jpg)\n\nNotice that in this example the parent \"4\" is now red, meaning it belongs to\nits parent node in the (2,4) tree. The double red violation may propagate to\nthis parent in the RBT, which corresponds to the overflow propagating up the\n(2,4) tree, requiring further repair.\n\n#### Time Complexity of RBT Insertion\n\nWe already established that insertion in (2,4) trees is Θ(lg _n_) due to their\nheight. Since RBTs are only at most twice as high, we might expect this result\nto transfer, and it does, but it needs to be shown separately since the\nmanipulations of the RBT are different. So:\n\n  * The algorithm first searches for the insertion location, which will require visiting _h_ = Θ(lg _n_) nodes on the way down the tree (since we are searching for a leaf node and the tree is balanced).\n  * Adding the item takes O(1). \n  * Recolorings and restructurings are Θ(1) each, and we perform at most O(lg _n_) recolorings and _one_ restructuring propagating structural changes back up the tree.\n\nThus insertion is **Θ(lg _n_).**\n\nNote: A top-down version of this algorithm is also possible, restructuring on\nthe way down and requiring only one pass through the tree. See the Sedgewick\nreading distributed.\n\n###  Deletion in Red-Black Trees\n\nTo remove item with key _k_, we first perform the BST deletion (modified for\nour representational changes using T.nil).\n\nBecause deletion of a node higher in the tree involves replacing it with its\nsuccessor, which is then deleted, deletion always involves an internal and an\nexternal node.\n\nWe can preserve the RBT properties at the new internal location of the\nsuccessor by giving the successor the color of the node deleted, so we need\nonly be concerned with possible violations of RBT properties at the bottom of\nthe tree, where the successor was moved from, or where a node without a\nsuccessor was deleted.\n\nLet _v_ be the internal node removed, _w_ the external node removed, and _r_\nthe sibling of _w_:\n\n    \n    \n        x       \n         \\               x                    \n          v       ==>     \\\n         / \\               r \n        r   w\n    \n\nIf either _v_ or _r_ was red, we color _r_ black and we are done (the number\nof black nodes has not changed).\n\nElse (_v_ and _r_ were both black), we have removed a black node, violating\nthe depth property. We fix this by coloring _r_ **double black,** a fictional\ncolor. (Intuitively, the black of both _v_ and _r_ have been absorbed into\n_r_.) Now we have the correct \"amount\" of black on this path from root to\nleaf, but the double black violates the color property.\n\nFixing this will require a reorganization of the tree. Example: deletion of 8\ncauses a double black:\n\n![](fig/GT-RBT-deletion-double-black.jpg)\n\nA double black corresonds to _**underflow**_ in (2,4) trees (and here the\nimages I am borrowing from Goodrich & Tamassia go to greyscale!):\n\n![](fig/GT-double-black-as-underflow.jpg)\n\nGoodrich & Tamassia's algorithm for remedying a double black node _w_ with\nsibling _y_ considers _three cases_, discussed below. _(Note that these are\ndifferent from CLRS's four cases!)_\n\n**_Case 1:_** _y_ is black and has a red child: Perform a RBT **restructuring**, equivalent to a (2,4) **transfer**, and we are done.\n\nFor example, if we have the RBT on the left corresponding to underflow in the\n(2,4) tree on the right:\n\n![](fig/GT-RBT-DB-remedy-case-1-1.jpg)\n\n... we do the following restructuring:\n\n![](fig/GT-RBT-DB-remedy-case-1-2.jpg)\n\n**_Case 2:_** _y_ is black and its children are both black: Perform a RBT **recoloring**, equivalent to a (2,4) **fusion**, which may propagate up the double black violation.\n\nIf the double-black reaches the root we can just remove it, as it is now on\n_all_ of the paths from the root to the leaves, so does not affect property 5,\nthe depth property.\n\nFor example, if we have the RBT on the left corresponding to underflow in the\n(2,4) tree on the right:\n\n![](fig/GT-RBT-DB-remedy-case-2-1.jpg)\n\n... we do the following recoloring: the black node _y_ is colored red, and the\ndouble black node _r_ is colored ordinary black:\n\n![](fig/GT-RBT-DB-remedy-case-2-2.jpg)\n\nThe root of the above subtree takes on an extra black, which propagates only\nif it was previously black and is not the root. If it was red it merely turns\nblack; if it was the root the extra black no longer affects the balanced black\nheight of the tree.\n\n**_Case 3:_** _y_ is red: Perform a RBT **adjustment**, equivalent to choosing a different representation of a 3-node, after which either Case 1 or Case 2 applies.\n\n![](fig/GT-RBT-DB-remedy-case-3.jpg)\n\nThese are both representations of the following 2-4 tree, but the\ntransformation allows one of the other cases to apply, reducing duplication of\ncases.\n\n![](fig/GT-RBT-DB-remedy-case-3-2-4.jpg)\n\nThe CLRS chapter divides the situation up into four cases: try to see whether\nyou can map between the above cases and theirs!\n\n#### Time Complexity of RBT Deletion\n\nThe analysis is similar to the previous ones: Θ(lg _n_) search to find the\ndeletion point (the item to delete may be in an internal node, but we always\nfind its successor in any case, which is at the bottom of the tree), followed\nby deletion and restructuring O(1) operations that are propagated at most up\nO(lg _n_) levels. Deletion is **Θ(lg _n_)**.\n\n### RBT Animations\n\nYou may want to look at these:\n\n<http://secs.ceas.uc.edu/~franco/C321/html/RedBlack/redblack.html>\n\n    A java applet. You can go step by step and it tells you the rules violated and the fixes. Must click on \"next step\" until done with process. To delete, click on Delete and then on the node to be deleted.\n<http://www.csanimated.com/animation.php?t=Red-black_tree>\n\n     A flash animation: slides with voice-over. It goes kind of fast (little time to figure out what property is being fixed in each case), and does not let you control slide by slide. \n\n* * *\n\n## Related Data Structures\n\n**AVL Trees,** named for their authors, are the oldest balanced trees. They are binary trees with the requirement that the heights of the left and right subtree of any given node differ at most by 1\\. A small amount of extra storage is needed to record height differences. Their operations are O(lg _n_) like RBTs, but may require O(lg _n_) rotations to rebalance. \n\n**Splay Trees** are binary trees in which an adjustment moving a node towards the root called _splaying_ is done after every access (including search). There are no rules about properties to maintain and no labels. Amazingly, splaying alone is enough to guarantee O(lg _n_) behavior in an amortized sense: we will use these as an example when we cover chapter 17 Amortized analysis. They also make frequently accessed items more accessible. \n\n**B-Trees,** covered in Chapter 18 of Cormen et al. (but not in this course), are balanced multi-way trees that allow up to M keys per node for large M. They are used for trees in external (disk) storage, where speed is optimized by making the size of a node be the same as the size of a block read in by one disk read.\n\n* * *\n\n##  Comparison of Dictionary Implementations\n\nFirst, here is a summary of the correspondence between (2,4) and Red-Black\ntree operations:\n\n<table width=\"100%\" border=\"1\">\n  <tr>\n    <th colspan=\"3\" scope=\"col\"><div align=\"left\">Insertion: Remedy double red</div></th>\n  </tr>\n  <tr>\n    <th scope=\"row\"><div align=\"left\">(2,4) tree action</div></th>\n    <td><div align=\"left\"><strong>Red-Black Tree Action</strong></div></td>\n    <td><div align=\"left\"><strong>Result</strong></div></td>\n  </tr>\n  <tr>\n    <td>Change of 4-node representation</td>\n    <td>Restructuring</td>\n    <td>Double red removed</td>\n  </tr>\n  <tr>\n    <td>Split</td>\n    <td>Recoloring</td>\n    <td>Double red removed or propagated up</td>\n  </tr>\n  <tr>\n    <th colspan=\"3\" scope=\"row\">&nbsp;</th>\n  </tr>\n  <tr>\n    <th colspan=\"3\" scope=\"row\"><div align=\"left\">Deletion: Remedy double black</div></th>\n  </tr>\n  <tr>\n    <th scope=\"row\"><div align=\"left\">(2,4) tree action</div></th>\n    <td><strong>Red-Black Tree Action</strong></td>\n    <td><strong>Result</strong></td>\n  </tr>\n  <tr>\n    <td>Transfer</td>\n    <td>Restructuring</td>\n    <td>Double black removed</td>\n  </tr>\n  <tr>\n    <td>Fusion</td>\n    <td>Recoloring</td>\n    <td>Double black removed or propagated up</td>\n  </tr>\n  <tr>\n    <td>Change of 3-node representation</td>\n    <td>Adjustment</td>\n    <td>Restructuring or recoloring follows</td>\n  </tr>\n</table>\n\n\n### A comparison of run times.\n\n<table width=\"100%\" border=\"1\">\n  <tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Search</th>\n    <th scope=\"col\">Insert</th>\n    <th scope=\"col\">Delete</th>\n    <th scope=\"col\">Notes</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">Hash Table</th>\n    <td>O(1) expected</td>\n    <td>O(1) expected</td>\n    <td>O(1) expected</td>\n    <td><p>No ordered dictionary methods. Simple to implement.</p>\n    </td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Doubly Linked List</th>\n    <td>O(<i>n</i>)</td>\n    <td>O(1) if not sorted; O(<i>n</i>) if sorted </td>\n    <td>&Theta;(1) if node given, O(<i>n</i>) otherwise</td>\n    <td>Simple to implement.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Skip List</th>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>Randomized insertion. Simple to implement.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Binary Tree</th>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>Moderately complex to implement deletion.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Red-Black Tree</th>\n    <td>O(lg <i>n</i>) worst case</td>\n    <td>&Theta;(lg <i>n</i>)</td>\n    <td>&Theta;(lg <i>n</i>) </td>\n    <td>Complex to implement.</td>\n  </tr>\n</table>\n\n\nFrom this we can see that hash tables are most efficient expected behavior\nwhen no ordered methods are needed, and red-black trees give us the best\nguarantee when ordering matters.\n\n* * *\n\nDan Suthers Last modified: Mon Mar 3 20:13:52 HST 2014  \nImages are from lecture slides provided by Michael Goodrich and Roberto\nTamassia, and from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//110.balanced-trees/reading-notes.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to (2,4) Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11a",
 "morea_summary"=>"Basic ideas about balanced trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=N-Sot-yf3As",
 "morea_labels"=>["Screencast", "Suthers", "11 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-a.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Insertion and deletion in (2,4) Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11b",
 "morea_summary"=>"Conceptual overview of balanced tree operations",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=W49P7wqdIuE",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-b.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Red-Black Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11c",
 "morea_summary"=>"Red-Black trees as 2-4 and BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=a6LaiKa3ES0",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-c.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Red-Black Tree Mutation",
 "published"=>true,
 "morea_id"=>"reading-screencast-11d",
 "morea_summary"=>"Red-Black tree insertion and deletion",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=HlwQ_n56MHQ",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-d.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-sedgewick.html</h2>

<pre>Hash
{"title"=>"Sedgewick 15 - Balanced Trees",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-15",
 "morea_summary"=>"Top-down 2-3-4 trees, red-black trees, other algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-sedgewick.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-sedgewick.md"}
</pre>

<h2>/morea/120.dynamic-programming/experience-2.html</h2>

<pre>Hash
{"title"=>"Dynamic programming sample problem: matrix chain multiplication",
 "published"=>true,
 "morea_id"=>"experience-dynamic-programming-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply dynamic programming principles again",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/120.dynamic-programming/experience-2.html",
 "url"=>"/morea/120.dynamic-programming/experience-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### Dynamic Programming\n\n#### 2\\. (8 pts) Matrix Chain\n\nThe Matrix Chain multiplication problem is the classic dynamic programming\nproblem: every algorithms textbook I have seen uses it as an example. Thus you\nshould at least be familiar with it, and this gives us a chance to get a feel\nfor what a Dynamic Programming computation is like.\n\nRead the Matrix Chain section of the book first (it's not in my lectures) so\nyou understand the optimal substructure (page 373-374, summarized in formula\n15.7) and how the matrices **_m_** and **_s_** function in the code.\n\n![](fig/equation-matrix-decomposition.jpg)\n\n_Briefly:_ Given we have an optimal solution that includes the optimal choice\nof where to put the parentheses at the top level, the solutions to the\nsubproblems of how to recursively parenthesize within each half must also be\noptimal, because otherwise we could substitute in a subproblem solution with\nfewer multiplications: the top level number of multiplications would not\nchange, so there would be fewer multiplications overall, contradicting the\nassumption that the solution was optimal. But we don't know what the optimal\ntop level choice is until we have solved all the subproblems: this is the min\nin the bottom half of formula 15.7. So we will compute the cost of multiplying\neach pair of matrices together, then optimize multiplying 3 matrices together,\nthen optimize 4, on up to 5, building a table of the costs as we go.\n\n![](fig/code-matrix-chain-order.jpg)\n\nThe code is shown to the right. A table **_p_** of dimensions is given. The\nalgorithm iterates for chains of length **_l_**, starting with 2. Variables\n**_i_** and **_j_** control the left and right boundaries of the chain, and\n**_k_** is the current split being considered. The algorithm records the\nminimal number of _m_ultiplications needed for each chain in matrix **_m_**,\nand where we _s_plit the chain into two with the parentheses in matrix\n**_s_**.\n\nYou will solve it for this chain:\n\n> A1(5x15), A2(15x2), A3(2x10), A4(10x5), A5(5x100)\n\nThe chain is represented by this table **_p_**:\n\n![](fig/Problem-Set-07-Matrix-Chain-HW-p-table.jpg)\n\nI provide the solution and the full computations for **_l_=2** below, and give\nyou the template for the computations for **_l_=3**. You will:\n\n**(a)** Fill out the computations for **_l_=3**, put the results in the table, and continue for **_l_=4** and **_l_=5** to complete the table.\n\n**(b)** Write down the output of `Print-Optimal-Parens(_s_,1,5)` assuming the table you produced in (a) is `_s_`. \n\n![](fig/Problem-Set-07-Matrix-Chain-HW-Start.jpg)\n\n    \n    \n    n = 5 \n    \n    ---\n    l=2 // compute the optimal way to multiply each pair\n        // I demonstrate this for you \n    \n      i=1, j=2, k=1:\n        q = m[1,1] + m[2,2] + p0*p1*p2\n          = 0 + 0 + 150 \n          = 150 \n      i=2, j=3, k=2:\n        q = m[2,2] + m[3,3] + p1*p2*p3 \n          =  0 + 0 + 300\n          = 300 \n      i=3, j=4, k=3:\n        q = m[3,3] + m[4,4] + p2*p3*p4 \n          = 0 + 0 + 100\n          = 100\n      i=4, j=5, k=4:\n        q = m[4,4] + m[5,5] + p3*p4*p5 \n          = 0 + 0 + 5000\n          = 5000\n    \n    \n    ---\n    l=3 // compute the optimal way to multiply each triplet\n        // there will be more than one value of k: choose the minimum result\n        // this will indicate the top level parenthesization of the chain\n      i=1, j=3 \n        k=1: \n          q = m[1,1] + m[2,3] + p0*p1*p3 \n            = \n        k=2: \n          q = m[1,2] + m[3,3] + p0*p2*p3 \n            = \n      i=2, j=4 \n        k=2: \n          q = m[2,2] + m[3,4] + p1*p2*p4 \n            = \n        k=3: \n          q = m[2,3] + m[4,4] + p1*p3*p4 \n            = \n      i=3, j=5 \n        k=3: \n          q = m[3,3] + m[4,5] + p2*p3*p5 \n            = \n        k=4: \n          q = m[3,4] + m[5,5] + p2*p4*p5 \n            = \n    \n    ---\n    l=4 // compute the optimal way to multiply each set of 4 matrices \n    \n    What's the pattern? fill out as above, but now you go over 3 values of k. \n    \n    ---\n    l=5 // compute the optimal way to multiply all 5 matrices\n        // then you are ready to give the answer in part (b) \n    \n\n\n\n* * *\n\n### Meet Mr. Fibonacci\n\nHe has some numbers he is proud of (they [seem to show up in nature a\nlot](http://jwilson.coe.uga.edu/emat6680/parveen/fib_nature.htm)), but needs\nyour help in generating and storing them. His first and second numbers are 1,\nand then each successive number is generated by adding up the previous two\nnumbers. He has written a recursive procedure that generates these numbers:\n\n    \n    \n    Fibonacci (n)\n        if n < 2\n            return 1\n        else\n            return Fibonacci (n-1) + Fibonacci(n-2) \n    \n\nBut it is very slow!\n\n#### 3\\. (4 pts) Analysis of Recursive Fibonacci\n\nExplain to him the asymptotic complexity of his algorithm, as follows.\n(Formula A.5 will come in handy.)\n\n**(a)** Draw the recursion tree.\n\n**(b)** Identify these quantities: \n\n  * How many edges go down the left hand side, following the recursive calls for _n_−1? This is an upper bound on tree height.\n  * How many edges go down the right hand side, following the recursive calls for _n_−2? This is a lower bound on tree height.\n  * How many vertices are at level _i_ (where root is level 0)? \n\n**(c)** Then, assuming Θ(1) work at each vertex, how much work is in the tree, as determined by the upper and lower bounds on tree height and work per level that you just computed?\n\n#### 4\\. (4 pts) Dynamic Programming Solution\n\n**(a)** _Rewrite his algorithm to use dynamic programming,_ saving and re-using previous values rather than re-computing them: it's a simple iterative solution. (Did I mention that the solution is _simple_? If you are doing anything complicated you're over-thinking it.) \n\n**(b)** _What's the asymptotic complexity of your re-written algorithm?_ Justify your conclusion.\n\n#### 5\\. (4 pts) Huffman Coding of Fibonacci Numbers\n\nNow that we can efficiently generate his numbers, he has observed that certain\nconfigurations in the flowers in his garden occur with frequencies following\nhis number. He has given each configuration letters. Configuration \"A\" and \"B\"\noccur only once. Configuration \"C\" occurs twice, \"D\" 3 times, \"E\" 5 times, \"F\"\n8 times, \"G\" 13 times and \"H\" 21 times. Show him how he can encode his\nconfigurations with less space using Huffman coding.\n\n**(a)** _Draw the Huffman Tree for these first 8 letters using the observed frequencies._   \n_Note:_ Assume a heap implementation of a min-priority queue where _keys of\nthe same value come out in FIFO order_. For example, when letters A and B are\nmerged to form a node of weight 2, the node for C of weight 2 will be dequeued\nbefore that for A and B. This means that singletons will always be the left\nchild and subtrees the right child. Then you will get a prettier tree and the\npattern will be clear.\n\n**(b)** If we extended this to _n_ letters, _describe the pattern for what the code will look like for any _i_th Fibonacci number_, _i_ ≤ _n_.\n\n\n\n* * *\n\nDan Suthers Last modified: Wed Apr 16 14:38:28 HST 2014\n\n",
 "path"=>"morea//120.dynamic-programming/experience-2.md"}
</pre>

<h2>/morea/120.dynamic-programming/experience.html</h2>

<pre>Hash
{"title"=>"Dynamic programming sample problem: longest simple path",
 "published"=>true,
 "morea_id"=>"experience-dynamic-programming",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply dynamic programming principles to a sample problem",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/120.dynamic-programming/experience.html",
 "url"=>"/morea/120.dynamic-programming/experience.html",
 "content"=>
  "# Longest Simple Path in a Directed Acylic Graph\n\nGiven a directed weighted acyclic graph G=(V,E) with real valued edge weights\nrepresenting the \u0093length\u0094 of each edge and two vertices s (start) and t\n(target), develop a dynamic programming approach for finding a longest\nweighted simple path from s to t.\n\nDefinitions:\n\n  * directed: it has arrows on the edges and you only go in the direction of the arrows \n\n  * acyclic: one can never get back to where one started\n\n  * path: a sequence of vertices connected by edges, e.g., ⟨u, v, w⟩ when (u,v) and (v,w) are in E. \n\n  * simple path: a path that does not repeat vertices. \n\nNote: in a DAG without self loops (v, v) all paths are simple. We\u0092ll assume no\nself loops.\n\n  \n\nNotations:\n\n  * Write |V| for number of vertices and |E| for number of edges. \n\n  * (u, v) \\- the directed edge from vertex to vertex v. \n\n  * w(u, v) \\- the weight on the edge (u, v), here interpreted as length.\n\n  * G.V \\- a list of vertices in G. \n\n  * G.Adj[u] \\- a list of edges in G that begin on u (e.g., (u,v), (u,w), (u,x) ). \n\nAlso, let\u0092s assume that vertices are named by integers {1, 2,  |V|} so we can\nuse vertices as indices into arrays. And draw pictures! After all, these are\ngraphs!\n\n  \n\nFollow these steps:\n\n  \n\n1. Characterize the Structure of an Optimal Solution\n\n  \n\nSince we need to reason about subpaths, we\u0092ll start at u (which can be s or\nany other vertex). Let p be a longest path from u to t. If u =  t then p is\nsimply ⟨u⟩ and has zero weight. Consider when u ≠ t. Then p has at least two\nvertices and looks like:\n\n  \n\np = ⟨u, v  t⟩   (it is possible that v = t).\n\n  \n\nLet p\u0092 = ⟨v  t⟩ and prove that p\u0092 must be a longest simple path from v to t.\n\n  \n  \n\n2\\. Recursively define the value of an optimal solution:\n\n  \n\nLet dist[u] be the distance of a longest path from u to t.  Fill out the\ndefinition to reflect the above structure: (You will need mathematical\nnotation that is easier to write on paper. Do it on paper first and figure out\nGoogle equations only if you have time):\n\n  \n\ndist[u] =\n\n  \n\n3\\. Compute the value of an optimal solution (simple recursive version):\n\n  \n\nWrite a recursive procedure that computes the value of an optimal solution as\ndefined by the above recursive definition. Do not memoize yet; that\u0092s the next\nstep.\n\n  \n\nLongest-Path-Value-Recursive (G, u, t)\n\n  \n  \n\n4\\. Compute the value of an optimal solution (dynamic programming version):\n\n  \n\nNow memoize your procedure above by passing the array dist[1..|V|] that\nrecords longest path distances dist[u] from each vertex u to t so you don\u0092t\nhave to repeat computations. We will assume that the caller has initialized\nall entries of dist to -∞.\n\n  \n\nLongest-Path-Value-Memoized (G, u, t, dist)\n\n  \n  \n\n5\\. Analyze the runtime of your solution in #4 in terms of |V| and |E|.\n\n  \n\nInclude the runtime (a) to initialize dist and (b) of Longest-Path-Value-\nMemoized.\n\n  \n  \n\n6\\. Extra Credit: Recover a Solution\n\n  \n\nRewrite Longest-Path-Value-Memoized to Longest-Path-Memoized that takes an\nadditional parameter next[1..|V|], and records the next vertex in the path\nfrom any given vertex u in next[u]. Assume that all entries of next are\ninitialized to -1 by the caller.\n\n  \n\nLongest-Path-Memoized (G, u, t, dist, next)\n\n  \n\nOnce that is done, you can easily write a procedure that recovers (e.g.,\nprints) the path from s to t by tracing through next (but you don\u0092t need to do\nit here).\n\n  \n\n7\\. To Think About\n\n  \n\nHow would you analyze the runtime of procedure Longest-Path-Recursive in terms\nof |V| and |E|? How does it compare to that of Longest-Path-Memoized?\n\n\n\n",
 "path"=>"morea//120.dynamic-programming/experience.md"}
</pre>

<h2>/morea/120.dynamic-programming/module.html</h2>

<pre>Hash
{"title"=>"Dynamic Programming",
 "published"=>true,
 "morea_id"=>"dynamic-programming",
 "morea_outcomes"=>["outcome-dynamic-programming"],
 "morea_readings"=>
  ["reading-screencast-12a",
   "reading-screencast-12b",
   "reading-screencast-12c",
   "reading-screencast-12d",
   "reading-cormen-15",
   "reading-sedgewick-37",
   "reading-notes-12"],
 "morea_experiences"=>
  ["experience-dynamic-programming", "experience-dynamic-programming-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/120.dynamic-programming/logo.gif",
 "morea_sort_order"=>120,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/120.dynamic-programming/module.html",
 "content"=>
  "Cut rod problem, longest common subsequence, matrix-chain multiplication, knapsack problem, optimal substructure.\n",
 "path"=>"morea//120.dynamic-programming/module.md"}
</pre>

<h2>/modules/dynamic-programming/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Dynamic Programming",
 "url"=>"/modules/dynamic-programming/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/dynamic-programming/index.html"}
</pre>

<h2>/morea/120.dynamic-programming/outcome.html</h2>

<pre>Hash
{"title"=>"Use dynamic programming for problem solving",
 "published"=>true,
 "morea_id"=>"outcome-dynamic-programming",
 "morea_type"=>"outcome",
 "morea_sort_order"=>120,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/120.dynamic-programming/outcome.html",
 "content"=>
  "Be able to implement solutions for simple optimization problems based upon dynamic programming techniques.",
 "path"=>"morea//120.dynamic-programming/outcome.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 15 - Dynamic Programming",
 "published"=>true,
 "morea_id"=>"reading-cormen-15",
 "morea_summary"=>
  "Rod cutting, matrix-chain multiplication, elements of DP, longest common subsequence, optimal BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "55 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-cormen.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-notes-12",
 "morea_summary"=>"Derivations of dynamic programming",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/120.dynamic-programming/reading-notes.html",
 "url"=>"/morea/120.dynamic-programming/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Problem Solving Methods and Optimization Problems \n  2. Introducing DP with the Rod Cutting Example \n  3. Illustrating DP with the Longest Common Subsequence Example \n  4. Summary and Comments on Optimal Substructure\n\n## Readings \n\n  * Read all of CLRS Chapter 15. The focus is on the problem solving strategy: Read the examples primarily to understand the Dynamic Programming strategy rather than to memorize the specifics of each problem (although you will be asked to trace through some of the algorithms). \n\n  * I have also posted a chapter by Sedgewick in Laulima. In this case, I don't think that Sedgewick is any clearer than Cormen et al. The Rod-Cutting example in Cormen et al. illustrates the basics of DP quite well. Also, although usually it is easier to understand examples first, DP examples involve tedious combinations of subproblems, so you may be better off trying to understand the gist of the strategy first in this case.\n\n\n* * *\n\n##  Setting the Context\n\n### Problem Solving Methods\n\nIn this course we study many well defined algorithms, including (so far) those\nfor ADTs, sorting and searching, and others to come to operate on graphs.\nQuality open source implementations exist: you often don't need to implement\nthem.\n\nBut we also study problem solving methods that guide the design of algorithms\nfor your specific problem. Quality open source implementations may not exist\nfor your specific problem: you may need to:\n\n  * Understand and identify characteristics of your problem\n  * Match these characteristics to algorithmic design patterns.\n  * Use the chosen design patterns to design a custom algorithm.\n\nSuch problem solving methods include divide & conquer, dynamic programming,\nand greedy algorithms (among others to come).\n\n### Optimization Problems\n\nAn **optimization problem** requires finding a/the \"best\" of a set of\nalternatives (alternative approaches or solutions) under some quality metric\n(which we wish to maximize) or cost metric (which we wish to minimize).\n\nDynamic Programming is one of several methods we will examine. (Greedy\nalgorithms and linear programming can also apply to optimization problems.)\n\n### Basic Idea of Dynamic Programming\n\nDynamic programming solves optimization problems by combining solutions to\nsubproblems.\n\nThis sounds familiar: **divide and conquer** also combines solutions to\nsubproblems, but **_applies when the subproblems are disjoint_**. For example,\nhere is the recursion tree for merge sort on an array A[1..8]. Notice that the\nindices at each level do not overlap):\n\n![](fig/merge-sort-recursive-structure.jpg)\n\n**Dynamic programming _applies when the subproblems overlap_**. For example, here is the recursion tree for a \"rod cutting\" problem to be discussed in the next section (numbers indicate lengths of rods). Notice that not only do lengths repeat, but also that there are entire subtrees repeating. It would be redundant to redo the computations in these subtrees. \n\n![](fig/Fig-15-3-alt-recursion-tree.jpg)\n\n**Dynamic programming _solves each subproblem just once, and saves its answer in a table_**, to avoid the recomputation. It uses additional memory to save computation time: an example of a **time-memory tradeoff**.\n\nThere are many examples of computations that require exponential time without\ndynamic programming but become polynomial with dynamic programming.\n\n* * *\n\n##  Example: Rod Cutting\n\nThis example nicely introduces key points about dynamic programming.\n\nSuppose you get different prices for steel rods of different lengths. Your\nsupplier provides long rods; you want to know how to cut the rods into pieces\nin order to maximize revenue. Each cut is free. Rod lengths are always an\nintegral number of length units (let's say they are centimeters).\n\n> **Input:** A length _n_ and a table of prices _pi_ for _i_ = 1, 2, ..., _n_.  \n  \n**Output:** The maximum revenue obtainable for rods whose lengths sum to _n_, computed as the sum of the prices for the individual rods. \n\nWe can choose to cut or not cut at each of the _n_-1 units of measurement.\nTherefore one can cut a rod in 2_n_-1 ways.\n\nIf _pn_ is large enough, an optimal solution might require no cuts.\n\n### Example problem instance\n\n![](fig/cut-rod-8-prices.jpg)\n\nSuppose we have a rod of length 4. There are 2_n_-1 = 23 = 8 ways to cut it up\n(the numbers show the price we get for each length, from the chart above):\n\n![](fig/Fig-15-2-alt-rod-cutting.jpg)\n\nHaving enumerated all the solutions, we can see that for a rod of length 4 we\nget the most revenue by dividing it into two units of length 2 each: _p_2 \\+\n_p_2 = 5 + 5 = 10.\n\n###  Optimal Substructure of Rod Cutting\n\nAny optimal solution (other than the solution that makes no cuts) for a rod of\nlength > 2 results in at least one subproblem: a piece of length > 1 remaining\nafter the cut.\n\n_Claim:_ The optimal solution for the overall problem must include an optimal\nsolution for this subproblem.\n\n_Proof:_ The proof is a \"cut and paste\" proof by contradiction: if the overall\nsolution did not include an optimal solution for this problem, we could cut\nout the nonoptimal subproblem solution, paste in the optimal subproblem\nsolution (which must have greater value), and thereby get a better overall\nsolution, contradicting the assumption that the original cut was part of an\noptimal solution.\n\nTherefore, rod cutting exhibits **optimal substructure: _The optimal solution\nto the original problem incorporates optimal solutions to the subproblems,\nwhich may be solved independently._** This is a hallmark of problems amenable\nto dynamic programming. (Not all problems have this property.)\n\n###  Continuing the example\n\n![](fig/cut-rod-8-prices.jpg)\n\nHere is a table of _ri_, the maximum revenue for a rod of length _i_, for this\nproblem instance.\n\n![](fig/cut-rod-8-manual-solutions.jpg)\n\nTo solve a problem of size 7, find the best solution for subproblems of size\n7; 1 and 6; 2 and 5; or 3 and 4. Each of these subproblems also exhibits\noptimal substructue.\n\nOne of the optimal solutions makes a cut at 3cm, giving two subproblems of\nlengths 3cm and 4cm. We need to solve both optimally. The optimal solution for\na 3cm rod is no cuts. As we saw above, the optimal solution for a 4cm rod\ninvolves cutting into 2 pieces, each of length 2cm. These subproblem optimal\nsolutions are then used in the solution to the problem of a 7cm rod.\n\n###  Quantifying the value of an optimal solution\n\nThe next thing we want to do is write a general expression for the value of an\noptimal solution that captures its recursive structure.\n\nFor any rod length _n_, we can determine the optimal revenues _rn_ by taking\nthe maximum of:\n\n  * _pn_: the price we get by not making a cut, \n  * _r_1 \\+ _r__n_-1: the maximum revenue from a rod of 1cm and a rod of _n_-1cm, \n  * _r_2 \\+ _r__n_-2: the maximum revenue from a rod of 2cm and a rod of _n_-2cm, .... \n  * _r__n_-1 \\+ _r_1\n\nSo, _rn_ = max (_pn_, _r_1 \\+ _r__n_-1, _r_2 \\+ _r__n_-2, .... _r__n_-1 \\+\n_r_1).\n\nThere is redundancy in this equation: if we have solved for _ri_ and _r__n_-\n_i_, we don't also have to solve for _r__n_-_i_ and _ri_.\n\n#### A Simpler Decomposition\n\nRather than considering all ways to divide the rod in half, leaving two\nsubproblems, consider all ways to cut off the first piece of length _i_,\nleaving only one subproblem of length _n_ \\- _i_:\n\n![](fig/equation-cut-rod-decomposition.jpg)\n\nWe don't know in advance what the first piece of length _i_ should be, but we\ndo know that one of them must be the optimal choice, so we try all of them.\n\n### Recursive Top-Down Solution\n\nThe above equation leads immediately to a direct recursive implementation (_p_\nis the price vector; _n_ the problem size):\n\n![](fig/code-cut-rod.jpg)\n\nThis works but is inefficient. It calls itself repeatedly on subproblems it\nhas already solved (circled). Here is the recursion tree for _n_ = 4:\n\n![](fig/Fig-15-3-alt-recursion-tree-annotated.jpg)\n\nIn fact we can show that the growth is exponential. Let _T_(_n_) be the number\nof calls to Cut-Rod with the second parameter = _n_.\n\n![](fig/recurrence-rod-cutting.jpg)\n\nThis has solution 2_n_. (Use the inductive hypothesis that it holds for _j_ <\n_n_ and then use formula A5 of Cormen et al. for an exponential series.)\n\n### Dynamic Programming Solutions\n\nDynamic programming arranges to solve each sub-problem just once by saving the\nsolutions in a table. There are two approaches.\n\n#### Top-down with memoization\n\nModify the recursive algorithm to store and look up results in a table _r_.\n**Memoizing** is remembering what we have computed previously.\n\n![](fig/code-memoized-cut-rod.jpg) \n\n![](fig/code-memoized-cut-rod-aux.jpg)\n\nThe top-down approach has the advantages that it is easy to write given the\nrecursive structure of the problem, and only those subproblems that are\nactually needed will be computed. It has the disadvantage of the overhead of\nrecursion.\n\n#### Bottom-up\n\nOne can also sort the subproblems by \"size\" (where size is defined according\nto which problems use which other ones as subproblems), and solve the smaller\nones first.\n\n![](fig/code-bottom-up-cut-rod.jpg)\n\nThe bottom-up approach requires extra thought to ensure we arrange to solve\nthe subproblems before they are needed. (Here, the array reference _r_[_j_ \\-\n_i_] ensures that we only reference subproblems smaller than _j_, the one we\nare currently working on.)\n\nThe bottom-up approach can be more efficient due to the iterative\nimplementation (and with careful analysis, unnecessary subproblems can be\nexcluded).\n\n#### Asymptotic running time\n\nBoth the top-down and bottom-up versions run in Θ(_n_2) time.\n\n  * _Bottom-up:_ there are doubly nested loops, and the number of iterations for the inner loop forms an arithmetic series. \n  \n\n  * _Top-down:_ Each subproblem is solved just once. Subproblems are solved for sizes 0, 1, ... _n_. To solve a subproblem of size _n_, the `for` loop iterates _n_ times, so over all recursive calls the total number of iterations is an arithmetic series. (This uses aggregate analysis, covered in a later lecture.) \n\n### Constructing a Solution\n\nThe above programs return the value of an optimal solution. To construct the\nsolution itself, we need to record the choices that led to optimal solutions.\nUse a table _s_ to record the place where the optimal cut was made (compare to\nBottom-Up-Cut-Rod):\n\n![](fig/code-extended-bottom-up-cut-rod.jpg)\n\nFor our problem, the input data and the tables constructed are:\n\n![](fig/cut-rod-8-prices.jpg) \n\n![](fig/cut-rod-8-computed-solutions.jpg)\n\nWe then trace the choices made back through the table _s_ with this procedure:\n\n![](fig/code-print-cut-rod-solution.jpg)\n\nTrace the calls made by `Print-Cut-Rod-Solution(_p_, 8)`...\n\n* * *\n\n## Four Steps of Problem Solving with Dynamic Programming\n\nIn general, we follow these steps when solving a problem with dynamic\nprogramming:\n\n  1. **Characterize the structure of an optimal solution**: \n    * How are optimal solutions composed of optimal solutions to subproblems?\n    * Assume you have an optimal solution and show how it must decompose\n    * Sometimes it is useful to write a brute force solution, observe its redunancies, and characterize a more refined solution\n    * e.g., our observation that a cut produces one to two smaller rods that can be solved optimally\n  \n\n  2. **Recursively define the value of an optimal solution**: \n    * Write a recursive cost function that reflects the above structure\n    * e.g., the recurrence relation shown\n  \n\n  3. **Compute the value of an optimal solution**: \n    * Write code to compute the recursive values, memoizing or solving smaller problems first to avoid redundant computation\n    * e.g., `Bottom-Up-Cut-Rod`\n  \n\n  4. **Construct an optimal solution from the computed information**: \n    * Augment the code as needed to record the structure of the solution\n    * e.g., `Extended-Bottom-Up-Cut-Rod` and `Print-Cut-Rod-Solution`\n\n![](fig/equation-cut-rod-decomposition.jpg)\n\nThe steps are illustrated in the next example.\n\n* * *\n\n##  Example: Longest Common Subsequence\n\nA **subsequence** of sequence _S_ leaves out zero or more elements but\npreserves order.\n\n_Z_ is a ** common subsequence ** of _X_ and _Y_ if _Z_ is a subsequence of\nboth _X_ and _Y_.  \n_Z_ is a **longest common subsequence** if it is a subsequence of maximal\nlength.\n\n### The LCS Problem\n\nGiven two sequences _X_ = ⟨ _x_1, ..., _x__m_ ⟩ and _Y_ = ⟨ _y_1, ..., _y__n_\n⟩, find a subsequence common to both whose length is longest. Solutions to\nthis problem have applications to DNA analysis in bioinformatics. The analysis\nof optimal substructure is elegant.\n\n#### Examples\n\n![](fig/LCS-examples.jpg)\n\n### Brute Force Algorithm\n\nFor every subsequence of _X_ = ⟨ _x_1, ..., _x__m_ ⟩, check whether it is a\nsubsequence of _Y_ = ⟨ _y_1, ..., _y__n_ ⟩, and record it if it is longer than\nthe longest previously found.\n\n  * There are 2_m_ subsequences of _X_ to check. \n  * For each subsequence, scan _Y_ for the first letter. From there scan for the second letter, etc., up to the _n_ letters of _Y_. \n  * Therefore, Θ(_n_2_m_). \n\nThis involves a lot of redundant work.\n\n  * If a subsequence _Z_ of _X_ fails to match _Y_, then any subsequence having _Z_ as a prefix will also fail. \n  * If a subsequence _Z_ of _X_ matches _Y_, then there is no need to check prefixes of _Z_. \n\nMany problems to which dynamic programming applies have exponential brute\nforce solutions that can be improved on by exploiting redundancy in subproblem\nsolutions.\n\n### Step 1. Optimal Substructure of LCS\n\nThe first step is to characterize the structure of an optimal solution,\nhopefully to show it exhibits optiomal stubstructure.\n\nOften when solving a problem we start with what is known and then figure out\nhow to contruct a solution. The optimal substructure analysis takes the\nreverse strategy: _ _assume_ you have found an optional solution_ (Z below)\n_and figure out what you must have done to get it_!\n\nNotation:\n\n  * _Xi_ = prefix ⟨ _x_1, ..., _x__i_ ⟩\n  * _Yi_ = prefix ⟨ _y_1, ..., _y__i_ ⟩\n\n**_Theorem:_ ** Let _Z_ = ⟨ _z_1, ..., _z__k_ ⟩ be any LCS of _X_ = ⟨ _x_1, ..., _x__m_ ⟩ and _Y_ = ⟨ _y_1, ..., _y__n_ ⟩. Then \n\n  1. If _xm_ = _yn_, then _zk_ = _xm_ = _yn_, and _Z__k_-1 is an LCS of _X__m_-1 and _Y__n_-1.\n  2. If _xm_ ≠ _yn_, then _zk_ ≠ _xm_ ⇒ _Z_ is an LCS of _X__m_-1 and _Y_. \n  3. If _xm_ ≠ _yn_, then _zk_ ≠ _yn_ ⇒ _Z_ is an LCS of _X_ and _Y__n_-1. \n\n_Sketch of proofs:_\n\n(1) can be proven by contradiction: if the last characters of _X_ and _Y_ are\nnot included in _Z_, then a longer LCS can be constructed by adding this\ncharacter to _Z_, a contradiction.\n\n(2) and (3) have symmetric proofs: Suppose there exists a subsequence _W_ of\n_X__m_-1 and _Y_ (or of _X_ and _Y__n_-1) with length > _k_. Then _W_ is a\ncommon subsequence of _X_ and _Y_, contradicting _Z_ being an LCS.\n\nTherefore, **an LCS of two sequences contains as prefix an LCS of prefixes of\nthe sequences.** We can now use this fact construct a recursive formula for\nthe value of an LCS.\n\n### Step 2. Recursive Formulation of Value of LCS\n\nLet _c_[_i_, _j_] be the length of the LCS of prefixes _Xi_ and _Yj_. The\nabove recursive substructure leads to the definition of _c_:\n\n![](fig/LCS-recursive-formulation.jpg)\n\nWe want to find _c_[_m_, _n_].\n\n### Step 3. Compute Value of Optimal Solution to LCS\n\nA recursive algorithm based on this formulation would have lots of repeated\nsubproblems, for example, on strings of length 4 and 3:\n\n![](fig/LCS-4-3-recurrence-tree.jpg)\n\n![](fig/LCS-recursive-formulation.jpg) Dynamic programming avoids\nthe redundant computations by storing the results in a table. We use\n_c_[_i_,_j_] for the length of the LCS of prefixes _Xi_ and _Yj_ (hence it\nmust start at 0). (_b_ is part of the third step and is explained next\nsection.)\n\nTry to find the correspondence betweeen the code below and the recursive\ndefinition shown in the box above.\n\n![](fig/code-LCS-length.jpg)\n\nThis is a bottom-up solution: Indices _i_ and _j_ increase through the loops,\nand references to _c_ always involve either _i_-1 or _j_-1, so the needed\nsubproblems have already been computed.\n\nIt is clearly **Θ(_m__n_)**; _much better than Θ(_n_2_m_)_!\n\n### Step 4. Construct an Optimal Solution to LCS\n\nIn the process of computing the _value_ of the optimal solution we can also\nrecord the _choices_ that led to this solution. Step 4 is to add this latter\nrecord of choices and a way of recovering the optimal solution at the end.\n\nTable _b_[_i_, _j_] is updated above to remember whether each entry is\n\n  * a common substring of _X__i_-1 and _Y__j_-1 (diagonal arrow), in which case the common character _xi_ = _yj_ is included in the LCS;\n  * a common substring of _X__i_-1 and _Y_ (↑); or\n  * a common substring of _X_ and _Y__j_-1 (<-).\n\nWe reconstruct the path by calling Print-LCS(_b_, _X_, _n_, _m_) and following\nthe arrows, printing out characters of _X_ that correspond to the diagonal\narrows (a Θ(_n_ \\+ _m_) traversal from the lower right of the matrix to the\norigin):\n\n![](fig/code-print-LCS.jpg)\n\n### Example of LCS\n\nWhat do \"spanking\" and \"amputation\" have in common?\n\n![](fig/LCS-spanking-amputation.jpg)\n\n* * *\n\n##  Other Applications\n\nTwo other applications are covered in the Cormen et al. text, and many others\nin the Problems at the end of the chapter. I omit them to keep this lecture\nfrom being too long, and trust that the student will read them in the text.\n\n### Optimizing Matrix-Chain Multiplication\n\nMany scientific and business applications involve multiplication of chains of\nmatrices ⟨ A1, A2, A3, ... A_n_ ⟩. Since matrix multiplication is associative,\nthe matrices can be multiplied with their neighbors in this sequence in any\norder. The order chosen can have a huge difference in the number of\nmultiplications required. For example suppose you have A, a 2x100 matrix, B\n(100x100) and C (100x20). To compute A*B*C:\n\n> (A*B) requires 2*100*100 = 20000 multiplications, and results in a 2x100\nmatrix. Then you need to multiply by C: 2*100*20 = 4000 multiplications, for a\ntotal of 24,000 multiplications (and a 2x20 result).\n\n> (B*C) requires 100x100x20 = 200000 multiplications, and results in a 100x20\nmatrix. Then you need to multiply by A: 2*100*20 = 4000 multiplications, for a\ntotal of 204,000 multiplications (and the same 2x20 result).\n\nThe Matrix-Chain Multiplication problem is to determine the optimal order of\nmultiplications (_not_ to actually do the multiplications). For three matrices\nI was able to figure out the best sequence by hand, but some problems in\nscience, business and other areas involve many matrices, and the number of\ncombinations to be checked grows exponentially.\n\nPlanning matrix multiplication is perhaps the most \"canonical\" example of\ndynamic programming: it is used in most introductory presentations. I chose to\npresent LCS instead because matrix multiplication optimization will be built\ninto turnkey software, and current students will more likely be interested in\nbioinformatics applications\n\n###  Optimal Binary Search Tree\n\nWe saw in [Topic\n8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html) that\nan unfortunate order of insertions of keys into a binary search tree (BST) can\nresult in poor performance (e.g., linear in _n_). If we know all the keys in\nadvance and also the probability that they will be searched, we can optimize\nthe construction of the BST to minimize search time in the aggregate over a\nseries of queries. An example application is when we want to construct a\ndictionary from a set of terms that are known in advance along with their\nfrequency in the language. The reader need only try problem 15.5-2 from the\nCormen et al. text (manual simulation of the algorithm) to appreciate why we\nwant to leave this tedium to computers!\n\n* * *\n\n##  Further Observations Concerning Optimal Substructure\n\nTo use dynamic programming, we must show that any optimal solution involves\nmaking a choice that leaves one or more subproblems to solve, and the\nsolutions to the subproblems used within the optimal solution must themselves\nbe optimal.\n\n### The optimal choice is not known before solving the subproblems\n\nWe may not know what that first choice is. Consequently:\n\n  * To show that there is optimal substructure, we suppose that the choice has been made, and show that the subproblems that result must also be solved optimally. This argument is often made using a cut-and-paste proof by contradiction.\n  * Then when writing the code, we must ensure that enough potential choices and hence their supbproblems are considered that we find the optimal first choice. This usually shows up as iteration in which we find the maximum or minimum according to some objective function across all choices.\n\n### Optimal substructure varies across problem domains:\n\nHow many subproblems are used in an optimal solution may vary:\n\n  * Rod Cutting: 1 subproblem (of size _n_ \\- _i_)\n  * LCS: 1 subproblem (LCS of the prefix sequence(s).) \n  * Optimal BST: 2 subproblems (given _kr_ has been chosen as the root, _ki_ ..., _k__r_-1 and _k__r_+1 ..., _kj_) \n\nHow many choices in determining which subproblem(s) to use may vary:\n\n  * Rod cutting: _n_ choices (for each value of _i_)\n  * LCS: Either 1 choice (if _xi_ = _yj_, take LCS of _X__i_-1 and _Y__j_-1), or 2 choices (if _xi_ ≠ _yj_, check both LCS of _X__i_-1 and _Y_, and LCS of _X_ and _Y__j_-1)\n  * Optimal BST: _j_ \\- _i_ \\+ 1 choices for the root _kr_ in _ki_ ..., _kj_: see text.\n\nInformally, running time depends on (# of subproblems overall) x (# of\nchoices).\n\n  * Rod Cutting: Θ(_n_) subproblems overall, ≤ _n_ choices for each ⇒ O(_n_2) running time.\n  * LCS: Θ(_m__n_) subproblems overall; ≤ 2 choices for each ⇒ O(_m__n_) running time.\n  * Optimal BST: Θ(_n_2) subproblems overall; O(_n_) choices for each ⇒ O(_n_3) running time. \n\n(We'll have a better understanding of \"overall\" when we cover amortized\nanalysis.)\n\n### Not all optimization problems have optimal substructure\n\n![](fig/shortest-path-optimal-substructure.jpg)\n\nWhen we study graphs, we'll see that finding the **shortest path** between two\nvertices in a graph has optimal substructure: if _p_ = _p_1 \\+ _p_2 is a\nshortest path between _u_ and _v_ then _p_1 must be a shortest path between\n_u_ and _w_ (etc.). Proof by cut and paste.\n\nBut finding the **longest simple path** (the longest path not repeating any\nedges) between two vertices is not likely to have optimal substructure.\n\n![](fig/longest-path-nonoptimal-substructure.jpg)\n\nFor example, _q_ -> _s_ -> _t_ -> _r_ is longest simple path from _q_ to _r_,\nand _r_ -> _q_ -> _s_ -> _t_ is longest simple path from _r_ to _t_, but the\ncomposed path is not even legal: the criterion of simplicity is violated.\n\nDynamic programming requires _overlapping_ yet _independently solveable_\nsubproblems.\n\nLongest simple path is NP-complete, a topic we will cover at the end of the\nsemester, so is unlikely to have any efficient solution.\n\n### Dynamic programming uses optimal substructure bottom up\n\nAlthough we wrote the code both ways, in terms of the order in which solutions\nare found, dynamic programming _first_ finds optimal solutions to subproblems\nand _then_ choses which to use in an optimal solution to the problem. It\napplies when one cannot make the top level choice until subproblem solutions\nare known.\n\nIn [Topic\n13](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-13.html),\nwe'll see that ** greedy algorithms** work top down: _first_ make a choice\nthat looks best, _then_ solve the resulting subproblem. Greedy algorithms\napply when one can make the top level choice without knowing how subproblems\nwill be solved.\n\n* * *\n\n##  Summary\n\nDynamic Programming applies when the problem has these characteristics:\n\n**Recursive Decomposition**\n    The problem has recursive structure: it breaks down into smaller problems of the same type. _This characterisic is shared with divide and conquer, but dynamic programming is distinguished from divide and conquer by the next item._\n**Overlapping Subproblems**\n    The subproblems solved by a recursive solution overlap (the same subproblems are revisited more than once). _This means we can save time by preventing the redundant computations._\n**Optimal Substructure**\n    Any optimal solution involves making a choice that leaves one or more subproblems to solve, and the solutions to the subproblems used within the optimal solution must themselves be optimal. _This means that optimized recursive solutions can be used to construct optimized larger solutions._\n  \n\nDynamic programming can be approached top-down or bottom-up:\n\n**Top-Down with memoization:**\n    Write a recursive procedure to solve the problem, computing subproblems as needed. Each time a sub-problem is encountered, see whether you have stored it in a table, and if not, solve it and store the solution.\n  \n**Bottom-Up:**\n    Order the subproblems such that \"smaller\" problems are solved first, so their solutions are available in the table before \"larger\" problems need them. (This ordering need not be based on literal size.) \n\nBoth have the same asympotic running time. The top-down procedure has the\noverhead of recursion, but computes only the subproblems that are actually\nneeded. Bottom-up is used the most in practice.\n\nWe problem solve with dynamic programming in four steps:\n\n  1. **Characterize the structure of an optimal solution**: \n    * How are optimal solutions composed of optimal solutions to subproblems?\n  2. **Recursively define the value of an optimal solution**: \n    * Write a recursive cost function that reflects the above structure\n  3. **Compute the value of an optimal solution**: \n    * Write code to compute the recursive values, memoizing or solving smaller problems first to avoid redundant computation\n  4. **Construct an optimal solution from the computed information**: \n    * Augment the code as needed to record the structure of the solution\n\n* * *\n\n## Wrapup\n\nThere is an online presentation focusing on LCS at [ http://www.csanimated.com\n/animation.php?t=Dynamic_programming](http://www.csanimated.com/animation.php?\nt=Dynamic_programming).\n\nIn the next Topic 13 we look at a related optimization strategy: greedy\nalgorithms.\n\n* * *\n\nDan Suthers Last modified: Sun Mar 2 05:24:02 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//120.dynamic-programming/reading-notes.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-screencast-12a",
 "morea_summary"=>"Example of dynamic programming using the cut rod problem.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=RYPsOJmhwgE",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-a.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Dynamic programming using steps and LCS",
 "published"=>true,
 "morea_id"=>"reading-screencast-12b",
 "morea_summary"=>"Example of dynamic programming using steps and LCS.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=0tQ3ah0Fddw",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-b.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Dynamic programming using LCS (continued)",
 "published"=>true,
 "morea_id"=>"reading-screencast-12c",
 "morea_summary"=>"Example of dynamic programming using LCS.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=dFObo5BeJ0k",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-c.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Dynamic programming applications, substructure, and summary",
 "published"=>true,
 "morea_id"=>"reading-screencast-12d",
 "morea_summary"=>"Summary of dynamic programming.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=QzgqDJIJtNY",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-d.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-sedgewick.html</h2>

<pre>Hash
{"title"=>"Sedgewick 37 - Dynamic Programming",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-37",
 "morea_summary"=>
  "Knapsack problem, matrix chain product, optimal BSTs, shortest paths, time and space requirements",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-sedgewick.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-sedgewick.md"}
</pre>

<h2>/morea/130.greedy-algorithms/experience.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms sample problem: activity scheduling",
 "published"=>true,
 "morea_id"=>"experience-greedy-algorithms",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply the greedy strategy to activity scheduling",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/130.greedy-algorithms/experience.html",
 "url"=>"/morea/130.greedy-algorithms/experience.html",
 "content"=>
  "### Activity Scheduling\n\nThe activity scheduling problem is to _**find the largest possible set **_\n(maximum _number_, not duration) _**of activities that do not overlap with\neach other**_, given their start and finish times.\n\nCLRS show that this problem has the _**greedy choice property**_: a globally\noptimal solution can be assembled from locally optimal choices. They show it\nby example with this _greedy strategy_: Always select the remaining compatible\nactivity that ends first, and then solve the subproblem of scheduling the\nactivities that start after this activity. The local optimization is to select\nthe activity that finishes first, and the global optimization is scheduling\nthe maximum number of activities possible.\n\nThere are other greedy strategies, but some work and some don't.\n\n#### Alternative Greedy Strategies\n\nIn the following three problems you determine whether alternative locally\ngreedy strategies lead to globally optimal solutions. For each strategy below,\n\n  * either show that the strategy leads to an optimal solution by outlining the algorithm or approach,\n  \n\n  * or give a counterexample: write down a set of activities (defined by their start and finish times) that the strategy fails on, and show what goes wrong.\n\n**1.** _Greedy strategy_: Always select the remaining compatible activity that has the **least duration**. _Rationale:_ Leave the most time remaining for other activities. \n\n**2.** _Greedy strategy_: Always select the remaining compatible activity that has the **latest start time**. _Rationale:_ Leave the most time remaining at the beginning for other activities.\n\n**3.** _Greedy strategy_: Always select the remaining compatible activity that **overlaps with the fewest number of remaining activities**. _Rationale:_ Eliminate the fewest number of remaining activities from consideration.\n\n####  Value Optimization\n\n**4.** Suppose now that different activities earn different amounts of revenue. In addition to their start and finish times _s__i_ and _f__i_, each activity _a__i_ has _revenue_ _r__i_, and our objective is now to maximize the total revenue Σ_a__i_∈_A_ _r__i_. \n\n  * Does this problem exhibit the greedy choice property?\n  * If so, do any of the above strategies apply, or another greedy strategy you can think of?\n  * If not, why not, and what alternative problem solving strategy would work on this problem? \n\n\n",
 "path"=>"morea//130.greedy-algorithms/experience.md"}
</pre>

<h2>/morea/130.greedy-algorithms/module.html</h2>

<pre>Hash
{"title"=>"Greedy Algorithms",
 "published"=>true,
 "morea_id"=>"greedy-algorithms",
 "morea_outcomes"=>["outcome-greedy-algorithms"],
 "morea_readings"=>
  ["reading-screencast-13a",
   "reading-screencast-13b",
   "reading-screencast-13c",
   "reading-cormen-16",
   "reading-notes-13"],
 "morea_experiences"=>["experience-greedy-algorithms"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/130.greedy-algorithms/logo.png",
 "morea_sort_order"=>130,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/130.greedy-algorithms/module.html",
 "content"=>
  "Dynamic programming, activity scheduling, the greedy strategy, Huffman codes.\n",
 "path"=>"morea//130.greedy-algorithms/module.md"}
</pre>

<h2>/modules/greedy-algorithms/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Greedy Algorithms",
 "url"=>"/modules/greedy-algorithms/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/greedy-algorithms/index.html"}
</pre>

<h2>/morea/130.greedy-algorithms/outcome.html</h2>

<pre>Hash
{"title"=>"Use greedy algorithms for problem solving",
 "published"=>true,
 "morea_id"=>"outcome-greedy-algorithms",
 "morea_type"=>"outcome",
 "morea_sort_order"=>120,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/130.greedy-algorithms/outcome.html",
 "content"=>
  "Be able to implement solutions for simple optimization problems based upon greedy programming techniques.",
 "path"=>"morea//130.greedy-algorithms/outcome.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 16 - Greedy algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-16",
 "morea_summary"=>
  "Activity selection problems, elements of the greedy algorithm strategy, huffman codes (16.1 -16.3)",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-cormen.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on greedy algorithms",
 "published"=>true,
 "morea_id"=>"reading-notes-13",
 "morea_summary"=>
  "Greedy algorithms, the activity selection problem, greedy strategy, and Huffman codes.",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/130.greedy-algorithms/reading-notes.html",
 "url"=>"/morea/130.greedy-algorithms/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Prelude: Greedy Algorithms and Dynamic Programming\n  2. Example: Activity Selection Problem \n  3. Greedy Strategy \n  4. Huffman Codes \n\n## Readings and Screencasts\n\n  * Read the first three sections of CLRS Chapter 16, although you need not read the details of the proofs. We are not covering Matroids (the 4th section).\n\nThis presentation follows the CLRS reading fairly closely, selecting out the\nmost relevant parts and explaining a few things in more detail. (The\nassociated videos change the ordering somewhat: 13A provides a conceptual\nintroduction, leaving the activity selection example for 13B.)\n\n* * *\n\n##  Prelude: Greedy Algorithms and Dynamic Programming\n\nBoth Dynamic Programming and Greedy Algorithms are ways of solving\n_**optimization problems**_: a solution is sought that optimizes (minimizes or\nmaximizes) an _**objective function**_.\n\n**Dynamic Programming:**\n\n  * Finds solutions bottom-up (solves subproblems before solving their super-problem) \n  * Exploits overlapping subproblems for efficiency (by reusing solutions)\n  * Can handle subproblem interdependence \n\n**Greedy Algorithms** \"greedily\" take the choice with the most immediate gain. \n\n  * Find solutions top-down (commit to a choice, then solve sub-problems) \n  * Assume that if the objective function is optimized locally it will be optimized globally\n  * Cannot handle interdependent subproblems \n\nFor some problems, but not all, local optimization actually results in global\noptimization.\n\nWe'll use an example to simultaneously review dynamic programming and motivate\ngreedy algorithms, as the two approaches are related (but distinct).\n\n* * *\n\n##  Activity Selection Problem\n\nSuppose that _activities_ require exclusive use of a common resource, and you\nwant to schedule as many as possible.\n\nLet _S_ = {_a_1, ..., _a__n_} be a set of _n_ activities.\n\nEach activity _ai_ needs the resource during a time period starting at _si_\nand finishing before _fi_, i.e., during [_si_, _fi_).\n\n(_Why not_ [_si_, _fi_]?)\n\nThe optimization problem is to select the largest set of non-overlapping\n(mutually compatible) activities from _S_.\n\nWe assume that activities are sorted by finish time _f_1 ≤ _f_2 ≤ ... _f__n_-1\n≤ _f__n_ (this can be done in Θ(_n_ lg _n_)).\n\n### Example\n\nConsider these activities:\n\n![](fig/activities.jpg)\n\nHere is a graphic representation:\n\n![](fig/activity-timeline.jpg)\n\nSuppose we chose one of the activities that _start first_, and then look for\nthe next activity that starts after it is done. This could result in {_a_4,\n_a_7, _a_8}, but this solution is not optimal.\n\nAn optimal solution is {_a_1, _a_3 _a_6, _a_8}. (It maximizes the objective\nfunction of number of activities scheduled.)\n\nAnother one is {_a_2, _a_5, _a_7, _a_9}. (Optimal solutions are not\nnecessarily unique.)\n\nHow do we find (one of) these optimal solutions? Let's consider it as a\ndynamic programming problem ...\n\n### Optimal Substructure Analysis\n\nA dynamic programming analysis begins by identifying the choices to be made,\nand assuming that you can make an optimal choice (without yet specifying what\nthat choice is) that will be part of an optimal solution.\n\nIt then specifies the possible subproblems that result in the most general way\n(to ensure that possible components of optimal solutions are not excluded),\nand shows that an an optimal solution must recursively include optimal\nsolutions to the subproblems. (This is done by reasoning about the value of\nthe solutions according to the objective function.)\n\nWe'll approach Activity Selection similarly. I'll try to clarify the reasoning\nin the text ...\n\nFor generality, we define the problem in a way that applies both to the\noriginal problem and subproblems.\n\nSuppose that due to prior choices we are working on a time interval from _i_\nto _j_. This could be after some already-scheduled activity _ai_ and before\nsome already-scheduled event _aj_, or for the original problem we can define\n_i_ and _j_ to bound the full set of activities to be considered.\n\nThen the candidate activities to consider are those that start after _ai_ and\nend before _aj_:\n\n![](fig/compatible-activities.jpg)  \n![](fig/compatible-activities-timeline.jpg)\n\nNow let's define _Aij_ to be an optimal solution, i.e., a maximal set of\nmutualy compatible activities in _Sij_. What is the structure of this\nsolution?\n\nAt some point we will need to make a choice to include some activity _ak_ with\nstart time _sk_ and finishing by _fk_ in this solution. This choice will leave\ntwo sets of compatible candidates after _ak_ is taken out:\n\n  * _Sik_ : activities that start after _ai_ finishes, and finish before _ak_ starts \n  * _Skj_ : activities that start after _ak_ finishes, and finish before _aj_ starts \n\n(Note that _Sij_ may be a proper superset of _Sik_ ∪ {_ak_} ∪ _Skj_, as\nactivities incompatible with _ak_ are excluded.)\n\nUsing the same notation as above, define the optimal solutions to these\nsubproblems to be:\n\n  * _Aik_ = _Aij_∩ _Sik_: the optimal solution to _Sik_\n  * _Akj_ = _Aij_ ∩ _Skj_: the optimal solution to _Skj_\n\nSo the structure of an optimal solution _Aij_ is:\n\n> _Aij_ = _Aik_ ∪ {_ak_} ∪ _Akj_\n\nand the number of activities is:\n\n> |_Aij_| = |_Aik_| + 1 + |_Akj_|\n\nBy the \"cut and paste argument\", an optimal solution _Aij_ for _Sij_ must\ninclude the optimal solutions _Aik_ for _Sik_ and _Akj_ for _Skj_, because if\nsome suboptimal solution _A'ik_ were used for _Sik_ (or similarly _A'kj_ for\n_Skj_), where |_A'ik_| < |_Aik_|, we could substitute _Aik_ to increase the\nnumber of activities (a contradiction to optimality).\n\nTherefore the Activity Scheduling problem exhibits optimal substructure.\n\n### Recursive Solution\n\nSince the optimal solution _A__ij_ must include optimal solutions to the\nsubproblems for _S__ik_ and _S__kj_, we could solve by dynamic programming.\n\nLet _c_[_i_, _j_] = size of optimal solution for _S__ij_ (_c_[_i_, _j_] has\nthe same value as |_Aij_|, but apparently we are switching notation to\nindicate that this is for any optimal solution). Then\n\n> _c_[_i_, _j_] = _c_[_i_, _k_] + _c_[_k_, _j_] + 1   (the 1 is to count\n_ak_).\n\nWe don't know which activity _ak_ to choose for an optimal solution, so we\ncould try them all:\n\n![](fig/activity-scheduling-recurrence-16-2.jpg)\n\nThis suggests a recursive algorithm that can be memoized, or we could develop\nan equivalent bottom-up approach, filling in tables in either case.\n\nBut it turns out we can solve this without considering multiple subproblems.\n\n### Being Greedy\n\nWe are trying to optimize the number of activities. Let's be greedy!\n\n  * The more time that is left after running an activity, the more subsequent activities we can fit in. \n  * If we **choose the first activity to _finish,_** the most time will be left.\n  * Since activities are sorted by finish time, we will always start with _a_1. \n  * Then we can solve the single subproblem of activity scheduling in this remaining time.\n\nSince there is only a single subproblem, the _Sij_ notation, bounding the set\nat both ends, is more complex than we need. We'll simplify the notation to\nindicate the activities that start after _ak_ finishes:\n\n> _S_k = {_ai_ ∈ _S_ : _si_ ≥ _fk_}\n\nSo, after choosing _a_1 we just have _S_1 to solve (and so on after choices in\nrecursive subproblems).\n\nBy optimal substructure, _if_ _a_1 is part of an optimal solution, then an\noptimal solution to the original problem consists of _a_1 plus all activities\nin an optimal solution to _S_1.\n\nBut we need to prove that _a_1 is always part of some optimal solution (i.e.,\nto prove our original intuition).\n\n_**Theorem:**_ If _S_k is nonempty and _am_ has the earliest finish time in\n_S_k, then _am_ is included in some optimal solution.\n\n_Proof:_ Let _Ak_ be an optimal solution to _S_k, and let _aj_ ∈ _Ak_ have the\nearliest finish time in _Ak_. If _aj_ = _am_ we are done. Otherwise, let\n_A'__k_ = (_Ak_ \\- {_aj_}) ∪ {_am_} (substitute _am_ for _aj_).\n\n> _Claim:_ Activities in _A'k_ are disjoint.\n\n>\n\n> _Proof of Claim:_ Activities in _Ak_ are disjoint because it was a solution.  \nSince _aj_ is the first activity in _Ak_ to finish, and fm ≤ fj (_am_ is the\nearliest in _Sk_), _am_ cannot overlap with any other activities in _A'k_.  \nNo other changes were made to _Ak_, so _A'k_ must consist of disjoint\nactivities.\n\nSince |_A'k_| = |_Ak_| we can conclude that _A'k_ is also an optimal solution\nto _S_k, and it includes _am_.\n\nTherefore we don't need the full power of dynamic programming: we can just\nrepeatedly choose the activity that finishes first, remove any activities that\nare incompatible with it, and repeat on the remaining activities until no\nactivities remain.\n\n### Greedy Algorithm Solution\n\nLet the start and finish times be represented by arrays _s_ and _f_, where _f_\nis assumed to be sorted in monotonically increasing order.\n\nAdd a fictitious activity _a_0 with _f_0 = 0, so _S_0 = _S_ (i.e., the entire\ninput sequence).\n\nOur initial call will be RECURSIVE-ACTIVITY-SELECTOR(_s_, _f_, 0, _n_).\n\n![](fig/pseudocode-recursive-activity.jpg)\n\nThe algorithm is Θ(_n_) because each activity is examined exactly once across\nall calls: each recursive call starts at _m_, where the previous call left\noff. (Another example of aggregate analysis.)\n\nIf the activities need to be sorted, the overall problem can be solved in\nΘ(_n_ lg _n_)).\n\nThis algorithm is nearly tail recursive, and can easily be converted to an\niterative version:\n\n![](fig/pseudocode-greedy-activity.jpg)\n\nLet's trace the algorithm on this:\n\n![](fig/activity-timeline.jpg)\n\n* * *\n\n##  A Closer Look at the Greedy Strategy\n\nInstead of starting with the more elaborate dynamic programming analysis, we\ncould have gone directly to the greedy approach.\n\nTypical steps for designing a solution with the greedy strategy (and two\nproperties that are key to determining whether it might apply to a problem):\n\n  1. Consider how we can make a greedy choice (local optimization of the objective function), leaving one subproblem to solve.\n  2. **Greedy Choice Property:** Prove that the greedy choice is always part of some optimal solution.\n  3. **Optimal Substructure:** Demonstrate that an optimal solution to the problem contains within it optimal solutions to the subproblems.\n\nThen we can construct an algorithm that combines the greedy choice with an\noptimal solution to the remaining problem.\n\n###  Dynamic Programming compared to Greedy Strategy:\n\nBoth require optimal substructure, but ...\n\n**Dynamic Programming**\n\n  * Each choice depends on knowing the optimal solutions to subproblems.\n  * Bottom-up: Solve subproblems first\n\n**Greedy Strategy**\n\n  * Each choice depends only on local optimization \n  * Top-down: Make choice before solving subproblems \n\n### Example: Knapsack Problems\n\nThese two problems demonstrate that the two strategies do not solve the same\nproblems. Suppose a thief has a knapsack of fixed carrying capacity, and wants\nto optimize the value of what he takes.\n\n![](fig/Fig-16-2-0-1-a-knapsack-example.jpg)\n\n#### 0-1 knapsack problem:\n\nThere are _n_ items. Item _i_ is worth $_vi_ and weighs _wi_ pounds. The thief\nwants to take the most valuable subset of items with weight not exceeding _W_\npounds. It is called 0-1 because the thief must either not take or take each\nitem (they are discrete objects, like gold ingots).\n\nIn the example, item 1 is worth $6/pound, item 2 $5/pound and item 3 $4/pound.\n\nThe greedy strategy of optimizing value per unit of weight would take item 1\nfirst.\n\n#### Fractional knapsack problem:\n\nThe same as the 0-1 knapsack problem except that the thief _can take a\nfraction of each item_ (they are divisible substances, like gold powder).\n\nBoth have optimal substructure _(why?)._\n\nOnly the fractional knapsack problem has the greedy choice property:\n\n_Fractional:_ One can fill up as much of the most valuable substance by weight\nas one can hold, then as much of the next most valuable substance, etc., until\n_W_ is reached:\n\n![](fig/Fig-16-2-0-1-c-knapsack-example.jpg)\n\n_0-1:_ A greedy strategy could result in empty space, reducing the overall\ndollar density of the knapsack. After choosing item 1, the optimal solution\n(shown third) cannot be achieved:\n\n![](fig/Fig-16-2-0-1-b-knapsack-example-reordered.jpg)\n\n* * *\n\n##  Huffman Codes\n\nWe are going to see several greedy algorithms throughout the semester. The\nactivity scheduler was good for illustration, but is not important in\npractice. We should look at one important greedy algorithm today ...\n\nHuffman codes provide an efficient way to compress text, and are constructed\nusing a greedy algorithm. We only have time to review how this important\nalgorithm works; see the text for analysis.\n\n### Binary Codes\n\n**Fixed-length binary codes** (e.g., ASCII) represent each character with a fixed number of bits (a binary string of fixed length called a **codeword**).\n\n**Variable-length binary codes** can vary the number of bits allocated to each character. This opens the possibility of space efficiency by using fewer bits for frequent characters.\n\nExample: Suppose we want to encode documents with these characters:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n\nWith a 3 bit code it would take 300,000 bits to code a file of 100,000\ncharacters, but the variable-length code shown requires only 224,000 bits.\n\n**Prefix codes** (better named **prefix-free codes**) are codes in which no codeword is a prefix of another.\n\nFor any data, it is always possible to construct a prefix code that is optimal\n(though not all prefix codes are optimal, as we will see below).\n\nPrefix codes also have the advantage that each character in an input file can\nbe \"consumed\" unambiguously, as the prefix cannot be confused with another\ncode.\n\n### Binary Tree Representation of Prefix Codes\n\nWe can think of the 0 and 1 in a prefix code as directions for traversing a\nbinary tree: 0 for left and 1 for right. The leaves store the coded character.\nFor example, here is the fixed-length prefix code from the table above\nrepresented as a binary tree:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n![](fig/Fig-16-4-a-coding-tree.jpg)\n\nConsuming bits from an input file, we traverse the tree until the character is\nidentified, and then start over at the top of the tree for the next character.\n\n_Exercise: Decode 101100100011_\n\nBut the above tree uses three bits per character: it is not optimal. It can be\nshown that an optimal code is always represented by a full binary tree (every\nnon-leaf node has two children).\n\nFor example, an optimal prefix code (from the table reproduced again here) is\nrepresented by this tree:\n\n![](fig/Fig-16-4-b-coding-tree.jpg) \n![](fig/Fig-16-3-character-coding-problem.jpg)\n\n_Exercise: Decode 10111010111_\n\n### Huffman's Algorithm\n\nHuffman's greedy algorithm constructs optimal prefix codes called **Huffman\nCodes**.\n\nIt is given a set _C_ of _n_ characters, where each character has frequency\n_c.freq_ in the \"text\" to be encoded.\n\nThe optimality of a code is relative to a \"text\", which can be what we\nnormally think of as texts, or can be other data encoded as sequences of bits,\nsuch as images.\n\n  * We can construct a generic Huffman code for a universe of texts, such as all texts in English, by estimating the frequency of characters in this universe of texts.\n  * More commonly, we contruct Huffman codes optimized for particular documents. Then the document-specific code needs to be passed on along with the compressed document.\n\nThe algorithm creates a binary tree leaf node for each character, annotated\nwith its frequency, and the tree nodes are then put on a min-priority queue\n(this is only implied in line 2 below).\n\nThen the first two subtrees on the queue (those with minimum frequency) are\ndequeued with `Extract-Min`, merged into a single tree, annotated with the sum\nof their frequencies, and this single node is re-queued.\n\nThis process is repeated until only one tree node remains on the queue (the\nroot). Since a tree is being constructed and |_E_| = |_V_|−1 we can just run\nthe loop until _n_−1 and know that there will be one node left at this point.\n\nHere is the algorithm:\n\n![](fig/pseudocode-huffman.jpg)\n\n#### Informal Correctness\n\nThe \"greedy\" aspect is the choice to merge min-frequency nodes first, and\nassume that this local minimization will result in an optimal global solution.\n\nIntuitively, this approach will result in an optimal solution because the\nlowest frequency items will be \"pushed down\" deeper in the tree, and hence\nhave longer codes; while higher frequency items will end up nearer the root,\nand hence have the shortest codes.\n\nCormen et al. prove correctness with two Lemmas for the two properties:\n\n  * Greedy choice property: there exists an optimal prefix code where two characters having the lowest frequency in _C_ are encoded with equal length strings that differ only in the last bit, as they are leaf nodes. \n  * Optimal-substructure property: if the tree constructed by merging two nodes is optimal it must have been constructed from an optimal tree for the subproblem.\n\n#### Complexity\n\nThe initial BUILD-MIN-HEAP implied by line 2 requires O(_n_) time.  \nThe loop executes _n_ times, with O(lg _n_) required for each heap operation.  \nThus, HUFFMAN is O(_n_ lg _n_).\n\n### An Example of Huffman Coding\n\nThe characters are in a min priority queue by frequency:\n\n![](fig/Fig-16-5-a-huffman-trace.jpg)\n\nTake out the two lowest frequency items and make a subtree that is put back on\nthe queue as if it is a combined character:\n\n![](fig/Fig-16-5-b-huffman-trace.jpg) \n![](fig/pseudocode-huffman.jpg)\n\nCombine the next lowest frequency characters:\n\n![](fig/Fig-16-5-c-huffman-trace.jpg)\n\nContinuing, tree fragments themselves become subtrees:\n\n![](fig/Fig-16-5-d-huffman-trace.jpg)\n\nTwo subtrees are merged next:\n\n![](fig/Fig-16-5-e-huffman-trace.jpg)\n\nThe highest frequency character gets added to the tree last, so it will have a\ncode of length 1:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n![](fig/Fig-16-5-f-huffman-trace.jpg)\n\nOne might wonder why the second most frequent character does not have a code\nof length 2. This would force the other characters to be deeper in the tree,\ngiving them excessively long codes.\n\n* * *\n\n## Wrapup\n\nWe will encounter several examples of greedy algorithms later in the course,\nincluding classic algorithms for finding minimum spanning trees (Topic\n[17](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-17.html))\nand shortest paths in graphs (Topics\n[18](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html)\nand\n[19](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-19.html)).\n\n* * *\n\nDan Suthers Last modified: Sat Mar 1 17:53:22 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//130.greedy-algorithms/reading-notes.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms and dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-screencast-13a",
 "morea_summary"=>"Dynamic programming and greedy algorithsm.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=hWtmauUMYD8",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-a.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms and activity scheduling",
 "published"=>true,
 "morea_id"=>"reading-screencast-13b",
 "morea_summary"=>"Illustration of the greedy strategy.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=_-1nKy0bnwE",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-b.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Huffmann Codes",
 "published"=>true,
 "morea_id"=>"reading-screencast-13c",
 "morea_summary"=>"Design and implementation of Huffman Codes.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=UgJk87jazLQ",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-c.md"}
</pre>

<h2>/morea/140.graphs/experience-1.html</h2>

<pre>Hash
{"title"=>"Breadth first search and depth first search",
 "published"=>true,
 "morea_id"=>"experience-graph-bfs-dfs",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with breadth first search and depth first search",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-1.html",
 "url"=>"/morea/140.graphs/experience-1.html",
 "content"=>
  "## Breadth first search and depth first search\n\n### Peer Credit Assignment\n\n**1.** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate across all members (_NOT_ 3 points per member!).\n  * If two members besides yourself were present, you have a total of 4 points to allocate across all members.\n  * If only one other member was present, you have a total of 6 points to allocate across all members.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### Breadth first search\n\n#### 2 points\n\n![](fig/pseudocode-BFS-Smaller.jpg)\n\nThe text (p. 597) analyzes Breadth-First-Search (BFS), using an _aggregate\nanalysis_ (what you are reading about for Tuesday's upcoming class) to show\nthat because the procedure scans the adjacency list of each vertex only when\nthe vertex is dequeued, and this happens only once, the total time spent\nscanning adjacency lists is O(E). Along with O(V) initialization this gives\nO(V+E) for the total running time.\n\n**2.** What would be the running time of BFS if we replace the adjacency list with an adjacency matrix and modify it at line 12 to handle this form of input? Justify your answer. \n\n(_Note:_ Change of representation is a common situation and I will also ask\nyou about the implications of change of representation on the next exam.)\n\n### Depth first search and Cycles\n\n#### 8 points; 2 each\n\n![](fig/pseudocode-DFS-DFS-Visit.jpg)\n\nDFS classifies edges as tree edges, back edges, forward edges, and cross edges\n(see p. 609).\n\n**3.** How can you modify DFS to detect back-edges? Say what code you would modify or insert, referencing CLRS line numbers.\n\n**4.** How can you modify DFS to determine whether a graph has a cycle? Again, say what code you would modify or insert, referencing CLRS line numbers. \n\n**5.** What is the run time of this modified cycle-detecting algorithm on _Directed_ graphs? Assume that it exits the DFS as soon as a cycle is found. Justify your answer.\n\n**6.** What is the run time of this modified cycle-detecting algorithm on _**Un**directed_ graphs? Assume that it exits the DFS as soon as a cycle is found. Justify your answer. \n\n* * *\n\nDan Suthers Last modified: Sat Mar 29 21:44:46 EDT 2014\n\n",
 "path"=>"morea//140.graphs/experience-1.md"}
</pre>

<h2>/morea/140.graphs/experience-2.html</h2>

<pre>Hash
{"title"=>"Strongly connected components",
 "published"=>true,
 "morea_id"=>"experience-graph-scc",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Finding the strongly connected components through depth first search",
 "morea_sort_order"=>2,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-2.html",
 "url"=>"/morea/140.graphs/experience-2.html",
 "content"=>
  "## Finding SCC of a graph with DFS\n\n**1.** _Run DFS on this graph_. To make grading easier, _visit vertices in alphabetical order_ (both in the main loop of DFS and the adjacency list loop of DFS-Visit). For each vertex, show values _d_ (discovery), _f_ (finish), and π (parent). \n\n![](fig/SCC-Class-Problem-Graph.jpg)\n\nThe finish times from largest to smallest order the nodes in a manner\nconsistent with a topological sort of their SCCs.\n\n**2.** Now run DFS on the transpose graph, visiting vertices in order of finish time (highest to lowest) from the DFS of step 1 (as required by the SCC algorithm). Again, show values _d_ (discovery), _f_ (finish), and π (parent).\n\n![](fig/SCC-Class-Problem-Graph-Transpose.jpg)\n\n**3.** List the strongly connected components you found by first _listing the tree edges_ in the transpose graph that define the SCC, and then _listing the vertices_ in the SCC (the first is shown): \n    \n    \n    SCC 1:  Tree edges: { }; Vertices: {l} \n    SCC 2:  Tree edges: {...}; Vertices: {...} \n    SCC 3:  ... \n    \n\n\n",
 "path"=>"morea//140.graphs/experience-2.md"}
</pre>

<h2>/morea/140.graphs/experience-3.html</h2>

<pre>Hash
{"title"=>"Computing Transpose",
 "published"=>true,
 "morea_id"=>"experience-graph-transpose",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply transpose under different representations",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-3.html",
 "url"=>"/morea/140.graphs/experience-3.html",
 "content"=>
  "### Computing Transpose Under Different Representations\n\nThe **transpose** of a directed graph _G_ = (_V_, _E_) is the graph _G__T_ =\n(_V_, _E__T_), where _E__T_ = {(_v_, _u_) such that (_u_, _v_) ∈ _E_}. In\nother words, _G__T_ is the graph _G_ with all of its edges reversed.\n\nAlgorithms for computing transpose by copying to a new graph are trivial to\nwrite under matrix and adjacency list representations. However, we will be\nworking with very large graphs and want to avoid copies. Can we transpose a\ngraph \"in place\" without making a copy, and minimizing the amount of extra\nspace needed? What is the time and space cost to do so?\n\n(In the following, you are writing algorithms that work \"inside\" the ADT: you\nare allowed to access the matrix and list representations directly.)\n\n**1.** Describe an efficient algorithm for computing _G__T_ from _G_, _in place_ (don't make a copy), for the _adjacency-matrix_ representation of _G_. Do this by actually _modifying the matrix_. Analyze the space and time requirements of your algorithm. \n\n**2.** Now describe a way to compute _G__T_ from _G_ under the _adjacency-matrix_ representation _without modifying the matrix_, and by _using only one bit_ of extra storage! Analyze space and time requirements. (_Hint:_ You need only make it _appear_ to the client to be transposed.)\n\n**3.** Describe an efficient algorithm for computing _G__T_ from _G_ using the _adjacency-list_ representation of _G_, using _as little space as possible_. Can you do it \"in place\"? Analyze the space and time requirements of your algorithm. \n\n**4.** Now suppose you have an _edge-list_ representation of _G_. What algorithm would efficiently compute _G__T_ with the edge-list, and with what space and time requirements? \n\n\n",
 "path"=>"morea//140.graphs/experience-3.md"}
</pre>

<h2>/morea/140.graphs/module.html</h2>

<pre>Hash
{"title"=>"Graphs",
 "published"=>true,
 "morea_id"=>"graphs",
 "morea_outcomes"=>["outcome-graphs"],
 "morea_readings"=>
  ["reading-screencast-14a",
   "reading-screencast-14b",
   "reading-screencast-14c",
   "reading-screencast-14d",
   "reading-screencast-14e",
   "reading-screencast-14f",
   "reading-cormen-22",
   "reading-sedgewick-32",
   "reading-sedgewick-wayne-4",
   "reading-goodrich-graphs",
   "reading-notes-14"],
 "morea_experiences"=>
  ["experience-graph-transpose",
   "experience-graph-scc",
   "experience-graph-bfs-dfs"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/140.graphs/logo.png",
 "morea_sort_order"=>140,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/140.graphs/module.html",
 "content"=>
  "Definitions, methods, breadth first search, depth first search, shortest path, asymptotic complexity, topological sort, strongly connected components. \n",
 "path"=>"morea//140.graphs/module.md"}
</pre>

<h2>/modules/graphs/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Graphs",
 "url"=>"/modules/graphs/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/graphs/index.html"}
</pre>

<h2>/morea/140.graphs/outcome.html</h2>

<pre>Hash
{"title"=>"Understand graphs",
 "published"=>true,
 "morea_id"=>"outcome-graphs",
 "morea_type"=>"outcome",
 "morea_sort_order"=>140,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/140.graphs/outcome.html",
 "content"=>
  "Be able to assess when to use different graph algorithms and why they might be preferred for a given problem context. ",
 "path"=>"morea//140.graphs/outcome.md"}
</pre>

<h2>/morea/140.graphs/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 22 - Elementary graph algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-22",
 "morea_summary"=>
  "Representations of graphs, breadth-first search, depth-first search, topological sort, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "24 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-cormen.md"}
</pre>

<h2>/morea/140.graphs/reading-goodrich.html</h2>

<pre>Hash
{"title"=>"Goodrich and Tommassia 13 - Graphs",
 "published"=>true,
 "morea_id"=>"reading-goodrich-graphs",
 "morea_summary"=>
  "The graph ADT, data structures for graphs, graph traversals, directed graphs, weighted graphs, shortest paths, minimum spanning trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://ww0.java4.datastructures.net/",
 "morea_labels"=>["Textbook", "70 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-goodrich.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-goodrich.md"}
</pre>

<h2>/morea/140.graphs/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on graphs",
 "published"=>true,
 "morea_id"=>"reading-notes-14",
 "morea_summary"=>
  "Graph definitions, examples, the ADT, representations, breadth first seach, depth first search, topological sort, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>10,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/reading-notes.html",
 "url"=>"/morea/140.graphs/reading-notes.html",
 "content"=>
  "## Outline\n\nTuesday\n\n  1. Graph Definitions & examples\n  2. Graph ADT\n  3. Representations (Implementations) of Graph ADT\n  4. Breadth-first Search \n\nThursday\n\n  1. Depth-first Search \n  2. Topological Sort \n  3. Strongly Connected Components \n  4. Related Concepts \n\nThe video lectures and notes below provide material not found in the textbook:\ndefining graphs, an ADT, and implementations. This material is important for\nProject 2.\n\n##  Graphs\n\n### Definitions\n\nA graph _G_ is a pair\n\n> _G_ = (_V_, _E_)\n\nwhere\n\n> _V_ = {_v_1, _v_2, ... _v_n}, a set of **vertices**\n\n> _E_ = {_e_1, _e_2, ... _e_m} ⊆ _V_ ⊗ _V_, a set of **edges**.\n\n#### Undirected Graphs\n\nIn an **undirected graph** the edge set E consists of _unordered pairs_ of\nvertices. That is, they are sets _e_ = {_u_, _v_}. Edges can be written with\nthis notation when clarity is desired, but we will often use parentheses (_u_,\n_v_).\n\nNo self loops are allowed in undirected graphs. That is, we cannot have (_v_,\n_v_), which would not make as much sense in the set notation {_v_, _v_}.\n\nWe say that _e_ = {_u_, _v_} is **incident on** _u_ and _v_, and that the\nlatter vertices are **adjacent**. The **degree** of a vertex is the number of\nedges incident on it.\n\nThe **handshaking lemma** is often useful in proofs:\n\n> Σ_v_∈_V_degree(_v_) = 2|_E_|\n\n(Each edge contributes two to the sum of degrees.)\n\n#### Directed Graphs\n\nIn a **directed graph** or **digraph** the edges are ordered pairs (_u_, _v_).\n\nWe say that _e_ = (_u_, _v_) is **incident from** or **leaves** _u_ and is\n**incident to** or **enters** _v_. The **in-degree** of a vertex is the number\nof edges incident to it, and the **out-degree** of a vertex is the number of\nedges incident from it.\n\n**Self loops** (_v_, _v_) are allowed in directed graphs.\n\n#### Paths\n\nA **path** of length _k_ is a sequence of vertices ⟨_v_0, _v_1, _v_2, ...\n_v_k⟩ where (_v__i_-1, _v__i_) ∈ _E_, for _i_ = 1, 2, ... _k_. (Some authors\ncall this a \"walk\".) The path is said to **contain** the vertices and edges\njust defined.\n\nA **simple path** is a path in which all vertices are distinct. (The \"walk\"\nauthors call this a \"path\").\n\nIf a path exists from _u_ to _v_ we say that _v_ is **reachable** from _u_.\n\nIn an undirected graph, a path ⟨_v_0, _v_1, _v_2, ... _v_k⟩ forms a **cycle**\nif _v_0 = _v_k and _k_ ≥ 3 (as no self-loops are allowed).\n\nIn a directed graph, a path forms a **cycle** if _v_0 = _v_k and the path\ncontains at least one edge. (This is clearer than saying that the path\ncontains at least two vertices, as self-loops are possible in directed\ngraphs.) The cycle is **simple** if _v_1, _v_2, ... _v_k are distinct (i.e.,\nall but the designated start and end _v_0 = _v_k are distinct). A directed\ngraph with no self-loops is also **simple**.\n\nA graph of either type with no cycles is **acyclic**. A directed acyclic graph\nis often called a **dag**.\n\n#### Connectivity\n\nA graph _G'_ = (_V'_, _E'_) is a **subgraph** of _G_ = (_V_, _E_) if _V'_ ⊆\n_V_ and _E'_ ⊆ _E_.\n\nAn undirected graph is **connected** if every vertex is reachable from all\nother vertices. In any connected undirected graph, |_E_| ≥ |_V_| - 1 (see also\n[discussion of tree properties](http://www2.hawaii.edu/~suthers/courses/ics311\ns14/Notes/Topic-08.html)). The **connected components** of _G_ are the maximal\nsubgraphs _G_1 ... _G__k_ where every vertex in a given subgraph is reachable\nfrom every other vertex in that subgraph, but not reachable from any vertex in\na different subgraph.\n\nA directed graph is **strongly connected** if every two vertices are reachable\nfrom each other. The **strongly connected components** are the subgraphs\ndefined as above. A directed graph is thus strongly connected if it has only\none strongly connected component. A directed graph is **weakly connected** if\nthe underlying undirected graph (converting all tuples (_u_, _v_) ∈ _E_ into\nsets {_u_, _v_} and removing self-loops) is connected.\n\n#### Variations\n\nA **bipartite** graph is one in which _V_ can be partitioned into two sets\n_V_1 and _V_2 such that every edge connects a vertex in _V_1 to one in _V_2.\nEquivalently, there are no odd-length cycles.\n\nA **complete** graph is an undirected graph in which every pair of vertices is\nadjacent.\n\nA **weighted** graph has numerical weights associated with the edges. (The\nallowable values depend on the application. Weights are often used to\nrepresent distance, cost or capacity in networks.)\n\n### Graph Size in Analysis\n\nAsymptotic analysis is often in terms of both |_V_| and |_E_|. Within\nasymptotic notation we leave out the \"|\" for simplicity, for example, writing\nO(_V_ \\+ _E_), O(_V_2 lg _E_), etc.\n\n### Many Applications ...\n\n![](fig/GT-Map-Graph-Example.jpg) ![](fig/GT-Circuit-Graph-Example.jpg) ![](fig/social-network.jpg)\n![](fig/240px-Internet_map_1024.jpg)\n\n* * *\n\n## Graph ADT\n\nThese are detailed slightly more in [Project\n2](http://www2.hawaii.edu/~suthers/courses/ics311s14/Projects/Project-2.html),\nand in the Goodrich & Tamassia excerpt uploaded to Laulima.\n\n#### Graph Accessors\n\n**numVertices()**  \n    Returns the number of vertices |_V_|\n\n**numEdges()**   \n    Returns the number of edges |_E_|\n\n**vertices()**  \n    Returns an iterator over the vertices _V_\n\n**edges()**  \n    Returns an iterator over the edges _E_\n\n#### Accessing Undirected Graphs\n\n**degree(_v_)**  \n    Returns the number of edges (directed and undirected) incident on _v_.\n\n**adjacentVertices(_v_)**  \n    Returns an iterator of the vertices adjacent to _v_.\n\n**incidentEdges(_v_)**  \n    Returns an iterator of the edges incident on _v_. \n\n**endVertices(_e_)**  \n    Returns an array of the two end vertices of _e_.\n\n**opposite(_v_,_e_)**  \n    Given _v_ is an endpoint of _e_.  \n    Returns the end vertex of _e_ different from _v_.   \n    Throws InvalidEdgeException when _v_ is not an endpoint of _e_.\n\n**areAdjacent(_v_1,_v_2)**  \n    Returns true iff _v_1 and _v_2 are adjacent by a single edge. \n\n####  Accessing Directed Graphs\n\n**directedEdges()**  \n    Returns an iterator over the directed edges of _G_. \n\n**undirectedEdges()**  \n    Returns an iterator over the undirected edges of _G_. \n\n**inDegree(_v_)**  \n    Returns the number of directed edges (arcs) incoming to _v_. \n\n**outDegree(_v_)**  \n    Returns the number of directed edges (arcs) outgoing from _v_.\n\n**inAdjacentVertices(_v_)**  \n    Returns an iterator over the vertices adjacent to _v_ by incoming edges. \n\n**outAdjacentVertices(_v_)**  \n    Returns an iterator over the vertices adjacent to _v_ by outgoing edges. \n\n**inIncidentEdges(_v_)**  \n    Returns an iterator over the incoming edges of _v_. \n\n**outIncidentEdges(_v_)**  \n    Returns an iterator over the outgoing edges of _v_. \n\n**destination(_e_)**  \n    Returns the destination vertex of _e_, if _e_ is directed.  \n    Throws InvalidEdgeException when _e_ is undirected.\n\n**origin(_e_)**  \n    Returns the origin vertex of _e_, if _e_ is directed.  \n    Throws InvalidEdgeException when _e_ is undirected. \n\n**isDirected(_e_)**  \n    Returns true if _e_ is directed, false otherwise\n\n#### Mutators (Undirected and Directed)\n\n**insertEdge(_u_,_v_)**  \n**insertEdge(_u_,_v_,_o_)**  \n    Inserts a new undirected edge between two existing vertices, optionally containing object _o_.  \n    Returns the new edge. \n\n**insertVertex()**  \n**insertVertex(_o_)**  \n    Inserts a new isolated vertex optionally containing an object _o_ (e.g., the label associated with the vertex).  \n    Returns the new vertex. \n\n**insertDirectedEdge(_u_,_v_)**  \n**insertDirectedEdge(_u_,_v_,_o_)**  \n    Inserts a new directed edge from an existing vertex to another.  \n    Returns the new edge. \n\n**removeVertex(_v_)**  \n    Deletes a vertex and all its incident edges.  \n    Returns object formerly stored at _v_.\n\n**removeEdge(_e_)**  \n    Removes an edge.  \n    Returns the object formerly stored at _e_.\n\n#### Annotators (for vertices and all types of edges)\n\nMethods for annotating vertices and edges with arbitrary data.\n\n**setAnnotation(Object _k_, _o_)**  \n    Annotates a vertex or edge with object _o_ indexed by key _k_.\n\n**getAnnotation(Object _k_)**  \n    Returns the object indexed by _k_ annotating a vertex or edge.\n\n**removeAnnotation(Object _k_)**  \n    Removes the annotation on a vertex or edge indexed by _k_ and returns it.\n\n#### Changing Directions\n\nThere are various methods for changing the direction of edges. I think the\nonly one we will need is:\n\n**reverseDirection(_e_)**  \n    Reverse the direction of an edge.  \n    Throws InvalidEdgeException if the edge is undirected\n\n* * *\n\n##  Graph Representations\n\nThere are two classic representations: the adjacency list and the adjacency\nmatrix.\n\nIn the **adjacency list**, vertices adjacent to vertex _v_ are listed\nexplicitly on linked list _G_.Adj[_v_] (assuming an array representation of\nlist headers).\n\nIn the **adjacency matrix**, vertices adjacent to vertex _v_ are indicated by\nnonzero entries in the row of the matrix indexed by v, in the columns for the\nadjacent vertices.\n\nAdjacency List and Matrix representations of an undirected graph:\n\n![](fig/Fig-22-1-representations-undirected.jpg)\n\nAdjacency List and Matrix representations of a directed graph:\n\n![](fig/Fig-22-2-representations-directed.jpg)\n\n_Consider this before reading on: What are the asymptotic complexities of\nthese methods in each representation? _\n\n  * List vertices/edges \n  * areAdjacent \n  * access (out)AdjacentVertices or (out)IncidentEdges (outdegree) \n  * access (in)AdjacentVertices or (in)IncidentEdges (indegree) \n\n_Are edges first class objects in the above representations? Where do you\nstore edge information in the undirected graph representations?_\n\n### Complexity Analysis\n\n#### Adjacency List\n\nSpace required: Θ(_V_ \\+ _E_).\n\nTime to list all vertices adjacent to u: Θ(degree(_u_)).\n\nTime to determine whether (_u_, _v_) ∈ _E_: O(degree(_u_)).\n\n#### Adjacency Matrix\n\nSpace required: Θ(_V_2).\n\nTime to list all vertices adjacent to _u_: Θ(_V_).\n\nTime to determine whether (_u_, _v_) ∈ _E_: Θ(1).\n\nSo the matrix takes more space and more time to list adjacent matrices, but is\nfaster to test adjacency of a pair of matrices.\n\n### \"Modern\" Adjacency Representation\n\nGoodrich & Tamassia (reading in Laulima) propose a representation that\ncombines an edge list, a vertex list, and an adjacency list for each vertex:\n\n![](fig/GT-adjacency-list-structure.jpg)\n\nThe sets _V_ and _E_ can be represented using a dictionary ADT (from\nImplementation Project #1). In many applications, it is especially important\nfor _V_ to enable fast access by key, and may be important to access in order.\nEach vertex object has an adjacency list _I_ (I for incident), and the edges\nreference both the vertices they connect and the entries in this adjacency\nlist. There's a lot of pointers to maintain, but this enables fast access in\nany direction you need, and for large sparse graphs the memory allocation is\nstill less than for a matrix representation.\n\nSee also Newman (2010) chapter 9, posted in Laulima, for discussion of graph\nrepresentations.\n\n* * *\n\n## BFS and DFS Overview\n\nBefore starting with Cormen et al.'s more complex presentation, let's discuss\nhow BFS and DFS can be implemented with nearly the same algorithm, but using a\nqueue for BFS and a stack for DFS. You should be comfortable with this\nrelationship between BFS/queues and DFS/stacks.\n\nSketch of both algorithms:\n\n  1. Pick a starting vertex and put it on the queue (BFS) or stack (DFS) \n  2. Repeat until the queue/stack is empty:\n    1. Dequeue (BFS) or pop (DFS) the next vertex _v_ from the appropriate data structure\n    2. If _v_ is unvisited, \n      * Mark _v_ as visited (and process it as needed for the specific application). \n      * Find the unvisted neighbors of _v_ and queue (BFS) or push (DFS) them on the appropriate data structure. \n\nTry starting with vertex q and run this using both a stack and a queue:\n\n![](fig/small-example-digraph.jpg)\n![](fig/small-example-digraph.jpg)\n\nBFS's FIFO queue explores nodes at each distance before going to the next\ndistance. DFS's LIFO stack explores the more distant neighbors of a node\nbefore continuing with nodes at the same distance (\"goes deep\").\n\nSearch in a directed graph that is weakly but not strongly connected may not\nreach all vertices.\n\n* * *\n\n## Breadth-first Search\n\nGiven a graph _G_ = (_V_,_E_) and a source vertex _s_ ∈ _V_, output _v.d_, the\nshortest distance (# edges) from _s_ to _v_, for all _v_ ∈ _V_. Also record\n_v.π_ = _u_ such that (_u_,_v_) is the last edge on a shortest path from _s_\nto _v_. (We can then trace the path back.)\n\n_Analogy_ Send a \"tsunami\" out from _s_ that first reaches all vertices 1 edge\nfrom _s_, then from them all vertices 2 edges from _s_, etc. Like a tsunami,\nequidistant destinations are reached at the \"same time\".\n\nUse a FIFO queue _Q_ to maintain the wavefront, such that _v_ ∈ _Q_ iff the\ntsunami has hit _v_ but has not come out of it yet.\n\n![](fig/bfs-nature-of-computation.jpg) ![](fig/pseudocode-BFS.jpg)\n\nAt any given time _Q_ has vertices with _d_ values _i, i, ... i, i+1, i+1, ...\ni+1_. That is, there are at most two distances on the queue, and values\nincrease monotonically.\n\n### Examples\n\n#### Book's Example: Undirected Graph\n\n![](fig/Fig-22-3-operation-BFS.jpg)\n\n#### A directed example:\n\nLet's do another (number the nodes by their depth, then click to compare your\nanswer):\n\n![](fig/BFS-directed-graph-example-1.jpg)\n\n### Time Analysis\n\n(This is an aggregate analysis.) Every vertex is enqueued at most once. We\nexamine edge (_u_, _v_) only when _u_ is dequeued, so every edge is examined\nat most once if directed and twice if undirected. Therefore, O(_V_ \\+ _E_).\n\n### Shortest Paths\n\n**Shortest distance** δ(_s_, _v_) from _s_ to _v_ is the minimum number of edges across all paths from _s_ to _v_, or ∞ if no such path exists.\n\nA **shortest path** from _s_ to _v_ is a path of length δ(_s_, _v_).\n\nIt can be shown that BFS is guaranteed to find the shortest paths to all\nvertices from a start vertex _s_: _v_._d_ = δ(_s_, _v_), ∀ _v_ at the\nconclusion of the algorithm. See book for tedious proof.\n\nInformally, we can see that all vertices at distance 1 from _s_ are enqueued\nfirst, then via them all nodes of distance 2 are reached and enqueued, etc.,\nso inductively it would be a contradiction if BFS reached a vertex _c_ by a\nlonger path than the shortest path because the last vertex _u_ on the shortest\npath to the given vertex _v_ would have been enqueued first and then dequeued\nto reach _v_.\n\n### Breadth-First Trees\n\nThe **predecessor subgraph** of G is\n\n> Gπ = (Vπ, Eπ) where  \nVπ= {_v_ ∈ V : _v_.π ≠ NIL} ∪ {_s_} and  \nEπ = {(_v_.π, _v_) : _v_ ∈ Vπ \\- {_s_}}\n\nA predecessor subgraph Gπ is a **breadth-first tree** if Vπ consists of\nexactly all vertices reachable from _s_ and for all _v_ in Vπ the subgraph Gπ\ncontains unique simple and _ shortest_ paths from _s_ to _v_.\n\nBFS constructs π such that Gπ is a breadth-first tree.\n\n* * *\n\n## Depth-first Search\n\nGiven _G_ = (_V_, _E_), directed or undirected, DFS explores the graph from\nevery vertex (no source is vertex given), constructing a _forest_ of trees and\nrecording two time stamps on each vertex:\n\n  * _v.d_ = discovery time\n  * _v.f_ = finishing time\n\nTime starts at 0 before the first vertex is visited, and is incremented by 1\nfor every discovery and finishing event (as explained below). These attributes\nwill be used in other algorithms later on.\n\nSince each vertex is discovered once and finished once, discovery and\nfinishing times are unique integers from 1 to 2|_V_|, and for all _v_, _v.d_ <\n_v.f_.\n\n_(Some presentations of DFS pose it as a way to visit nodes, enabling a given\nmethod to be applied to the nodes with no output specified. Others present it\nas a way to construct a tree. The CLRS presentation is more complex but\nsupports a variety of applications.)_\n\nDFS explores _every_ edge and starts over from different vertices if necessary\nto reach them (unlike BFS, which may fail to reach subgraphs not connected to\n_s_).\n\nAs it progresses, every vertex has a color:\n\n> WHITE = undiscovered\n\n>  \nGRAY = discovered, but not finished (still exploring vertices reachable from\nit)\n\n>     _v_._d_ records the moment at which _v_ is _discovered_ and colored\ngray.\n\n>  \nBLACK = finished (have found everything reachable from it)\n\n>     _v_._f_ records the moment at which _v_ is _finished_ and colored black.\n\n![](fig/dfs-nature-of-computation.jpg)\n\n### Pseudocode\n\n![](fig/pseudocode-DFS.jpg) ![](fig/pseudocode-DFS-Visit.jpg)\n\nWhile BFS uses a queue, DFS operates in a stack-like manner (using the\nimplicit recursion stack in the algorithm above).\n\n  * BFS's FIFO queue explores nodes at each distance before going to the next distance\n  * DFS's implicit LIFO stack explores the more distant neighbors of a node before continuing with nodes at the same distance (\"goes deep\").\n\nAnother major difference in the algorithms as presented here is that DFS will\nsearch from every vertex until all edges are explored, while BFS only searches\nfrom a designated start vertex.\n\n  * This is not an essential difference: both could be either restricted to a start vertex or run from all vertices; \n  * This reflects how the algorithms are used in practice (BFS for finding shortest paths; DFS for exposing structure of the graph, as will be explained shortly). \n\n### Example:\n\nOne could start DFS with any arbitrary vertex, and continue at any remaining\nvertex after the first tree is constructed. Regularities in the book's\nexamples (e.g., processing vertices in alphabetical order, or always starting\nat the top of the diagram) do not reflect a requirement of the algorithm.\n\n![](fig/Fig-22-4-operation-DFS-a.jpg)\n![](fig/Fig-22-4-operation-DFS-b.jpg)\n![](fig/Fig-22-4-operation-DFS-c.jpg)\n![](fig/Fig-22-4-operation-DFS-d.jpg)\n![](fig/Fig-22-4-operation-DFS-e.jpg)\n![](fig/Fig-22-4-operation-DFS-f.jpg)\n![](fig/Fig-22-4-operation-DFS-g.jpg)\n![](fig/Fig-22-4-operation-DFS-h.jpg)\n![](fig/Fig-22-4-operation-DFS-i.jpg)\n![](fig/Fig-22-4-operation-DFS-j.jpg)\n![](fig/Fig-22-4-operation-DFS-k.jpg)\n![](fig/Fig-22-4-operation-DFS-l.jpg)\n![](fig/Fig-22-4-operation-DFS-m.jpg)\n![](fig/Fig-22-4-operation-DFS-n.jpg)\n![](fig/Fig-22-4-operation-DFS-o.jpg)\n![](fig/Fig-22-4-operation-DFS-p.jpg)\n\nLet's do this example (start with the upper left node, label the nodes with\ntheir d and f, then click to compare your answer):\n\n![](fig/DFS-directed-graph-example-0.jpg)\n\n#### Time Analysis\n\n![](fig/pseudocode-DFS-DFS-Visit.jpg)\n\nThe analysis uses aggregate analysis, and is similar to the BFS analysis,\nexcept that DFS is guaranteed to visit every vertex and edge, so it is Θ not\nO:\n\n> Θ(_V_) to visit all vertices in lines 1 and 5 of `DFS`;  \n  \nΣ_v_∈_V_ |Adj(_v_)| = Θ(_E_) to process the adjacency lists in line 4 of `DFS-\nVisit`.  \n  \n(_Aggregate analysis:_ we are not attempting to count how many times the loop\nof line 4 executes each time it is encountered, as we don't know |Adj(_v_)|.\nInstead, we sum the number of passes through the loop in total: all edges will\nbe processed.)  \n  \nThe rest is constant time.\n\nTherefore, Θ(_V_ \\+ _E_).\n\n### Classification of Edges\n\nThis classification will be useful in forthcoming proofs and algorithms.\n\n  * **Tree Edge**: in the **depth-first forest** constructed by DFS: found by exploring (_u_,_v_). \n  * **Back Edge**: (_v_,_u_), where _v_ is a descendant of _u_.\n  * **Forward Edge**: (_u_,_v_), where _v_ is a descendant of _u_ but not a tree edge.\n  * **Cross Edge**: any other edge. They can go between vertices in the same depth-first tree or in different depth-first trees.\n\nHere's a graph with edges classified, and redrawn to better see the structural\nroles of the different kinds of edges:\n\n![](fig/Fig-22-5-DFS-properties-a.jpg)\n![](fig/Fig-22-5-DFS-properties-c.jpg)\n\n### DFS Properties\n\nThese theorems show important properties of DFS that will be used later to\nshow how DFS exposes properties of the graph.\n\n#### Parentheses Theorem\n\nAfter any DFS of a graph _G_, for any two vertices _u_ and _v_ in _G_, exactly\none of the following conditions holds:\n\n  * The intervals [_u_._d_, _u_._f_] and [_v_._d_, _v_._f_] are entirely disjoint, and neither _u_ nor _v_ is a descendant of the other in the DFS forest.\n  * The interval [_u_._d_, _u_._f_] is contained entirely within the interval [_v_._d_, _v_._f_], and _u_ is a descendant of _v_ in a DFS tree. \n  * The interval [_v_._d_, _v_._f_] is contained entirely within the interval [_u_._d_, _u_._f_], and _v_ is a descendant of _u_ in a DFS tree. \n\nEssentially states that the _d_ and _f_ visit times are well nested. See text\nfor proof. For the above graph:\n\n![](fig/Fig-22-5-DFS-properties-b.jpg)\n![](fig/Fig-22-5-DFS-properties-a.jpg)\n\n#### Corollary: Nesting of Descendant's Intervals\n\nVertex _v_ is a **_proper descendent_** of vertex _u_ in the DFS forest of a\ngraph iff _u_._d_ < _v_._d_ < _v_._f_ < _u_._f_. (Follows immediately from\nparentheses theorem.)\n\nAlso, (_u_, _v_) is a **_back edge_** iff _v_._d_ ≤ _u_._d_ < _u_._f_ ≤\n_v_._f_; and a **_cross edge_** iff _v_._d_ < _v_._f_ < _u_._d_ < _u_._f_.\n\n#### White Path Theorem\n\nVertex _v_ is a descendant of _u_ iff at time _u.d_ there is a path from _u_\nto _v_ consisting of only white vertices (except for _u_, which was _just_\ncolored gray).\n\n(Proof in textbook uses _v_._d_ and _v_._f_. Metaphorically and due to its\ndepth-first nature, if a search encounters an unexplored location, all the\nunexplored territory reachable from this location will be reached before\nanother search gets there.)\n\n#### DFS Theorem\n\nDFS of an undirected graph produces only tree and back edges: never forward or\ncross edges.\n\n(Proof in textbook uses _v_._d_ and _v_._f_. Informally, this is because the\nedges being bidirectional, we would have traversed the supposed forward or\ncross edge earlier as a tree or back edge.)\n\n* * *\n\n##  Topological Sort\n\nA **directed acyclic graph** (DAG) is a good model for processes and\nstructures that have partial orders: You may know that _a_ > _c_ and _b_ > _c_\nbut may not have information on how _a_ and _b_ compare to each other.\n\nOne can always make a **total order** out of a partial order. This is what\ntopological sort does. A **topological sort** of a DAG is a linear ordering of\nvertices such that if (_u_, _v_) ∈ _E_ then _u_ appears somewhere before _v_\nin the ordering.\n\n### Outline of Algorithm:\n\n`Topological-Sort(G)` is actually a modification of `DFS(G)` in which each\nvertex _v_ is inserted onto the front of a linked list as soon as finishing\ntime _v_._f_ is known.\n\n### Examples\n\nSome real world examples include\n\n  * Scheduling 100,000 independent tasks on a high performance computing system (research by Dr. Henri Casanova) \nProducing 5,000,000 documents that reference each other such that each\ndocument is produced before the ones that reference it.\n\nHere is the book's example ... a hypothetical professor (not me!) getting\ndressed _(what node did they start the search at? Could it have been done\ndifferently?)_:\n\n![](fig/Fig-22-7-topological-sort.jpg)\n\nWe can make it a bit more complex, with catcher's outfit (click to compare\nyour answer):\n\n![](fig/DAG-topological-sort-example-1.jpg)\n\n_The answer given starts with the batting glove and works left across the\nunvisted nodes. What if we had started the search with socks and worked right\nacross the top nodes? If you put your clothes on differently, how could you\nget the desired result? Hint: add an edge._\n\nAs noted previously, one could start with any vertex, and once the first tree\nwas constructed continue with any artibrary remaining vertex. It is not\nnecessary to start at the vertices at the top of the diagram. _Do you see\nwhy?_\n\n![](fig/pseudocode-topological-sort.jpg)\n\n### Time Analysis\n\nTime analysis is based on simple use of DFS: Θ(_V_ \\+ _E_).\n\n### Correctness\n\n**_Lemma_**: A directed graph _G_ is acyclic iff a DFS of _G_ yields no back edges. \n\nSee text for proof, but it's quite intuitive:\n\n> ⇒ A back edge by definition is returning to where one started, which means\nit completes a cycle.  \n⇐ When exploring a cycle the last edge explored will be a return to the vertex\nby which the cycle was entered, and hence classified a back edge.\n\n**_Theorem:_** If `G` is a DAG then `Topological-Sort(G)` correctly produces a topological sort of `G`. \n\nIt sufficies to show that\n\n> if (_u_, _v_) ∈ _E_ then _v.f_ < _u.f_\n\nbecause then the linked list ordering by _f_ will respect the graph topology).\n\nWhen we explore (_u_, _v_), what are the colors of _u_ and _v_?\n\n  * _u_ is gray, because it is being explored when (_u_, _v_) is found. \n  * Can _v_ be gray too? No, because then _v_ would be an ancestor of _u_, meaning (_u_, _v_) is a back edge, contradicting the DAG property by the lemma above.\n  * Is _v_ white? Then it becomes a descendant of _u_. By the parentheses theorem, _u.d_ < _v.d_ <**_v.f_ < _u.f_**. \n  * Is _v_ black? Then _v_ is finished. Since we are exploring (_u_, _v_) we have not finished _u_. Therefore **_v.f_ < _u.f_**.\n\n* * *\n\n##  Strongly Connected Components\n\nGiven a directed graph _G_ = (_V_, _E_), a **strongly connected component\n(SCC)** of _G_ is a maximal set of vertices _C_ ⊆ _V_ such that for all _u_,\n_v_ ∈ _C_, there is a path both from _u_ to _v_ and from _v_ to _u_.\n\n#### Example:\n\nWhat are the Strongly Connected Components? (Click to see.)\n\n![](fig/SCC-example-0.jpg)\n\n### Algorithm\n\nThe algorithm uses _GT_= (_V_, _ET_), the **transpose** of _G_ = (_V_, _E_).\n_GT_ is _G_ with all the edges reversed.\n\n    \n    \n    Strongly-Connected-Components (G)\n    1.  Call DFS(G) to compute finishing times _u_._f_ for each vertex _u_ ∈ _E_. \n    2.  Compute _GT_\n    3.  Call modified DFS(_GT_) that considers vertices\n        in order of decreasing _u_._f_ from line 1.\n    4.  Output the vertices of each tree in the depth-first forest\n        formed in line 3 as a separate strongly connected component. \n    \n\n### Example 1\n\n#### First Pass of DFS:\n\n![](fig/Fig-22-9-SCC-by-DFS-a.jpg)\n\n#### Second Pass of DFS:\n\n![](fig/Fig-22-9-SCC-by-DFS-b.jpg)\n\n### Why it Works\n\n#### Informal Explanation\n\n_(This is from my own attempt to understand the algorithm. It differs from the\nbook's formal proof.)_\n\n_G_ and _GT_ have the same SCC.     _Proof:_\n\n  * If _u_ and _v_ are in the same SCC in _G_, then there is a path _p_1 from _u_ to _v_ and a path _p_2 from _v_ to _u_.\n  * Reversing the edges, path _p_1 becomes a path from _v_ to _u_ and _p_2 becomes a path from _u_ to _v_.\n\nA DFS from any vertex _v_ in a SCC _C_ will reach _all_ vertices in _C_ (by\ndefinition of SCC).\n\n  * Then why can't we call DFS on unvisited vertices to find the SCCs in the first pass, line 1? \n  * Because this first unconstrained DFS could also get vertices _not_ in _C_, as there may be a path from _v_ in _C_ to _v'_ where there is no path from _v'_ to _v_ (so _v'_ is not in _C_)!\n\nSo how does the second search on _GT_ help avoid inadvertent inclusion of _v'_\nin _C_?\n\n  * _v'_ will have an earlier finishing time than some of the other vertices in _C_, because at least some of those vertices (in particular, _v_ from which _v'_ was reached) are still active (gray) when _v'_ is finished (Parentheses Theorem). \n  * In the second search, the component _C_ to which _v_ belongs is processed before _v'_ and its component, because _v_ has a later finishing time, so the entire component will be explored before other components (in particular, that containing _v'_). \n  * Since _GT_ has the same SCC as _G_, the component found from _v_ in the second search is the same component as in the previous search. \n  * But in this second search, _v'_ will _not_ be reached. Why? Because we are using reversed edges in _GT_. If _v'_ could be reached from _C_ in _GT_, then _v_ would be reachable from _v'_ in _G_, and so _v'_ would be a member of _C_, a contradiction.\n  * So, due to the topological sort, the trees constructed in the second search cannot contain vertices _v'_ that do not belong to the SCC of the other vertices in the tree. This along with the fact that any DFS from a vertex _v_ in a SCC _C_ will find _all_ vertices in _C_ means that the trees constructed by the second DFSs find exactly the vertices in the SCCs.\n\nIn the example above, notice how node _c_ corresponds to _v_ and _g_ to _v'_\nin the argument above. But we need to also say why nodes like _b_ will never\nbe reached from _c_ in the second search. It is because _b_ finished later in\nthe first search, so was processed earlier and already \"consumed\" by the\ncorrect SCC in the second search, before the search from _c_ could reach it.\nThe following fact is useful in understanding why this would be the case.\n\n#### Component Graph\n\n  * Define _GSCC_ to be a graph of the SCCs of G obtained by collapsing all the vertices in each SCC into one vertex for the component but retaining the edges between SCCs.\n  * Then this **component graph _GSCC_ is a Directed Acyclic Graph**. (If there were any cycles, vertices in each component would be reachable from all others, so they would be one component.) \n\nHere is _GSCC_ for the above example:\n\n![](fig/Fig-22-9-SCC-by-DFS-c.jpg)\n\nThe first pass of the SCC algorithm essentially does a topological sort of the\ngraph _GSCC_ (by doing a topological sort of constituent vertices). The second\npass visits the components of _GTSCC_ in topologically sorted order such that\n**_each component is searched before any component that can reach that\ncomponent_**.\n\nThus, for example, the component _abe_ is processed first in the second\nsearch, and since this second search is of _GT_ (reverse the arrows above) one\ncan't get to _cd_ from _abe_. When _cd_ is subsequently searched, one can get\nto _abe_ but it's vertices have _already been visited_ so can't be incorrectly\nincluded in _cd_.\n\n### Example 2\n\nStart at the node indicated by the arrow; conduct a DFS; then click to compare\nyour answer:\n\n![](fig/SCC-example-0-start.jpg)\n\n### Analysis\n\nWe have provided an informal justification of correctness: please see the CLRS\nbook for a formal proof of correctness for the SCC algorithm.\n\n![](fig/pseudocode-SCC.jpg)\n\nThe CLRS text says we can create _GT_ in Θ(_V_ \\+ _E_) time using adjacency\nlists.\n\n  * The easy approach is to simply copy the graph, but given the size of some graphs we work with, it would be much better to reverse the edges in place (and reverse them back when done). \n  * _ Problem for class: How can this be done? (A naive implementation could end up undoing some of its own work, as it confuses already-reversed edges with those to be reversed.)_\n\nThe SCC algorithm also has two calls to DFS, and these are Θ(V + E) each.\n\nAll other work is constant, so the overall time complexity is Θ(V + E).\n\n* * *\n\n##  Related Graph Concepts\n\n![](fig/Fig-22-10-articulations-bridges-biconnected.jpg)\n\nAn **articulation point** or **cut vertex** is a vertex that when removed\ncauses a (strongly) connected component to break into two components.\n\nA **bridge** is an edge that when removed causes a (strongly) connected\ncomponent to break into two components.\n\nA **biconnected component** is a maximal set of edges such that any two edges\nin the set lie on a common simple cycle. This means that there is no bridge\n(at least two edges must be removed to disconnect it). This concept is of\ninterest for network robustness.\n\n* * *\n\n##  Up Next\n\nWe take a brief diversion from graphs to introduce amortized analysis and\nefficient processing of union and find operations on sets, both of which will\nbe used in subsequent work on graphs. Then we return to graphs with concepts\nof minimum spanning trees, shortest paths, and flows in networks.\n\n* * *\n\nDan Suthers Last modified: Sat Mar 8 00:37:35 HST 2014  \nSome images are from the instructor's material for Cormen et al. Introduction\nto Algorithms, Third Edition.  \n\n",
 "path"=>"morea//140.graphs/reading-notes.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Graph definitions",
 "published"=>true,
 "morea_id"=>"reading-screencast-14a",
 "morea_summary"=>"Graph definitions",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=IdStgDmUlXM",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-a.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Graph ADT and representations",
 "published"=>true,
 "morea_id"=>"reading-screencast-14b",
 "morea_summary"=>"Methods in the graph ADT",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=pyDxf58rK_0",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-b.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Breadth first search",
 "published"=>true,
 "morea_id"=>"reading-screencast-14c",
 "morea_summary"=>
  "Breadth first search and applications to finding the shortest path",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=A8SKOFseOyU",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-c.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-d.html</h2>

<pre>Hash
{"title"=>"Depth first search",
 "published"=>true,
 "morea_id"=>"reading-screencast-14d",
 "morea_summary"=>"Depth first search and its asymptotic complexithy",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=emiWSGizJlc",
 "morea_labels"=>["Screencast", "Suthers", "10 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-d.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-d.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-e.html</h2>

<pre>Hash
{"title"=>"Depth first search: example and properties",
 "published"=>true,
 "morea_id"=>"reading-screencast-14e",
 "morea_summary"=>"Depth first search trace.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://www.youtube.com/watch?v=HMPaUoGfsPo",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-e.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-e.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-f.html</h2>

<pre>Hash
{"title"=>"Topological sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-14f",
 "morea_summary"=>"Topological sort and strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://www.youtube.com/watch?v=TZDQHplPrNo",
 "morea_labels"=>["Screencast", "Suthers", "26 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-f.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-f.md"}
</pre>

<h2>/morea/140.graphs/reading-sedgewick-2.html</h2>

<pre>Hash
{"title"=>"Sedgewick and Wayne 4 - Graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-wayne-4",
 "morea_summary"=>
  "Undirected graphs, directed graphs, minimum spanning trees, shortest paths",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://algs4.cs.princeton.edu/40graphs/",
 "morea_labels"=>["Textbook", "12 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-sedgewick-2.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-sedgewick-2.md"}
</pre>

<h2>/morea/140.graphs/reading-sedgewick.html</h2>

<pre>Hash
{"title"=>"Sedgewick 32 - Directed graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-32",
 "morea_summary"=>
  "Depth first search, transitive closure, topological sorting, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "12 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-sedgewick.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-sedgewick.md"}
</pre>

<h2>/morea/150.amortized-analysis/experience.html</h2>

<pre>Hash
{"title"=>"The accounting method",
 "published"=>true,
 "morea_id"=>"experience-amortized-analysis",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with amortized analysis",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/150.amortized-analysis/experience.html",
 "url"=>"/morea/150.amortized-analysis/experience.html",
 "content"=>
  "## Amortized Analysis: Accounting Method\n\n_(This is similar to the problem that was solved using aggregate analysis in\nmy lecture notes, but the numbers are slightly different: in the lecture, the\ntable grows at sizes 2i-1; here at 2i; and here you use the accounting method.\nDon't do it with aggregate analysis: that won't count! I provide guidance on\napproaching the accounting analysis. The problem is relevant to Java hash\ntables.)_\n\nSuppose we perform a sequence of _n_ operations on a data structure in which\nthe _i_th operation costs _i_ if _i_ is an exact power of 2, and 1 otherwise.\n_Use the **accounting method** to determine the amortized cost per operation._\n\n**(a)** Choose the (fixed) cost per operation ĉ that you will charge. \n\n**(b)** Make a table showing \n\n  * operation number (_i_ = 1, 2, 3, ... up to about 20 to see the pattern), \n  * actual cost of the _i_th operation\n  * credit available at the conclusion of the _i_th operation (credit_i_ = credit_i_−1 \\+ ĉ − actual cost). \n\n**(c)** Write an expression for credit at each power of two, 2_i_. Does this credit increase, decrease, or stay the same? \n\n**(d)** Does credit increase, decrease or stay the same _between_ each power of 2? \n\n**(e)** Say why this shows that the amortized cost ĉ that you chose in step (a) is an upper bound on the actual cost. \n\n**(f)** Conclude by giving the big-O upper bound on amortized per-operation cost that (a) and (e) imply. \n\n\n",
 "path"=>"morea//150.amortized-analysis/experience.md"}
</pre>

<h2>/morea/150.amortized-analysis/module.html</h2>

<pre>Hash
{"title"=>"Amortized analysis",
 "published"=>true,
 "morea_id"=>"amortized-analysis",
 "morea_outcomes"=>["outcome-amortized-analysis"],
 "morea_readings"=>
  ["reading-screencast-15a",
   "reading-screencast-15b",
   "reading-cormen-17",
   "reading-notes-15"],
 "morea_experiences"=>["experience-amortized-analysis"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/150.amortized-analysis/logo.gif",
 "morea_sort_order"=>150,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/150.amortized-analysis/module.html",
 "content"=>
  "Aggregate method, accounting method, potential method, dynamic tables, multipop example.\n",
 "path"=>"morea//150.amortized-analysis/module.md"}
</pre>

<h2>/modules/amortized-analysis/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Amortized analysis",
 "url"=>"/modules/amortized-analysis/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/amortized-analysis/index.html"}
</pre>

<h2>/morea/150.amortized-analysis/outcome.html</h2>

<pre>Hash
{"title"=>"Understand amortized analysis",
 "published"=>true,
 "morea_id"=>"outcome-amortized-analysis",
 "morea_type"=>"outcome",
 "morea_sort_order"=>150,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/150.amortized-analysis/outcome.html",
 "content"=>"Understand when and how to apply amortized analysis.",
 "path"=>"morea//150.amortized-analysis/outcome.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 17 - Amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-cormen-17",
 "morea_summary"=>
  "aggregate analysis, the accounting method, the potential method, dynamic tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "30 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-cormen.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-notes-15",
 "morea_summary"=>
  "The general idea, multipop example, aggregate analysis, accounting method, potential method, dynamic tables.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/150.amortized-analysis/reading-notes.html",
 "url"=>"/morea/150.amortized-analysis/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Amortized Analysis: The General Idea \n  2. Multipop Example \n  3. Aggregate Analysis \n  4. Accounting Method\n  5. Potential Method\n  6. Dynamic Table Example (first look)\n  7. Other Examples\n\n##  Amortized Analysis: The General Idea\n\nWe have already used _aggregate_ analysis several times in this course. For\nexample, when analyzing the BFS and DFS procedures, instead of trying to\nfigure out how many times their inner loops\n\n> `for each _v_ ∈ G.Adj[_u_]`\n\nexecute (which depends on the degree of the vertex being processed), we\nrealized that no matter how the edges are distributed, there are at most |_E_|\nedges, so in aggregate across all calls the loops will execute |_E_| times.\n\nBut that analysis concerned itself only with the complexity of a single\noperation. In practice a given data structure will have associated with it\nseveral operations, and they may be applied many times with varying frequency.\n\nSometimes a given operation is designed to pay a larger cost than would\notherwise be necessary to enable other operations to be lower cost.\n\n_Example:_ Red-black tree insertion. We pay a greater cost for balancing so\nthat future searches will remain O(lg _n_).\n\n_Another example:_ Java Hashtable.\n\n  * These grow dynamically when a specified load factor is exceeded.\n  * Copying into a new table is expensive, but copying is infrequent and table growth makes access operations faster.\n\nIt is \"fairer\" to average the cost of the more expensive operations across the\nentire mix of operations, as all operations benefit from this cost.\n\nHere, \"average\" means average cost in the worst case (thankfully, no\nprobability is involved, which greatly simplifies the analysis).\n\nWe will look at three methods. The notes below use Stacks with Multipop to\nillustrate the methods. See the text for binary counter examples.\n\n(We have already seen examples of aggregate analysis throughout the semseter.\nWe will see examples of amortized analysis later in the semester.)\n\n* * *\n\n##  Multipop Example\n\nWe already have the stack operations:\n\n  * `Push(_S, x_)`: O(1) each call, so O(_n_) for any sequence of _n_ operations.\n  * `Pop(_S_)`: O(1) each call, so O(_n_) for any sequence of _n_ operations.\n\nSuppose we add a `Multipop` (this is a generalization of `ClearStack`, which\nempties the stack):\n\n![](fig/Fig-17-1-multipop.jpg) ![](fig/pseudocode-multipop.jpg)\n\nThe example shows a `Multipop(S,4)` followed by another where _k_ ≥ 2.\n\nRunning time of `Multipop`:\n\n  * Linear in number of `Pop` operations (one per loop iteration)\n  * Number of iterations of `while` loop is min(_s_, _k_), where _s_ = number of items on the stack\n  * Therefore, total cost = min(_s_, _k_). \n\nWhat is the worst case of a sequence of _n_ `Push`, `Pop` and `Multipop`\noperations?\n\nUsing our existing methods of analysis we can identify a _loose bound:_:\n\n  * The most expensive operation is `Multipop`, potentially O(_n_).\n  * Therefore, potentially O(_n2_) over _n_ operations.\n\n* * *\n\n##  Aggregate Analysis\n\nWe can tighten this loose bound by aggregating the analysis over all _n_\noperations:\n\n  * Each object can only be popped once per time that it is pushed.\n  * There are at most _n_ `Push`es, so at most n `Pop`s, including those in `Multipop`\n  * Therefore, total cost = O(_n_)\n  * Averaging over the _n_ operations we get O(1) per operation.\n\nThis analysis shows O(1) per operation on average in a sequence of _n_\noperations without using any probabilities of operations.\n\nSee text for example of aggregate analysis of binary counting. An example of\naggregate analysis of dynamic tables is at the end of these notes.\n\nSome of our previous analyses with indicator random variables have been a form\nof aggregate analysis, e.g., our analysis of the expected number of inversions\nin sorting, [Topic 5\nNotes](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-05.html).\n\nAggregate analysis treats all operations equally. The next two methods let us\ngive different operations different costs, so are more flexible.\n\n* * *\n\n##  Accounting Method\n\n#### Metaphor:\n\n  * View the computer as a coin operated appliance that requires one _cyber-dollar_ (CY$) per basic operation.\n  * The banks are wary of making loans these days, so when an operation is to be performed we must have enough cyber-dollars available to pay for it. \n  * We are permitted to charge some operations more than they actually cost so we can save enough to pay for the more expensive operations.\n\n**Amortized cost** = amount we charge each operation.\n\nThis differs from aggregate analysis:\n\n  * In aggregate analysis, all operations have the same cost.\n  * In the accounting method, different operations can have different costs.\n\nWhen an operation is overcharged (amortized cost > actual cost), the\ndifference is associated with _specific objects_ in the data structure as\n_credit_.\n\nWe use this credit to pay for operations whose actual cost > amortized cost.\n\nThe credit must never be negative. Otherwise the amortized cost may not be an\nupper bound on actual cost for some sequences.\n\nLet\n\n  * _ci_ = actual cost of _i_th operation. \n  * _ĉi_ = amortized cost of _i_th operation _(notice the 'hat')_. \n\nRequire ∑_i_=1,_n__ĉi_   ≥   ∑_i_=1,_n__ci_ for all sequences of _n_\noperations. That is, the difference between these sums always ≥ 0: we never\nowe anyone anything.\n\n### Stack Example\n\nWhenever we `Push` an object (at actual cost of 1 cyberdollar), we potentially\nhave to pay CY$1 in the future to `Pop` it, whether directly or in `Multipop`.\n\nTo make sure we have enough money to pay for the `Pops`, we charge `Push`\nCY$2.\n\n  * CY$1 pays for the push\n  * CY$1 is prepayment for the object being popped (metaphorically, this CY$1 is stored \"on\" the object).\n\nSince each object has CY$1 credit, the credit can never go negative.\n\n![](fig/stack-amortized-cost-table.jpg)\n\nThe total amortized cost _ĉ_ = ∑_i_=1,_n__ĉi_ for _any_ sequence of _n_\noperations is an upper bound on the total actual cost _c_ = ∑_i_=1,_n__ci_ for\nthat sequence.\n\nSince _ĉ_ = O(_n_), also _c_ = O(_n_).\n\nNote: we don't actually store cyberdollars in any data structures. This is\njust a metaphor to enable us to compute an amortized upper bound on costs.\n\n* * *\n\n##  Potential Method\n\nInstead of credit associated with objects in the data structure, this method\nuses the metaphor of _potential_ associated with the _entire data structure._\n\n(I like to think of this as potential _energy,_ but the text continues to use\nthe monetary metaphor.)\n\nThis is the most flexible of the amortized analysis methods, as it does not\nrequire maintaining an object-to-credit correspondence.\n\nLet\n\n  * D0 = initial data structure \n  * D_i_ = data structure after _i_th operation\n  * _ci_ = actual cost of _i_th operation. \n  * _ĉi_ = amortized cost of _i_th operation. \n\nPotential Function **Φ**: D_i_ -> ℜ, and we say that Φ(D_i_) is the\n**potential** associated with data structure D_i_.\n\nWe define the amortized cost _ĉi_ to be the actual cost _ci_ plus the change\nin potential due to the _i_th operation:\n\n> _ĉi_ = _ci_ \\+ Φ(D_i_) − Φ(D_i-1_)\n\n  * If at the _i_th operation, Φ(D_i_) − Φ(D_i-1_) is positive, then the amortized cost _c_'_i_ is an _overcharge_ and the potential of the data structure increases.\n  * On the other hand, if Φ(D_i_) &minus: Φ(D_i-1_) is negative then _c_'_i_ is an undercharge, and the decrease of the potential of the data structure pays for the difference (as long as it does not go negative). \n\nThe total amortized cost across _n_ operations is:\n\n> ∑_i_=1,_n__ĉi_   =   ∑_i_=1,_n_(_ci_ \\+ Φ(D_i_) - Φ(D_i-1_))   =\n(∑_i_=1,_n__ci_) + (Φ(D_n_) - Φ(D0))\n\n(The last step is taken because the middle expression involves a telescoping\nsum: every term other than D_n_ and D0 is added once and subtracted once.)\n\nIf we require that Φ(D_i_) ≥ Φ(D0) for all _i_ then the amortized cost will\nalways be an upper bound on the actual cost no matter which _i_th step we are\non.\n\nThis is usually accomplished by defining Φ(D0) = 0 and then showing that\nΦ(D_i_) ≥ 0 for all _i_. (Note that this is a constraint on Φ, not on _ĉ_. _ĉ_\ncan go negative as long as Φ(D_i_) never does.)\n\n### Stack Example\n\nDefine Φ(D_i_) = number of objects in the stack.\n\nThen Φ(D0) = 0 and Φ(D_i_) ≥ 0 for all _i_, since there are never less than 0\nobjects on the stack.\n\nCharge as follows (recalling that _ĉi_ = _ci_ \\+ Φ(D_i_) - Φ(D_i-1_)):\n\n![](fig/stack-potential-table.jpg)\n\nSince we charge 2 for each `Push` and there are O(n) Pushes in the worst case,\nthe amortized cost of a sequence of _n_ operations is O(_n_).\n\nDoes it seem strange that we charge `Pop` and `Multipop` 0 when we know they\ncost something?\n\n  * Remember that this is just a way of counting the total cost over a sequence of operations more precisely.\n  * It is not a claim about the actual cost of a specific procedural call.\n  * Like with the accounting method, we are guaranteeing that we have just enough credit on hand to pay for the operations when they happen.\n  * The methods give a tight bound on amortized cost, but with much easier counting than if we had to reason about probability distributions, etc.\n\n* * *\n\n## Application: Dynamic Tables\n\nThere is often a tradeoff between time and space, for example, with hash\ntables. Bigger tables give faster access but take more space.\n\nDynamic tables, like the Java Hashtable, grow dynamically as needed to keep\nthe load factor reasonable.\n\nReallocation of a table and copying of all elements from the old to the new\ntable is expensive!\n\nBut this cost is amortized over all the table access costs in a manner\nanalogous to the stack example: We arrange matters such that table-filling\noperations build up sufficient credit before we pay the large cost of copying\nthe table; so the latter cost is averaged over many operations.\n\n### A Familiar Definition\n\n**Load factor α** = _num_/_size_, where _num_ = # items stored and _size_ = the allocated size of the table.\n\nFor the boundary condition of _size_ = _num_ = 0, we will define α = 1.\n\nWe never allow α > 1 (no chaining).\n\n### Insertion Algorithm\n\nWe'll assume the following about our tables. (See Exercises 17.4-1 and 17.4-3\nconcerning different assumptions.):\n\nWhen the table becomes full, we double its size and reinsert all existing\nitems. This guarantees that α ≥ 1/2, so we are not wasting a lot of space.\n\n    \n    \n    Table-Insert (T,x)\n    1   if T.size == 0\n    2       allocate T.table with 1 slot \n    3       T.size = 1\n    4   if T.num == T.size\n    5       allocate newTable with 2*T.size slots\n    6       insert all items in T.table into newTable\n    7       free T.table\n    8       T.table = newTable \n    9       T.size = 2*T.size \n    10  insert x into T.table \n    11  T.num = T.num + 1\n    \n\nEach _elementary insertion_ has unit actual cost. Initially _T.num_ = _T.size_= 0.\n\n### Aggregate Analysis of Dynamic Table Expansion\n\nCharge 1 per elementary insertion. Count only elementary insertions, since all\nother costs are constant per call.\n\n**_ci_** = actual cost of _i_th operation.\n\n![](fig/pseudocode-table-insert.jpg)\n\n  * If the table is not full, _ci_ = 1 (for lines 1, 4, 10, 11). \n  * If full, there are _i_ \\- 1 items in the table at the start of the _i_th operation. Must copy all of them (line 6), and then insert the _i_th item. Therefore _ci_ = _i_ \\- 1 + 1 = _i_. \n\nA sloppy analysis: In a sequence of _n_ operations where any operation can be\nO(_n_), the sequence of _n_ operations is O(_n_2).\n\nThis is \"correct\", but inprecise: we rarely expand the table! A more precise\naccount of _ci_:\n\n![](fig/c_i-definition.jpg)\n\nThen we can sum the total cost of all _ci_ for a sequence of _n_ operations:\n\n![](fig/analysis-table-expansion.jpg)\n![](fig/formula-A-5.jpg)\n\n_Explain:_ Why the _n_? What is the summation counting? Why does the summation\nstart at _j_ = 0? Why does it end at _j_ = lg _n_?\n\nTherefore, the amortized cost per operation = 3: we are only paying a small\nconstant price for the expanding table.\n\nThe text also gives accounting and potential analyses of table expansion.\n\nThis analysis assumed that the table never shrinks. See section 17.4 (and your\nhomework) for an analysis using the potential method that covers shrinking\ntables.\n\n* * *\n\n## Other Examples\n\nHere are some other algorithms for which amortized analysis is useful:\n\n### Red-Black Trees\n\nAn amortized analysis of Red-Black Tree restructuring (Problem 17-4 of CLRS)\nimproves upon our analysis earlier in the semester:\n\n  * Any sequence of _m_ `RB-Insert` and `RB-Delete` operations performs O(_m_) structural modifications (rotations), \n  * This each operation does **O(1) structural modifications on average**, regardless of the size of the tree!\n  * An operation still may need to do O(lg _n_) recolorings, but these are very simple operations.\n\n### Self-Organizing Lists\n\n  * Self-organizing lists use a **_move-to-front heuristic_**: Immediately after searching for an element, it is moved to the front of the list.\n  * This makes frequently accessed items more readily available near the front of the list.\n  * An amortized analysis (Problem 17-5) shows that the heuristic is no more than 4 times worse than optimal.\n\n### Splay Trees\n\n  * Splay trees are ordinary binary search trees (no colors, no height labels, etc.)\n  * After every access (every insertion, deletion, or search), the element operated on (or its parent in the case of deletion) is moved towards the top of the tree.\n  * This movement uses three **_splaying_** operations called \"zig\", \"zig-zig\" and \"zig-zag\".\n  * Although in the worst case a splay tree can degenerate into an O(_n_) linked list, amortized analysis shows that the expected case is O(lg _n_)\n  * Randomization can be used to make the worst case very unlikely.\n  * If a single element is accessed at least _m_/4 times where _m_ is the number of operations, then the amortized running time of each of these accesses is O(1).\n  * Thus, splay-trees self-organize to provide fast access to frequently accessed items.\n  * This makes them good for locality of reference in memory, but multithreaded access must be implemented carefully.\n\n### To Be Continued\n\nAmortized analysis will be used in analyses of\n\n  * Graph search (Topic 14, Ch. 22) \n  * Disjoint set operations (Topic 16, Ch. 21) \n  * Dijkstra's Algorithm for Shortest Paths (Topic 18, Ch. 24) \n\n* * *\n\nDan Suthers Last modified: Sun Mar 16 02:03:09 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//150.amortized-analysis/reading-notes.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-15a",
 "morea_summary"=>"Aggregate, accounting, and potential methods",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=rlIka0W814U",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-screencast-a.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Amortized analysis example",
 "published"=>true,
 "morea_id"=>"reading-screencast-15b",
 "morea_summary"=>"aggregate analysis of dynamic tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=iy-WhloN6vA",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-screencast-b.md"}
</pre>

<h2>/morea/160.disjoint-sets/experience.html</h2>

<pre>Hash
{"title"=>"Disjoint sets",
 "published"=>true,
 "morea_id"=>"experience-disjoint-sets",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with disjoint sets",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/160.disjoint-sets/experience.html",
 "url"=>"/morea/160.disjoint-sets/experience.html",
 "content"=>
  "## Disjoint sets\n\n![](fig/pseudocode-disjoint-set-forest.jpg)\n\n\n### 1. State of the Union\n\n**Show the result of Union(e,j) on this forest.**\n\n![](fig/disjoint-set-forest-1.jpg)\n\n\n### 2. Iterative Find-Set\n\nFind-Set as written in the text is not tail recursive, so a compiler won't be\nable to automatically generate efficient iterative executable code. **Write an\niterative version of Find-Set.**\n\n\n### 3. Connected-Components \n  \n![](fig/pseudocode-connected-components.jpg) \n\nDuring the execution\nof Connected-Components on an undirected graph _G_=(_V_,_E_) with _k_\nconnected components,\n\n**(a) How many times is Find-Set called directly by Connected-Components? **  \n  \n**(b) How many times is Union called directly by Connected-Components? **\n\nExpress your answer in terms of _V_, _E_ and _k_ (but NOT asymptotic\nnotation). (Hint: solve for _k_=1 first and then think about what happens when\n_k_=2.)\n\n\n",
 "path"=>"morea//160.disjoint-sets/experience.md"}
</pre>

<h2>/morea/160.disjoint-sets/module.html</h2>

<pre>Hash
{"title"=>"Disjoint sets",
 "published"=>true,
 "morea_id"=>"disjoint-sets",
 "morea_outcomes"=>["outcome-disjoint-sets"],
 "morea_readings"=>
  ["reading-screencast-16a", "reading-cormen-21", "reading-notes-16"],
 "morea_experiences"=>["experience-disjoint-sets"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/160.disjoint-sets/logo.jpg",
 "morea_sort_order"=>160,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/160.disjoint-sets/module.html",
 "content"=>
  "Union-find, linked list representation, forest representation, finding connected components.\n",
 "path"=>"morea//160.disjoint-sets/module.md"}
</pre>

<h2>/modules/disjoint-sets/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Disjoint sets",
 "url"=>"/modules/disjoint-sets/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/disjoint-sets/index.html"}
</pre>

<h2>/morea/160.disjoint-sets/outcome.html</h2>

<pre>Hash
{"title"=>"Understand disjoint sets",
 "published"=>true,
 "morea_id"=>"outcome-disjoint-sets",
 "morea_type"=>"outcome",
 "morea_sort_order"=>160,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/160.disjoint-sets/outcome.html",
 "content"=>"Understand the principles and applications of disjoint sets. ",
 "path"=>"morea//160.disjoint-sets/outcome.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 21 - Data structures for disjoint sets",
 "published"=>true,
 "morea_id"=>"reading-cormen-21",
 "morea_summary"=>
  "disjoint set operations, linked-list representation of disjoint sets, disjoint-set forests, analysis of union by rank with path compression",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "25 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/160.disjoint-sets/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//160.disjoint-sets/reading-cormen.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on disjoint sets",
 "published"=>true,
 "morea_id"=>"reading-notes-16",
 "morea_summary"=>
  "Disjoint sets, finding connected components, linked list representations, and forest representations",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/160.disjoint-sets/reading-notes.html",
 "url"=>"/morea/160.disjoint-sets/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Disjoint Dynamic Sets \n  2. Finding Connected Components with Disjoint Sets \n  3. Linked List Representations of Disjoint Sets \n  4. Forest Representations of Disjoint Sets\n\n\n##  Dynamic Disjoint Sets (Union Find)\n\nTwo sets _A_ and _B_ are **disjoint** if they have no element in common.\n\nSometimes we need to group n distinct elements into a collection \u008A of disjoint\nsets \u008A = {_S_1, ..., _Sk_} that may change over time.\n\n  * \u008A is a set of sets: { { _x_, ... }, ..., { _y_, ... } } \n  * Each set _Si_ ∈ \u008A is identified by a **representative**, which is some member of the set (e.g., _x_ and _y_).\n  * It does not matter which member is the representative, as long as the representative remains the same while the set is not modified.\n\nDisjoint set data structures are also known as **Union-Find** data structures,\nafter the two operations in addition to creation. (Applications often involve\na mixture of searching for set membership and merging sets.)\n\n### Operations\n\n> **Make-Set(_x_)**: make a new set _Si_ = {_x_} _(_x_ will be its\nrepresentative)_ and add _Si_ to \u008A.\n\n> **Union(_x_, _y_)**: if _x_ ∈ _Sx_ and _y_ ∈ _Sy_, then \u008A <- \u008A − _Sx_ − _Sy_\n∪ {_Sx_ ∪ _Sy_} _(that is, combine the two sets _Sx_ and _Sy_)_.\n\n>\n\n>   * The representative of _Sx_ ∪ _Sy_ is any member of that new set\n(implementations often use the representative of one of _Sx_ or _Sy_.)\n\n>   * Destroys _Sx_ and _Sy_, since the sets must be disjoint (they cannot co-\nexist with _Sx_ ∪ _Sy_).\n\n> **Find-Set(_x_)**: return the representative of the set containing _x_.\n\n### Analysis\n\nWe analyze in terms of:\n\n  * _n_ = number of Make-Set operations, i.e., the number of sets initially involved\n  * _m_ = total number of operations\n\nSome facts we can rely on:\n\n  * _m_ ≥ _n_\n  * Can have at most _n_−1 Union operations, since after _n_−1 Unions, only 1 set remains.\n  * It can be helpful for analysis to assume that the first _n_ operations are Make-Set operations (put all the elements we will be working with in singleton sets to start with).\n\n* * *\n\n##  Applications of Disjoint Sets\n\nUnion-Find on disjoint sets is used to find structure in other data\nstructures, such as a graph. We initially assume that all the elements are\ndistinct by putting them in singleton sets, and then we merge sets as we\ndiscover the structure by which the elements are related.\n\n### Finding Connected Components\n\nRecall from [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)\nthat for a graph _G_ = (_V_, _E_), vertices _u_ and _v_ are in the same\n**connected component** if and only if there is a path between them.\n\nHere are the algorithms for computing connected components and then for\ntesting whether two items are in the same component:\n\n![](fig/pseudocode-connected-components.jpg) ![](fig/pseudocode-same-components.jpg)\n\n_Would that work with a directed graph?_\n\n#### Example\n\n![](fig/Fig-21-1-finding-connected-components.jpg)\n![](fig/ti-chats.jpg)\n\nAlthough it is easy to see the connected components above, the utility of the\nalgorithm becomes more obvious when we deal with large graphs (such as\npictured)!\n\n#### Alternatives\n\nIn a _static_ undirected graph, it is faster to run Depth-First Search\n(exercise 22.3-12), or for static directed graphs the strongly connected\ncomponents algorithm of [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)\n(section 22.5), which consists of two DFS. But in some applications edges may\nbe added to the graph. In this case, union-find with disjoint sets is faster\nthan re-running the DFS.\n\n### Minimum Spanning Trees\n\nNext week we cover algorithms to find _minimum spanning trees_ of graphs.\nKruskal's algorithm will use Union-Find operations.\n\n* * *\n\n##  Linked List Representations of Disjoint Sets\n\nOne might think that lists are the simplest approach, but there is a better\napproach that is not any more complex: this section is mainly for comparision\npurposes.\n\n### Representation\n\nEach set is represented using an unordered singly linked list. The list object\nhas attributes:\n\n  * **head**: pointing to the first element in the list, the set's representative.\n  * **tail**: pointing to the last element in the list.\n\n![](fig/Fig-21-2-linked-list-S1.jpg)\n\nEach object in the list has attributes for:\n\n  * **next**\n  * The **set member** (e.g., the vertex in the graph being analyzed)\n  * A pointer to the list object that represents the **set**\n\n### Operations\n\nFirst try:\n\n  * Make-Set(_x_): create a singleton list containing _x_\n  * Find-Set(_x_): follow the pointer back to the list object, and then follow the `head` pointer to the representative\n  * Union(_x_, _y_): append _y_'s lists onto the end of _x_'s list. \n    * Use _x_'s tail pointer to find the end.\n    * Need to update the pointer back to the set object for every node on _y_'s list.\n\nFor example, let's take the union of _S_1 and _S_2, replacing _S_1:\n\n![](fig/Fig-21-2-linked-list-representation.jpg)\n\nThis can be slow for large data sets. For example, suppose we start with _n_\nsingletons and always happend to append the larger list onto the smaller one\nin a sequence of merges:\n\n![](fig/Fig-21-3-worst-case-alt.jpg)\n\nIf there are _n_ Make-Sets and _n_ Unions, the amortized time per operation is\nO(_n_)!\n\nA **weighted-union heuristic** speeds things up: always append the smaller\nlist to the larger list (so we update fewer set object pointers). Althought a\nsingle union can still take Ω(_n_) time (e.g., when both sets have _n_/2\nmembers), a sequence of _m_ operations on _n_ elements takes O(_m_ \\+ _n_ lg\n_n_) time.\n\n**_Sketch of proof:_** Each Make-Set and Find-Set still takes O(1). Consider how many times each object's set representative pointer must be updated during a sequence of _n_ Union operations. It must be in the smaller set each time, and after each Union the size of this smaller set is at least double the size. So: \n\n![](fig/representative-update-bound.jpg)\n\nEach representative set for a given element is updated ≤ lg _n_ times, and\nthere are _n_ elements plus _m_ operations. However, we can do better!\n\n* * *\n\n##  Forest Representations of Disjoint Sets\n\nThe following is a classic representation of Union-Find, due to Tarjan (1975).\nThe set of sets is represented by a forest of trees. The code is as simple as\nthe analysis of runtime is complex.\n\n### Representation\n\n![](fig/disjoint-set-forest.jpg)\n\n  * Each tree represents a set.\n  * The root of the tree is the set representative.\n  * Each node points only to its parent (no child pointers needed).\n  * The root points to itself as parent. \n\n### Operations\n\n  * Make-Set(_x_): create a single node tree with _x_ at the root\n  * Find-Set(_x_): follow parent pointers back to the root\n  * Union(_x_, _y_): make one root a child of the other. (This in itself could degenerate to a linear list-like tree, but we will fix this below.)\n\n![](fig/disjoint-set-union-alt.jpg)\n\n#### Heuristics\n\nIn order to avoid degeneration to linear trees, and achieve amazing amortized\nperformance, these two heuristics are applied:\n\n**Union by Rank**: make the root of the \"smaller\" tree a child of the root of the \"larger\" tree. But rather than size we use **rank**, an upper bound on the height of each node (stored in the node).\n\n  * Rank of singleton sets is 0.\n  * When taking the Union of two trees of equal rank, choose one arbitrarily to be the parent and increment its rank by one. _(Why is it incremented?)_\n  * When taking the Union of two trees of unequal rank, the tree with lower rank becomes the child, and ranks are unchanged. _(Why does this make sense?)_\n\n**Path Compression**: When running Find-Set(_x_), make all nodes on the path from _x_ to the root direct children of the root. For example, Find-Set(a):\n\n![](fig/Fig-21-5-path-compression-alt.jpg)\n\n### Algorithms\n\nThe algorithms are very simple! (But their analysis is complex!) We assume\nthat nodes _x_ and _y_ are tree nodes with the client's element data already\ninitialized.\n\n![](fig/pseudocode-disjoint-set-forest.jpg)\n\nLink implements the union by rank heuristic.\n\nFind-Set implements the path compression heuristic. It makes a recursive pass\nup the tree to find the path to the root, and as recursion unwinds it updates\neach node on the path to point directly to the root. (This means it is not\ntail recursive, but as the analysis shows, the paths are very unlikely to be\nlong.)\n\n### Time Complexity\n\nThe analysis can be found in section 21.4. It is very involved, and I only\nexpect you to know what is discussed below. It is based on a very fast growing\nfunction:\n\n![](fig/A_k-j.jpg)\n\n_Ak_(_j_) is a variation of **Ackerman's Function**, which is what you will\nfind in most classic texts on the subject. The function grows so fast that\n_A_4(1) = 16512 is _much_ larger than the number of atoms in the observable\nuniverse (1080)!\n\nThe result uses **α(_n_),** a single parameter inverse of _Ak_(_j_) defined as\nthe lowest _k_ for which _Ak_(1) is at least _n_:\n\n![](fig/growth-inverse-ackermann.jpg)\n\n> α(_n_) = min{_k_ : _Ak_(1) ≥ _n_}\n\nα(_n_) grows _very_ slowly, as shown in the table. We are highly unlikely to\never encounter α(_n_) > 4 (we would need input size much greater than the\nnumber of atoms in the universe). Although its growth is strictly larger than\na constant, for all practical purposes we can treat α(_n_) as a constant.\n\nThe analysis of section 21.4 shows that the running time is **O(_m_ α(_n_))**\nfor a sequence of _m_ `Make-Set`, `Find-Set` and `Union` operations. Thus for\nall practical purposes, the cost of a sequence of _m_ such operations is\nO(_m_), or O(1) amortized cost per operation!\n\n* * *\n\n## Wrapup\n\nWe now return to Graphs. We'll see Union-Find used when we cover minimum\nspanning trees.\n\n* * *\n\nDan Suthers Last modified: Thu Apr 17 15:35:22 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//160.disjoint-sets/reading-notes.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Sets and union-find",
 "published"=>true,
 "morea_id"=>"reading-screencast-16a",
 "morea_summary"=>"Introduction to disjoint sets",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=RUBU6eHAAaU",
 "morea_labels"=>["Screencast", "Suthers", "27 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/160.disjoint-sets/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//160.disjoint-sets/reading-screencast-a.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/experience-2.html</h2>

<pre>Hash
{"title"=>"More minimum spanning trees",
 "published"=>true,
 "morea_id"=>"experience-minimum-spanning-tree-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Minimum spanning trees and Kruskal's algorithm",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/170.minimum-spanning-tree/experience-2.html",
 "url"=>"/morea/170.minimum-spanning-tree/experience-2.html",
 "content"=>
  "## Minimum Spanning Graph\n\n#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n#### 2\\. (4 pts) Positive and Negative Weights: Minimum Spanning Graph?\n\nGiven a graph _G_ = (_V_, _E_), must any subset of edges _T_ ⊆ _E_ that\nconnects all vertices in V and have minimal total weight be a tree, or can it\nbe some other subgraph? Answer this question separately for two cases:\n\n> **a.** All of the edges have positive weight.  \n  \n**b.** Some edges may have negative weights. \n\nIf you think \"yes it must be a tree\" then argue why this is the case (hint:\nsuppose it's not a tree: find a contradiction to connectedness or minimality).\nIf you think \"no, it need not be a tree\" then give an example where a\nconnected graph that is not a tree has lower weight.\n\n#### 3\\. (1 pt) Kruskal Alternative Spanning Tree\n\nThe graph of Figure 23.4 has several edges of equal weight. Sometimes both are\nused; sometimes one can't be used because it forms a cycle; and sometimes the\nchoice is arbitrary: either edge could have been used. In this third case, the\nchoice of which one was used depends on the order in which edges are sorted in\nline 4 (nondecreasing order by weight).\n\n![](fig/Fig-23-4-Kruskal-Example-n.jpg)\n\nThe actual ordering used in the book's example is:\n\n> ` (g, h), (c, i), (f, g), (a, b), (c, f), (g, i), (c, d),  \n(h, i), (a, h), (b, c), (d, e), (e, f), (b, h), (d, f) `\n\nGive an ordering that would result in a different minimum spanning tree than\nthe one shown in figure 23.4 by rewriting the above list swapping just one\npair of edges.\n\n#### 4\\. (5 pts) Building Low Cost Bridges\n\nSuppose we are in Canada's Thousand Islands National Park, and in one\nparticular lake there are eight small islands that park officials want to\nconnect with floating bridges so that people can experience going between\nislands without a canoe. The cost of constructing a bridge is proportional to\nits length, and the table below shows the distances between pairs of islands\nin meters (Canada is trying to free itself of the archaic British system of\nmeasurement based on the sizes of a dead king's body parts.)\n\nWhich bridges should they build to connect the eight islands at minimal cost?\nSay in a sentence or two how you found the solution, and give the solution as\na list of pairs of islands, for example, (A, B) ....\n\n<table width=\"50%\" border=\"1\">\n  <tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">A</th>\n    <th scope=\"col\">B</th>\n    <th scope=\"col\">C</th>\n    <th scope=\"col\">D</th>\n    <th scope=\"col\">E</th>\n    <th scope=\"col\">F</th>\n    <th scope=\"col\">G</th>\n    <th scope=\"col\">H</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">A</th>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">240</div></td>\n    <td><div align=\"center\">210</div></td>\n    <td><div align=\"center\">340</div></td>\n    <td><div align=\"center\">280</div></td>\n    <td><div align=\"center\">200</div></td>\n    <td><div align=\"center\">345</div></td>\n    <td><div align=\"center\">120</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">B</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">265</div></td>\n    <td><div align=\"center\">175</div></td>\n    <td><div align=\"center\">215</div></td>\n    <td><div align=\"center\">180</div></td>\n    <td><div align=\"center\">185</div></td>\n    <td><div align=\"center\">155</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">C</th>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">260</div></td>\n    <td><div align=\"center\">115</div></td>\n    <td><div align=\"center\">350</div></td>\n    <td><div align=\"center\">435</div></td>\n    <td><div align=\"center\">195</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">D</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">160</div></td>\n    <td><div align=\"center\">330</div></td>\n    <td><div align=\"center\">295</div></td>\n    <td><div align=\"center\">230</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">E</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">360</div></td>\n    <td><div align=\"center\">400</div></td>\n    <td><div align=\"center\">170</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">F</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">175</div></td>\n    <td><div align=\"center\">205</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">G</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">305</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">H</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n  </tr>\n</table>",
 "path"=>"morea//170.minimum-spanning-tree/experience-2.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/experience.html</h2>

<pre>Hash
{"title"=>"Minimum spanning trees",
 "published"=>true,
 "morea_id"=>"experience-minimum-spanning-tree",
 "morea_type"=>"experience",
 "morea_summary"=>"Basics of minimum spanning trees",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/170.minimum-spanning-tree/experience.html",
 "url"=>"/morea/170.minimum-spanning-tree/experience.html",
 "content"=>
  "## Basics of minimum spanning trees\n\nBelow is pseudocode for three different algorithms. Each one takes a connected\ngraph _G_ = (_V_,_E_) and a weight function _w_ as input and returns a set of\nedges _T_.\n\n#### Is it an MST algorithm?\n\nFor each algorithm, either prove that the set of edges _T_ produced by the\nalgorithm is always a minimum spanning tree, or find a counter-example where\n_T_ is not a minimum spanning tree\n\n**1\\. `Maybe-MST-A`**\n    \n    \n    Maybe-MST-A (G, w)\n    1  sort the edges into nonincreasing order of edge weights w\n    2  T = E\n    3  for each edge e, taken in sorted order\n    4      if T − {e} is a connected graph\n    5          T = T − {e}\n    6  return T\n    \n\n**2\\. `Maybe-MST-B`**\n    \n    \n    Maybe-MST-B (G, w)\n    1  T = {} // empty set\n    2  for each edge e ∈ E, taken in arbitrary order\n    3      if T ∪ {e} has no cycles\n    4          T = T ∪ {e}\n    5  return T\n    \n\n**3\\. `Maybe-MST-C`**\n    \n    \n    Maybe-MST-C (G, w)\n    1  T = {}\n    2  for each edge e ∈ E, taken in arbitrary order\n    3      T = T ∪ {e}\n    4      if T has a cycle _c_\n    5          let e' be a maximum weight edge on c\n    6          T = T − {e'}\n    7  return T\n    \n\n#### Efficient Implementation\n\n**4.** Once you are done, pick on of the above algorithms that you think computes a MST. Then describe an efficient implementation of that algorithm. Each algorithm above uses utility methods such as sorting, testing connectivity, detecting cycles, and finding the maximum weight edge on a cycle. Your response should focus on efficient implementations of these utility methods. \n\n\n",
 "path"=>"morea//170.minimum-spanning-tree/experience.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/module.html</h2>

<pre>Hash
{"title"=>"Minimum spanning tree",
 "published"=>true,
 "morea_id"=>"minimum-spanning-tree",
 "morea_outcomes"=>["outcome-minimum-spanning-tree"],
 "morea_readings"=>
  ["reading-screencast-17a",
   "reading-screencast-17b",
   "reading-screencast-17c",
   "reading-cormen-23",
   "reading-sedgewick-31",
   "reading-notes-17"],
 "morea_experiences"=>
  ["experience-minimum-spanning-tree", "experience-minimum-spanning-tree-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/170.minimum-spanning-tree/logo.png",
 "morea_sort_order"=>170,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/170.minimum-spanning-tree/module.html",
 "content"=>
  "Generic algorithm, safe edge algorithm, Kruskal's algorithm, Prim's algorithm, shortest path, dense paths.",
 "path"=>"morea//170.minimum-spanning-tree/module.md"}
</pre>

<h2>/modules/minimum-spanning-tree/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Minimum spanning tree",
 "url"=>"/modules/minimum-spanning-tree/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/minimum-spanning-tree/index.html"}
</pre>

<h2>/morea/170.minimum-spanning-tree/outcome.html</h2>

<pre>Hash
{"title"=>"Understand minimum spanning tree",
 "published"=>true,
 "morea_id"=>"outcome-minimum-spanning-tree",
 "morea_type"=>"outcome",
 "morea_sort_order"=>170,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/170.minimum-spanning-tree/outcome.html",
 "content"=>"Understand when, why, and how to use minimum spanning trees. ",
 "path"=>"morea//170.minimum-spanning-tree/outcome.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 23 - Minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-23",
 "morea_summary"=>
  "Growing a minimum spanning tree, the algorithms of Kruskal and Prim.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "19 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-cormen.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-notes-17",
 "morea_summary"=>
  "Minimum spanning trees, the generic algorithm, Kruskal's and Prim's algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/170.minimum-spanning-tree/reading-notes.html",
 "url"=>"/morea/170.minimum-spanning-tree/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Minimum Spanning Trees \n  2. Generic Algorithm and Safe Edge Theorem \n  3. Kruskal's Algorithm \n  4. Prim's Algorithm \n\n##  Minimum Spanning Trees\n\n### Spanning Trees\n\nA **spanning tree** _T_ for a connected graph _G_ is a tree that includes all\nthe vertices of _G_: it _spans_ the graph.\n\nWithout calling them such, we have already encountered two kinds of spanning\ntrees in the introduction to graphs ([Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)):\nthose generated by breadth-first search and depth-first search. We saw that _\nbreadth-first trees _ are one way of finding shortest paths in a graph, and _\ndepth-first forests _ (a collection of spanning trees, one for each connected\ncomponent) are good for uncovering the structure of a graph such as\ntopological sort and connectivity. These were defined on unweighted graphs.\n\n### Minimum Spanning Trees\n\nMany application areas (e.g., in communications, electronics, and\ntransportation) require finding the lowest cost way to connect a set of\nobjects or locations. For example, the cost may be measured in terms of\ncurrency or distance. We can model such situations with _**weighted graphs**_,\nintroduced in [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html) as\ngraphs where a real-valued number is associated with each edge. Then we want\nto find a spanning tree of minimum cost.\n\nMore formally, we can pose this as a problem on a graph representation _G_ =\n(_V_, _E_):\n\n  * The objects or locations are vertices _V_ and the available connections are edges _E_.\n  * A weight function _w_(_u_,_v_) gives the weight on each edge (_u_,_v_) ∈ _E_.\n  * We seek _T_ ⊆ _E_ such that \n    * _T_ connects all the vertices _V_ of _G_. \n    * The sum of weights _w_(_T_) = Σ(_u_,_v_)∈_T_ _w_(_u_,_v_) is minimized. \n\nA few facts can be noted:\n\n  * _G_ must be connected (consist of a single connected component) in order for _T_ to be possible. \n  * However, if _G_ is not connected we can generalize the problem to one of finding _T_1 ... _Tc_ for each of _c_ connected components of _G_.\n  * A subgraph of _G_ that connects its vertices _V_ at minimal cost will always be a tree. _Why?_\n\nTherefore we call this the **minimum spanning tree (MST)** problem (and the\ngeneralized version the minimum spanning forest problem).\n\nHere is an example of a minimum spanning tree (the shaded edges represent\n_T_):\n\n![](fig/example-MST-1.jpg)\n\n_Are minimum spanning trees unique?_\n\nLook at edges (_e_,_f_) and (_c_,_e_).\n\n* * *\n\n##  Generic Algorithm and Safe Edge Theorem\n\nWe specify a generic greedy algorithm for solving the MST problem. The\nalgorithm will be \"greedy\" in terms of always choosing a lowest cost edge.\nThis algorithm is instantiated into two versions, Kruskal's and Prim's\nalgorithms, which differ in how they define from what set of edges the lowest\ncost edge is chosen.\n\nLet's start by noting some properties that MSTs of _G_ = (_V_, _E_) must have\n\n  * A MST for _G_ has |_V_| − 1 edges. (See properties of trees, [Topic 8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html).)\n  * Any tree (and hence any MST) has no cycles. It has only one path between any two vertices.\n  * There might be more than one MST for _G_.\n\n### Building a Solution\n\n  * We will build a set of edges _A_. \n  * Initially _A_ has no edges.\n  * As we add edges to _A_, we maintain a loop invariant: _A_ is a subset of _some_ MST for _G_.\n\nDefine an edge (_u_,_v_) to be **safe** for _A_ iff _A_ ∪ {(_u_,_v_)} is also\na subset of some MST for _G_.\n\n(BTW, \"iff\" is not a spelling error: it is shorthand for \"if and only if\"\ncommonly used in proofs.\n\nIf we only add safe edges to _A_, once |_V_| − 1 edges have been added we have\na MST for _G_. This motivates the ...\n\n### Generic MST Algorithm\n\n![](fig/pseudocode-generic-MST.jpg)\n\n_Loop Invariant:_ _A_ is a subset of some MST for _G_\n\n  * _Initialization:_ The initially empty set trivially satisfies the loop invariant.\n  * _Maintenance:_ Since we add only safe edges, _A_ remains a subset of some MST.\n  * _Termination:_ We stop when _A_ is a spanning tree (|_A_| = |_V_| − 1), and it is a subset of itself.\n\nOK, great, but how do we find safe edges?\n\n### Finding Safe Edges\n\nEach time we add an edge we are connecting two sets of vertices that were not\npreviously connected by _A_. (Otherwise we would be forming a cycle.) A greedy\nalgorithm might try to keep the cost down by choosing the lowest cost edge\nthat connects previously unconnected vertices. (Perhaps we should call it a\n\"stingy\" algorithm!)\n\nBut is this greedy strategy \"safe\"? How do we know that after adding this edge\nwe still have a subset of an MST?\n\nFirst some definitions:\n\n  * A **cut** (_S_, _V_ − _S_) is a partition of vertices into disjoint sets _S_ and _V_ − _S_. \n  * Edge (_u_,_v_) ∈ _E_ **crosses** cut (_S_, _V_ − _S_) if one endpoint is in _S_ and the other is in _V_ − _S_. \n  * A cut **respects** _A_ iff no edge in _A_ crosses the cut.\n  * An edge is a **light edge** crossing a cut iff its weight is minimum over all edges crossing the cut. (There may be more than one light edge for a given cut.) \n\nThe following illustrates a cut and will be used in the proof below. There are\ntwo sets of vertices _S_ and _V_ − _S_. Four edges cross the cut (_S_, _V_ \\-\n_S_). Whether or not this respects _A_ depends on what is in _A_.\n\n![](fig/illustration-safe-edge-theorem.jpg)\n\n_Suppose A is the shaded edges. Does this cut respect A?_\n\n#### Safe Edge Theorem\n\nLet _G_ = (_V_, _E_) be a graph, _A_ be a subset of some MST for _G_, (_S_,\n_V_ − _S_) be a cut that respects _A_, and (_u_,_v_) be a light edge crossing\n(_S_, _V_ − _S_). Then (_u_,_v_) is safe for _A_.\n\n(_A light edge that crosses a cut that respects _A_ is safe for _A_._)\n\n_**Proof:**_ Let _T_ be a MST that includes _A_. Consider two cases:\n\nCase 1: _T_ contains (_u_,_v_). Then the theorem is proven, since _A_ ∪\n{(_u_,_v_)} ⊆ _T_ is a subset of some MST for _G_.\n\nCase 2: _T_ does not contain (_u_,_v_). We will show that we can construct a\ntree _T'_ that is a MST for _G_ and that contains _A_ ∪ {(_u_,_v_)}.\n\n![](fig/illustration-safe-edge-theorem.jpg)\n\nSince _T_ is a tree it contains a unique path _p_ between _u_ and _v_. Path\n_p_ must cross the cut (_S_, _V_ − _S_) at least once (otherwise _T_ would be\ndisconnected). Let (_x_,_y_) be an edge of _p_ that crosses the cut.\n\n(Except for the dashed edge (_u_,_v_), all the edges shown in the figure are\nin _T_. _A_ is not shown in the figure, but it cannot contain any edges that\ncross the cut, since the cut respects _A_. Shaded edges are the path _p_.)\n\nSince the cut respects _A_, edge (_x_,_y_) is not in _A_.\n\nTo form _T'_ from _T_: Remove (_x_,_y_). This breaks _T_ into two components.\nAdd (_u_,_v_). This reconnects the tree. So _T'_ = T - {(_x_,_y_)} ∪ (_u_,_v_)\nis a spanning tree.\n\nTo show that _T'_ is a minimal spanning tree: _w_(_T'_) = _w_(_T_) -\n_w_(_x_,_y_) + _w_(_u_,_v_) ≤ _w_(_T_) since (_u_,_v_) is light.\n\nWe still need to show that (_u_,_v_) is safe for _A_. Since _A_ ⊆ _T_ and\n(_x_,_y_) ∉ _A_ then A ⊆ _T'_. Therefore _A_ ∪ {(_u_,_v_)} ⊆ _T'_, a MST. ♦\n\n#### Further Observations\n\n_A_ is a forest containing connected components. Initially each component is a\nsingle vertex. Any safe edge merges two of these components into one. Each\ncomponent so constructed is a tree. Since an MST has exactly |_V_| − 1 edges,\nthe loop iterates |_V_| − 1 times before we are down to one component.\n\n#### Corollary\n\nIf _C_ = (_VC_, _EC_) is a connected component in the forest _GA_ = (_V_, _A_)\nand (_u_,_v_) is a light edge connecting _C_ to some other component _C'_ in\n_GA_ \\-- that is, (_u_,_v_) is a light edge crossing the cut (_VC_, _V_ \\-\n_VC_) -- then (_u_,_v_) is safe for _A_.\n\n_Proof:_ Set _S_ = _VC_ in the theorem. ♦\n\nThis idea (of thinking in terms of components rather than vertices) leads to\nKruskal's algorithm ...\n\n* * *\n\n##  Kruskal's Algorithm\n\nKruskal's algorithm starts with each vertex being its own component, and\nrepeatedly merges two components into one by choosing the light edge that\nconnects them. It does this greedily (or stingily?) by scanning the edges in\nincreasing order by weight. A disjoint-set data structure is used to determine\nwhether an edge connects vertices in two different components.\n\nThis algorithm has similarities with the connected components algorithm we\npreviously saw in [Topic\n16](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-16.html):\n\n![](fig/pseudocode-connected-components.jpg)\n\nHere is Kruskal's version:\n\n![](fig/pseudocode-Kruskal-MST.jpg)\n\n### Example\n\nLet's start with this example. The first edge has been chosen.\n\n![](fig/Fig-23-4-Kruskal-Example-a.jpg)\n\nAdd 4 more edges (notice we could add edges of weight 2 in either order, and\nsimilarly for 4) ...\n\n![](fig/Fig-23-4-Kruskal-Example-e.jpg)\n\nThe next edge considered is not added because it would connect already\nconnected vertices:\n\n[ ![](fig/Fig-23-4-Kruskal-Example-f.jpg)](http://www2.hawaii.edu/~\nsuthers/courses/ics311s14/Notes/Topic-17/Topic-17-K.html)\n\nKeep going until the MST is constructed, and click to see the final tree.\n\n### Analysis\n\n![](fig/pseudocode-Kruskal-MST.jpg)\n\nThe costs are:\n\n  * Initialize _A_: O(1)\n  * First `for` loop: |_V_| `MAKE-SET` operations\n  * Sort _E_: O(_E_ lg _E_) \n  * Second `for` loop: O(_E_) `FIND-SETs` and `UNIONs`\n\nIf we use the tree implementation of the disjoint-set data structure with\nunion by rank and path compression ([Topic\n16](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-16.html)),\nthe amortized cost per `MAKE-SET`, `UNION` and `FIND-SET` operation (across\n|_E_| operations) is O(α(_V_)), where α is a _very_ slowly growing function,\nthe inverse of Ackermann's function. (Lemma 21.11 states that MAKE-SET in\nisolation is O(1), but here we must treat it as O(α(_V_)) since we are making\na statement about the amortized cost per operation in a _sequence_ of _m_\noperations: see section 24.1. Also, using O(α(_V_)) simplifies the expression\nbelow.)\n\nDroping the lower order O(1) and substituting α(_V_) for the disjoint-set\noperations, the above list of costs sums to O((_V_ \\+ _E_)⋅α(_V_))+ O(_E_ lg\n_E_).\n\nSince G is connected, |_E_| ≥ |_V_| − 1, so we can replace _V_ with _E_ to\nsimplify the first term for the disjoint-set operations, O((_V_ \\+\n_E_)⋅α(_V_)), to O((_E_ \\+ _E_)⋅α(_V_)) or O(_E_⋅α(_V_)).\n\nFurthermore, α(_V_) = O(lg _V_) = O(lg _E_), so O(_E_⋅α(_V_)) is O(_E_ lg\n_E_), and hence the entire expression we started with, O((_V_ \\+ _E_)⋅α(_V_))+\nO(_E_ lg _E_), simplifies to O(_E_ lg _E_).\n\nFinally, since |_E_| ≤ |_V_|2, lg |_E_| = O(2 lg _V_) = O(lg _V_), so we can\nwrite the result as **O(_E_ lg _V_)** to obtain the growth rate in terms of\nboth |_E_| and |_V_|.\n\n(It is usually a good idea to include both _V_ and _E_ when giving growth\nrates for graph algorithms, unless one of them can be strictly limited to the\nother. Shortly we will see that O(_E_ lg _V_) enables comparison to Prim's\nalgorithm.)\n\n* * *\n\n##  Prim's Algorithm\n\nThis algorithm is also a greedy (stingy) algorithm, but it builds one tree,\nchoosing the lightest edge incident on the growing tree, so the set _A_ is\nalways a tree. The tree is initialized to be a single vertex, designated _r_\nfor root.\n\nAt each step it finds a light edge crossing the cut (_VA_, _V_ \\- _VA_), where\n_VA_= vertices that are incident on _A_, and adds this edge to _A_.\n(Initially, _A_ = {} and _VA_ = {_r_}.)\n\n![](fig/illustration-Prims-algorithm.jpg)\n\n(Edges of _A_ are shaded in the illustration.)\n\n### General Idea\n\nTo find the light edge quickly we use a priority queue _Q_ ([Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html)):\n\n  * Each queued object is a vertex in _V_ − _VA_ (the vertices that have not yet been connected to _A_). \n  * The key is the minimum weight of any edge (_u_,_v_) where _u_ ∈ _VA_. (We update this weight for _v_ whenever a new edge is found that reaches _v_ at a lower cost than before.) \n  * Thus the vertex returned by `EXTRACT-MIN` is for _v_ such that ∃ _u_ ∈ _VA_ and (_u_,_v_) is a light edge crossing (_VA_, _V_ \\- _VA_). \n  * If _v_ is not adjacent to any vertices in _VA_, the key of _v_ is ∞\n\nThe edges of _A_ will form a rooted tree with root _r_, given as input (_r_\ncan be any vertex).\n\n  * Each vertex keeps track of its parent by the attribute _v_.π = parent of _v_, or NIL if _v_ = _r_ or has no parent yet.\n  * As the algorithm progresses, _A_ = {(_v_, _v_.π) : _v_ ∈ _V_ \\- {_r_} − _Q_}. \n  * At termination, _Q_ is empty, so _A_ is a MST.\n\n### Pseudocode\n\nThis code _differs from the book's version_ in having explicit calls to the\nheap methods:\n\n![](fig/pseudocode-Prim-MST-improved.jpg)\n\nNotice that it is possible for the last `if` to execute multiple times for a\ngiven _v_. In other words, we may find an edge reaching vertex _v_, but before\nwe choose to use it (because other edges have lower key values), we find\nanother edge reaching _v_ for lower cost (key value). _Watch for this\nsituation in the example below._\n\n### Example\n\nLet's try it with this graph. The first three steps are shown. Every time a\nvertex is dequed, it is colored black and the cost of all adjacent vertices\nare updated as needed. For example, when **a** is dequeued, the cost of **b**\nis updated from infinite to 4, and the cost of **h** is updated from infinite\nto 8. Then when **b** is dequeued, its neighbors are updated and so on.\n\n![](fig/Fig-23-5-Prim-Example-a-d.jpg)\n\n_Did you see where a vertex's key was lowered from one non-infinite value to\nanother? Which one? _\n\n_Now finish it and click on the image to see final solution._\n\n![](fig/pseudocode-Prim-MST-improved.jpg)\n\n###  Analysis\n\nPerformance depends on the priority queue implementation. With a binary heap\nimplementation ([Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html)),\nthe costs are:\n\n  * Initialize _Q_ and iterate over |_V_| vertices in first `for` loop to insert in queue, each insert being O(lg _V_): O(_V_ lg _V_) total.\n  * Decrease key of r: O(lg _V_)\n  * The `while` loop has |_V_| `EXTRACT-MIN` calls -> O(_V_ lg _V_)\n  * By amortized analysis, the inner `for each` loop processes Θ(|_E_|) edges, O(_E_) of which result in O(lg _V_) `DECREASE-KEY` calls -> O(_E_ lg _V_)\n\nThe sum of the dominating terms is O(_V_ lg _V_) + O(_E_ lg _V_).\n\nIf G is connected, |_E_| ≥ |_V_| − 1, so we can replace O(_V_ lg _V_) with\nO(_E_ lg _V_), and the total is **O(_E_ lg _V_)**.\n\nThis is asympotitically the same as Kruskal's algorithm. A faster\nimplementation of O(_E_ \\+ _V_ lg _V_) is possible with Fibonacci Heaps, as\nexplained in the text.\n\n* * *\n\nDan Suthers Last modified: Thu Apr 3 12:36:42 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//170.minimum-spanning-tree/reading-notes.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-17a",
 "morea_summary"=>"Including the generic algorithm and the safe edge theorem",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=Mx-dvvSE4Qc",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-a.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Kruskal's algorithm",
 "published"=>true,
 "morea_id"=>"reading-screencast-17b",
 "morea_summary"=>
  "Kruskal's minimum spanning tree algorithm, with example and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=1BnpXYm7LKY",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-b.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Prim's algorithm",
 "published"=>true,
 "morea_id"=>"reading-screencast-17c",
 "morea_summary"=>
  "Prim's minimum spanning tree algorithm, with example and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=qegY0R78QMQ",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-c.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-sedgewick.html</h2>

<pre>Hash
{"title"=>"Sedgewick 31 - Weighted graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-31",
 "morea_summary"=>
  "Minimum spanning trees, shortest path, dense paths, geometric problems",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-sedgewick.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-sedgewick.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/experience-2.html</h2>

<pre>Hash
{"title"=>"Experience single source shortest paths (again)",
 "published"=>true,
 "morea_id"=>"experience-single-source-shortest-paths-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Playing with Bellman-Ford, Dijkstra's algorithm, and DAGs ",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/180.single-source-shortest-paths/experience-2.html",
 "url"=>"/morea/180.single-source-shortest-paths/experience-2.html",
 "content"=>
  "### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### 2\\. DAG-Shortest-Paths for Job Scheduling\n\nIn class we learned how the problem of scheduling a set of jobs with time\nrequired and precedence constraints can be modeled using a Directed Acyclic\nGraph.\n\n  * Edges from j_i_s to j_i_f are weighted with the time required to do job j_i_.\n  * If job j_j_ cannot start until job j_i_ finishes, a 0 weighted edge is put from j_i_f to j_j_s. \n  * There is an implicit 0 weight edge from start node s to every j_j_ s and from every j_j_ f to f.\n\n![](fig/PS-11-Jobs.jpg)\n\nWe also discussed how it would be preferable to use an existing algorithm so\nwe don't have to go to the trouble of developing a new one and proving that it\nis correct. The `DAG-Shortest-Paths` algorithm is promising, but it computes\nshortest paths while we need longest paths in order to find the longest chain\nof precedence constraints. A simple transformation to the problem\nrepresentation makes it possible to use `DAG-Shortest-Paths`: negate the\nweights!\n\nYour problem here is to _**solve the job scheduling problem for the jobs shown\nin the table using `DAG-Shortest-Paths`**_. You will fill in the edge weights,\nrun `DAG-Shortest-Paths` on the modified DAG, and transform the results into a\ntable for each job of the time that it can start running and the time it will\nfinish. (The start time of s is time 0.)\n\n#### DAG with edge weights filled in:\n\n![](fig/PS-11-DAG-Template.jpg)\n\n#### Start and End Times for Jobs:\n\n    \n    \n    job    start   end \n    -----  -----  -----\n    j1\n    j2\n    j3\n    j4\n    j5\n    j6\n    j7\n    \n    Last job finishes at: \n    \n\n\n\n* * *\n\n### 3\\. Dijkstra's Algorithm on Negative Weight Graphs\n\n![](fig/PS-11-Graph.jpg)\n\nRun **Dijkstra's algorithm** on the graph shown, using vertex `a` as the start\nvertex. For each step of the algorithm, show what vertex is dequeued and the\ndistance estimate _v_._d_ it had at the time it is put into _S_. (To help the\nTA debug, you may wish to show the entire graph each iteration.) Then identify\nthe final distance estimates and explain what distance estimate is in error\nand why.\n\n    \n    \n    Step  v put in S  v.d\n    ----  ----------  ---\n    1.\n    2.\n    3.\n    4.\n    5.\n    \n    (Final distance estimates are in column v.d.)\n    **What distance estimate(s) is(are) in error, and explanation of why:**\n    \n\n\n\n* * *\n\n#### The next two problems are the steps of Johnson's Algorithm that fix the\nproblem we just showed above.\n\n* * *\n\n### 4\\. Bellman-Ford on Johnson's _G_'\n\n**(a)** First we construct the graph _G_' defined in the first step of Johnson's algorithm by adding a new vertex `s` with edges of cost 0 to all other vertices. Run `Initialize-Single-Source` (the first step of `Bellman-Ford`) on this graph (so vertices are labeled by _v_._d_, either 0 or ∞). Use the template below to show the resulting graph. (Large version of templates will be emailed to you.)\n\n#### Graph _G_' after Bellman-Ford calls Initialize-Single-Source:\n\n![](fig/PS-11-Graph-Prime.jpg)\n\n**(b)** Complete running the **`Bellman-Ford`** algorithm on the graph you just constructed, using vertex `s` as the start vertex. For Pass #0 (first line), copy the values of _v_.d from the graph above and nil for _v_.π. Then show updated values of _d_ and π for each vertex after each pass of loop at line 2 (at the end, _v_.d = δ(_s_,_v_), which will be used in problem 5). To make it easier for the TA to grade, let's all relax the edges in the same order, namely lexical order of pairs: ` (a,b), (a,c), (a,d), (b,c), (b,e), (c,d), (d,e), (e,d), (s,a), (s,b), (s,c), (s,d), (s,e).`\n    \n    \n    Pass#    a._d_  a.π    b._d_  b.π    c._d_  c.π    d._d_  d.π    e._d_  e.π\n    -----    ---  ---    ---  ---    ---  ---    ---  ---    ---  ---\n    0.\n    1.\n    2.\n    3.\n    4.\n    5.\n    \n\n#### Graph _G_' after Bellman-Ford completes (vertices labeled by _h_(_v_) =\nδ(_s_,_v_)):\n\n![](fig/PS-11-Graph-Prime.jpg)\n\n* * *\n\n### 5\\. Iterated Dijkstra with _ŵ_\n\n**(a)** Using the results of problem 4 and defining _h_(_v_) = δ(_s_,_v_) ∀ _v_ ∈ _V_, draw the revised graph _G_ (with `s` removed) with edge weights _ŵ_(_u_,_v_) = _w_(_u_,_v_) + _h_(_u_) − _h_(_v_) and after `Initialize-Single-Source` (vertices are labeled by _v_.d, either 0 or ∞). (Refer to 4b above for _h_ values.) \n\n#### Graph _G_ showing edge weights _ŵ_(_u_,_v_) after Initialize-Single-\nSource:\n\n![](fig/PS-11-Graph-NoWeights.jpg)\n\n**(b)** Run Dijkstra's algorithm on the resulting graph, again using vertex `a` as the start vertex. Show the resulting δ̂(_a_,_v_) as labels inside the vertices.\n    \n    \n    Step  v put in S   v.d\n    ----  ----------  -----\n    1.\n    2.\n    3.\n    4.\n    5. \n    \n\n#### Graph _G_ with edge weights _ŵ_(_u_,_v_) and vertex labels δ̂(_a_,_v_):\n\n![](fig/PS-11-Graph-NoWeights.jpg)\n\n**(c)** Now map the distances above back to what they would be under _w_ by using δ(_a_,_v_) = δ̂(_a_,_v_) − _h_(_u_) + _h_(_v_), referring to your results of 4b for _h_. Note that this should get the correct path that was missed in problem 2! \n\n#### Graph _G_ with edge weights _w_(_u_,_v_) and vertex labels δ(_a_,_v_):\n\n![](fig/PS-11-Graph-NoWeights.jpg)\n\n\n\n* * *\n\nJohnson's algorithm would now repeat 5(a) and 5(b) on each of the other\nvertices b, c, d, and e. However, you do not need to turn in the other runs of\nDijkstra. We'll compute the other paths below.\n\n* * *\n\n### 6\\. Floyd-Warshall\n\nConstruct and show the matrix _D_(0) for the graph shown again here. Then run\nFloyd-Warshall, showing the matrix _D_(_k_) for each value of _k_. The final\nmatrix should have the same values as those computed in problem 5 for the\nstart vertex _a_, as well as values for all other start vertices. To make it\nclear what order to process the vertices and easier for the TA to grade, we\nwill number the vertices in alphabetical order: for _k_=1, _k_ is vertex a;\nfor _k_=2, _k_ is vertex b, etc.\n\n#### _D_(0)\n\n![](fig/PS-11-Graph.jpg) ![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n#### _D_(1)\n\n![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n#### _D_(2)\n\n![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n#### _D_(3)\n\n![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n#### _D_(4)\n\n![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n#### _D_(5)\n\n![](fig/PS-11-Floyd-Warshall-Matrix-Template.jpg)\n\n* * *\n\nDan Suthers Last modified: Sun Apr 20 03:32:41 HST 2014\n\n",
 "path"=>"morea//180.single-source-shortest-paths/experience-2.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/experience.html</h2>

<pre>Hash
{"title"=>"Experience single source shortest paths",
 "published"=>true,
 "morea_id"=>"experience-single-source-shortest-paths",
 "morea_type"=>"experience",
 "morea_summary"=>"Playing with Bellman-Ford, Dijkstra's algorithm, and DAGs ",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/180.single-source-shortest-paths/experience.html",
 "url"=>"/morea/180.single-source-shortest-paths/experience.html",
 "content"=>
  "### Bellman-Ford\n\n####  1\\. Modifying Bellman-Ford to exit early when all paths are done.\n\nSuppose we have modified the Relax procedure to return TRUE if it changed an\nedge weight and FALSE otherwise. This enables us to write a better\nencapsulated version of Bellman-Ford, shown below.\n\n    \n    \n    Bellman-Ford(G,w,S)\n    1  Initialize-Single-Source(G,s)\n    2  for i = 1 to |G.V| - 1\n    3      for each edge (u,v) in G.E\n    4          Relax(u,v,w) \n    5  for each edge (u,v) in G.E\n    6      if Relax(u,v,w) \n    7          return FALSE\n    8  return TRUE\n    \n\nBut Bellman-Ford still does unnecessary work if all shortest paths are\nsignificantly shorter than |G.V| - 1. Modify the code above to exit the\nalgorithm early with the correct value if there will be no further changes. Do\nthis by modifying Bellman-Ford alone: don't change the Relax procedure.\n\n### Dijkstra's Algorithm\n\n#### 2\\. Dijkstra's algorithm and negative weight edges.\n\nDijkstra's algorithm assumes (based on the triangle inequality) that once a\nvertex v is moved to S, v.d = δ(s, v). It is not supposed to be able to handle\ngraphs with negative weight edges, such as the graph shown below, because they\nviolate the triangle inequality.\n\n![](fig/code-Dijkstra.jpg) ![](fig/Dijkstra-False-Counterexample.jpg)\n\nBut nothing in Dijkstra's algorithm says lines 7-8 can't update weights of\nvertices that are already in S. If you simulate the algorithm, you will see\nthat when vertex y is dequeued, vertex z (which is already in S) should be\nupdated to its correct distance δ(s, z). Why won't this always happen? Can you\nmodify the graph to be one for which v.d ≠ δ(s, v) at the end for some v?\n\n### DAG Shortest Paths\n\n#### 3\\. Parallel scheduling with critical paths in a DAG\n\nThe parallel scheduling problem is to take a set of interdependent jobs with\nknown execution time and determine when to schedule each job so that the last\njob finishes as soon as possible while respecting the interdependency\nconstraints.\n\nWe can model such problems using weighted DAGs as follows:\n\n  1. Create a DAG with a start node _s_, a finish node _f_, and two vertices for each job _j_: a job start vertex _j_s and a job end vertex _j_f.\n  2. For each job, add an edge from its start vertex to its end vertex with weight equal to its duration. (This models the time required for the job.) \n  3. For each precedence constraint where job _i_ must finish before job _j_ starts, add a zero-weight edge from _i_f to _j_s. (This models precedence constraints where one job relies on another having been finished, but can start immediately after it finished: hence the 0.)\n  4. Also add zero-weight edges from the start node _s_ to every job start node _j_s and from every job finish node _j_f to the finish node _f_. (The start vertex gives us a place to begin the shortest paths algorithm and the finish vertex gives us a place where we will assign the finishing time.) \n\n![](fig/PS-11-Jobs.jpg)\n\nBut we need an algorithm to do the scheduling. Rather than write an algorithm\nfrom scratch, we will see how we can do this with DAG-Shortest-Paths. Some\nadjustment may be required to make the data structure fit the algorithm.\n\nSome jobs are shown in the table to the right. The first job should start at\ntime 0. Your task is to schedule the jobs such that they are all completed in\nthe minimum amount of time while respecting the constraints. You will complete\nthis on your homework: here we will ensure we understand the approach.\n\n**(a)** Do we need to find the shortest paths or the longest paths, and why? \n\n**(b)** Do you need to change the weights in any way to solve it with DAG-Shortest-Paths, and if so how and why? \n\n**(c)** After running DAG-Shortest-Paths, how do you extract the solution, specifically the start time of each job and the time at which all jobs will be finished? \n\n",
 "path"=>"morea//180.single-source-shortest-paths/experience.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/module.html</h2>

<pre>Hash
{"title"=>"Single source shortest paths",
 "published"=>true,
 "morea_id"=>"single-source-shortest-paths",
 "morea_outcomes"=>["outcome-single-source-shortest-paths"],
 "morea_readings"=>
  ["reading-screencast-18a",
   "reading-screencast-18b",
   "reading-screencast-18c",
   "reading-cormen-24",
   "reading-sedgewick-31",
   "reading-notes-18"],
 "morea_experiences"=>
  ["experience-single-source-shortest-paths",
   "experience-single-source-shortest-paths-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/180.single-source-shortest-paths/logo.gif",
 "morea_sort_order"=>180,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/180.single-source-shortest-paths/module.html",
 "content"=>
  "Bellman-Ford algorithm, shortest paths in direct acyclic graphs, Dijsktra's algorithm.",
 "path"=>"morea//180.single-source-shortest-paths/module.md"}
</pre>

<h2>/modules/single-source-shortest-paths/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Single source shortest paths",
 "url"=>"/modules/single-source-shortest-paths/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/single-source-shortest-paths/index.html"}
</pre>

<h2>/morea/180.single-source-shortest-paths/outcome.html</h2>

<pre>Hash
{"title"=>"Understand single source shortest paths",
 "published"=>true,
 "morea_id"=>"outcome-single-source-shortest-paths",
 "morea_type"=>"outcome",
 "morea_sort_order"=>180,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/180.single-source-shortest-paths/outcome.html",
 "content"=>
  "Understand when, why, and how to use single source shortest paths.",
 "path"=>"morea//180.single-source-shortest-paths/outcome.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 24 - Single Source Shortest Paths (24.1 - 24.3)",
 "published"=>true,
 "morea_id"=>"reading-cormen-24",
 "morea_summary"=>
  "Bellman-Ford algorithm, SSSP in directed acyclic graphs, Dijkstra's algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "20 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/180.single-source-shortest-paths/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//180.single-source-shortest-paths/reading-cormen.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on single source shortest paths",
 "published"=>true,
 "morea_id"=>"reading-notes-18",
 "morea_summary"=>
  "Shortest paths problem, Bellman-Ford algorithm, Shortest paths in a DAG, Dijkstra's algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/180.single-source-shortest-paths/reading-notes.html",
 "url"=>"/morea/180.single-source-shortest-paths/reading-notes.html",
 "content"=>
  "## Outline\n\nToday's Theme: Relax!\n\n  1. Shortest Paths Problems \n  2. Bellman-Ford Algorithm\n  3. Shortest Paths in a DAG \n  4. Dijkstra's Algorithm \n\n##  Shortest Paths Problems\n\nor how to get there from here ...\n\n### Definition\n\nInput is a directed graph _G_ = (_V_, _E_) and a **_weight function_** _w_:\n_E_ -> ℜ.\n\nDefine the **_path weight_ _w_(_p_) ** of path _p_ = ⟨_v_0, _v_1, ... _vk_⟩ to\nbe the sum of edge weights on the path:\n\n![](fig/sum-of-weights.jpg)\n\nThen the **_shortest path weight_** from _u_ to _v_ is:\n\n![](fig/shortest-path-definition.jpg)\n\nA **shortest path** from _u_ to _v_ is any path such that _w_(_p_) = δ(_u_,\n_v_).\n\n### Examples\n\nIn our examples the shortest paths will always start from _s_, the\n**_source_**. The δ values will appear inside the vertices, and shaded edges\nshow the shortest paths.\n\n![](fig/Fig-24-2-shortest-paths-example-alt.jpg)\n\nAs can be seen, shortest paths are not unique.\n\n### Variations\n\n  * **_Single-Source:_** from _s_ to every _v_ ∈ _V_ (the version we consider)\n  * **_Single-Destination:_** from every _v_ ∈ _V_ to some _d_. (Solve by reversing the links and solving single source.) \n  * **_Single-Pair:_** from some _u_ to some _v_. Every known algorithm takes just as long as solving Single-Source. (_Why might that be the case?_)\n  * **_All-Pairs:_** for every pair _u_, _v_ ∈ _V_. Next lecture.\n\n### Negative Weight Edges\n\nThese are OK as long as no negative-weight cycles are reachable from the\nsource _s_. Fill in the blanks:\n\n![](fig/Fig-24-1-negative-weights-2.jpg)\n\nIf a negative-weight cycle is accessible, it can be iterated to make _w_(_s_,\n_v_) arbitarily small for all _v_ on the cycle:\n\n![](fig/Fig-24-1-negative-weights-3.jpg)\n\nSome algorithms can detect negative-weight cycles and others cannot, but when\nthey are present shortest paths are not well defined.\n\n### Cycles\n\nShortest paths cannot contain cycles.\n\n  * We already ruled out negative-weight cycles.\n  * If there is a positive-weight cycle we can get a shorter path by omitting the cycle, so it can't be a shortest path with the cycle.\n  * If there is a zero-weight cycle, it does not affect the cost to omit them, so we will assume that solutions won't use them.\n\n### Optimal Substructure\n\nThe shortest paths problem exhibits **_optimal substructure_**, suggesting\nthat greedy algorithms and dynamic programming may apply. Turns out we will\nsee examples of both (Dijkstra's algorithm in this chapter, and Floyd-Warshall\nin the next chapter, respectively).\n\n![](fig/lemming.jpg)\n\n**_Lemma:_ Any subpath of a shortest path is a shortest path.**\n\n**_Proof_** is by cut and paste. Let path _puv_ be a shortest path from _u_ to _v_, and that it includes subpath _pxy_ (this represents subproblems):\n\n![](fig/subpath-lemma-a.jpg)\n\nThen δ(_u_, _v_) = _w_(_p_) = _w_(_pux_) + _w_(_pxy_) + _w_(_pyv_).\n\nNow, for proof by contradiction, suppose that substructure is not optimal,\nmeaning that for some choice of these paths there exists a shorter path _p'xy_\nfrom _x_ to _y_ that is shorter than _pxy_. Then _w_(_p'xy_) < _w_(_pxy_).\n\nFrom this, we can construct _p'_:\n\n![](fig/subpath-lemma-b.jpg)\n\nThen\n\n![](fig/subpath-lemma-c.jpg)\n\nwhich contradicts the assumption that _puv_ is a shortest path.\n\n### Algorithms\n\nAll the algorithms we consider will have the following in common.\n\n#### Output\n\nFor each vertex _v_ ∈ _V_, we maintain these attributes:\n\n**_v.d_** is called the **_shortest path estimate_**. \n\n  * Initially, _v.d_ = ∞\n  * _v.d_ may be reduced as the algorithm progresses, but _v.d_ ≥ δ(_s_, _v_) is always true.\n  * We want to show that at the conclusion of our algorithms, _v.d_ = δ(_s_, _v_).\n\n**_v._π** = the predecessor of _v_ by which it was reached on the shortest path known so far. \n\n  * If there is no predecessor, _v._π = NIL.\n  * We want to show that at the conclusion of our algorithms, _v._π = the predecessor of _v_ on the shortest path from _s_.\n  * If that is true, π induces a **_shortest path tree_** on _G_. (See text for proofs of properties of π.) \n\n#### Initialization\n\nAll the shortest-paths algorithms start with this:\n\n![](fig/code-initialize-single-source.jpg)\n\n#### Relaxation\n\nThey all apply the relaxation procedure, which essentially asks: can we\nimprove the current shortest-path estimate for _v_ by going through _u_ and\ntaking (_u_, _v_)?\n\n![](fig/code-relax.jpg) ![](fig/Fig-24-3-relaxation-alt.jpg)\n\nThe algorithms differ in the order in which they relax each edge and how many\ntimes they do that.\n\n### Shortest Paths Properties\n\nAll but the first of these properties assume that `INIT-SINGLE-SOURCE` has\nbeen called once, and then `RELAX` is called zero or more times.\n\n![](fig/properties.jpg)\n\nProofs are available in the text. Try to explain informally why these are\ncorrect.\n\n* * *\n\n##  Bellman-Ford Algorithm\n\nEssentially a **brute force strategy**: relax systematically enough times that\nyou can be sure you are done.\n\nThe algorithm can also be considered a dynamic programming algorithm for\nreasons discussed below.\n\n  * Allows negative-weight edges\n  * Computes _v_._d_ and _v_.π for all _v_ ∈ _V_.\n  * Returns True (and a solution embedded in the graph) if no negative-weight cycles are reachable from _s_, and False otherwise.\n\n![](fig/code-Bellman-Ford.jpg) ![](fig/code-relax.jpg)\n\nThe first `for` loops do the work of relaxation. _How does the last `for` loop\nhelp -- how does it work?_\n\n### Analysis:\n\n`RELAX` is O(1), and the nested `for` loops relax all edges |_V_| - 1 times,\nso `BELLMAN-FORD` is Θ(_V E_).\n\n### Examples:\n\nExample from the text, relaxed in order (t,x), (t,y), (t,z), (x,t), (y,x)\n(y,z), (z,x), (z,s), (s,t), (s,y):\n\n![](fig/Fig-24-4-Bellman-Ford-example.jpg)\n\nTry this other example (click for answer):\n\n![](fig/code-Bellman-Ford.jpg) ![](fig/Bellman-Ford-Example-2-1.jpg)\n\n###  Correctness\n\nThe values for _v_._d_ and _v_.π are guaranteed to converge on shortest paths\nafter |_V_| - 1 passes, assuming no negative-weight cycles.\n\nThis can be proven with the path-relaxation property, which states that if we\nrelax the edges of a shortest path ⟨_v_0, _v_1, ... _vk_⟩ in order, even if\ninterleaved with other edges, then _vk_._d_ = δ(_s_,_vk_) after _vk_ is\nrelaxed.\n\n![](fig/code-Bellman-Ford.jpg)\n\nSince the list of edges is relaxed as many times as the longest possible\nshortest path (|_V_|- 1), it must converge by this property.\n\n  * First iteration relaxes (_v_0, _v_1)\n  * Second iteration relaxes (_v_1, _v_2)\n  * ... \n  * _k_th iteration relaxes (_v__k_-1, _v__k_)\n\nThis is why the Bellman Ford algorithm can be considered to be a dynamic\nprogramming algorithm:\n\n  * After the first pass, paths of length 1 are correct and are used to construct longer paths;\n  * after the second pass, paths of length 2 are correct and are used to construct longer paths; etc.\n\nup until _n_−1, which is the longest possible path.\n\nWe also must show that the True/False values are correct. Informally, we can\nsee that if _v_._d_ is still getting smaller after it should have converged\n(see above), then there must be a negative weight cycle that continues to\ndecrement the path.\n\nThe full proof of correctness may be found in the text.\n\nThe values computed on each pass and how quickly it converges depends on order\nof relaxation: it may converge earlier.\n\n_How can we use this fact to speed the algorithm up a bit?_\n\n![](fig/pillow_talk.jpg)\n\n* * *\n\n![](fig/dawg.jpg)\n\n##  Shortest Paths in a DAG\n\nLife is easy when you are a DAG ...\n\nThere are no cycles in a Directed Acyclic Graph. Thus, negative weights are\nnot a problem. Also, vertices must occur on shortest paths in an order\nconsistent with a topological sort.\n\nWe can do something like Bellman-Ford, but don't need to do it as many times,\nand don't need to check for negative weight cycles:\n\n![](fig/code-DAG-Shortest-Paths.jpg)\n\n#### Analysis:\n\nGiven that topological sort is Θ(_V_ \\+ _E_), what's the complexity of `DAG-\nSHORTEST-PATHS`? _This one's on you: what's the run-time complexity?_ Use\naggregate analysis ...\n\n#### Correctness:\n\nBecause we process vertices in topologically sorted order, edges of _any_ path\nmust be relaxed in order of appearance in the path.\n\nTherefore edges on any shortest path are relaxed in order.\n\nTherefore, by the path-relaxation property, the algorithm terminates with\ncorrect values.\n\n![](fig/code-DAG-Shortest-Paths.jpg)\n\n### Examples\n\nFrom the text:\n\n![](fig/Fig-24-5-Shortest-Paths-in-DAG.jpg)\n\nNotice we could not reach _r_!\n\nLet's try another example (click for answer):\n\n![](fig/DAG-example-2-1.jpg)\n\n* * *\n\n![](fig/Dijkstra.jpg)\n\n##  Dijkstra's Algorithm\n\nThe algorithm is essentially a weighted version of breadth-first search: BFS\nuses a FIFO queue; while this version of Dijkstra's algorithm uses a priority\nqueue.\n\nIt also has similarities to Prim's algorithm, being greedy, and with similar\niteration.\n\nAssumes there are no negative-weight edges.\n\n### Algorithm\n\n  * _S_ = set of vertices whose final shortest-path weights are determined.\n  * _Q_ = _V_ \\- _S_ is the priority queue. \n  * Priority queue keys are shortest path estimates _v_._d_. \n\n![](fig/pseudocode-Prim-MST.jpg)\n\nHere it is, with Prim on the right for comparison:\n\n![](fig/code-Dijkstra.jpg)\n\nDijkstra's algorithm is greedy in choosing the closest vertex in _V_ \\- _S_ to\nadd to _S_ each iteration. The difference is that\n\n  * For Prim \"close\" means the cost to take one step to include the next cheapest vertex:   \n` if _w_(_u_,_v_) < _v_.key`\n\n  * for Dijkstra \"close\" means the cost from the source vertex _s_ to _v_: this is in the RELAX code   \n`if _v_._d_ > _u_._d_ \\+ _w_(_u_,_v_)`.\n\n### Examples\n\nFrom the text (black vertices are set _S_; white vertices are on _Q_; shaded\nvertex is the min valued one chosen next iteration):\n\n![](fig/Fig-24-6-Dijkstra-Example.jpg)\n\nLet's try another example (click for answer):\n\n![](fig/code-Dijkstra.jpg) ![](fig/Dijkstra-Example-2-1.jpg)\n\nHere's a graph with a negative weight: try it from _s_ and see what happens:\n\n![](fig/Dijkstra-negative-weight-example.jpg)\n\n### Correctness\n\n![](fig/code-Dijkstra.jpg)\n\nThe proof is based on the following loop invariant at the start of the `while`\nloop:\n\n> _v_._d_ = δ(_s_, _v_) for all _v_ ∈ _S_.\n\n**_Initialization:_** Initially _S_ = ∅, so trivially true. \n\n**_Maintenance:_** We just sketch this part (see text). Need to show that _u_._d_ = δ(_s_, _u_) when _u_ is added to _S_ in each iteration. The upper bound property says it will stay the same thereafter.\n\nSuppose (for proof by contradiction) that ∃ _u_ such that _u_._d_ ≠ δ(_s_,\n_u_) when added to _S_. Without loss of generality, let _u_ be the first such\nvertex added to _S_.\n\n![](fig/Fig-24-7-Dijkstra-Correctness.jpg)\n\n  * _u_ ≠ _s_, since _s_._d_ = δ(_s_, _s_) = 0. Therefore _s_ ∈ _S_ ≠ ∅. \n  * So there is a path from _s_ to _u_. This means there must be a shortest path _p_ from _s_ to _u_. \n  * The proof decomposes _p_ into a path _s_ to _x_, (_x_, _y_), and a path from _y_ to _u_. (Some but not all of these can be null.)\n  * _y_._d_ = δ(_s_, _y_) when _u_ added to _S_. (By hypothesis, _x_._d_ = δ(_s_, _x_) when _x_ was added. Relaxation of (_x_, _y_) extends this to _y_ by the convergence property.)\n  * Since _y_ appears before _u_ on a shortest path with non-negative weights, δ(_s_,_y_) ≤ δ(_s_,_u_), and we can show that _y_._d_ ≤ _u_._d_ by the triangle inequality and upper-bound properties.\n  * But _u_ being chosen first from _Q_ means _u_._d_ ≤ _y_._d_; so must be that _u_._d_ = _y_._d_. \n  * Therefore _y_._d_ = δ(_s_, _y_) = δ(_s_, _u_) = _u_._d_. \n  * This contradicts the assumption that _u_._d_ ≠ δ(_s_, _u_)\n\n**_Termination:_** At the end, _Q_ is empty, so _S_ = _V_, so _v_._d_ = δ(_s_, _v_) for all _v_ ∈ _V_\n\n### Analysis\n\nThe run time depends on the implementation of the priority queue.\n\n![](fig/code-Dijkstra.jpg)\n\nIf **_binary min-heaps_** are used:\n\n  * The `EXTRACT-MIN` in line 5 and the implicit `DECREASE-KEY` operation that results from relaxation in line 8 are each O(lg _V_).\n  * The `while` loop over |_V_| elements of _Q_ invokes |_V_| O(log _V_) `EXTRACT-MIN` operations. \n  * Switching to aggregate analysis for the `for` loop in lines 7-8, there is a call to `RELAX` for each of O(_E_) edges, and each call may result in an O(log _V_) `DECREASE-KEY`.\n  * The total is **O((_V_ \\+ _E_) lg _V_)**.\n  * If the graph is connected, there are at least as many edges as vertices, and this can be simplified to **O(_E_ lg _V_)**, which is faster than `BELLMAN-FORD`'s O(_E_ _V_). \n\nWith **_Fibonacci heaps_** (which were developed specifically to speed up this\nalgorithm), O(_V_ lg _V_ \\+ _E_) is possible. _(Do not use this result unless\nyou are specifically using Fibonacci heaps!)_\n\n* * *\n\nDan Suthers Last modified: Mon Apr 14 03:36:49 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//180.single-source-shortest-paths/reading-notes.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to single source shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-18a",
 "morea_summary"=>"Properties of the problem, supporting algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=S7rzpWwICo8",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/180.single-source-shortest-paths/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//180.single-source-shortest-paths/reading-screencast-a.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Bellman-Ford and DAGS shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-18b",
 "morea_summary"=>"The Bellman-Ford algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=4Iy0RalrsXo",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/180.single-source-shortest-paths/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//180.single-source-shortest-paths/reading-screencast-b.md"}
</pre>

<h2>/morea/180.single-source-shortest-paths/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Dijkstra's algorithm for single source shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-18c",
 "morea_summary"=>"Dijkstra's algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=0wfNtfhHlqE",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/180.single-source-shortest-paths/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//180.single-source-shortest-paths/reading-screencast-c.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/experience.html</h2>

<pre>Hash
{"title"=>"Experience all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"experience-all-pairs-shortest-paths",
 "morea_type"=>"experience",
 "morea_summary"=>"Floyd-Warshall and Johnson's algorithm",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/190.all-pairs-shortest-paths/experience.html",
 "url"=>"/morea/190.all-pairs-shortest-paths/experience.html",
 "content"=>
  "### Floyd-Warshall\n\n**1.** What do the main diagonal entries of the matrix constructed by the Floyd-Warshall algorithm mean? \n\n**2.** How can we use the diagonal entries to detect negative weight cycles? \n\n### Johnson's Algorithm\n\nThese questions are related to each other. Answering one might help answer the\nother.\n\n**3.** We put 0-weighted links from _s_ to every other vertex _v_, so isn't δ(_s_, _v_) always 0? When is it not? \n\n**4.** Suppose that G=(V,E) has no negative weight edges. What is the relationship between w and ŵ for G, and why? \n\n**5.** What is the purpose of adding the new vertex _s_ to _V_, to construct _V_'? Why don't we just pick an arbitrary vertex in _V_ to start from? \n\n\n",
 "path"=>"morea//190.all-pairs-shortest-paths/experience.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/module.html</h2>

<pre>Hash
{"title"=>"All pairs shortest paths",
 "published"=>true,
 "morea_id"=>"all-pairs-shortest-paths",
 "morea_outcomes"=>["outcome-all-pairs-shortest-paths"],
 "morea_readings"=>
  ["reading-screencast-19a",
   "reading-screencast-19b",
   "reading-screencast-19c",
   "reading-cormen-25",
   "reading-notes-19"],
 "morea_experiences"=>["experience-all-pairs-shortest-paths"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/190.all-pairs-shortest-paths/logo.jpg",
 "morea_sort_order"=>190,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/190.all-pairs-shortest-paths/module.html",
 "content"=>
  "Johnson's algorithm, Floyd-Warshall algorithm, dynamic programming for dense graphs, transitive closure",
 "path"=>"morea//190.all-pairs-shortest-paths/module.md"}
</pre>

<h2>/modules/all-pairs-shortest-paths/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"All pairs shortest paths",
 "url"=>"/modules/all-pairs-shortest-paths/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/all-pairs-shortest-paths/index.html"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/outcome.html</h2>

<pre>Hash
{"title"=>"Understand all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"outcome-all-pairs-shortest-paths",
 "morea_type"=>"outcome",
 "morea_sort_order"=>190,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/190.all-pairs-shortest-paths/outcome.html",
 "content"=>"Understand when, why, and how to use all pairs shortest paths.",
 "path"=>"morea//190.all-pairs-shortest-paths/outcome.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 25 - All Pairs Shortest Paths",
 "published"=>true,
 "morea_id"=>"reading-cormen-25",
 "morea_summary"=>
  "Shortest paths and matrix multiplication, Floyd-Warshall algorithm, Johnson's algorithm for sparse graphs",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/190.all-pairs-shortest-paths/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//190.all-pairs-shortest-paths/reading-cormen.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"reading-notes-19",
 "morea_summary"=>
  "All pairs shortest paths problem, using single source algorithms, Johnson's bright idea, Floyd-Warshall: dynamic programming for dense graphs, transitive closure",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/190.all-pairs-shortest-paths/reading-notes.html",
 "url"=>"/morea/190.all-pairs-shortest-paths/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. All-Pairs Shortest Paths Introduction\n  2. Using Single-Source Algorithms\n  3. Johnson's Bright Idea\n  4. Floyd-Warshall: Dynamic Programming for Dense Graphs\n  5. Transitive Closure (Briefly Noted)\n\n##  All-Pairs Shortest Paths\n\nThe problem is an extension of Single-Source Shortest Paths to all sources. We\nstart by repeating the definition.\n\n#### Path Weights and Shortest Paths\n\nInput is a directed graph G = (_V_, _E_) and a **_weight function_** _w_: _E_\n-> ℜ. Define the **_path weight_ _w_(_p_)** for _p_ = ⟨_v_0, _v_1, ... _vk_⟩\nto be the sum of edge weights on the path:\n\n![](fig/sum-of-weights.jpg) ![](fig/Fig-25-1-Directed-Weighted-Graph.jpg)\n\nThe **_shortest path weight_** from _u_ to _v_ is:\n\n![](fig/shortest-path-definition.jpg)\n\nA **_shortest path_** from _u_ to _v_ is any path such that _w_(_p_) = δ(_u_,\n_v_).\n\n#### All-Pairs Shortest Paths\n\nThen the **all-pairs shortest paths problem** is to find a shortest path and\nthe shortest path weight for every pair _u_, _v_ ∈ _V_.\n\n_(Consider what this means in terms of the graph shown above right. How many\nshortest path weights would there be? How many paths?)_\n\n#### Applications\n\nAn obvious real world application is computing **mileage charts**.\n\nUnweighted shortest paths are also used in social network analysis to compute\nthe **betweeness centrality** of actors. (Weights are usually tie strength\nrather than cost in SNA.) The more shortest paths between other actors that an\nactor appears on, the higher the betweeness centrality. This is usually\nnormalized by number of paths possible. This measure is one estimate of an\nactor's potential control of or influence over ties or communication between\nother actors.\n\n* * *\n\n##  All-Pairs Shortest Paths Using Single-Source Algorithms\n\nSince we already know how to compute shortest paths from _s_ to every _v_ ∈\n_V_ (the Single Source version from the [last lecture](http://www2.hawaii.edu/\n~suthers/courses/ics311s14/Notes/Topic-18.html)), why not just iterate one of\nthese algorithms for each vertex _v_ ∈ _V_ as the source?\n\nThat will work, but let's look at the complexity and constraints.\n\n### Iterated Bellman-Ford\n\nBellman-Ford is O(_V E_), and it would have to be run |_V_| times, so the cost\nwould be **O(_V_2_E_)** for any graph.\n\n  * On _dense graphs_, |_E_| = O(_V_2), so this would be O(_V_4). Ouch! \n  * But it will work on graphs with negative weight edges.\n\n![](fig/Dijkstra-Iterated.jpg)\n\n### Iterated Dijkstra\n\nOn sparser graphs, Dijkstra has better asymptotic performance. Dijkstra's is\nO(_E_ lg _V_) with the binary min-heap (faster with Fibonacci heaps).\n\n  * |_V_| iterations gives **O(_V_ _E_ lg _V_)**, which is O(_V_3 lg _V_) in dense graphs (already better), and will be lower in very sparse graphs. (This can be done in O(_V_2 lg _V_ \\+ _VE_) with Fibonacci heaps.) \n  * But it will **_not_** work on graphs with negative weight edges.\n\nWhat a pity. But why can't we just get rid of those pesky negative weights?\n\n![](fig/Fig-25-1-Directed-Weighted-Graph.jpg)\n\n### Eliminating Negative Weights\n\n**_Proposal:_ ** How about adding a constant value to every edge? \n\n  * Find the smallest (most negative) weight, and negate it to get a positive number _c_.\n  * Add _c_ to every edge weight. (If we are using a matrix representation in which a sentinel value such as ∞ is used to represent the absence of an edge, this value is not changed.) \n  * Every weight will be 0 or more, i.e., non-negative. \n\nSince we have added the same constant value to everything, we are just scaling\nup the costs linearly and should obtain the same solutions, right?\n\nFor example, in this graph the shortest path from s to z is **`s--x--y--z`**,\nbut [Dijkstra's algorithm](http://www2.hawaii.edu/~suthers/courses/ics311s14/N\notes/Topic-18.html#Dijkstra) can't find it because there is a negative weight\n(_why? what goes wrong?_):\n\n![](fig/addweight-counterexample-1.jpg)\n\nSo, let's add 10 to every edge:\n\n![](fig/addweight-counterexample-2.jpg)\n\nand the shortest path is .... Oops! **`s--z`**!\n\nThe strategy suggested above does not work because it does not add a constant\namount to each _path_; rather it adds a constant to each _edge_ and hence\nlonger paths are penalized disproportionately.\n\nPerhaps because of this, the first algorithm for all-pairs shortest paths (in\nthe 1960's) by Floyd based on Warshall's work took a dynamic programming\napproach. (We'll get to that later.) But then Johnson had a bright idea in\n1977 that salvaged the greedy approach.\n\n* * *\n\n##  Johnson's Bright Idea\n\nDonald Johnson figured out how to make a graph that has all edge weights ≥ 0,\nand is also equivalent for purposes of finding shortest paths.\n\n### Definitions\n\nWe have been using a weight function _w_ : _V_⊗_V_ -> ℜ that gives the weight\nfor each edge (_i_, _j_) ∈ _E_, or has value ∞ otherwise. (When working with\nadjacency list representations, it may be more convenient to write _w_ : _E_\n-> ℜ and ignore (_i_, _j_) ∉ _E_.)\n\nWe want to find a **modified weight function _ŵ_** that has these properties:\n\n  1. **For all _u_, _v_ ∈ _V_, _p_ is a shortest path from _u_ to _v_ using _w_ _iff_ _p_ is a shortest path from _u_ to _v_ using _ŵ_**.   \n_(A shortest path under each weight function is a shortest path under the\nother weight function.)_\n\n  \n\n  2. **For all (_u_, _v_) ∈ _E_, _ŵ_(_u_, _v_) ≥ 0.**   \n_(All weights are non-negative, so Dijkstra's efficient algorithm can be\nused.)_\n\nIf property 1 is met, it suffices to find shortest paths with _ŵ_. If property\n2 is met, we can do so by running Dijkstra's algorithm from each vertex. But\nhow do we come up with _ŵ_? That's where Johnson can help ...\n\nJohnson figured out that if you add a weight associated with the source and\nsubtract one associated with the target, you preserve shortest paths.\n\n![](fig/lemming.jpg)\n\n### Reweighting Lemma\n\nGiven a directed, weighted graph _G_ = (_V_, _E_), _w_ : _E_ -> ℜ, let _h_ be\n_any_ function (bad-ass lemming don't care) such that _h_ : _V_ -> ℜ.\n\nFor all (_u_, _v_) ∈ _E_ define\n\n> **_ŵ_(_u_, _v_)   =   _w_(_u_, _v_)   \\+   _h_(_u_)   −   _h_(_v_). **\n\nLet _p_ = ⟨_v_0, _v_1, ..., _v__k_⟩ be any path from _v_0 to _v__k_.\n\nThen _p_ is a shortest path from _v_0 to _v__k_ under _w_ **_iff_** _p_ is a\nshortest path from _v_0 to _v__k_ under _ŵ_.\n\nFurthermore, _G_ has a negative-weight cycle under _w_ **_iff_** _G_ has a\nnegative-weight cycle under _ŵ_.\n\n_**Proof:**_ First we'll show that _ŵ_(_p_) = _w_(_p_) + _h_(_v_0) −\n_h_(_v__k_); that is, that the defined relationship transfers to paths.\n\n![](fig/reweighting-lemma-sums.jpg)\n\nTherefore, any path from _v_0 to _v__k_ has _ŵ_(_p_) = _w_(_p_) + _h_(_v_0) −\n_h_(_v__k_).\n\nSince _h_(_v_0) and _h_(_v__k_) don't depend on the path from _v_0 to _v__k_,\nif one path from _v_0 to _v__k_ is shorter than another with _w_, it will also\nbe shorter with _ŵ_.\n\nNow we need to show that ∃ negative-weight cycle with _w_ **_iff_** ∃\nnegative-weight cycle with _ŵ_.\n\nLet cycle _c_ = ⟨_v_0, _v_1, ..., _v__k_⟩ where _v_0 = _v__k_. Then:\n\n![](fig/reweighting-lemma-cycles.jpg)\n\nTherefore, _c_ has a negative-weight cycle with _w_ **_iff_** it has a\nnegative-weight cycle with _ŵ_.\n\n**_Implications:_** It's remarkable that under this definition of _ŵ_, _h_ can assign _any_ weight to the vertices and shortest paths and negative weight cycles will be preserved. This gets us Property 1. How can we choose _h_ to get Property 2?\n\n### Johnson's _h_(_v_)\n\nProperty 2 states that ∀ (_u_, _v_) ∈ _E_, _ŵ_(_u_, _v_) ≥ 0.\n\nSince we have defined _ŵ_(_u_, _v_) = _w_(_u_, _v_) + _h_(_u_) − _h_(_v_), to\nget property 2 we need an _h_ : _V_ -> ℜ for which we can show that _w_(_u_,\n_v_) + _h_(_u_) − _h_(_v_) ≥ 0.\n\nThe motivation for how this is done derives from a section on difference\nconstraints in Chapter 24 that we did not cover, so we'll just have to take\nthis as an insight out of the blue ....\n\n![](fig/Fig-25-6-Johnsons-Example-preview.jpg)\n\nDefine _G'_ = (_V'_, _E'_)\n\n  * _V'_ = _V_ ∪ {_s_}, where _s_ is a new vertex.\n  * _E'_ = _E_ ∪ {(_s_, _v_) : _v_ ∈ _V_}.\n  * _w_(_s_, _v_) = 0 for all _v_ ∈ _V_.\n\nSince no edges enter _s_, _G'_ has the same cycles as _G_, including negative\nweight cycles if they exist.\n\n**Define _h_(_v_) = δ(_s_, _v_) for all _v_ ∈ _V_. **\n\n_(We put a 0-weighted link from _s_ to every other vertex _v_, so isn't δ(_s_,\n_v_) always 0? When is it not? What does this tell us?)_\n\n#### Correctness (proof that we have property 2)\n\n_**Claim:**_ _ŵ_(_u_, _v_)   =   _w_(_u_, _v_)   \\+   _h_(_u_)   −   _h_(_v_)\n≥   0.\n\n_**Proof:**_ By the triangle inequality,\n\n> δ(_s_, _v_)   ≤   δ(_s_, _u_)   \\+   _w_(_u_, _v_),\n\nSubstituting _h_(_v_) = δ(_s_, _v_) (as defined above) and similarly for _u_,\n\n> _h_(_v_)   ≤   _h_(_u_)   \\+   _w_(_u_, _v_).\n\nSubtracting _h_(_v_) from both sides,\n\n> _w_(_u_, _v_)   \\+  _h_(_u_)   −   _h_(_v_)   ≥   0\\.\n\n![](fig/algorithm-Johnson.jpg)\n\n### The Algorithm\n\nThe algorithm constructs the augmented graph _G_' (line 1), uses Bellman-Ford\nfrom _s_ to check whether there are negative weight cycles (lines 2-3), and if\nthere are none this provides the δ(_s_, _v_) values needed to compute _h_(_v_)\n(lines 4-5).\n\nThen it does the weight adjustment with _h_ (lines 6-7), and runs Dijkstra's\nalgorithm from each start vertex (lines 9-10), reversing the weight adjustment\nto obtain the final distances put in a results matrix D (lines 11-12).\n\n### Example\n\nLet's start with this graph:\n\n![](fig/Fig-25-1-Directed-Weighted-Graph.jpg)\n![](fig/Fig-25-6-Johnsons-Example-a.jpg)\n\nFirst we construct _G_' by adding _s_ (the black node) and edges of weight\nfrom _s_ 0 to all other vertices. The original weights are still used. This\nnew graph G' is shown to the right. Vertex numbers have been moved outside of\nthe nodes.\n\nThen we run Bellman-Ford on this graph with _s_ (the black node) as the start\nvertex. The resulting path distances δ(_s_, _v_) are shown inside the nodes to\nthe right. Remember that _h_(_v_) = δ(_s_, _v_), so that these are also the\nvalues we use in adjusting edge weights (next step).\n\n![](fig/Fig-25-6-Johnsons-Example-b-no-s.jpg)\n\nIn the next graph to the left, the edge weights have been adjusted to _ŵ_(_u_,\n_v_) = _w_(_u_, _v_) + _h_(_u_) − _h_(_v_). For example, the edge (1, 5),\npreviously weighted -4, has been updated to -4 + 0 − (-4) = 0.\n\nAll weights are positive, so we can now run Dijkstra's algorithm from each\nvertex _u_ as source (shown in black in the next step) using _ŵ_.\n\n![](fig/Fig-25-6-Johnsons-Example-d.jpg)\n\nTo the right is an example of one pass, starting with vertex 2.\n\nWithin each vertex _v_ the values δ̂(2, _v_) and δ(2, _v_) = δ̂(2, _v_) +\n_h_(2) − _h_(_u_) are separated by a slash.\n\nThe values for δ̂(_2_, _v_) were computed by running Dijkstra's algorithm with\nstart vertex 2, using the modified weights _ŵ_. But to get the correct path\nlengths in the original graph we have to map this back to _w_.\n\nOf course, node 2 is labeled \"0/0\" for δ̂(_2_, _2_) and δ(_2_, _2_),\nrespectively, because it costs 0 to get from a vertex to itself in any graph\nthat does not have negative weight cycles.\n\nThe cost to get to vertex 4 is 0 in the modified graph. To get the cost in the\noriginal graph, we reverse the adjustment that was done in computing _w_': we\nnow _subtract_ the source vertex weight _h_(2) = -1 (from figure above) and\n_add_ the target vertex weight _h_(4) = 0, so 0 − (-1) + 0 = 1. That is where\nthe \"1\" on node 4 came from.\n\nBut that example was for a path of length 1: let's look at a longer one. Node\n5 has \"2/-1\". Dijkstra's algorithm found the lowest cost path ((2, 4), (4, 1),\n(1, 5)) to vertex 5, at a cost of 2 using the edge weights _w_'. To convert\nthis into the path cost under edge weights _w_, we do _not_ have to subtract\nthe source vertex weight _h_(_u_) and add the target vertex weight _h_(_v_)\nfor every edge on the path, because it is a telescoping sum. We only have to\nsubtract the source vertex weight _h_(2) = -1 for the start of the _path_ and\nadd the target vertex weight _h_(5) = -4 for the end of the path.\n\nThus δ(5) = δ̂(5) − _h_(2) + _h_(5) = 2 − (-1) + (-4) = -1.\n\nSimilarly, the numbers after the \"/\" on each node are δ(_v_) in the original\ngraph: these are the \"answers\" for the start vertex used in the given\nDijkstra's run. We collect all these answers in matrix _D_ across all\nvertices.\n\n![](fig/algorithm-Johnson.jpg)\n\n### Time\n\nΘ(_V_) to compute _G'_; O(_V E_) to run Bellman-Ford; Θ(_E_) to compute _ŵ_;\nand Θ(_V_2) to compute _D_; but these are all dominated by **O(_V E_ lg _V_)**\nto run Dijkstra |_V_| times with a binary min-heap implementation.\n\nNot surprisingly, this is the same as iterated Dijkstra's, but it will handle\nnegative weights.\n\nAsymptotic performance can be improved to O(_V_2 lg _V_ \\+ _V E_) using\nFibonacci heaps.\n\n  \n\n* * *\n\n##  Dynamic Programming Approaches and Matrix Multiplication\n\nWe should also be aware of dynamic programming approaches to solving all-pairs\nshortest paths. We already saw in [Topic 18](http://www2.hawaii.edu/~suthers/c\nourses/ics311s14/Notes/Topic-18.html#optimal) that any subpath of a shortest\npath is a shortest path; thus there is optimal substructure. There are also\noverlapping subproblems since we can extend the solution to shorter paths into\nlonger ones. Two approaches differ in how they chararacterize the recursive\nsubstructure.\n\nCLRS first develop a dynamic programming solution that is similar to matrix\nmultiplication. Matrices are a natural representation for all-pairs shortest\npaths as we need O(_V_2) memory elements just to represent the final results,\nso it isn't terribly wasteful to use a non-sparse graph representation\n(although for very large graphs once can use a sparse matrix representation).\n\n### Optimal Substructure\n\nA shortest path _p_ between distinct vertices _i_ and _j_ can be decomposed\ninto a shortest path from _i_ to some vertex _k_, plus the final edge from _k_\nto _j_. In case that _i_ is directly connected to _j_, then _k_=_j_ and we\ndefine the length of a shortest path from a vertex to itself to be 0.\n\n### Extending Shortest Paths\n\nThis dynamic programming approach builds up shortest paths that contain at\nmost _m_ edges. For _m_ = 0, all the shortest paths from vertices to\nthemselves are of length 0; and others are infinite. For _m_ = 1, the\nadjacency matrix gives the shortest paths between an pair of vertices _i_ and\n_j_ (namely, the weight on the edge between them). For _m_ > 1, an algorithm\nis developed that takes the minimum of paths of length _m_−1 and those that\ncan be obtained by extending these paths one more step via an intermediate\nvertex _k_.\n\nWe will leave the details to the text, but it turns out that this algorithm\nfor extending paths one step has structure almost identical to that for\nmultiplying square matrices. The operations are different (min instead of\naddition, addition instead of multiplication), but the structure is the same.\nBoth algorithms have three nested loops, so are O(_V_3).\n\nAfter |_V_|−1 extensions, the paths will not get any shorter (assuming no\nnegative weight edges), so one can iterate the path extending algorithm\n|_V_|−1 times, for an O(_V_4) algorithm overall: not very efficient.\n\nHowever, the path extension algorithm, like matrix mutliplication, is\nassociative, and we can use this fact along with the fact that results won't\nchange after |_V_|−1 extensions to speed up the algorithm. We modify it to be\nlike **_repeated squaring_**, essentially multiplying the resulting matrix by\nitself repeatedly. Then one needs only lg(_V_) \"multiplications\" (doubling of\npath length) to have paths longer than |_V_|, so the runtime overall is O(_V_3\nlg _V_).\n\nBut we can do better with a different way of characterizing optimal\nsubstructure; one that does not just extend paths at their end, but rather\nallows two paths of length greater than 1 to be combined.\n\n* * *\n\n##  Floyd-Warshall: Dynamic Programming for Dense Graphs\n\nThe textbook first develops a more complex version of this algorithm that\nmakes multiple copies of matrices, and then notes in exercise 25.2-4 that we\ncan reduce space requirements by re-using matrices. Here I go directly to that\nsimpler version.\n\n### Dynamic Programming Analysis\n\nAssume that _G_ is represented as an adjacency matrix of weights _W_ =\n(_wij_), with vertices numbered from 1 to _n_.\n\n![](fig/W-matrix-definition.jpg)\n\nWe have _**optimal substructure**_ because subpaths of shortest paths are\nshortest paths ([previous lecture](http://www2.hawaii.edu/~suthers/courses/ics\n311s14/Notes/Topic-18.html#optimal)), and we have _**overlapping\nsubproblems**_ because a shortest path to one vertex may be extended to reach\na further vertex. We need the recursive structure that exploits this.\n\nThe subproblems are defined by computing, for 1 ≤ _k_ ≤ |_V_|, the shortest\npath from each vertex to each other vertex that uses _only_ vertices from {1,\n2, ..., _k_}. That is:\n\n  * first find the shortest paths from each _i_ to each _j_ that go through no vertices (i.e., the direct edges);\n  * then find the shortest paths from each _i_ to each _j_ that go either direct or only via vertex 1;\n  * then find the shortest paths from each _i_ to each _j_ that go either direct or only via vertices 1 and 2; ...\n  * ... and so on until we are considering solutions via all vertices.\n\nImportantly, each step we can use what we just computed in the previous step,\nconsidering whether the _k_th vertex improves on paths found using vertices {1\n... _k_-1}. This is what enables us to leverage dynamic programming's ability\nto save and re-use solutions to subproblems.\n\nThe basic insight is that the shortest path from vertex _i_ to vertex _j_\nusing only vertices from {1, 2, ..., _k_} is either:\n\n  * the shortest path _p_ from vertex **_i_** to vertex **_j_** using only vertices from {1, 2, ..., _k_−1}, or \n  * a path _p_ composed of the shortest path _p1_ from vertex **_i_** to vertex **_k_** using only vertices from {1, 2, ..., _k_−1} and the shortest path _p2_ from vertex **_k_** to vertex **_j_** using only vertices from {1, 2, ..., _k_−1}\n\n![](fig/Fig-25-3-Structure-Shortest-Paths.jpg)\n\nThis way of characterizing optimal substructure allows the Floyd-Warshall\nalgorithm to consider more ways of combining shortest subpaths than the\nmatrix-multiplication-like algorithm did.\n\n###  Algorithm\n\nThis leads immediately to the classic Floyd-Warshall algorithm (as presented\nin excercise 25.2-4 and its public solution, as well as many other texts):\n\n![](fig/algorithm-Floyd-Warshall-Prime.jpg)\n\n### Run Time Analysis\n\n_It's trivial; you tell me._\n\n### Constructing the Shortest Paths\n\nAlthough one can infer the shortest paths from the final weight matrix _D_, it\nis perhaps more straightforward to maintain a matrix of predecessor pointers\njust like we maintain predecessor pointers on individual vertices in the\nsingle-source version of shortest paths.\n\nWe update a matrix Π that is the same dimensions as _D_, and each entry π_i,j_\ncontains the predecessor of vertex _j_ on a shortest path from _i_ (the\npredecessor on shortest paths from other vertices may differ).\n\nThe CLRS textbooks presentation shows us making a series of matrices Π(0) ...\nΠ(_n_), but as with the weight matrix _D_ we can actually do this in one\nmatrix Π, and we can understand the superscripts (0) ... (_n_) as merely\nrepresenting states of this matrix.\n\n### Example\n\nExamples of Floyd-Warshall, like of other dynamic programming problems, are\ntedious to work through. I invite you to trace though the example in the text,\nfollowing the algorithm literally, and be prepared to do another example on\nhomework. I won't talk through it here.\n\n![](fig/Fig-25-4-Floyd-Warshall-Example-0.jpg)  \n![](fig/Fig-25-4-Floyd-Warshall-Example-1.jpg)  \n![](fig/Fig-25-4-Floyd-Warshall-Example-2.jpg)  \n![](fig/Fig-25-4-Floyd-Warshall-Example-3.jpg)  \n![](fig/Fig-25-4-Floyd-Warshall-Example-4.jpg)  \n![](fig/Fig-25-4-Floyd-Warshall-Example-5.jpg)  \n  \n\n* * *\n\n##  Transitive Closure\n\nSuppose we have a graph _G_ and we want to compute the **transitive closure**\n\n> _G*_ = (_V_, _E*_) of _G_, where   (_u_, _v_) ∈ _E*_   _**iff**_   ∃ path\nfrom _u_ to _v_ in _G_.\n\nWe can do this by assigning a weight of 1 to each edge, running the above\nalgorithm, and then concluding there is a path for any (_i_, _j_) that have\nnon-infinite path cost.\n\nIf all we care about is transitivity rather than path length, we can reduce\nspace requirements and possibly speed up the algorithm by representing all\nedges as boolean values (1 for connected; 0 for not connected), and then\nmodify Floyd-Warshall to use boolean OR rather than min and AND rather than\naddition. This reduces the space requirements from one numeric word to one bit\nper edge weight, and may be faster on machines for which boolean operations\nare faster than addition. See text for discussion.\n\n* * *\n\nDan Suthers Last modified: Fri Apr 11 02:23:25 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//190.all-pairs-shortest-paths/reading-notes.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-19a",
 "morea_summary"=>"Introduction.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=Ku24z4v9YF8",
 "morea_labels"=>["Screencast", "Suthers", "10 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/190.all-pairs-shortest-paths/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//190.all-pairs-shortest-paths/reading-screencast-a.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Johnson's algorithm for all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-19b",
 "morea_summary"=>"Johnson's algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=z0reAeK4kl0",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/190.all-pairs-shortest-paths/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//190.all-pairs-shortest-paths/reading-screencast-b.md"}
</pre>

<h2>/morea/190.all-pairs-shortest-paths/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Floyd-Warshall algorithm for all pairs shortest paths",
 "published"=>true,
 "morea_id"=>"reading-screencast-19c",
 "morea_summary"=>"Floyd-Warshall's algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=dzf7x5Z8Ui8",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/190.all-pairs-shortest-paths/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//190.all-pairs-shortest-paths/reading-screencast-c.md"}
</pre>

<h2>/morea/200.maximum-flow/experience-2.html</h2>

<pre>Hash
{"title"=>"Experience maximum flow (again)",
 "published"=>true,
 "morea_id"=>"experience-maximum-flow-2",
 "morea_type"=>"experience",
 "morea_summary"=>"maximum flow",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/200.maximum-flow/experience-2.html",
 "url"=>"/morea/200.maximum-flow/experience-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n#### 2\\. (10 pts) Flow with Edmunds-Karp\n\n**(a)** (8 pts) **Run Edmunds-Karp on the flow network shown below with **s** as the source and **t** as the sink.** Following the template below, draw the flow graph on the left hand side of the page with flows and capacities labeled as shown, and the residual graph on the right hand side of the page with capacities shown. (The first line is already done for you up to this point.) Then on each pass: \n\n  1. Mark the shortest augmenting path in the residual graph with a thick line or circles.\n  2. Use this augmenting path to update flow in the original graph on the left hand side of the next line.\n  3. Draw the updated residual graph on the right in that next line.\n  4. Repeat at 1, redrawing both updated graphs on a new line until you can't find an augmenting path.\n\nPlease use the visual templates provided to make this easier to grade (.jpg,\n.graffle, .vdx. and .svg versions of templates are available.)\n\n       _**_G_ (flow network)**_\n       \n       _**_G__f_ (residual network)**_\n\n![](fig/Problem-Set-12-HW-Flow-Graph-Start.jpg)\n\n       \n![](fig/Problem-Set-12-HW-Residual-Graph-Start.jpg)\n\n\n\n       \n\n\n![](fig/Problem-Set-12-HW-Flow-Graph-Template.jpg)\n\n       \n![](fig/Problem-Set-12-HW-Residual-Graph-Template.jpg)\n\n\n\n       \n\n\n                ... repeat as needed ...\n       \n                ... repeat as needed\n\nWhen you can't update the graph any more (be sure you can say on an exam _why_\nyou can't update it any more!), answer the following:\n\n**(b)** (1 pt) Write down the value of the flow |_f_| that was achieved. \n\n**(c)** (1 pt.) Draw a line in the final graph showing the min cut that corresponds to this max flow. (Be careful: all outgoing edges your line crosses must be filled to capacity.) \n\n* * *\n\n#### 3\\. (8 pts) Reducing Bipartite Matching to Flow Maximization\n\n![](fig/Problem-Set-12-Matching-Flow-Graph.jpg)\n\nIn this problem you will solve a bipartite matching problem by converting it\nto a flow maximization problem and then to a linear program. This shows the\npower of \"problem reduction\": converting a problem of one type into a\ndifferent problem for which you have a problem solving tool.\n\nTo the right is a **bipartite matching problem** (page 732). Adele will dance\nwith William and Xavier; Betty will dance with William and Yuhan, and Cindy\nwill dance with Xavier and Zachary. We want to maximize the dancing pairs.\n\n**(a)** (2 pts) Convert this problem to a maximum flow problem by adding vertices and specifying edge capacities (page 733). \n\n**(b)** (4 pts) Convert the maximum flow problem to a linear program. _NOTE:_ Do not just write down the general formulas on page 860. Write out the specific formulas for each edge and vertex, using _f_AW etc. Specifically, you will write:\n\n  * (1 pt) The expression to be maximized (use literal expressions, no Σ notation).\n  * (2 pts) The capacity constraints: one equation per each edge.\n  * (2 pts) The flow conservation constraints: one equation per each vertex other than _s_ and _t_.\n  * (1 pt) The nonnegativity constraints for each edge: here you may use abbreviated notation.\n\n_Comment:_ Although linear programming has great generality, now you can see\nwhy it might be easier to just run Ford-Fulkerson. However, linear programming\nmay be easier and perhaps even necessary when there are other constraints,\nsuch as in the problem below.\n\n* * *\n\n#### 4\\. (2 pts) Minimum Cost Multi-Commodity Flow\n\nOn page 861, CLRS discuss **minimum-cost flow,** in which different edges\ncharge different costs per unit of flow and we wish to minimize cost. On page\n862, CLRS discuss **multicommodity flow,** in which we want to transport\nmultiple kinds of commodities from different sources and targets over the same\nnetwork with delivery guarantees, and ask whether a flow that meets these\nguarantees exists. The only known algorithms for solving this problem are\nlinear programming solutions (we can no longer handle it with a modified graph\nand Edmunds-Karp).\n\nThe **minimum-cost multi-commodity flow problem** is the combination of these\nproblems:\n\n> Given a graph _G_ = (_V_, _E_),\n\n>\n\n>   * Each edge (_u_, _v_) ∈ _E_ has _capacity_ _c_(_u_, _v_) ≥ 0 and _cost_\n_a_(_u_, _v_) > 0\\.\n\n>   * We wish to transport _k_ _commodities_ _K_1, _K_2, .... _K__k_ over the\nnetwork\n\n>   * Each commodity _K__i_ = (_s__i_, _t__i_, _d__i_) must be transported\nfrom a commodity-specific _start vertex_ _s__i_ to _target vertex_ _t__i_, and\nwe need a _delivery guarantee_ that _d__i_ units are delivered.\n\n>   * The _aggregate flow_ _f__u,v_ on edge (_u_, _v_) is the sum of the flow\nfor each commodity _f__i_.\n\n>   * A solution is _feasible_ if _f__u,v_ ≤ _c_(_u_, _v_).\n\n>   * We want to meet the delivery demands _d__i_ _and_ minimize the cost of\nthe flow, which is the sum of the cost time aggregate flow for each edge:\n\n>\n\n>> Σ_u,v_∈_V_ _a_(u_,_ _v_) _f__u,v_\n\n**How can the solutions in CLRS be combined to solve the minimum-cost multi-commodity flow problem?** (It's easier than this looks at first.) \n\n* * *\n\nDan Suthers Last modified: Sat Apr 19 02:16:44 HST 2014\n\n",
 "path"=>"morea//200.maximum-flow/experience-2.md"}
</pre>

<h2>/morea/200.maximum-flow/experience.html</h2>

<pre>Hash
{"title"=>"Experience maximum flow",
 "published"=>true,
 "morea_id"=>"experience-maximum-flow",
 "morea_type"=>"experience",
 "morea_summary"=>"maximum flow",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/200.maximum-flow/experience.html",
 "url"=>"/morea/200.maximum-flow/experience.html",
 "content"=>
  "## Flow \n\n**1.** (2 pts) Suppose that _f_1 and _f_2 are flows in a network _G_ (NOT in the residual graph). Assume that _f_1 and _f_2 individually satisfy the conservation property and capacity constraints, and we computed the augmented flow _f_1↑_f_2. \n\n**(a)** Does this augmented flow _f_1↑_f_2 necessarily satisfy the conservation property? Show why (proof) or why not (counter-example).\n\n**(b)** Does _f_1↑_f_2 necessarily satisfy the capacity constraint? Show why (proof) or why not (counter-example).\n\n**(c)** How does finding augmented flows in the residual graph _G__f_ prevent the problem you identified in one of the above questions?\n\n**2.** (3 pts) Professor Addams' children, Pugsley and Wednesday, dislike each other intensely. When walking to school (they go to the same school), they refuse to walk down any block that the other child has walked down, though strangely they have no problem crossing the same corner. Professor Addams wants to figure out how to get his children to school. Fortunately both their house and the school are on corners. The professor has a map of his town. Show how to formulate the problem of determining whether both of his children can go to the same school as a maximum flow problem. \n\n_Note: If you answer with either \"yes they can go to school\" or \"no they\ncannot go to school\" you will get 0 points for this question, as you cannot\nsolve the problem without the map. You are showing the Professor how to model\nthe problem as a flow problem so that he can solve it using his map._\n\n#### If you have more time, discuss this problem:\n\n**3.** Suppose that, in addition to edge capacities, a flow network has **vertex capacities**. We will extend the capacity function _c_ to work on vertices as well as edges: _c_(_v_) gives the amount of flow that can pass through _v_. How would you transform a flow network _G_=(_V_,_E_) with vertex capacities into an equivalent flow network _G_*=(_V*_,_E*_) and a _c*_ that is defined only on edges (without vertex capacities) such that a maximum flow in _G*_ has the same value as a maximum flow in _G_.   \n\n\n",
 "path"=>"morea//200.maximum-flow/experience.md"}
</pre>

<h2>/morea/200.maximum-flow/module.html</h2>

<pre>Hash
{"title"=>"Maximum flow",
 "published"=>true,
 "morea_id"=>"maximum-flow",
 "morea_outcomes"=>["outcome-maximum-flow"],
 "morea_readings"=>
  ["reading-screencast-20a",
   "reading-screencast-20b",
   "reading-screencast-20c",
   "reading-cormen-26",
   "reading-notes-20"],
 "morea_experiences"=>["experience-maximum-flow", "experience-maximum-flow-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/200.maximum-flow/logo.png",
 "morea_sort_order"=>200,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/200.maximum-flow/module.html",
 "content"=>
  "Flow networks, maximum flow problem, Ford-Fulkerson algorithm, Edmonds-Karp algorithm, maximum bipartite matching",
 "path"=>"morea//200.maximum-flow/module.md"}
</pre>

<h2>/modules/maximum-flow/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Maximum flow",
 "url"=>"/modules/maximum-flow/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/maximum-flow/index.html"}
</pre>

<h2>/morea/200.maximum-flow/outcome.html</h2>

<pre>Hash
{"title"=>"Understand maximum flow",
 "published"=>true,
 "morea_id"=>"outcome-maximum-flow",
 "morea_type"=>"outcome",
 "morea_sort_order"=>200,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/200.maximum-flow/outcome.html",
 "content"=>"Understand when, why, and how to use the maximum flow algorithm.",
 "path"=>"morea//200.maximum-flow/outcome.md"}
</pre>

<h2>/morea/200.maximum-flow/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 26 - Maximum flow (26.1 - 26.3)",
 "published"=>true,
 "morea_id"=>"reading-cormen-26",
 "morea_summary"=>
  "Flow networks, Ford-Fulkerson method, Maximum bipartite matching.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "29 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/200.maximum-flow/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//200.maximum-flow/reading-cormen.md"}
</pre>

<h2>/morea/200.maximum-flow/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on maximum flow",
 "published"=>true,
 "morea_id"=>"reading-notes-20",
 "morea_summary"=>
  "Flow networks, maximum flow problem, Ford-Fulkerson algorithm, Edmonds-Karp algorithm, maximum bipartite matching",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/200.maximum-flow/reading-notes.html",
 "url"=>"/morea/200.maximum-flow/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Flow Networks and Maximum Flow Problem \n  2. Ford Fulkerson Method and Concepts\n  3. Ford Fulkerson Algorithm\n  4. Edmonds-Karp Algorithm (FF on a leash)\n  5. Maximum Bipartite Matching\n\n##  Flow Networks and Maximum Flow Problem\n\nMany problems involve modeling flow through networks, to maximize flow or look\nfor vulnerabilities.\n\nIncludes liquids through pipes, materials through transportation networks, and\ncommunication networks.\n\nFlow algorithms also have applications to problems that don't look like flow,\nsuch as scheduling.\n\n### Flow Networks\n\nA **flow network** is a directed graph _G_ = (_V_, _E_) where each edge (_u_,\n_v_) has a **capacity** _c_(_u_, _v_) ≥ 0, and:\n\n  * If (_u_, _v_) ∉ _E_ then _c_(_u_, _v_) = 0.\n  * If (_u_, _v_) ∈ _E_ then reverse edge (_v_, _u_) ∉ _E_. [*]\n  * A vertex _s_ is designated as the **source vertex**.\n  * A vertex _t_ is designated as the **sink vertex** (or _t_ for \"target\").\n\nComments:\n\n  * [*] Can work around this restriction. (Also, some authors _require_ that (_v_, _u_) ∈ _E_, but this is because they are using _G_ simultaneously for what we will do with two graphs.)\n  * We assume that each vertex _v_ lies on a path from source _s_ to sink _t_. (A _v_ that does not will not participate in any flow, so can be ignored.) \n\nA simple example: the trucking capacity network:\n\n![](fig/Fig-26-1-a-Flow-Network.jpg)\n\n### Flow (Not Csikszentmihalyi's!)\n\nA **flow** for a network is a function _f_ : _V_ x _V_ -> ℜ _ (that is, _f_\nassigns numbers to edges)_ satisfying:\n\n  * **Capacity Constraint**: ∀ _u_, _v_ ∈ _V_,  0   ≤   _f_(_u_, _v_)   ≤   _c_(_u_, _v_).   \n_(Can't push more over an edge than its capacity.)_\n\n  \n\n  * **Flow Conservation**: ∀ _u_ ∈ _V_ \\- {_s_, _t_},  \n\n![](fig/equation-flow-conservation.jpg)  \n\n_(Flow into a vertex must equal flow out of it, except for the source and\nsink.)_\n\nExample with flow/capacity:\n\n![](fig/Fig-26-1-b-Flow.jpg)\n\n_Let's check flow conservation in this network. Also, are we making maximum\nuse of the network? How can we improve it?_\n\n#### Value of Flow\n\nThe value of flow _f_ = |_f_| is the flow out of source minus the flow into\nthe source:\n\n![](fig/equation-flow-value.jpg)\n\n_What is the value of flow in the example above?_\n\n###  Excluded Variations\n\nOur formulation disallows **anti-parallel edges:**\n\n![](fig/Fig-26-2-a-Antiparallel-Edges.jpg)\n\nFortunately they are easy to eliminate. _How would you do it? Why not just\nsubtract 4 from 10 to get 6? Click on image to see an alternate solution._\n\nWe also require that there be a single source and sink. We can easily convert\nnetworks with multiple sources and sinks:\n\n![](fig/Fig-26-3-Conversion-to-Single-Source-Sink.jpg)\n\n### Maximum Flow Problem\n\nGiven _G_, _s_, _t_, and _c_, find a flow _f_ whose value is maximum.\n\n* * *\n\n## Cuts and Flow\n\nWe take a brief diversion into some relevant graph theory.\n\nA **cut** (_S_, _T_) of a flow network _G_ = (_V_, _E_) is a partition of _V_\ninto _S_ and _T_ = _V_ \\- _S_ such that _s_ ∈ _S_ and _t_ ∈ _T_.\n\n![](fig/Fig-26-5-Cut-Example.jpg)\n\nHere is an example of a cut:\n\nThe **net flow** across cut (_S_, _T_) for flow _f_ is:\n\n![](fig/equation-flow-across-cut.jpg)\n\nThe **capacity** of cut (_S_, _T_) is:\n\n![](fig/equation-cut-capacity.jpg)\n\n_What is the net flow for the cut in this example? The capacity?_\n\nNote the **_assymetry between net flow and capacity of cut_**: For capacity,\ncount only edges going from _S_ to _T_, ignoring those in reverse direction.\nFor net flow, count flow on all edges across the cut: flow on edges from _S_\nto _T_ minus flow on edges from _T_ to _S_. _Why does this assymetry make\nsense?_\n\n###  Examples\n\nConsider the cut _S_ = {_s_, _w_, _y_}, _T_ = {_x_, _z_, _t_} in the network\nshown.\n\n![](fig/equations-cut-example-1.jpg) ![](fig/example-flow-network-2-with-flow-and-cut1.jpg)\n\nNow consider the cut _S_ = {_s_, _w_, _x_, _y_}, _T_ = {_z_, _t_}.\n\n![](fig/equations-cut-example-2.jpg) ![](fig/example-flow-network-2-with-flow-and-cut2.jpg)\n\nWe get the same flow as the previous cut, but higher capacity. It is not an\naccident that changing cut can change capacity but not flow. _Can you explain\nwhy?_\n\n#### Minimum Cut\n\nA **minimum cut** of _G_ is a cut whose capacity is minimum over all cuts of\n_G_.\n\n###  Useful Facts\n\nThe proofs of these are straightforward but involve long manipulations of\nsummations: see text.\n\n![](fig/lemming.jpg)\n\n#### Lemma\n\nFor any cut (_S_, _T_),   _f_(_S_, _T_)   =   |_f_|  \n_(the net flow across any cut equals the value of the flow)._\n\nThe intuition is that no matter where you cut the pipes in a network, you'll\nsee the same flow volume coming out of the openings. If you did not,\nconservation would be violated at some nonempty subset of the vertices.\n\n#### Corollary\n\nThe value of any flow ≤ capacity of any cut.\n\nThis is again intuitive under the plumbing analogy: if it were false, you\ncould push more flow through the pipes than they can hold.\n\n* * *\n\n##  Ford Fulkerson Method and Concepts\n\nThis is a method, not an algorithm, because there are many ways to do it.\n\n![](fig/code-Ford-Fulkerson-Method.jpg)\n![](fig/Fig-26-1-b-Flow.jpg)\n\nThe intuition behind this method is simple: Find a pathway (an _augmenting\npath_) of unused capacity and increase the flow along that pathway. Repeat\nuntil no such pathways are found.\n\nWhat makes this nontrivial is an apparent paradox: overall flow can sometimes\nbe increased by decreasing flow along certain edges (because they flow in the\n\"wrong\" direction or move capacity to a part of the network that can't handle\nit as well).  \n_ See whether you can find an example in the graph shown._\n\nFord Fulkerson manages this by constructing a parallel network of the\navailable or _residual_ capacity. We will return to the method after\nexplaining these concepts.\n\n###  Residual Network\n\nGiven a flow _f_ in a network _G_ = (_V_, _E_), consider a pair of vertices\n_u_, _v_ ∈ _V_. How much additional flow can we push directly from _u_ to _v_?\nThis is the **residual capacity**:\n\n![](fig/equation-residual-capacity.jpg)\n\nThe first case says that if we have not used the full capacity _c_(_u_, _v_)\nof an edge (_u_, _v_) in _E_ then we can increase it by the difference.\n\nThe second case says that if we are using _f_(_v_, _u_) of the capacity of\n(_v_, _u_) in _E_ then we have the residual \"capacity\" of reversing\n(cancelling) that much flow in the reverse direction (_u_, _v_) (_notice that\nthe letters are swapped_).\n\nOtherwise there is no residual capacity between _u_ and _v_.\n\nWe record these capacities in the **residual network** _Gf_ = (_V_, _Ef_),\nwhere\n\n> **_Ef_** = {(_u_, _v_) ∈ _V_ x _V_ : _cf_(_u_, _v_) > 0}.\n\nEach edge of the residual network can admit a positive flow.\n\n#### Example\n\nA flow network is on the left, and its residual network on the right.\n\n![](fig/example-flow-network-2-with-flow.jpg) ![](fig/example-flow-network-3-residual.jpg)\n\nFor example, _G__f_ says that we can add two more units from _s_ to _w_ in _G_\nor we can take one unit back. _(Take a little time to understand the\nrelationship between the two graphs: it's critical, so don't go on until you\ndo!)_\n\nEvery edge (_u_, _v_) ∈ _Ef_ corresponds to (_u_, _v_) ∈ _E_ or (_v_, _u_) ∈\n_E_ or both. So, |_Ef_| ≤ 2 |_E_|\n\nA residual network is similar to a flow network, except that it may contain\nantiparallel edges.\n\nWe can define flow in a residual network that satisfies the definition of\nflow, but with respect to _cf_ in _Gf_.\n\n###  Augmentation and Augmenting Paths\n\nGiven flows _f_ in _G_ and _f '_ in _Gf_, define the **augmentation** of _f_\nby _f '_,   ** _f_ ↑ _f '_**,   to be a function _V_ x _V_ -> ℜ:\n\n![](fig/equation-augmentation-flow.jpg)\n\nfor all _u_, _v_ ∈ _V_.\n\n**_In English:_** Increase the flow on (_u_, _v_) by _f '_(_u_, _v_), but _decrease_ it by _f '_(_v_, _u_) because pushing flow on the reverse edge in the residual network decreases the flow in the original network.\n\n![](fig/lemming.jpg)\n\n#### Another Lemma\n\nGiven flow network _G_, flow _f_ in _G_, and residual network _Gf_, let _f '_\nbe a flow in _Gf_. Then _f_ ↑ _f '_ is a flow in _G_ with value   |_f_ ↑ _f\n'_|   =   |_f_| + |_f '_|.\n\n_(A proof with lots of summations in the CLRS book shows that the capacity\nconstraint and flow conservation properties are met, and demonstrates that the\nvalue of _f_ ↑ _f '_ is correct. The proof is easy to follow but more than I\nwant to write here; see CLRS.)_\n\n#### Augmenting Paths\n\nAny simple path _p_ from _s_ to _t_ in _Gf_ is an **augmenting path**.\n\nAugmenting paths admit more flow along each edge in _Gf_ (because all the\nedges have positive capacity).\n\nHow much more flow can we push from _s_ to _t_ along an augmenting path _p_?\nThe \"weakest link\" principle applies:\n\n> _cf_ (_p_) = min{_cf_(_u_, _v_) : (_u_, _v_) is on _p_}.\n\n#### Example\n\nHere is a flow network (left) and a residual network (right).\n\n![](fig/example-flow-network-2-with-flow.jpg) ![](fig/example-flow-network-3-residual.jpg)\n\nConsider the augmenting path _p_ = ⟨_s_, _w_, _y_, _z_, _x_, _t_⟩ in _Gf_. The\nminimum residual capacity of this path is ..._what???_\n\n![](fig/example-flow-network-4-augmented.jpg)\n\nPush that much additional flow along _p_ in _G_. Notice that the path in _Gf_\ngoes over _G_'s edge (_y_, _w_) in the reverse direction, so we subtract that\nmuch flow from the edge in _G_.\n\nAs a result, edge (_y_, _w_) has _f_(_y_, _w_) = 0, so we omit the flow,\nshowing only _c_(_y_, _w_) = 3 in the revised _G_ to the left.\n\n![](fig/example-flow-network-5-residual.jpg)  \n\nNow let's update the residual network _Gf_. _Make sure you understand how we\ngot the graph to the right before going on._\n\n_Is there an augmenting path in Gf?. How can we tell?_\n\nNotice that no edges cross the cut ({_s_, _w_}, {_x_, _y_, _z_, _t_}) in the\nforward direction in _Gf_, so no path can get from _s_ to _t_.\n\nSince no further augmentation is possible, we claim that the flow shown in G\nis a maximal flow. This theorem tells us we are right.\n\n### Max-Flow Min-Cut Theorem (Important!)\n\nThe **following are equivalent** (see text for lemma, corollary and proof):\n\n\n  1. _f_ is a maximum flow\n  \n\n  2. _Gf_ has no augmenting path\n  \n\n  3. |_f_| = _c_(_S_, _T_) for some cut (_S_, _T_).\n\n\nThis means that if (2) we can't find augmenting paths _or_ (3) have achieved a\nflow equivalent to the capacity of some cut, then we are done: (1) we have\nfound the maximum flow.\n\n(3) also lets us predict what the max flow will be: it will be equal to the\ncapacity of the _minimum_ cut (as measured by capacity). Hence **\"max flow is\nmin cut\"**.\n\n* * *\n\n##  Ford Fulkerson Algorithm\n\nIntuition: keep augmenting flow along an augmenting path until there is no\naugmenting path. The flow attribute is represented using dot notation on\nedges: (_u_, _v_)._f_.\n\n![](fig/code-Ford-Fulkerson-simple.jpg)\n\nor in more detail\n\n![](fig/code-Ford-Fulkerson.jpg)\n\n(Line 7: adding flow. Line 8: reducing flow.)\n\n#### Analysis\n\nRuntime depends on what costs can be, and method used to find paths.\n\nBest to use integer weights when possible. (If capacities are irrational\nnumbers, Ford-Fulkerson might never terminate!)\n\nThe initialization lines 1-2 is O(_E_).\n\nThe cost to find a path _p_ from _s_ to _t_ in line 3 depends on the method\nused. Breadth-First-Search or Depth-First-Search will work, and these are\nO(_V_ \\+ _E_). This is a connected graph, so |_E_| ≥ |_V_| − 1, so this\nreduces to O(_E_).\n\nThe rest of the work in the `while` loop is of lower complexity, so the work\nof each pass of the `while` loop is O(_E_).\n\nHow many times will the `while` loop run? The worst case scenario is:\n\n  * If all capacities are integer, each augmenting path raises |_f_| ≥ 1.\n  * In the worst case, it is possible for each augmenting path to raise |_f_| by _only_ 1, so if the maximum flow is _f*_ then _f*_ iterations may be needed. \n  * Each iteration costs _E_, so worst case is **O(_E_ _f*_)**.\n\n![](fig/example-FF-worst-case.jpg)\n\nThe example to the right illustrates the classic worst case scenario. One\ncould:\n\n  * find augmenting path ⟨(A,B), (B,C), (C,D)⟩, increasing flow by 1 to 1; then\n  * find augmenting path ⟨(A,C), (C,B), (B,D)⟩ _(notice that this uses the antiparallel edge in the residual graph to subtract flow from (B,C))_, increasing flow by 1 to 2; then \n  * find augmenting path ⟨(A,B), (B,C), (C,D)⟩, increasing flow by 1 to 3; then\n  * find augmenting path ⟨(A,C), (C,B), (B,D)⟩, increasing flow by 1 to 4; then ...\n\n... requiring 2000 iterations due to the unlucky choice of augmenting paths.\n\n* * *\n\n##  Edmonds-Karp Algorithm (FF on a leash)\n\nEdmonds-Karp come to the rescue with an insight that controls the order in\nwhich Ford-Fulkerson explores paths.\n\nNotice that in the example above, if the shortest paths (by number of edges,\nnot considering weight) are considered first, then the anomaly does not occur.\nWe would find augmenting path ⟨(A,B), (B,D)⟩ to increase flow by 1000, then\nfinish the job with augmenting path ⟨(A,C), (C,D)⟩, or find the second and\nthen the first.\n\nEdmonds-Karp is the Ford-Fulkerson algorithm but with the constraint that\naugmenting paths are computed by Breadth-First Search of _Gf_. (_ I told you\nthat those search algorithms are widely useful!_)\n\nA proof in the CLRS text shows that the number of flow augmentations performed\nby Edmunds-Karp is O(_V__E_). Since each BFS is still O(_E_) in a connected\ngraph, Edunds-Karp runs in **O(_V_ _E_2)** time. The proof in CLRS works by\nbounding distances to vertices in _Gf_.\n\nEven better bounds are possible: this has been a very active area of algorithm\ndevelopment. Sections 26.4-26.5 of CLRS describe **push-relabel** algorithms\nthat are as fast as O(_V_3). The notes at the end of the chapter discuss\nfaster algorithms.\n\nThere are many variations of Maximum Flow, such as including multiple sources\nand sinks; including costs and trying to minimize cost; including different\nkinds of material that take different capacities to transport; etc. Some can\nbe very difficult to solve.\n\n* * *\n\n##  Maximum Bipartite Matching\n\nMaximum Flow can also be used to solve problems that don't look like flow\nproblems. Here is an example.\n\nSuppose we want to maximize ...\n\n  * The number of boys and girls who can dance, given a list of who is willing to dance with whom (a.k.a. the \"marriage problem\").\n  * The number of classes that can be scheduled, given a list of which classes can be held in which rooms.\n  * The number of tasks that can be performed by some machines, given that some tasks can only be performed by some of the machines. \n\n... etc. We make a **bipartite** graph _G_ = (_V_, _E_) where _V_ = _L_ ∪ _R_\nsuch that all edges go between _L_ and _R_.\n\nA **matching** is a subset of the edges _M_ ⊆ _E_ such that for all _v_ ∈ _V_,\nzero or one edges of _M_ are incident on _v_. (0: _v_ is **unmatched**; 1: _v_\nis **matched**; > 1 is not allowed.) Here is one example with two solutions:\n\n![](fig/Fig-26-8-a-Maximum-Bipartite-Matching-Examples.jpg)\n\nOn the left we can see a nonmaximal matching, and on the right a **maximum\nmatching**, or matching of maximum cardinality: |_M_| ≥ |_M'_| ∀ matchings\n_M'_.\n\n### Solution\n\nGiven _G_, define flow network _G'_ = (_V'_, _E'_):\n\n  * _V'_ = _V_ ∪ {_s_, _t_}.\n  * _E'_ = _E_ augmented with edges from _s_ to every _u_ ∈ _L_ and from every _v_ in _R_ to _t_. \n  * _c_(_u_, _v_) = 1 ∀ (_u_, _v_) ∈ _E'_.\n\nThen just run Ford-Fulkerson (Edumunds-Karp is not required, as all edges have\nunit value):\n\n![](fig/Fig-26-8-b-Maximum-Bipartite-Matching-Solution.jpg)\n\nThis works because a maximum flow must use the maximum number of (unitary\ncapacity) edges across the cut (_L_, _R_).\n\n###  Run Time Complexity\n\nPreviously we established that Ford-Fulkerson is O(_E_ _f_*).\n\nIn the present problem we are running Ford-Fulkerson on _E'_, but _E'_ =\nO(_E_) since we are adding no more than _V_ edges (to vertices in _L_ and\nvertices in _R_). Also, the flow value _f_* = O(_V_) since edges are of unit\nvalue and you can't have flow across more edges than there are in min(|_L_|,\n|_R_|) = O(_V_).\n\nTherefore, bipartite matching can be computed with Ford-Fulkerson in\n**O(_V__E_)**.\n\n* * *\n\n## Wrapup\n\nWe have just seen an example of **problem reduction**: reducing the maximum\nbipartite matching problem to a flow problem and using a flow algorithm to\nsolve it. Last week we saw another problem reduction: solving job scheduling\nby modeling it as a shortest-paths problem.\n\n**_Problem reduction is a common theme in computer science._** In Topic 21, we will see how the flow problem reduces to the linear programming problem. In later topics, we'll consider reduction of classes of problems known as \"P\" and \"NP\", and encounter the greatest unsolved problem in computer science.\n\n* * *\n\nDan Suthers Last modified: Sun Apr 13 00:50:36 HST 2014  \nMost images are from the instructor's material for Cormen et al. Introduction\nto Algorithms, Third Edition. The counter-example is from Goodrich & Tamassia,\nand I found the Lemma lemming running around loose on the Internet somewhere.  \n\n",
 "path"=>"morea//200.maximum-flow/reading-notes.md"}
</pre>

<h2>/morea/200.maximum-flow/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to maximum flow",
 "published"=>true,
 "morea_id"=>"reading-screencast-20a",
 "morea_summary"=>"Flow networks and the maximum flow problem",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=3Q0IJgyuVEM",
 "morea_labels"=>["Screencast", "Suthers", "12 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/200.maximum-flow/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//200.maximum-flow/reading-screencast-a.md"}
</pre>

<h2>/morea/200.maximum-flow/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Residuals, augmenting flows.",
 "published"=>true,
 "morea_id"=>"reading-screencast-20b",
 "morea_summary"=>
  "Residual graphs, augmenting flows, and the min cut max flow theorem",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=TDO4dwq4fKI",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/200.maximum-flow/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//200.maximum-flow/reading-screencast-b.md"}
</pre>

<h2>/morea/200.maximum-flow/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"Flow algorithms and applications",
 "published"=>true,
 "morea_id"=>"reading-screencast-20c",
 "morea_summary"=>"Ford-Fulkerson, Edmonds-Karp and Bipartite Matching.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=TTFOk0miZ6k",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/200.maximum-flow/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//200.maximum-flow/reading-screencast-c.md"}
</pre>

<h2>/morea/210.linear-programming/experience.html</h2>

<pre>Hash
{"title"=>"Experience linear programming",
 "published"=>true,
 "morea_id"=>"experience-linear-programming",
 "morea_type"=>"experience",
 "morea_summary"=>"linear programming",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/210.linear-programming/experience.html",
 "url"=>"/morea/210.linear-programming/experience.html",
 "content"=>
  "### Modeling Shortest-Paths Problems with Linear Programming:\n\n![](fig/Problem-Set-12-Shortest-Paths-Graph-for-LP.jpg)\n\n**1\\. Single-Source, Single-Destination (2 pts)**  \n_Write the explicit linear program for finding the shortest path from node\n**a** to node **e**_ in the graph above (single-source, single destination\nshortest paths). By \"explicit\" I mean _write all the inequalities and\nequations_ using variables _d_a, _d_b, _d_c, _d_d, and _d_e, and the actual\nnumbers for _w_(_u_,_v_), not the generic form on page 860. The first line is\nshown.\n\n**Maximize:**  \n  \n**Subject to constraints:**  \n\n        _d__b_ ≤ _d__a_ \\+ 1 \n\n_Do you see how this gets _shortest_ paths even though we are maximizing\ndistances?_\n\n**2\\. Single-Source, All-Destinations (1 pt) **  \n\n_How would you generalize the program you wrote above to solve the single\nsource shortest paths problem from **a** to **all vertices**_ in the graph\nabove? Write the part that changes.\n\n**3\\. Slack Form (2 pts)**   \nRewrite your equations and inequalities to be in slack form. Every variable\nyou used above should appear in every equation, even if they have 0\ncoefficients. Make all the coefficients explicit, including 0 and 1\ncoefficients (this will help you see what goes into the matrix). Use slack\nvariables _y_1, _y_2, ... in addition to the variables above. There should be\nonly one constant on the right hand side.\n\n-1_d__a_ \\+ 1_d__b_ \\+ 0_d__c_ \\+ 0_d__d_ \\+ 0_d__e_ \\+ _y_1 = 1\n\n**Got time? All-Pairs Shortest Paths:** How would you need to modify the linear program you wrote in part 1 and 2 to solve the all-pairs shortest paths problem in the graph above?\n\n\n",
 "path"=>"morea//210.linear-programming/experience.md"}
</pre>

<h2>/morea/210.linear-programming/module.html</h2>

<pre>Hash
{"title"=>"Linear programming",
 "published"=>true,
 "morea_id"=>"linear-programming",
 "morea_outcomes"=>["outcome-linear-programming"],
 "morea_readings"=>
  ["reading-sedgewick-5",
   "reading-sedgewick-38",
   "reading-cormen-29",
   "reading-notes-21"],
 "morea_experiences"=>["experience-linear-programming"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/210.linear-programming/logo.jpg",
 "morea_sort_order"=>210,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/210.linear-programming/module.html",
 "content"=>"Gaussian elimination, simplex method.",
 "path"=>"morea//210.linear-programming/module.md"}
</pre>

<h2>/modules/linear-programming/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Linear programming",
 "url"=>"/modules/linear-programming/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/linear-programming/index.html"}
</pre>

<h2>/morea/210.linear-programming/outcome.html</h2>

<pre>Hash
{"title"=>"Understand linear programming",
 "published"=>true,
 "morea_id"=>"outcome-linear-programming",
 "morea_type"=>"outcome",
 "morea_sort_order"=>210,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/210.linear-programming/outcome.html",
 "content"=>"Understand when, why, and how to use linear programming.",
 "path"=>"morea//210.linear-programming/outcome.md"}
</pre>

<h2>/morea/210.linear-programming/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 29 - Linear programming (29.0 - 29.3)",
 "published"=>true,
 "morea_id"=>"reading-cormen-29",
 "morea_summary"=>
  "Standard and slack forms, formulating problems as linear programs, the simplex algorithm.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "35 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/210.linear-programming/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//210.linear-programming/reading-cormen.md"}
</pre>

<h2>/morea/210.linear-programming/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on linear programming",
 "published"=>true,
 "morea_id"=>"reading-notes-21",
 "morea_summary"=>
  "Introduction, formulating problems as linear programs, foundations in gaussian elimination, the simplex method",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/210.linear-programming/reading-notes.html",
 "url"=>"/morea/210.linear-programming/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Introduction to Linear Programming\n  2. Formulating Problems as Linear Programs \n  3. Foundations in Gaussian Elimination \n  4. The Simplex Method \n\n## Objectives\n\n  * Be aware of the range of problems to which linear programming can be applied.\n  * Understand the Simplex algorithm just enough to understand the format of linear equations used and what is done with them. \n  * Be able to write a simple linear program for a problem. \n\n## Readings\n\nIf you have a background in Gaussian Elimination and read and understand\nSections 29.0-29.3 of CLRS, up through the description of Simplex (you need\nnot read the proofs that follow), these objectives will be met. (The material\nof CLRS Sections 29.4-29.5 is excellent, but we don't need to see all the\nproofs concerning Simplex to use it.)\n\nIf you don't have a background in Gaussian Elimination, then reading and\nunderstanding Section 28.1 of CLRS would provide it. However, Section 28.1\nprovides more detail than is needed to get the gist of Gaussian Elimination\nand the Simplex. I found Sedgewick's (1984), Chapter 5 presentation of\nGaussian Elimination to be clear and sufficient. I also found his presentation\nof Linear Programming in Chapter 38 useful for its clear narrative around an\nexample.\n\nFor a full study of linear programming I recommend this reading sequence:\n\n  * **Chapter 5 of Sedgewick (1984) on Gaussian Elimination**\n  * **Chapter 38 of Sedgewick (1984) on Linear Programming**\n  * **Sections 29.0 through the first half of 29.3 of CLRS**\n\nIf you don't have time for the full reading:\n\n  * **Read the following web notes** (which summarize the main points from Sedgewick and some material from CLRS 29.0-29.3).\n  * Then **read 29.0, 29.1 and 29.2 of CLRS before class** (quiz questions and class problems are drawn from those sections). \n\n\n##  Introduction to Linear Programming\n\nThe following brief conceptual overview of Linear Programming and its roots\nin Gaussian Elimination is based largely on Chapters 5 and 38 of: Robert\nSedgewick (1983). Algorithms. Reading, MA: Addison-Wesley. First Edition\n(available on Internet), with some comments from CLRS Chapter 29. \n\n**Mathematical programming** is the process of modeling a problem as a set of mathematical equations. (The \"programming\" is in mathematics, not computer code.)\n\n**Linear programming** is mathematical programming where the equations are _linear equations_ in a set of variables that model a problem, and include:\n\n  * a set of **constraints** on the values of the variables (each constraint being expressed as a linear equation), and \n  * an **objective function** or linear function of these variables that is to be maximized subject to these constraints.\n\nA large and diverse set of problems can be expressed as linear programs and\nsolved. Examples include:\n\n  * _**Scheduling tasks,**_ such as in business, construction or manufacturing, for example, scheduling flight crews for an airline.\n  * _**Flows in a network,**_ including flows of multiple types of substances or commodities subject to various constraints (example to be given).\n  * _**Maximizing an outcome **_ given a set of constrained resources, such as deciding where to drill for oil for maximum expected payoff.\n\n### Simplex Algorithm\n\n  * A well established algorithm (actually, family of algorithms) for solving linear programming problems.\n  * Available in many computer packages.\n  * Not always the most efficient way to solve a problem (many of the algorithms we have studied are more efficient for their specialized problem), but is often the easiest feasible approach.\n  * Well studied, but analyzing its asymptotic complexity is still an active area of research, over 50 years after its invention!\n  * Examples have been given requiring exponential time, but Simplex has been repeatedly shown to have good performance in practice on real problems.\n\n### Examples\n\nWe begin with examples of problems for which we already have more efficient\nalgorithms. The point of revisiting them here with less efficient linear\nprogramming solutions is to show you how linear programming works in terms of\nfamiliar problems; and also to reinforce the recurring theme that problems can\nbe solved with different algorithms if you change problem representation.\n\n#### Linear Program for Single-Pair Shortest Paths\n\nThe [Bellman-Ford algorithm](http://www2.hawaii.edu/~suthers/courses/ics311s14\n/Notes/Topic-18.html#bellmanford) for single-source shortest paths uses the `[\nRelax`](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html#\nrelax) procedure to find a distance _v_._d_, where for every edge (_u_, _v_) ∈\n_E_, _v_._d_ ≤ _u_._d_ \\+ _w_(_u_, _v_) (since `Relax` changes _v_._d_\nprecisely when this is not true). Also, _s_._d_ for the source vertex _s_ is\nalways 0.\n\nWe can translate these observations directly into a linear program for the\n**single-_pair_ shortest-path** problem from _s_ to _t_. We will use notation\n_d__v_ instead of _v_._d_ to be consistent with typical linear programming\nnotation:\n\n> Maximize:     _d__t_  \nSubject to:  \n                    _d__v_ ≤ _d__u_\\+ _w_(_u_, _v_),   ∀ (_u_, _v_) ∈ _E_   \n                    _s_._d_ = 0. \n\nWhy are we _maximizing_ _d__t_ when we seek _shortest_ paths?\n\n  * If we minimized _d__t_, then there would be a trivial solution where _d__v_ = 0, ∀ _v_ ∈ _V_. \n  * The minimization that finds shortest paths is actually implicit in the first constraint.   \nEach _d__v_ will be given the maximum value that is yet ≤ the _smallest_\n_d__u_ \\+ _w_(_u_, _v_).\n\n(Compare to the fact that we needed to find _longest_ paths when determining\nthe shortest time in which a set of jobs could finish in the parallel\nscheduling problem given in class.)\n\nThe extension to **single-source all-destinations** is straightforward:\nmaximize the _sum_ of the destination distances.\n\nThe custom algorithms for [single-\nsource](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html)\nand indeed [all-\npairs](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-19.html)\nshortest paths will be more efficient than solving these problems with linear\nprogramming, but this example (and the next) illustrates how linear\nprogramming works in terms of a familiar example.\n\n#### Linear Program for Max Flow\n\nNext we show how to model a max-flow problem with linear programming. Instead\nof writing _f_(_s_,_a_) to indicate the flow over edge (_s_,_a_) (for\nexample), we follow the conventions of the linear programming literature and\nwrite _f__sa_. (Sedgewick uses _X_AB.) CLRS present a more general template\nfor any flow network, whereas here we look at a specific example:\n\n![](fig/flow-network-example.jpg)\n\n> Maximize:     _f_sa\\+ _f_sb  \nSubject to:  \n                    _f_sa ≤ 8             _f_sb ≤ 2   \n                    _f_ac ≤ 6             _f_da ≤ 3   \n                    _f_bd≤ 5             _f_cb ≤ 2   \n                    _f_ct ≤ 4             _f_dt ≤ 5   \n                    _f_sa \\+ _f_da = _f_ac         _f_sb \\+ _f_cb = _f_bd   \n                    _f_dt \\+ _f_da = _f_bd         _f_cb \\+ _f_ct = _f_ac   \n                    _f_sa,   _f_sb,   _f_ac,   _f_cb,   _f_ct,   _f_bd,   _f_da,   _f_dt   ≥   0\\. \n\nThe expression to be maximized,\n\n> _fsa_ \\+ _fsb_\n\nis the flow over the edges coming out of the source, and hence will be the\nflow of the entire network. If the linear program maximizes this, then we have\nfound the max flow. (If there are edges incoming to _s_ we can subtract these\nin the expression to be maximized.)\n\nThese inequalities capture edge capacities:\n\n> _fsa_ ≤ 8;   _fsb_ ≤ 2;   _fac_ ≤ 6;   _fda_ ≤ 3;   _fbd_ ≤ 5;   _fcb_ ≤ 2;\n_fct_ ≤ 4;   _fdt_ ≤ 5\\.\n\nThese equalities capture the conservation of flow at vertices (we write the\nsingle edge on the right hand side to give the equations uniform formats):\n\n> _fsa_ \\+ _fda_ = _fac_     (flow through a)  \n_fsb_ \\+ _fcb_ = _fbd_     (flow through b)  \n_fcb_ \\+ _fct_ = _fac_     (flow through c)  \n_fdt_ \\+ _fda_ = _fbd_     (flow through d)\n\nThe final eight inequalities (written in one line for brevity) express the\nconstraint that all flows must be positive:\n\n> _f_sa,   _f_sb,   _f_ac,   _f_cb,   _f_ct,   _f_bd,   _f_da,   _f_dt   ≥\n0\\.\n\nThe Simplex algorithm (discussed later and in the readings), when given a\nsuitable form of these equations (see section 29.1 CLRS), will return an\nassignment of values to variables _fsa_, ... _fdt_ that maximizes the\nexpression _fsa_ \\+ _fsb_ and hence flow.\n\nThe Edmonds-Karp flow algorithm is more efficient than the Simplex algorithm\nfor solving this version of the max-flow problem. However, Edmonds-Karp is\ndifficult to modify for problem variations such as multiple commodities or\ndealing with cost-benefit tradeoffs. These additional constraints are easy to\nadd to a linear program.\n\nIn general, if a problem can be expressed as a linear program it may be\nquicker from a development standpoint to do that rather than to invent a\ncustom algorithm for it. Linear programming covers a large variety of\nproblems.\n\nThe point here is to introduce linear programming with a familiar example, and\nto illustrate its generality, but this also provides another example of\n\"problem reduction\", a concept that will be at the core of the final topic of\nthis course on Complexity Theory & NP-Completeness.\n\n* * *\n\n##  Gaussian Elimination\n\nThe Simplex algorithm works in a manner similar to (derived from) Gaussian\nElimination for solving a set of linear equations.\n\nInvented by Chinese mathematicians a few thousand years ago, and in Europe by\nNewton and revised by Gauss, Gaussian elimination is a two part method for\nsolving a system of linear equations.\n\nAs a simple example, suppose we have the following system:\n\n> ** _x_ \\+ 3_y_ − 4_z_ = 8  \n_x_ \\+ _y_ − 2_z_ = 2  \n−_x_ − 2_y_ \\+ 5_z_ = −1 **\n\nThe goal is to find values of _x_, _y_, and _z_ that satisfy these equations.\n(Recall that there may be zero, one, or an infinite number of solutions, and\nyou need as many equations as variables to have a unique solution.)\n\nIf we think of the variables as subscripted as shown on the left, then we can\nrewrite the system of equations as a matrix equation without bothering with\nthe letters, as shown on the right:\n\n> _x_1 \\+ 3_x_2 − 4_x_3 = 8  \n_x_1 \\+ _x_2 − 2_x_3 = 2  \n−_x_1 − 2_x_2 \\+ 5_x_3 = −1\n\n> ![](fig/gauss-example-1c.jpg)\n\nThe following operations can be done on systems of linear equations such as\nthe above. (Later, in the section on linear programing, we'll drop the\nparentheses and put everything in one matrix. Then, the operations below will\nbe operations on rows and columns of the matrix.)\n\n  * _**Interchanging equations:**_ Since the order in which we write equations does not matter, we can reorder the rows.\n  * _**Renaming variables:**_ Swapping entire columns with each other. Swapping columns _i_ and _j_, what was formerly _xi_ becomes _xj_ and vice-versa.\n  * _**Multiplying equations by a constant:**_ Accomplished by multiplying all numbers in a row by that constant.\n  * _**Adding two equations and replacing one by the sum:**_ Since the two sides of an equation are equal, we can add them to the two sides of another equation without affecting equality. \n\n###  The Strategy\n\nGaussian elimination is a systematic way of applying these operations to make\nthe value of one variable obvious (_forward elimination_), and then\nsubstituting this value back into the other equations to expose their values\n(_backward substitution_).\n\n#### Forward Elimination (Triangulation)\n\nForward elimination turns the matrix into a triangular matrix, where there is\nonly one variable in the last equation, only that variable plus one more in\nthe next equation up, etc.\n\nFor example, replace the second equation by the difference between the first\ntwo:\n\nBefore:     ![](fig/gauss-example-1c.jpg)  \nAfter:       ![](fig/gauss-example-1d.jpg)\n\nOne term has gone to 0: this means _x1_ has been eliminated from the second\nequation. Let's eliminate _x1_ from the third equation by replacing the third\nby the sum of the first and the third:\n\n![](fig/gauss-example-1e.jpg)\n\nNow if we replace the third equation by the difference between the second and\ntwice the third, we can eliminate _x2_ from the third row, leaving a\n_triangular_ matrix. Writing the result as equations:\n\n> _x_1 \\+ 3_x_2 − 4_x_3 = 8  \n       2_x_2 − 2_x_3 = 6  \n              −4_x_3 = −8 \n\nAt the completion of the forward elimination phase, the equations are easy to\nsolve.\n\n#### Backward Substitution Phase\n\nIt is easy to determine from the third equation that _x3_ = 2. Substituting\nthat into the second equation, we can derive _x2_:\n\n> 2_x_2 − 4 = 6  \n        _x_2 = 5 \n\nSubstituting this and _x3_ = 2 into the equation above (rewritten below)\nsolves for _x1_:\n\n> _x_1 \\+ 3_x_2 − 4_x_3 = 8  \n      _x_1 \\+ 15 − 8 = 8  \n                    _x_1 = 1 \n\n###  The Algorithm\n\nSo, in general we can solve systems of linear equations as written on the left\nby converting them into matrices as written on the right:\n\n![](fig/gauss-general-example-a.jpg)         ![](fig/gauss-general-example-b.jpg)\n\nIt is convenient to represent this entire system in one _N_ x (_N_+1) matrix.\n\n#### Basic Algorithm for Gaussian Elimination\n\nWe can eliminate\n\n  * the _first variable_ from _all but the first equation_ by adding an appropriate multiple of the first equation to each of the second through _N_th equations (the multiple will be different for each equation);\n  * the _second variable_ from _all but the first two equations_ by adding an appropriate multiple of the second equation to the third through _N_th equations;\n  * and so on ...\n\nIn general, the algorithm for forward elimination eliminates the _i_th\nvariable in the _j_th equation by multiplying the _i_th equation by _aji_ /\n_aii_ and subtracting it from the _j_th equation, for _i_+1 ≤ _j_ ≤ _N_.\n\nWe use _aji_ / _aii_ because (_aji_ / _aii_) * _aii_ = _aji_, so when we\nsubtract row _i_ from row _j_ we get _aji_ \\- _aji_ = 0 in cell _j,i_.\n\nThe essential idea can be expressed in this pseudocode fragment (translated\nfrom Sedgewick's Pascal):\n\n    \n    \n        for i = 1 to N do\n            for j = i + 1 to N do\n                for k = N + 1 downto i do\n                    a[j,k] = a[j,k] − a[i,k] * a[j,i] / a[i,i] \n    \n\nThere are three nested loops. _Trivial Question: How do the loops grow with N?\nWhat's the complexity?_\n\n####  Elimination Elaborated\n\nThis code is too simple: In an actual implementation, various issues must be\ndealt with, including:\n\n  * If _aii_ = 0, cannot divide by 0. Need to swap rows to make _aii_ non-zero in the outer loop. If this is not possible, there is no unique solution.\n  * If _aii_ is very small, the scaling factor _aji_ / _aii_ could get very large, leading to rounding error in floating point representations used in computers. This is solved by always choosing the row in _i_+1 to _N_ with the largest absolute value.\n\nThe process of elimination is also called **pivoting**, a concept that shows\nup in the application to linear programming.\n\nSedgewick presents an improved version as a Pascal procedure. If you want to\nunderstand the algorithm at this level of detail you should read CLRS 28.1.\n\n* * *\n\n##  Linear Programming\n\nLinear programs are systems of linear equations, but with the additional\ntwists that\n\n  * The constraint equations may include inequalities.\n  * There is also a linear expression, the objective function, to be maximized.\n\nThese two are related:\n\n  * The constraints being inequalities means there is often no unique solution to the system of constraints.\n  * Maximizing the objective function helps us choose from among the infinite possible solutions.\n\nIn fact, these points capture our motivations, in many cases, for using linear\nprogramming for real-world problems! There are many ways to act (i.e., many\nsolutions), but we want to know which one is the best (i.e., maximized\nobjective function). The constraints model a set of possible solutions, and\nthe objective function helps us pick one that maximizes something we care\nabout. Linear programming is a _general_ way to approach any such situation\nthat can be modeled with linear equations.\n\n###  Example\n\nFor example, a simple linear program in two variables might look like this:\n\n![](fig/linear-programming-example-1b.jpg)\n\n>   −_x_1 \\+ _x_2 ≤ 5  \n  _x_1 \\+ 4_x_2 ≤ 45  \n  2_x_1 \\+ _x_2 ≤ 27  \n3_x_1 − 4_x_2 ≤ 24  \n  \n      _x_1, _x_2 ≥ 0 \n\n#### Geometric Interpretation\n\nWe can graph this example as shown:\n\nEach inequality divides the plane into one half in which a solution cannot lie\nand one in which it can.\n\nFor example, _x1_ ≥ 0 excludes solutions to the left of the _x2_ axis, and\n−_x1_ \\+ _x2_ ≤ 5 means solutions must lie below and to the right of the line\n−_x1_ \\+ _x2_ = 5, shown between (0,5) and (5,10).\n\n### Simplex\n\nSolutions must lie within this feasible region defined by intersecting regions\n(half-planes in this example). That region is called the **simplex**.\n\nThe simplex is a **convex region:** for any two points in the region, all\npoints on a line segment between them are also in the region. Convexness can\nbe used to show an important fact:\n\n#### Fundamental Theorem\n\n**_The objective function is always maximized at one of the vertices of the simplex._**\n\n![](fig/linear-programming-example-1b-small.jpg)\n\nThink of the objective function (here, _x1_ \\+ _x2_, the dotted line) as a\nline of known slope but unknown position. Imagine the line being slid towards\nthe simplex from infinity. If there is a solution, it will first touch the\nsimplex at one of the vertices (one solution) or coincide with an edge (many\nsolutions) that includes a vertex.\n\n_Where would this line touch the simplex?_\n\n_The algorithm does not actually slide a line_. Rather, this geometric\ninterpretation tells us that the algorithm need only need search for a\nsolution at the vertices of the convex simplex. _The simplex method\nsystematically searches the vertices, moving to new vertices on which the\nobjective function is no less, and is usually greater than the value for the\nprevious vertex_.\n\n#### Other Issues Exposed by the Geometric Interpretation\n\n  * **Linearity is important**: if either the objective function or the simplex were curved, it would be much harder to tell where they overlap optimally.\n  * If the intersection of the half-planes is empty, the linear program is **infeasible**.\n  * A constraint is **redundant** if the simplex defined by the other constraints lies entirely within its half-plane. Not a problem but the code must handle these situations.\n  * The simplex may be **unbounded**. As a result, the solution may be ill-defined, or even if it is well defined an algorithm may have difficulty with the unbounded portion.\n\n### Multiple Dimensions\n\nThe geometric interpretation extends to more variables = dimensions.\n\n**In three dimensions,**\n\n  * The simplex is a convex 3-D solid defined by the intersection of half-spaces defined by planes rather than lines.\n  * The objective function is a plane that we can imagine being brought in to intersect with a vertex of the solid.\n\n**In _n_ dimensions,**\n\n  * (_n_-1)-dimensional hyperplanes are intersected to define an _n_-dimensional simplex.\n  * The objective function is an _n_-1 dimensional hyper-plane brought from infinity to intersect with the simplex.\n\nThe anomalous situations get much harder to detect in advance as dimensions\nincrease, so it is important to handle them well in the code.\n\nAs an example, add the inequalities _x3_ ≤ 4 and _x3_ ≥ 0 to our previous\nexample. The simplex becomes a 3-D solid:\n\n![](fig/linear-programming-example-2a-nolines.jpg)\n\n>   −_x_1 \\+ _x_2 ≤ 5  \n  _x_1 \\+ 4_x_2 ≤ 45  \n  2_x_1 \\+ _x_2 ≤ 27  \n3_x_1 − 4_x_2 ≤ 24  \n            _x_3 ≤ 4  \n  \n  _x_1, _x_2, _x_3 ≥ 0\n\nIf the objective function is defined to be _x1_ \\+ _x2_ \\+ _x3_, this is a\nplane perpendicular to the line _x1_= _x2_ = _x3_. Imagine this plane being\nbrought from infinity to the origin: _where would it hit the simplex?_\n\nAgain, the algorithm we discuss below does not actually move planes from\ninfinity; this is just a way of visualizing the fact that an optimal solution\nmust lie on _some_ vertex of the _n_-dimensional simplex, so we need only\nsearch these vertices.\n\n### The Simplex Method\n\nNow we see how pivoting from Gaussian elimination is used. Pivoting is\nanalogous to moving between the vertices of the simplex, starting at the\norigin. First, we need to prepare the data ...\n\n#### Standard Form\n\n_(Note: Sedgewick does not distinguish between standard and slack forms; this\ndiscussion is based on CLRS section 29.1, to which the reader is referred for\ndetails.)_\n\nWhen equations are written to model a problem in a natural way, they may have\nvarious features that are not suitable for input to the Simplex Method. We\nbegin by conversion into **standard form**:\n\nGiven **_n_ real numbers _c_1, _c_2, ... _c__n_**     _(coefficients on\nobjective function)_,  \n\n**_m_ real numbers _b_1, _b_2, ... _b__m_**             _(constants on right hand side of equations)_,   \n\nand **_m__n_ real numbers _a__i__j_** for   _i_ = 1, 2 ... _m_ and _j_ = 1, 2,\n... _n_     _(coefficients on variables in equations)_,  \n\n**find real numbers _x_1, _x_2, ... _x__n_**           _(the variables)_\n\n\n**that maximize:   Σ_j_=1,_n_ _cj_ _xj_**     _(the objective function)_  \n  \n**subject to:   Σ_j_=1,_n_ _aij_ _xj_ ≤ _bj_   for _i_ = 1, 2, ... _m_**     _(regular constraints)_   \n  \n**and   _xj_ ≥ 0,   for _j_ = 1, 2, ... _n_**     _(nonnegativity constraints)_\n\nThe following conversions may be needed to convert a linear program into\nstandard form (see CLRS for details and justification):\n\n  1. If the objective function is to be minimized rather than maximized, negate the objective function (i.e., negate its coefficients). \n  2. Replace each variable _x_ that does not have a nonnegativity constraint with _x'_−_x''_, and introduce the constraints _x'_ ≥ 0 and _x''_ ≥ 0\\. \n  3. Convert equality constraints of form _f_(_x_1, _x_2, ... _x__n_) = _b_ into two inequality constraints _f_(_x_1, _x_2, ... _x__n_) ≤ _b_ and _f_(_x_1, _x_2, ... _x__n_) ≥ _b_. \n  4. Convert ≥ constraints (except the nonnegativity constraints) into ≤ constraints by multiplying the constraints by -1.\n\nOur example above is already in standard form, except that some of the\ncoefficients _aij_ are equal to 1 and are not written out, and we have not\nwritten terms with 0 coefficents. Making all _aij_ explicit, we would write:\n\n−1_x_1 \\+ 1_x_2 \\+ 0_x_3 ≤ 5  \n1_x_1 \\+ 4_x_2 \\+ 0_x_3 ≤ 45  \n2_x_1 \\+ 1_x_2 \\+ 0_x_3 ≤ 27  \n3_x_1 − 4_x_2 \\+ 0_x_3 ≤ 24  \n0_x_1 \\+ 0_x_2 \\+ 1_x_3 ≤ 4  \n  \n_x_1, _x_2, _x_3 ≥ 0\n\n#### Slack Form\n\nThe Simplex Method is based on methods (akin to Gaussian elimination) for\nsolving systems of linear equations that require that we work with equalities\nrather than inequalities (except for the constraints that the variables are\nnon-negative).\n\nWe can convert standard form into slack form by introducing **slack\nvariables**, one for each inequality, that \"take up the slack\" allowed by the\ninequality. (These will be allowed to range as needed to do so.)\n\nFor example, instead of _x_1\\+ 4_x_2 ≤ 45, we can write _x_1 \\+ 4_x_2 \\+ _y_ =\n45, where _y_ can range over the values needed to \"take up the slack\" between\ninequality and equality.\n\nApplying this idea to the 3-D example above, and using a different _yi_ for\neach equation, we can model that example with:\n\n> Maximize _x_1 \\+ _x_2 \\+ _x_3 subject to the constraints:\n\n>\n\n>> −1_x_1 \\+ 1_x_2 \\+ 0_x_3 \\+ _y_1 = 5  \n  1_x_1 \\+ 4_x_2 \\+ 0_x_3 \\+ _y_2 = 45  \n  2_x_1 \\+ 1_x_2 \\+ 0_x_3 \\+ _y_3 = 27  \n  3_x_1 − 4_x_2 \\+ 0_x_3 \\+ _y_4 = 24  \n  0_x_1 \\+ 0_x_2 \\+ 1_x_3 \\+ _y_5 =   4  \n  \n_x_1, _x_2, _x_3, _y_1, _y_2, _y_3, _y_4, _y_5 ≥ 0\n\nThere are _m_ equations in _n_ variables, including up to _m_ slack variables\n(one for each inequality). (Note: in using _n_ and _m_, I am following CLRS.\nSedgewick uses _M_ for number of variables and _N_ for number of equations.)\n\n  * We assume that _n_ > _m_ (more variables than equations), so there are many solutions possible. (In our example above, _n_ = 8 and _m_ = 5.) \n  * We assume that the origin ((0, 0, 0) in this example) is a point on the simplex, so we can use it as a starting point for the search for the best solution, which must lie on some vertex. (The assumption that the origin is a solution can be eliminated if needed.) \n\nWe can now write the slack-form system of equations (e.g., above) as a matrix\n(e.g., shown below), where the 0th row contains the negated coefficients of\nthe objective function. Sedgewick describes how this negation directs the\nprocedure to select the correct rows and columns for pivoting), and the\n(_n_+1)th column has the numbers on the right hand side of the equation.\n\n![](fig/linear-programming-example-2c.jpg)\n\nWe want to perform pivot operations, using the same row and column\nmanipulations as for Gaussian elimination.\n\n  * Instead of trying to make a triangular matrix we are trying to get each column corresponding to the non-slack variables _x_1, _x_2, and _x_3 to have exactly one \"1\" in it and all the rest \"0\"s.\n  * This is because the variables with one \"1\" in it and all the rest \"0\"s are the **basis variables**: their values give the solution if we set all other variables to 0.\n  * Then we will be able to read off the values of the variables in the (_n_+1)th or rightmost column. The value of variable _x__i_ will be found in row _i_ column _n_+1, or at _a__i_, _n_+1.\n  * We don't care what the values of the slack variables _yi_ are (they just move the solution around in the feasible inequality areas).\n\nAs we proceed, the upper right cell will have the current value of the\nobjective function. We always want to increase this. The question is what\nstrategy to take.\n\nThe most popular strategy is **greatest increment**:\n\n  * Choose the _column q_ with the smallest value in row 0 (the largest absolute value). The objective function will increase if we use any column with a negative entry in row 0. \n  * Choose the _row p_ from among those with positive values in the chosen column that has the smallest value when divided into the (_n_+1)th element in the same row. (Sedgewick discusses how this guarantees that the objective function increases and also that we stay in the simplex.)\n  * In the case of ties, choose the row that will result in the column of lowest index being removed from the basis (this policy prevents cycling). \n\nAn alternative strategy is ** steepest descent ** (actually ascent!): evaluate\nthe alternatives and choose the column that increases the objective function\nthe most.\n\n#### Example\n\nWe'll solve the example given above and copied below. Keep in mind that row\nindices start at 0, but column indices start at 1. (See Sedgewick for\ndiscussion of issues concerning staying in the simplex, detecting unbounded\nsimplexes, and avoiding circularity; and then CLRS if you want details and\nproofs.)\n\n![](fig/linear-programming-example-2c.jpg)\n\nThere are three columns with the smallest value (-1) in row 0; we choose to\noperate on the lowest indexed column 1. Dividing the last number by the\npositive values in this column, 45/1 = 45 (row 2), 27/2 = 13.5 (row 3) and\n24/3 = 8 (row 4), so we choose to pivot on row 4, as this has the smallest\nresult.\n\nPivot for row _p_= 4 and column _q_ = 1 by adding an appropriate multiple of\nthe fourth row to each of the other rows to make the first column 0 except for\na 1 in row 4):\n\n![](fig/linear-programming-example-2d.jpg)\n\nAfter that pivot, only _x_1 is a basis variable. Setting the others to 0, we\nhave moved to vertex (8,0,0) on the simplex (see figure), and the objecive\nfunction has value 8.00 (upper right corner of matrix above).\n\n![](fig/linear-programming-example-2a-small-1.jpg)\n\nNow, column 2 has the smallest value. Rows 2 and 3 are candidates: for row 2,\n37/5.33 = 6.94; and for row 3, 11/3.67 = 2.99. We choose row 3. Pivoting on\nrow _p_ = 3 and column _q_ = 2:\n\n![](fig/linear-programming-example-2e.jpg)\n\nAfter that pivot, _x_1 and _x_2 are basis variables, with values 12 and 3\nrespectively, so we are at vertex (12,3,0). The objecive function has value\n15.00. The figure to the right shows how we are moving through the space.\n\nNow pivot on column _q_ = 3 (it has -1 in row 0) and row _p_ = 5 (it has the\nonly positive value in column 3).\n\n![](fig/linear-programming-example-2f.jpg)\n\nNow all three _x__i_ are in the basis, and we are at vertex (12,3,4).\n\n![](fig/linear-programming-example-2a-small-2.jpg)\n\nBut we are not done: there is still a negative value in row 0 (at column 7),\nso we know that we can still increase the objective function. I leave it to\nyou to do the math to verify that row 2 will be selected. Pivoting on row _p_\n= 2 and column _q_ = 7, we get::\n\n![](fig/linear-programming-example-2g-solved.jpg)\n\nNow row 0 has no negative values, and the columns for the three variables of\ninterest are in the basis (all 0 except one 1 in each). We can read off the\nsolution: _x_1 = 9, _x_2 = 9, and _x_3 = 4, with optimum value 22.\n\n### Sedgewick's Code\n\n_(Here I briefly explain Sedgewick's Pascal code, but if you want to\nunderstand the algorithm in detail I recommend going to CLRS for a more\ncurrent treatment in pseudocode you are familiar with.)_\n\nKeep in mind that for Sedgewick there are _N_ equations in _M_ variables.\n\nThe main procedure finds values of _p_ and _q_ and calls `pivot`, repeating\nuntil the optimum is reached (`_q_=_M_+1`) or the simplex is found to be\nunbounded (`_p_=_N_+1`).\n\n![](fig/linear-programming-code-main.jpg)\n\n  * The first line finds _q_ by finding the first negative value in the 0th row.\n  * The second line finds the first positive value in the _q_th column. \n  * The `for` loop finds the best row _p_ for pivoting by searching for the smallest ratio with the value in _M_+1).\n  * If the conditions for continuation are met, `pivot` is called.\n\nThe `pivot` procedure has similarities to Gaussian elimination. (The `for`\nloops below correspond to the two innermost `for` loops of Gaussian\nelimination, and the outer `for` loop of Gauss corresponds to the `repeat`\nloop in the main procedure above):\n\n![](fig/linear-programming-code-pivot.jpg)\n\nThe innermost line is where one row is scaled and subtracted from another.\nOther details are discussed in Sedgewick's chapter, including the need to\nimplement cycle avoidance and test whether the matrix has a feasible basis\n(absent from the code above).\n\n* * *\n\n### What's Next\n\nAt this point, I highy recommend reading CLRS Sections 29.0 (the introduction\nto the chapter) through the middle of 29.3 (where the Simplex algorithm is\nintroduced: as a \"consumer\" of the algorithm you don't need to read the proofs\nthat follow in the rest of the section).\n\n* * *\n\nDan Suthers Last modified: Thu Apr 17 01:58:00 HST 2014  \nImages are from Sedgewick (1983). Algorithms. Reading, MA: Addison-Wesley.\nFirst Edition.  \n\n",
 "path"=>"morea//210.linear-programming/reading-notes.md"}
</pre>

<h2>/morea/210.linear-programming/reading-sedgewick-2.html</h2>

<pre>Hash
{"title"=>"Sedgewick 38 - Linear programming",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-38",
 "morea_summary"=>"Linear programs, geometric interpretation, simplex method",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "15 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/210.linear-programming/reading-sedgewick-2.html",
 "content"=>"",
 "path"=>"morea//210.linear-programming/reading-sedgewick-2.md"}
</pre>

<h2>/morea/210.linear-programming/reading-sedgewick.html</h2>

<pre>Hash
{"title"=>"Sedgewick 5 - Gaussian Elimination",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-5",
 "morea_summary"=>
  "A simple example, outline of the method, variations and extensions",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/210.linear-programming/reading-sedgewick.html",
 "content"=>"",
 "path"=>"morea//210.linear-programming/reading-sedgewick.md"}
</pre>

<h2>/morea/220.multithreading/module.html</h2>

<pre>Hash
{"title"=>"Multithreading",
 "published"=>true,
 "morea_id"=>"multithreading",
 "morea_outcomes"=>["outcome-multithreading"],
 "morea_readings"=>["reading-cormen-27", "reading-notes-22"],
 "morea_experiences"=>[],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/220.multithreading/logo.jpg",
 "morea_sort_order"=>220,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/220.multithreading/module.html",
 "content"=>
  "Concepts, modeling and measuring dynamic multithreading, analysis of multithreaded algorithms, matrix multiplication example, merge sort example.",
 "path"=>"morea//220.multithreading/module.md"}
</pre>

<h2>/modules/multithreading/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Multithreading",
 "url"=>"/modules/multithreading/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/multithreading/index.html"}
</pre>

<h2>/morea/220.multithreading/outcome.html</h2>

<pre>Hash
{"title"=>"Understand multithreading",
 "published"=>true,
 "morea_id"=>"outcome-multithreading",
 "morea_type"=>"outcome",
 "morea_sort_order"=>220,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/220.multithreading/outcome.html",
 "content"=>"Understand when, why, and how to use multithreading.",
 "path"=>"morea//220.multithreading/outcome.md"}
</pre>

<h2>/morea/220.multithreading/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 27 - Multithreading",
 "published"=>true,
 "morea_id"=>"reading-cormen-27",
 "morea_summary"=>
  "The basics of dynamic multithreading, multithreaded matrix multiplication, multithreaded merge sort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "36 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/220.multithreading/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//220.multithreading/reading-cormen.md"}
</pre>

<h2>/morea/220.multithreading/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on multithreading",
 "published"=>true,
 "morea_id"=>"reading-notes-22",
 "morea_summary"=>
  "Concepts of dynamic multithreading, modeling and measuring dynamic multithreading, analysis of multithreaded algorithms, matrix multiplication example, merge sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/220.multithreading/reading-notes.html",
 "url"=>"/morea/220.multithreading/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Concepts of Dynamic Multithreading \n  2. Modeling and Measuring Dynamic Multithreading \n  3. Analysis of Multithreaded Algorithms \n  4. Example: Matrix Multiplication\n  5. Example: Merge Sort\n\n##  Concepts of Dynamic Multithreading\n\nParallel Machines are getting cheaper and in fact are now ubiquitous ...\n\n  * supercomputers: custom architectures and networks\n  * computer clusters with dedicated networks (distributed memory)\n  * multi-core integrated circuit chips (shared memory)\n  * GPUs (graphics processing units) \n\n### Dynamic Multithreading\n\n**Static threading:** abstraction of virtual processors. But rather than managing threads explicitly, our model is **dynamic multithreading** in which programmers specify opportunities for parallelism and a **concurrency platform** manages the decisions of mapping these to static threads (load balancing, communication, etc.).\n\n#### Concurrency Constructs:\n\nThree keywords are added, reflecting current parallel-computing practice:\n\n  * **parallel**: add to loop construct such as `for` to indicate each iteration can be executed in parallel.\n  * **spawn**: create a parallel subprocess, then keep executing the current process (parallel procedure call).\n  * **sync**: wait here until all active parallel threads created by this instance of the program finish.\n\nThese keywords specify opportunities for parallelism without affecting whether\nthe corresponding sequential program obtained by removing them is correct. We\nexploit this in analysis.\n\n### Example: Parallel Fibonacci\n\nFor illustration, we take a really slow algorithm and make it parallel. (There\nare much better ways to compute Fibonacci numbers.) Here is the definition of\nFibonacci numbers:\n\n> F0 = 0.  \nF1 = 1.  \nF_i_ = F_i-1_ \\+ F_i-2_, for _i_ ≥ 2\\.\n\nHere is a recursive non-parallel algorithm for computing Fibonacci numbers\nmodeled on the above definition, along with its recursion tree:\n\n![](fig/code-Fib.jpg) ![](fig/Fig-27-1-Fib-Recursion-Tree-Small.jpg)\n\n**`Fib`** has recurrence relation T(_n_) = T(_n_ \\- 1) + T(_n_ \\- 2) + Θ(1), which has the solution T(_n_) = Θ(F_n_) (see the text for substitution method proof). This grows exponentially in _n_, so it's not very efficient. (A straightforward iterative algorithm is much better.) \n\nNoticing that the recursive calls operate independently of each other, let's\nsee what improvement we can get by computing the two recursive calls in\nparallel. This will illustrate the concurrency keywords and also be an example\nfor analysis:\n\n![](fig/code-P-Fib.jpg)\n\nNotice that without the keywords it is still a valid serial program.\n\n**Logical Parallelism**: The **spawn** keyword does not force parallelism: it just says that it is permissible. A scheduler will make the decision concerning allocation to processors.\n\nHowever, if parallelism is used, **sync** must be respected. For safety, there\nis an implicit sync at the end of every procedure.\n\nWe will return to this example when we analyze multithreading.\n\n### Scheduling\n\nScheduling parallel computations is a complex problem: see the text for some\ntheorems concerning the performance of a greedy centralized scheduler (i.e.,\none that has information on the global state of computation, but must make\ndecisions on-line rather than in batch).\n\nProfessor Henri Casanova does research in this area, and would be an excellent\nperson to talk to if you want to get involved.\n\n* * *\n\n##  Modeling and Measuring Dynamic Multithreading\n\nFirst we need a formal model to describe parallel computations.\n\n### A Model of Multithreaded Execution\n\n![](fig/code-P-Fib.jpg)\n\nWe will model a multithreaded computation as a **computation dag** (directed\nacyclic graph) _G_ = (_V_, _E_); an example for P-Fib(4) is shown:\n\n![](fig/Fig-27-2-P-Fib.jpg)\n\nVertices in _V_ are instructions, or **strands** = sequences of non-parallel\ninstructions.\n\nEdges in _E_ represent dependencies between instructions or strands: (_u_,\n_v_) ∈ _E_ means _u_ must execute before _v_.\n\n  * **Continuation Edges** (_u_, _v_) are drawn horizontally and indicate that _v_ is the successor to _u_ in the sequential procedure.\n  * **Call Edges** (_u_, _v_) point downwards, indicating that _u_ called _v_ as a normal subprocedure call.\n  * **Spawn Edges** (_u_, _v_) point downwards, indicating that _u_ spawned _v_ in parallel.\n  * **Return edges** point upwards to indicate the next strand executed after returning from a normal procedure call, or after parallel spawning at a sync point.\n\nA strand with multiple successors means all but one of them must have spawned.\nA strand with multiple predecessors means they join at a sync statement.\n\nIf _G_ has a directed path from _u_ to _v_ they are logically in **series**;\notherwise they are logically **parallel**.\n\nWe assume an ideal parallel computer with **sequentially consistent** memory,\nmeaning it behaves as if the instructions were executed sequentially in some\nfull ordering consistent with orderings within each thread (i.e., consistent\nwith the partial ordering of the computation dag).\n\n### Performance Measures\n\nWe write _T__P_ to indicate the running time of an algorithm on _P_\nprocessors. Then we define these measures and laws:\n\n#### Work\n\n**_T_1** = the total time to execute an algorithm on one processor. This is called _work_ in analogy to work in physics: the total amount of computational work that gets done.\n\nAn ideal parallel computer with _P_ processors can do at most _P_ units of\nwork in one time step. So, in _T__P_ time it can do at most _P_⋅_T__P_ work.\nSince the total work is _T_1,   _P_⋅_T__P_ ≥ _T_1,   or dividing by P we get\nthe **work law:**\n\n> _T__P_  ≥   _T_1 / _P_\n\nThe work law can be read as saying that the speedup for _P_ processors can be\nno better than the time with one processor divided by _P_.\n\n#### Span\n\n**_T_∞** = the total time to execute an algorithm on an infinite number of processors (or, more practically speaking, on just as many processors as are needed to allow parallelism wherever it is possible).\n\n![](fig/Fig-27-2-P-Fib.jpg)\n\n_T_∞ is called the _span_ because it corresponds to the longest time to\nexecute the strands along any path in the computation dag (the biggest\ncomputational span across the dag). It is the fastest we can possibly expect\n-- an Ω bound -- because no matter how many processors you have, the algorithm\nmust take this long.\n\nHence the **span law** states that a _P_-processor ideal parallel computer\ncannot run faster than one with an infinite number of processors:\n\n> _T__P_   ≥   _T_∞\n\nThis is because at some point the span will limit the speedup possible, no\nmatter how many processors you add.\n\n_What is the work and span of the computation dag for P-Fib shown?_\n\n#### Speedup\n\nThe ratio **_T_1 / _T__P_** defines how much _speedup_ you get with _P_\nprocessors as compared to one.\n\nBy the work law, _T__P_ ≥ _T_1 / _P_, so _T_1 / _T__P_ ≤ _P_: one cannot have\nany more speedup than the number of processors.\n\nThis is important: **_parallelism provides only constant time improvements_**\n(the constant being the number of processors) to any algorithm! **_Parallelism\ncannot move an algorithm from a higher to lower complexity class (e.g.,\nexponential to polynomial, or quadratic to linear)._** Parallelism is not a\nsilver bullet: good algorithm design and analysis is still needed.\n\nWhen the speedup _T_1 / _T__P_ = Θ(_P_) we have **linear speedup**, and when\n_T_1 / _T__P_ = _P_ we have **perfect linear speedup**.\n\n#### Parallelism\n\nThe ratio **_T_1 / _T_∞** of the work to the span gives the potential\nparallelism of the computation. It can be interpreted in three ways:\n\n  * _Ratio _: The average amount of work that can be performed for each step of parallel execution time.\n  * _Upper Bound _: the maximum possible speedup that can be achieved on any number of processors.\n  * _Limit_: The limit on the possibility of attaining perfect linear speedup. Once the number of processors exceeds the parallelism, the computation cannot possibly achieve perfect linear speedup. The more processors we use beyond parallelism, the less perfect the speedup.\n\nThis latter point leads to the concept of **parallel slackness**,\n\n> (_T_1 / _T_∞) / _P_   =   _T_1 / (_P_⋅_T_∞),\n\nthe factor by which the parallelism of the computation exceeds the number of\nprocessors in the machine. If slackness is less than 1 then perfect linear\nspeedup is not possible: you have more processors than you can make use of. If\nslackness is greater than 1, then the work per processor is the limiting\nconstraint and a scheduler can strive for linear speedup by distributing the\nwork across more processors.\n\n_What is the parallelism of the computation dag for P-Fib shown previously?\nWhat are the prospects for speedup at *this* n? What happens to work and span\nas n grows?_\n\n* * *\n\n## Analysis of Multithreaded Algorithms\n\nAnalyzing _work_ is simple: ignore the parallel constructs and analyze the\nserial algorithm. For example, the work of P-Fib(_n_) is T1(_n_) = T(_n_) =\nΘ(F_n_). Analyzing _span_ requires more work.\n\n### Analyzing Span\n\nIf in series, the span is the sum of the spans of the subcomputations. (This\nis similar to normal sequential analysis.)\n\nIf in parallel, the span is the _maximum_ of the spans of the subcomputations.\n(This is where analysis of multithreded algorithms differs.)\n\n![](fig/Fig-27-3-Composed-Subcomputations.jpg)\n\nReturning to our example, the span of the parallel recursive calls of\nP-Fib(_n_) is:\n\n> _T_∞ (_n_)   =   max(T∞(_n_−1), T∞ (_n_−2)) \\+ Θ(1)  \n              =   T∞(_n_−1) + Θ(1). \n\nwhich has solution Θ(_n_).\n\nThe parallelism of P-Fib(_n_) in general (not the specific case we computed\nearlier) is T1(_n_) / T∞   =   Θ(F_n_/_n_), which grows dramatically, as F_n_\ngrows much faster than _n_.\n\nThere is considerable parallel slackness, so above small _n_ there is\npotential for near perfect linear speedup: there is likely to be something for\nadditional processors to do.\n\n### Parallel Loops\n\nSo far we have used spawn, but not the **parallel** keyword, which is used\nwith loop constructs such as **for**. Here is an example.\n\nSuppose we want to multiply an _n_ x _n_ matrix _A_ = (_a__ij_) by an _n_-\nvector _x_ = (_x__j_). This yields an _n_-vector _y_ = (_y__i_) where:\n\n![](fig/equation-matrix-vector-mult.jpg)\n\nThe following algoirthm does this in parallel:\n\n![](fig/code-Mat-Vec.jpg)\n\nThe **parallel for** keywords indicate that each iteration of the loop can be\nexecuted concurrently. (Notice that the inner **for** loop is not parallel; a\npossible point of improvement to be discussed.)\n\n#### Implementing Parallel Loops\n\nIt is not realistic to think that all _n_ subcomputations in these loops can\nbe spawned immediately with no extra work. (For some operations on some\nhardware up to a constant _n_ this may be possible; e.g., hardware designed\nfor matrix operations; but we are concerned with the general case.) How might\nthis parallel spawning be done, and how does this affect the analysis?\n\nThis can be accomplished by a compiler with a divide and conquer approach,\nitself implemented with parallelism. The procedure shown below is called with\nMat-Vec-Main-Loop(_A_, _x_, _y_, _n_, 1, _n_). Lines 2 and 3 are the lines\noriginally within the loop.\n\n![](fig/code-Mat-Vec-Main-Loop.jpg) ![](fig/Fig-27-4-Mat-Vec-Main-Loop-small.jpg)\n\nThe computation dag is also shown. It appears that a lot of work is being done\nto spawn the _n_ leaf node computations, but the increase is not asymptotic.\n\nThe _work_ of Mat-Vec is T1(_n_) = Θ(_n_2) due to the nested loops in 5-7.\n\nSince the tree is a full binary tree, the number of internal nodes is 1 fewer\nthan the leaf nodes, so this extra work is also Θ(_n_).\n\nSo, the work of recursive spawning contributes a constant factor when\namortized across the work of the iterations.\n\nHowever, concurrency platforms sometimes coarsen the recursion tree by\nexecuting several iterations in each leaf, reducing the amount of recursive\nspawning.\n\nThe span is increased by Θ(lg _n_) due to the tree. In some cases (such as\nthis one), this increase is washed out by other dominating factors (e.g., the\ndoubly nested loops).\n\n#### Nested Parallelism\n\nReturning to our example, the span is Θ(_n_) because even with full\nutilization of parallelism the inner **for** loop still requires Θ(_n_). Since\nthe work is Θ(_n_2) the parallelism is Θ(_n_). Can we improve on this?\n\nPerhaps we could make the inner **for** loop parallel as well? Compare the\noriginal to this revised version:\n\n![](fig/code-Mat-Vec.jpg) ![](fig/code-Mat-Vec-Prime.jpg)\n\nWould it work? We need to introduce a new issue ...\n\n### Race Conditions\n\n**Deterministic** algorithms do the same thing on the same input; while ** nondeterministic** algorithms may give different results on different runs.\n\nThe above Mat-Vec' algorithm is subject to a potential problem called a\n**determinancy race**: when the outcome of a computation could be\nnondeterministic (unpredictable). This can happen when two logically parallel\ncomputations access the same memory and one performs a write.\n\nDeterminancy races are hard to detect with empirical testing: many execution\nsequences would give correct results. This kind of software bug is\nconsequential: Race condition bugs caused the Therac-25 radiation machine to\noverdose patients, killing three; and caused the North American Blackout of\n2003.\n\n![](fig/Fig-27-5-Race-Condition.jpg)\n\nFor example, the code shown below might output 1 or 2 depending on the order\nin which access to _x_ is interleaved by the two threads:\n\n![](fig/code-Race-Example.jpg)  \n\nAfter we understand that simple example, let's look at our matrix-vector\nexample again:\n\n![](fig/code-Mat-Vec-Wrong.jpg)\n\n* * *\n\n##  Example: Matrix Multiplication\n\n####  Multithreading the basic algorithm\n\nHere is an algorithm for multithreaded matrix multiplication, based on the\nT1(_n_)   =   Θ(_n_3) algorithm:\n\n![](fig/code-P-Square-Matrix-Multiply.jpg)\n\n_How does this procedure compare to MAT-VEC-WRONG? Is is also subject to a\nrace condition? Why or why not? _\n\nThe span of this algorithm is T∞(_n_)   =   Θ(_n_), due to the path for\nspawning the outer and inner parallel loop executions and then the _n_\nexecutions of the innermost **for** loop. So the parallelism is T1(_n_) /\nT∞(_n_)   =   Θ(_n_3) / Θ(_n_)   =   Θ(_n_2)\n\n_Could we get the span down to Θ(1) if we parallelized the inner **for** with\n**parallel for**?_\n\n#### Multithreading the divide and conquer algorithm\n\nHere is a parallel version of the divide and conquer algorithm from Chapter 4:\n\n![](fig/code-P-Matrix-Multiply-Recursive.jpg)\n\nSee the text for analysis, which concludes that the work is Θ(_n_3), while the\nspan is Θ(lg2_n_). Thus, while the work is the same as the basic algorithm the\nparallelism is Θ(_n_3) / Θ(lg2_n_), which makes good use of parallel\nresources.\n\n* * *\n\n##  Example: Merge Sort\n\nDivide and conquer algorithms are good candidates for parallelism, because\nthey break the problem into independent subproblems that can be solved\nseparately. We look briefly at merge sort.\n\n#### Parallelizing Merge-Sort\n\nThe dividing is in the main procedure `MERGE-SORT`, and we can parallelize it\nby spawning the first recursive call:\n\n![](fig/code-Merge-Sort-Prime.jpg)\n\n`MERGE` remains a serial algorithm, so its work and span are Θ(_n_) as before.\n\nThe recurrence for the _work_ MS'1(_n_) of `MERGE-SORT'` is the same as the\nserial version:\n\n![](fig/equation-Merge-Sort-Prime-work.jpg)\n\nThe recurrence for the _span_ MS'∞(_n_) of `MERGE-SORT'` is based on the fact\nthat the recursive calls run in parallel, so there is only one _n_/2 term:\n\n![](fig/equation-Merge-Sort-Prime-span.jpg)\n\nThe _parallelism_ is thus MS'1(_n_) / MS'∞(_n_)   =   Θ(_n_ lg _n_ / _n_)   =\nΘ(lg _n_).\n\nThis is low parallelism, meaning that even for large input we would not\nbenefit from having hundreds of processors. How about speeding up the serial\n`MERGE`?\n\n#### Parallelizing Merge\n\n`MERGE` takes two sorted lists and steps through them together to construct a\nsingle sorted list. This seems intrinsically serial, but there is a clever way\nto make it parallel.\n\nA divide-and-conquer strategy can rely on the fact that they are sorted to\nbreak the lists into four lists, two of which will be merged to form the head\nof the final list and the other two merged to form the tail.\n\nTo find the four lists for which this works, we\n\n  1. Choose the longer list to be the first list, T[_p_1 .. _r_1] in the figure below.\n  2. Find the middle element (median) of the first list (_x_ at _q_1).\n  3. Use binary search to find the position (_q_2) of this element if it were to be inserted in the second list T[_p_2 .. _r_2].\n  4. Recursively merge \n    * The first list up to just before the median T[_p_1 .. _q_1-1] and the second list up to the insertion point T[_p_2 .. _q_2-1].\n    * The first list from just after the median T[_q_1+1 .. _r_1] and the second list after the insertion point T[_q_2 .. _r_2].\n  5. Assemble the results with the median element placed between them, as shown below.\n\n![](fig/Fig-27-6-Multithreaded-Merge.jpg)\n\nThe text presents the `BINARY-SEARCH` pseudocode and analysis of Θ(lg _n_)\nworst case; this should be review for you. It then assembles these ideas into\na parallel merge procedure that merges into a second array Z at location _p_3\n(_r_3 is not provided as it can be computed from the other parameters):\n\n![](fig/code-P-Merge.jpg)\n\n#### Analysis\n\nMy main purpose in showing this to you is to see that even apparently serial\nalgorithms sometimes have a parallel alternative, so we won't get into\ndetails, but here is an outline of the analysis:\n\nThe span of `P-MERGE` is the maximum span of a parallel recursive call. Notice\nthat although we divide the first list in half, it could turn out that _x_'s\ninsertion point _q_2 is at the beginning or end of the second list. Thus\n(informally), the maximum recursive span is 3_n_/4 (as at best we have\n\"chopped off\" 1/4 of the first list).\n\nThe text derives the recurrence shown below; it does not meet the Master\nTheorem, so an approach from a prior exercise is used to solve it:\n\n![](fig/equation-P-Merge-span.jpg)\n\nGiven 1/4 ≤ α ≤ 3/4 for the unknown dividing of the second array, the work\nrecurrence turns out to be:\n\n![](fig/equation-P-Merge-work.jpg)\n\nWith some more work, PM1(_n_) = Θ(n) is derived. Thus the parallelism is Θ(n /\nlg2_n_)\n\nSome adjustment to the `MERGE-SORT'` code is needed to use this `P-MERGE`; see\nthe text. Further analysis shows that the work for the new sort, `P-MERGE-\nSORT`, is PMS1(_n_ lg _n_) = Θ(_n_), and the span PMS∞(_n_) = Θ(lg3_n_). This\ngives parallelism of Θ(_n_ / lg2_n_), which is much better than Θ(lg _n_) in\nterms of the potential use of additional processors as _n_ grows.\n\nThe chapter ends with a comment on coarsening the parallelism by using an\nordinary serial sort once the lists get small. One might consider whether `P\n-MERGE-SORT` is still a stable sort, and choose the serial sort to retain this\nproperty if it is desirable.\n\n* * *\n\nDan Suthers Last modified: Mon Jan 13 19:12:02 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//220.multithreading/reading-notes.md"}
</pre>

<h2>/morea/230.string-matching/module.html</h2>

<pre>Hash
{"title"=>"String matching",
 "published"=>true,
 "morea_id"=>"string-matching",
 "morea_outcomes"=>["outcome-string-matching"],
 "morea_readings"=>["reading-cormen-32", "reading-notes-23"],
 "morea_experiences"=>[],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/230.string-matching/logo.png",
 "morea_sort_order"=>230,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/230.string-matching/module.html",
 "content"=>
  "Naive (brute force) string matching, matching with finite state automata, Knuth-Morris-Pratt algorithm, Rabin-Karp algorithm",
 "path"=>"morea//230.string-matching/module.md"}
</pre>

<h2>/modules/string-matching/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"String matching",
 "url"=>"/modules/string-matching/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/string-matching/index.html"}
</pre>

<h2>/morea/230.string-matching/outcome.html</h2>

<pre>Hash
{"title"=>"Understand string matching",
 "published"=>true,
 "morea_id"=>"outcome-string-matching",
 "morea_type"=>"outcome",
 "morea_sort_order"=>220,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/230.string-matching/outcome.html",
 "content"=>"Understand when, why, and how to use string matching. ",
 "path"=>"morea//230.string-matching/outcome.md"}
</pre>

<h2>/morea/230.string-matching/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 32 - String matching",
 "published"=>true,
 "morea_id"=>"reading-cormen-32",
 "morea_summary"=>
  "The naive string matching algorithm, the Robin-Karp algorithm, string matching with finite automata, the Knuth-Morris-Pratt algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "29 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/230.string-matching/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//230.string-matching/reading-cormen.md"}
</pre>

<h2>/morea/230.string-matching/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on string matching",
 "published"=>true,
 "morea_id"=>"reading-notes-23",
 "morea_summary"=>
  "Naive (brute force) string matching, matching with finite state automata, Knuth-Morris-Pratt algorithm, Rabin-Karp algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/230.string-matching/reading-notes.html",
 "url"=>"/morea/230.string-matching/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. String Matching Introduction \n  2. Naive (Brute Force) String Matching \n  3. Matching with Finite State Automata \n  4. Knuth-Morris-Pratt Algorithm \n  5. FYI: Rabin-Karp Algorithm\n\n\n##  String Matching Introduction\n\nThe **string matching problem** is the problem of finding all occurrences of a\nstring _P_ (the **pattern**) in a target text _T_, also a string.\n\nWe treat both _T_ and _P_ as arrays _T_[1 .. _n_] and _P_[1 .. _m_], where _m_\n≤ _n_. The elements of the arrays are members of a finite alphabet Σ.\n\nMore precisely, we say that _P_ occurs with **shift** _s_ in _T_ (_P_ occurs\nbeginning at position _s_ \\+ 1 in _T_) if\n\n> 0   ≤   _s_   ≤   _n_ − _m_     _(one cannot fall off the end of T),_ and  \nT[_s_ \\+ 1 .. _s_ \\+ _m_]   =   _P_[1 .. _m_].\n\nThe **string matching problem** is the problem of finding _**all valid\nshifts**_.\n\nA simple example:\n\n![](fig/Fig-32-1-string-matching-example.jpg)\n\n### Applications\n\nString matching is obviously important for implementing search in _**text\nediting**_ programs, and for _**document search**_, whether searching within a\nsingle document or searching multiple documents for a topic of interest. This\nlatter application extends to _**internet search engines**_. Other specialized\napplications include processing _**DNA sequences**_ in bioinformatics.\n\n### Brief History\n\nIn 1970 Cook proved that an O(_M_+_N_) machine is possible. His theorem was\nnot intended to be practical, but Knuth and Pratt followed the structure of\nthe theorem to design an algorithm. Around the same time, Morris derived the\nalgorithm independently as part of his design of a text editor. Eventually\nthey found out about each other's work and published it in 1976. Also in 1976,\nBoyer and Moore found another efficient algorithm not covered here: see the\nSedgewick book. In 1980 Rabin & Karp came up with a modification to brute\nforce that searches on chunks of text of size _M_ using a hash function.\nResearchers continue to come up with new algorithms.\n\n### Algorithms\n\nThere are a surprising number of variations on string searching algorithms.\nSee [ http://www-igm.univ-mlv.fr/~lecroq/string/index.html](http://www-igm\n.univ-mlv.fr/~lecroq/string/index.html) for summary descriptions, code and\nanimations in Java. Find out what a \"**Backward Nondeterministic Dawg Matching\nalgorithm**\" is!\n\nMost algorithms require **preprocessing** of the pattern before entering the\n**matching** phase. Analysis must consider both costs, and applications must\ndetermine whether the preprocessing is worth the speedup in matching.\n\nThe algorithms we cover are summarized by this table from the text:\n\n![](fig/Fig-32-2-algorithms.jpg) ![](fig/no-lemmings.jpg)\n\nToday we will cover how these algorithms work, but not any proofs of\ncorrectness, which you may find in the text.\n\n### Notation and Terminology\n\n**Prefix:** _p_ ⊏ _t_ (_p_ is a prefix of _t_) if _t_ = _pw_ for some _w_ ∈ Σ*\n\n**Suffix:** _s_ ⊐ _t_ (_s_ is a suffix of _t_) if _t_ = _ws_ for some _w_ ∈ Σ*\n\nThe **empty string** is denoted ε.\n\nThe **_k_-character prefix** _T_[1 .. k] of any text or pattern _T_ is denoted\n_Tk_.\n\n**Comment on String Matching Time:** The test of whether \"_x_ == _y_\" takes Θ(_t_ \\+ 1) time, where _t_ is the length of the longest string _z_ such that _z_ ⊏ _x_ and _z_ ⊏ _y_. (The \"1\" is included to cover the case where _t_ = 0, since a positive amount of time must be expended to determine this fact.) This comparison loop will be implicit in some of our pseudocode.\n\n* * *\n\n##  Naive (Brute Force) String Matching\n\nIt is often instructive to start with a brute force algorithm, that we can\nthen examine for possible improvements and also use as a baseline for\ncomparison.\n\nThe obvious approach is to start at the first character of _T_, _T_[1], and\nthen step through _T_ and _P_ together, checking to see whether the characters\nmatch.\n\nOnce _P_ has been match at any given shift _T_[_s_], then go on to checking at\n_T_[_s_+1] (since we are looking for all matches), up until _s_ = |_T_| −\n|_P_| (which is _n_ − _m_).\n\n![](fig/code-naive-string-matcher.jpg)\n\n### Example\n\nSuppose _P_ = `aab` and _T_ = `acaabc`. There are four passes:\n\n![](fig/Fig-32-4-naive-string-matcher-a.jpg)\n![](fig/Fig-32-4-naive-string-matcher-b.jpg)\n![](fig/Fig-32-4-naive-string-matcher-c.jpg)\n![](fig/Fig-32-4-naive-string-matcher-d.jpg)\n\nYou an see C code and a Java applet animation at [ http://www-igm.univ-\nmlv.fr/~lecroq/string/node3.html](http://www-igm.univ-\nmlv.fr/~lecroq/string/node3.html)\n\n### Analysis\n\n![](fig/code-naive-string-matcher.jpg)\n\nNo preprocessing is required.\n\nFor each of _n_ − _m_ \\+ 1 start positions, potentially _m_ pattern characters\nare compared to the target text in the matching phase. Thus, the naive\nalgorithm is **O((_n_ \\- _m_ \\+ 1)_m_)** in the worst case.\n\n### Inefficiencies\n\nThe brute force method does not use information about what has been matched,\nand does not use information about recurrences within the pattern itself.\nConsideration of these factors leads to improvements.\n\nFor example, when we matched _P_ = `aab` at _s_ = 2, we found that _T_[5] =\n`b`:\n\n![](fig/Fig-32-4-naive-string-matcher-c.jpg)\n\nThen it is not possible for a shift of _s_ = 3 (or _s_ = 4 if _T_ were longer)\nto be valid, beause these shifts juxtapose _P_[2] = `a` (and _P_[1] = `a` if\napplicable) against _T_[5] = `b`:\n\n![](fig/Fig-32-4-naive-string-matcher-d.jpg)\n\nThis information is used in the finite state automata and Knuth-Morris-Pratt\napproaches.\n\n* * *\n\n##  Matching with Finite State Automata\n\nFinite state automata are machines that have a finite number of states, and\nmove between states as they read input one symbol at a time.\n\nAlgorithms that construct (or simulate) finite state automata can be very\nefficient matchers, but they require some preprocessing to construct.\n\n### Finite State Automata\n\nFinite state automata are widely used in computability theory as well as in\npractical algorithms. It's worth knowing about them even if you are not doing\nstring matching.\n\nA **finite state automaton (FSA) ** or **finite automaton** _M_ is a 5-tuple\n(_Q_, _q0_, _A_, Σ, δ) where\n\n  * **_Q_** is a finite set of _**states**_\n  \n\n  * **_q0_** ∈ _Q_ is the _**start state**_\n  \n\n  * **_A_** ⊆ _Q_ is a set of _**accepting states**_\n  \n\n  * **Σ** is a finite _**input alphabet**_\n  \n\n  * **δ** : _Q_ x Σ -> _Q_ is the _**transition function**_ of _M_. \n\nThe FSA starts in state _q0_. As each character (symbol) of the input string\nis read, it uses δ to determine what state to transition into. Whenever _M_ is\nin a state of _A_, the input read so far is **accepted**.\n\nHere is a simple example:   _q0_ = 0 and _A_ = {1}.   (_What are Q, Σ, and\nδ?_)\n\n![](fig/Fig-32-6-simple-fsa.jpg)\n\n_What strings does this FSA accept?_\n\nWe define the **final state function φ** : Σ* -> _Q_ such that φ(_w_) is the\nfinal state _M_ ends up in after reading _w_:\n\n> φ(ε)   =   _q_0  \nφ(_wa_)   =   δ(φ(_w_), _a_)   for _w_ ∈ Σ*, _a_ ∈ Σ\n\n### String Matching Automata\n\nLet's see how they work by an example. This is the string matching automaton\nfor _P_ = `ababaca`:\n\n![](fig/Fig-32-7-ababaca-fsa-a.jpg) ![](fig/Fig-32-7-ababaca-fsa-b.jpg)\n\nThe start state is 0 and the only accepting state is 7. Any transitions not\nshown (e.g., if `c` is read while in states 1, 2, 3, 4, 6, and 7) are assumed\nto go back to state 0.\n\nThis automaton can be represented with the table to the right. The shading\nshows the sequence of states through which a successful match transitions.\nThese transitions correspond to the darkened arrows in the diagram.\n\nWe can run this automaton continuously on a text _T_, and if and when state 7\nis entered output the relevant positions: the match will start with shift _i_\n− _m_ or at position _i_ − _m_ \\+ 1.\n\nThe following code simulates any FSA on input text _T_, given the FSA's table\nδ and pattern length _m_:\n\n![](fig/code-finite-automaton-matcher.jpg)\n\nFor example, below is a run on _T_ = `abababacaba`, which includes _P_ =\n`ababaca` starting at position 3: `ab ababaca ba`.\n\n![](fig/Fig-32-7-ababaca-fsa-c.jpg) ![](fig/Fig-32-7-ababaca-fsa-b.jpg)\n\nState 7 is reached at _i_ = 9, so the pattern occurs starting at _i_ − _m_ \\+\n1, or 9 − 7 + 1 = 3. The FSA keeps going after a match, as it may find other\noccurrences of the pattern.\n\nUnlike the brute force approach, past work is not thrown away: the transitions\nfollowing either failure to match a character or success to match the entire\npattern pick up with the input read so far.\n\nFor example,\n\n![](fig/Fig-32-7-ababaca-fsa-a-small.jpg)\n\n  * _After Failure:_ At _i_ = 5, `ababa` has been matched in _T_[1 .. 5] and `c` was expected but not found at _T_[6]. Rather than starting over, the FSA transitions to state δ(5, `b`) = 4 to indicate that the pattern prefix _P_4 = `abab` has matched the present text suffix _T_[3 .. 6]. \n  \n\n  * _After Success:_ At, _i_ = 9, we are in state 7 (success), and a `b` is seen. We need not start over: the FSA transitions to state δ(7, `b`) = 2 to reflect the fact that there is already a match to the prefix _P_2 = `ab` at _T_[9 .. 11].\n\nThis makes FSAs much more efficient than brute force in the matching phase. In\nfact, matching is Θ(_n_). But how do we build them?\n\n### Constructing Finite State Automata (Preprocessing Phase)\n\nIn general, the FSA is constructed so that the state number tells us how much\nof a prefix of _P_ has been matched.\n\n  * If the pattern _P_ is of length _m_ and the FSA is in state _m_, then the pattern has been matched.\n  \n\n  * If the state number is smaller than _m_, then the state number is the length of the prefix of _P_ matched.\n\nAnother definition is needed to formalize this.\n\n#### Definitions and Strategy\n\nThe **suffix function** corresponding to _P_ of length _m_ is σ_P_ : Σ* -> {0,\n1, ... _m_} such that σ_P_ (_w_) is the length of the longest prefix of _P_\nthat is also a suffix of _x_:\n\n> σ_P_(_w_) = max {_k_ : _Pk_ ⊐ _w_}.\n\nFor example, if _P_ = `ab` then\n\n  * σ(ε) = 0\n  * σ(`ccaca`) = 1\n  * σ(`ccab`) = 2\n\n(For simplicity, we leave out the subscript _P_ when it is clear from the\ncontext.) Then we can define the automaton for pattern _P_[1 .. _m_] as:\n\n  * _Q_ = {0, 1 .. _m_}\n  \n\n  * _q0_ = 0\n  \n\n  * _A_ = {_m_}\n  \n\n  * Σ is a superset of the characters in _P_\n  \n\n  * **δ(_q_, _a_) = σ(_Pq __a_)** for any state _q_ and character _a_. (_Pq __a_ is the concatenation of the first _q_ characters of _P_ with the character _a_.) \n\nBy defining δ(_q_, _a_) = σ(_Pq __a_), the state of the FSA keeps track of the\nlongest prefix of the pattern _P_ that has matched the input text _T_ so far.\n\nIn order for a substring of _T_ ending at _T_[_i_] to match some prefix _Pj_,\nthen this prefix _Pj_ must be a suffix of _T_[_i_].\n\nWe design δ such that the state _q_ = φ(_T_[_i_]) gives the length of the\nlongest prefix of _P_ that matches a suffix of _T_. We have:\n\n> _q_ = φ(_Ti_) = σ(_Ti_), and _Pq_ ⊐ _Ti_ (_Pq_ is a suffix of _Ti_).\n\nGiven a match so far to _Pq_ (which may be ε) and reading character _a_, there\nare two kinds of transitions:\n\n  * When _a_ = _P_[_q_ \\+ 1], _a_ continues to match the pattern, so δ(_q_, a) = _q_ \\+ 1 (going along the dark \"spine\" arrows of the example).\n  \n\n  * When _a_ ≠ _P_[_q_ \\+ 1], _a_ fails to match the pattern. The preprocessing algorithm given below **_matches the pattern against itself_** to identify the longest smaller prefix of _P_ that is still matched. \n\n![](fig/Fig-32-7-ababaca-fsa-a-small.jpg)\n\nAn example of this second case was already noted above for δ(5, `b`) = 4\\.\n\n#### Preprocessing Procedure\n\nThe following procedure computes the transition function δ from a pattern\n_P_[1 .. _m_].\n\n![](fig/code-compute-transition-function.jpg)\n\nThe nested loops process all combinations of states _q_ and characters _a_\nneeded for the cells of the table representing δ.\n\nLines 4-8 set δ(_q_, _a_) to the largest _k_ such that _Pk_ ⊐ _Pqa_.\n\n  * The preprocessor is _matching P against itself_.\n  * Thus, knowledge of the structure of P is used to retain information about the match so far, even when matches fail.\n  * By starting at the largest possible value of _k_ (line 4) and working down (lines 5-7) we guarantee that we get the longest prefix of _P_ that has been matched so far. \n    * If the match succeeds at _k_ = _q_ \\+ 1 then this transition indicates a successful match for the current _q_ and _a_. \n    * The loop is guaranteed to end at _k_ = 0, because _P0_ = ε is a suffix of any string.\n\n#### Analysis\n\n![](fig/code-compute-transition-function.jpg)\n\n`Compute-Transition-Function` requires _m_*|Σ| for the nested outer loops.\n\nWithin these loops, the inner `repeat` runs at most _m_ \\+ 1 times; and the\ntest on line 7 can require comparing up to _m_ characters. Together lines 5-7\ncontribute O(_m_2).\n\nTherefore, `Compute-Transition-Function` is **O(_m_3 |Σ|)**. This is rather\nexpensive preprocessing, but the Θ(_n_) matching is the best that can be\nexpected.\n\nYou an see C code and a Java applet animation at [ http://www-igm.univ-\nmlv.fr/~lecroq/string/nod43.html](http://www-igm.univ-\nmlv.fr/~lecroq/string/node4.html)\n\n* * *\n\n##  Knuth-Morris-Pratt Algorithm\n\nThe Knuth-Morris-Pratt algorithm improves on the FSA algorithm by avoiding\ncomputation of δ.\n\nInstead it precomputes an auxiliary **prefix function π_P_**, represented as\nan array π_P_[1 .. _m_], that can be computed from a pattern _P_ of length _m_\nin Θ(_m_) time. (We'll leave off the _P_ subscript from now on.)\n\nπ[_q_] is the length of the longest prefix of _P_ that is a proper suffix of\n_Pq_. This information enables fast amortized computation of δ(_q_, _a_) on\nthe fly.\n\nThe KMP matcher operates like the FSA matcher, but using π instead of δ. There\nis some additional overhead at matching time, but asymptotic performance of\nmatching remains Θ(_n_).\n\n### How π Works\n\nGiven _P_[1 .. _m_], the prefix function π for _P_ is π : {1, 2 ..., _m_} ->\n{0, 1, ..., _m_-1} such that\n\n> π[_q_] = max{_k_ : _k_ < _q_ and _Pk_ ⊐ _Pq_}\n\nFor example, for _P_ = `ababaca`, π is as follows (_let's use the formula\nabove to explain a few of the entries below, particularly for i=5._):\n\n![](fig/Fig-32-11-Lemma-32-5-a.jpg)\n\nThe table is structured such that a failure at a given position will drive the\nsystem to hop to the next smaller prefix that could be matched. (_Here I am\nskipping over the textbook's discussion of π* and Lemma 32.5, and using\nexamples instead_).\n\nFor example, for _P_ = `ababaca`, if _q_ = 5 and we are reading the 6th\ncharacter (to the right of the line), consider what happens when that\ncharacter is `c`, `b` or `a`:\n\n![](fig/Fig-32-11-Lemma-32-5-b.jpg)\n\nFrom _P_[6] we see that a `c` is expected next.\n\n![](fig/Fig-32-11-Lemma-32-5-a.jpg)\n\n  * If a `c` is read, we go to state 6 and continue.\n  * If an `a` or `b` is read, then there is a mismatch and we need to figure out what prefix of _P_ has still been matched. We look up π[5], which says to skip back to state 3.\n  * Repeating the test at state 3, the next character expected is _P_[4] = `b`. If we see a `b` we continue to state 4.\n  * Otherwise (the next character is `a`), we look up π[3] and skip back to state 1. Again, the next character does not match what is expected for state 1, and π[1] tells us to start over at state 0. In state 0, an `a` is expected, so it goes to state 1. \n\nThus, π helps us retain information from prior comparisions, shifting back to\nthe state for the maximum prefix matched so far, rather than starting over.\n\n### The KMP Matcher\n\nBoth the KMP matcher and the algorithm for computing π are similar to the\nFSA's `Compute-Transition-Function` (take the time to compare them when you\nread the text).\n\nThe KMP matching code is below. We can see the \"skipping\" discussed above\nhappening in lines 6-7, with successful matching of the next character handled\nin lines 8-9. After a successful match, we jump to the appropriate state to\ncontinue looking for the next item in line 12.\n\n![](fig/code-KMP-matcher-commented.jpg)\n\n### Computing π\n\nThe code for computing π is very similar to that of `KMP-Matcher`, because the\nconstructor is matching _P_ against itself:\n\n![](fig/code-compute-prefix-function.jpg)\n\nYou an see C code and a Java applet animation at [ http://www-igm.univ-\nmlv.fr/~lecroq/string/node8.html](http://www-igm.univ-\nmlv.fr/~lecroq/string/node8.html)\n\n### Analysis and Correctness\n\nThe text presents an analysis showing that `Compute-Prefix-Function` is\n**Θ(_m_)** \\-- a considerable improvement over the FSA's O(_m_3 |Σ|) -- and\n`KMP-Matcher` is **Θ(_n_)**.\n\nThese are good results, but in practice the FSA approach is still used when a\npattern will be used many times repeatedly, because of the greater simplicity\nof the `Finite-Automaton-Matcher` code.\n\nThe proof of correctness is accomplished by showing how KMP-Matcher simulates\nthe FSA matcher's operation. It's worth reading the informal discussion pages\n1009-1010, even if you don't wade into the formal proof.\n\n* * *\n\n##  Rabin-Karp Algorithm\n\nThere is an entirely different approach that functions in some respects like\nthe brute force approach, but instead of testing character by character it\ntests on whole substrings of length _m_ by using a hash function. If the hash\nfunction matches, it then uses brute force checking to verify the match.\n\n### Comparing Strings as Radix-_d_ Numbers\n\nGiven |Σ| = _d_, Rabin-Karp algorithm treats each string in Σ* as if it were a\nnumber in radix _d_ notation.\n\n  * For example, if Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} then _d_ = 10, and we interpret the string \"274\" to have value 2⋅d2 \\+ 7⋅d1 \\+ 4⋅d0 = 2⋅100 + 7⋅10 + 4⋅1\\. \n  \n\n  * Similarly, if Σ = {a, b} then _d_ = 2, and we map the characters of {a, b} to decimal values {0, 1}. Then \"bab\" = 1⋅d2 \\+ 0⋅d1 +1⋅d0 = 1⋅4 + 0⋅2 +1⋅1\\. (This is of course a binary representation of the decimal number 5.) \n  \n\n  * Hexadecimal notation uses Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F} and _d_ = 16.\n\nThis idea can be extended to large but finite |Σ|.\n\nThus, we can treat _P_[1 .. _m_] and substrings _T_[_s_+1 .. _s_+_m_] (0 ≤ _s_\n≤ _n_ − _m_) as _m_ digit numbers in radix-|Σ|.\n\nOur running example will assume Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} and _d_ =\n10.\n\nIf we can compare these numbers directly to each other, we can quickly\ndetermine whether _P_[1 .. _m_] matches _T_[_s_+1 .. _s_+_m_].\n\nTwo modifications need to be made to make this method suitable for a string\nmatching algorithm: shifting and hashing.\n\n### Shifting\n\nIf a match to _T_[_s_+1 .. _s_+_m_] fails, we need to compute the next number\nfor _T_[_s_+**2** .. _s_+_m_+1].\n\nRather than recomputing the entire sum, this can be done efficiently by a\nmathematical \"shift\":\n\n  * Subtract the value of the highest order digit _a_⋅_d__m_-1.\n  * Multiply the remaining value by _d_ to shift the values of the digits up by one position.\n  * Add the value of the new character _T_[_s_ \\+ _m_ \\+ 1].\n\nAn example is shown below, shifting a 5-digit number. (The mod will be\nexplained below.)\n\n![](fig/Fig-32-5-Rabin-Karp-Examples-c.jpg)\n![](fig/Fig-32-5-Rabin-Karp-Examples-d.jpg)\n\n### Hashing\n\nThese numbers can get very large. Even on a machine that allows arbitrarily\nlarge numbers, this is still a problem: we can no longer assume that comparing\ntwo numbers takes constant time.\n\nThe solution is to compute the numbers modulo the largest prime number _q_\nsuch that _dq_ still fits in one computer word (which can be manipulated with\none machine level instruction).\n\nNow you see why it is called \"hashing:\" it is like the division method of\nhashing.\n\nFor example, suppose we are matching to a 5 digit pattern _P_ = 31415 in radix\n_d_ = 10 we choose _q_ = 13, and we are comparing to string 14142. The hash\ncode for the pattern is 7, and for the target is 8, as shown in the previous\nexample. The hashes do not match, so the two patterns cannot be the same.\n\nHowever, the converse is not necessarily true. Hashing introduces a secondary\nproblem: collisions. Two different numbers may hash to the same value, as\nshown in the example below.\n\n![](fig/Fig-32-5-Rabin-Karp-Examples-b.jpg)\n\nTo solve this problem, when the hashes of _P_[1 .. _m_] and _T_[_s_+1 ..\n_s_+_m_] are equal, the Rabin-Karp algorithm verifies the match with a brute\nforce comparison.\n\nThis is still saving on comparisons over the brute force method, as we do not\ncompare characters for failed hash matches: the strings cannot be the same.\n\n### The Rabin-Karp Algorithm\n\nHere is the pseudocode:\n\n![](fig/code-Rabin-Karp-matcher.jpg)\n\n(The subscripts on _t_ are just for exposition and can be ignored.)\n\n  * Line 3 initializes _h_ to the value of the highest order digit that will be subtracted when shifting.\n  * Lines 4-8 compute the hash code _p_ for _P_ and the initial hash code _t_ for the substring of T to be compared in each iteration (_t_ will be updated by shifting digits as discussed).\n  * Like the brute-force algorithm, lines 9-14 iterate over all possible shifts _s_, but instead of comparing character by character, at first the hash codes are compared. If they match, brute force comparison is done.\n\nYou an see C code and a Java applet animation at [http://www-igm.univ-mlv.fr/~lecroq/string/node5.html](http://www-igm.univ-mlv.fr/~lecroq/string/node5.html)\n\n### Anaysis\n\nPreprocessing is Θ(_m_): the loop 6-8 executes _m_ times and with the modular\narithmetic keeping the numbers within the size of one computer word the\nnumerical work in the loop is O(1).\n\nThe worst case matching time is still Θ((_n_ − _m_ \\+ 1)_m_), like brute\nforce, because it is possible that every hash code matches and therefore every\nbrute force comparison has to be done.\n\nBut this is highly unlikely in any realistic application (where we are\nsearching for somewhat rare patterns in large texts). For constant number of\nvalid shifts the text offers an argument that the expected time is O(_n_ \\+\n_m_), or O(_n_) since _m_ ≤ _n_.\n\n### Extensions\n\nA major advantage of Rabin-Karp is that it has natural extensions to other\npattern matching problems, such as two-dimensional pattern matching (finding\nan _m_ x _m_ pattern in an array of _n_ x _n_ characters), or searching for\nmultiple patterns at once. Therefore it remains an important pattern matching\nalgorithm.\n\n* * *\n\nDan Suthers Last modified: Mon Jan 13 19:12:09 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//230.string-matching/reading-notes.md"}
</pre>

<h2>/morea/240.np-completeness/module.html</h2>

<pre>Hash
{"title"=>"NP-completeness",
 "published"=>true,
 "morea_id"=>"np-completeness",
 "morea_outcomes"=>["outcome-np-completeness"],
 "morea_readings"=>
  ["reading-screencast-24a",
   "reading-screencast-24b",
   "reading-screencast-24c",
   "reading-notes-24"],
 "morea_experiences"=>[],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/240.np-completeness/logo.jpg",
 "morea_sort_order"=>240,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/240.np-completeness/module.html",
 "content"=>
  "P and NP classes, encoding problems and polynomial time verification, constructing NPC ",
 "path"=>"morea//240.np-completeness/module.md"}
</pre>

<h2>/modules/np-completeness/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"NP-completeness",
 "url"=>"/modules/np-completeness/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/np-completeness/index.html"}
</pre>

<h2>/morea/240.np-completeness/outcome.html</h2>

<pre>Hash
{"title"=>"Understand np-completeness",
 "published"=>true,
 "morea_id"=>"outcome-np-completeness",
 "morea_type"=>"outcome",
 "morea_sort_order"=>240,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/240.np-completeness/outcome.html",
 "content"=>"Understand np-completeness.",
 "path"=>"morea//240.np-completeness/outcome.md"}
</pre>

<h2>/morea/240.np-completeness/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 34 - NP-completeness",
 "published"=>true,
 "morea_id"=>"reading-cormen-34",
 "morea_summary"=>
  "polynomial time, polynomial time verification, NP-completeness and reducibility, NP-completeness proofs, NP-complete problems",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "60 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/240.np-completeness/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//240.np-completeness/reading-cormen.md"}
</pre>

<h2>/morea/240.np-completeness/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on np-completeness",
 "published"=>true,
 "morea_id"=>"reading-notes-24",
 "morea_summary"=>
  "P and NP classes, encoding problems and polynomial time verification, constructing NPC",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/240.np-completeness/reading-notes.html",
 "url"=>"/morea/240.np-completeness/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Don't Let This Happen to You\n  2. Problem Classes\n  3. P and NP Classes\n  4. Encoding Problems and Polynomial Time Verification \n  5. NP Completeness \n  6. Constructing NPC \n  7. NP Complete Problems\n\n##  Don't Let This Happen to You\n\nSuppose you worked in industry, and your boss calls you to your office and\nsays they want to enter the emerging \"iThingy\" market. He wants to offer\ncustomized iThingies, and it's important to have an algorithm that, given a\nspecification, can efficiently construct a design that meets the maximum\nnumber of requirements in the lowest cost way.\n\n![](fig/garey-johnson-cartoon-1.jpg)\n\nYou try for weeks but just can't seem to find an efficient solution. Every\nsolution that you come up with amounts to trying all combinations of\ncomponents, in exponential time. More efficient algorithms fail to find\noptimal solutions.\n\nYou don't want to have to go your boss and say \"I'm sorry I can't find an\nefficient solution. I guess I'm just too dumb.\"\n\n![](fig/garey-johnson-cartoon-2-you.jpg)\n\nYou would like to be able to confidently stride into his office and say \"I\ncan't find an efficient solution because no such solution exists!\"\nUnfortunately, proving that the problem is inherently intractable is also very\nhard: in fact, no one has succeeded in doing so.\n\n![](fig/garey-johnson-cartoon-3.jpg)\n\nToday we introduce a class of problems that no one has been able to prove is\nintractable, but nor has anyone been able to find an efficient solution to any\nof them.\n\nIf you can show that the iThingy configuration problem is one of these\nproblems, you can go to your boss and say, \"I can't find an efficient\nsolution, but neither can all these smart and famous computer scientists!\"\n\n(Next week we'll cover what you would say next. Thanks to Garey & Johnson\n(1979) for the story & images.)\n\n* * *\n\n##  Problem Classes\n\nFor most of this semester, we abstracted away from the study of particular\n**_implementations_** to study the computational complexity of\n**_algorithms_** independently of their implementation. More precisely, we\nmade universally quantified statements to the effect that all possible\nimplementations of an algorithm would exhibit certain asymptotic growth within\nconstant factors of each other.\n\nNow we abstract further, to study the the computational complexity of\n**_problems_**, independently of the algorithms used to solve them. We will be\ntrying to make universally quantified statements about the computational\ncomplexity of all possible algorithms for a problem. (We won't always succeed\nin doing this.)\n\nFor example, when we showed that any comparison-based sorting algorithm is\nO(_n_ lg _n_) in [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html), we\nwere making a claim about a class of algorithms for a problem.\n\n### Tractability and Decidability\n\nBroadly, we are concerned with three classes of problems:\n\n  * **Tractable problems**, generally considered to be those solvable in polynomial time (most of the problems considered so far in this course). \n    * _Examples:_ Any problem studied this semester for which we have solutions bounded by _nk_ for fixed _k_. \n    * Some solutions are not strictly polynomial but are bounded above by a polynomial (e.g., _n_ lg _n_ for sorting is bounded by _n2_), so sorting is considered polynomial.\n  \n\n  * **Intractable problems** are those solvable in super-polynomial but not polynomial time. Today we will be concerned with the question of which problems are in this class. \n    * _Examples:_ Enumerate all binary strings of length _n_; Compute the power set (set of all sets) of _n_ items. The solutions consist of 2_n_ binary strings, or 2_n_ sets, respectively, so even if the solution were available instantly exponential time would be required just to output the result!\n    * _Unknown:_ Integer linear programming, finding longest simple paths in a graph, finding the optimal way to schedule tasks on factory machines under scheduling and deadline constraints, determining whether certain boolean formulas can be simultaneously satisfied, optimizing circuit layouts under certain constraints, finding the cheapest way to visit all of a set of cities without repeating any and returning to your starting city, finding the optimal partition of graphs under a modularity metric, and many many more ...\n  \n\n  * **Unsolvable problems** for which no algorithm can be given guaranteed to solve all instances of the problem. These problems are also known as _**Undecidable**_. \n    * Example: the _**halting problem**_ (given a description of a program and arbitrary input, decide whether the program will halt on that input), demonstrated by Alan Turing.\n\n### Hierarchy of Problem Classes\n\nThe hierarchy of problem classes was illustrated at the beginning of the\nsemester with this diagram:\n\n![](fig/Complexity-Hierarchy.jpg)\n\nWe have spent most of our time on problems in the lower half of this diagram.\nNow we consider whether there are problems that are _intrisically_ in the\nupper half.\n\n* * *\n\n##  P and NP Classes\n\n**P** denotes the class of problems solvable in polynomial time, such as most of the problems we have considered this semester.\n\n###  Nondeterministic Polynomial\n\n**NP** denotes the class of problems for which solutions are _**verifiable**_ in polynomial time: given a description of the problem _x_ and a \"certificate\" _y_ describing a solution (or providing enough information to show that a solution exists) that is polynomial in the size of _x_, we can check the solution (or verify that a solution exists) in polynomial time as a function of _x_ and _y_.\n\nProblems in NP are **decision problems**: we answer \"yes\" or \"no\" to whether\nthe certificate is a solution. The polynomial size requirement on _y_ rules\nout problems that take exponential time just to output their results, such as\nthe enumeration of all binary strings of length _n_. (One could check the\nsolution in time polynomial in _y_, but _y_ would be exponential in _x_, so\noverall the problem is not tractable.)\n\nThese problems are called \"nondeterministic polynomial\" because one way of\ndefining them is to suppose we have a **nondeterministic machine** that,\nwhenever faced with a choice, can guess the correct alternative, producing a\nsolution in polynomial time based on this guess. (Amazingly, no one has been\nable to prove that such a fanciful machine would help!)\n\nAnother way to think of this is that the machine can copy itself at each\nchoice point, essentially being multithreaded on an infinite number of\nprocessors and returning the solution as soon as the solution is found down a\nspan of polynomial depth (see [Topic on Multithreading](http://www2.hawaii.edu\n/~suthers/courses/ics311s14/Notes/Topic-22.html)).\n\n(Your text does not take either of these approaches, preferring the\n\"certificate\" definition. We elaborate on this later, but not in depth.)\n\n![](fig/money.jpg)\n\n### Relationship of P to NP\n\nClearly, P is a subset of NP.\n\nThe million dollar question (literally, see the [ Clay Mathematics Institute\nMillenium Prize](http://www.claymath.org/millennium-problems/rules-millennium-\nprizes) for the [ P vs NP Problem ](http://www.claymath.org/millenium-\nproblems/p-vs-np-problem)), is whether P is a strict subset of NP (i.e.,\nwhether there are problems in NP that are not in P, so are inherently\nexponential).\n\n![](fig/P-NP-NPC-NPH.jpg)\n\n### NP Completeness\n\nTheorists _have_ been able to show that there are problems that are just as\nhard as any problem in NP, in that if any of these **NP-Hard** problems are\nsolved then any problem in NP can be solved by reduction to (translating them\ninto instances of) the NP-Hard problem.\n\nThose NP-Hard problems that are also in NP are said to be ** NP Complete**,\ndenoted **NPC**, in that if _any_ NPC problem can be solved in polynomial time\nthen _all_ problems in NP can also be solved in polynomial time.\n\nThe study of NP Completeness is important: the _most cited reference in all of\nComputer Science_ is Garey & Johnson's (1979) book Computers and\nIntractability: A Guide to the Theory of NP-Completeness. (Your textbook is\nthe second most cited reference in Computer Science!)\n\nIn 1979 Garey & Johnson wrote, \"The question of whether or not the NP-complete\nproblems are intractable is now considered to be one of the foremost open\nquestions of contemporary mathematics and computer science.\"\n\n![](fig/shrug.jpg)\n\nOver 30 years later, in spite of a million dollar prize offer and intensive\nstudy by many of the best minds in computer science, this is still true: No\none has been able to either\n\n  * Prove that there are problems in NP that cannot be solved in polynomial time (which would mean P ≠ NP), or \n  * Find a polynomial time solution for a single NP Complete Problem (which would mean P=NP).\n\nAlthough either alternative is possible, most computer scientists believe that\nP ≠ NP.\n\n### Discriminating P and NP problems\n\nProblems that look very similar to each other may belong to different\ncomplexity classes (if P ≠ NP), for example:\n\n  * **Linear programming** can be solved in polynomial time (the [simplex algorithm](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-21.html)), but if we require that the assigned values be integers we have the **integer linear programming problem**, which is NP-Hard. \n  \n\n  * **Shortest paths** in a graph [can be found in O(_VE_) or better](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html), but the problem of finding **longest simple paths** is NP-Hard (deciding whether there is a path longer than length _k_ is NP-Complete).\n  \n\n  * An **Euler tour** that traverses each edge of a graph once can be found in O(_E_) time, but finding a **Hamiltonian cycle** that traverses each vertex exactly once via a simple cycle is NP-Hard.\n  \n\n  * We can determine whether a boolean formula in **2-Conjunctive Normal Form** is satisfiable in polynomial time, but doing the same for a formula in **3-Conjunctive Normal Form** is NP-Complete.\n\nClearly, it is important that we be able to recognize such problems when we\nencounter them and not share the fate of the iThingy algorithm designer. (In\n[the next\ntopic](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-25.html)\nwe'll discuss approximation algorithms for dealing with them in a practical\nway.)\n\n* * *\n\n## Encoding Problems and Polynomial Time Verification\n\n_This section discuses some concepts used in the study of complexity classes.\nWe are not delving into formal proofs such as those provided in the text, so\nsome of the material of this section is not essential to the discussion\nfollowing, but familiarity with the terminology may help you understand why\nthe remaining notes talk about \"languages\"._\n\n### Abstract Problems\n\nAn **abstract problem** _Q_ is a binary relation mapping problem **instances**\n_I_ to problem **solutions** _S_.\n\nNP Completeness is concerned with **decision problems**: those having a yes/no\nanswer, or in which _Q_ maps _I_ to {0, 1}.\n\nMany problems are **optimization problems**, which require that some value be\nminimized (e.g., finding shortest paths) or maximized (e.g., finding longest\npaths).\n\nWe can easily convert an optimization problem to a decision problem by asking:\nis there a solution that has value lower than (or higher than) a given value?\n\n### Encodings and Concrete Problems\n\nTo specify whether an abstract problem is solvable in polynomial time, we need\nto carefully specify the size of the input.\n\nAn **encoding** of a problem maps problem instances to binary strings. We\nconsider only \"reasonable\" encodings\n\n  * For example, we do not represent numbers in unary format. \n    * (If we did, the input to the problem of enumerating all binary strings of length _n_ would itself be of length 2_n_, so an \"O(_n_)\" solution would be possible!)\n  * A \"reasonable\" encoding an integer is polynomial related to its representation, and a set of objects polynomially related to its encoding. Standard codings such as ASCII meet this requirement.\n\nOnce problems and their solutions are encoded, we are dealing with **concrete\nproblem instances**. Then a problem _Q_ can be thought of as a function _Q_ :\n{0, 1}* -> {0, 1}*, or if it is a decision problem, _Q_ : {0, 1}* -> {0, 1}.\n(_Q_(_x_) = 0 for any string _x_ ∈ Σ* that is not a legal encoding.)\n\n### Accepting and Deciding Formal Langauges\n\nBy casting computational problems as decision problems, theorists can use\nconcepts from formal language theory in their proofs. We are not doing these\nproofs but you should be aware of the basic concepts and distinctions:\n\nA **language** _L_ over an alphabet Σ is a set of strings made up of symbols\nfrom Σ.\n\nAn algorithm _A_ ** accepts** a string _x_ ∈ {0, 1}* if given _x_ the output\nof _A_ is 1.\n\nThe **language _L_ accepted by _A_** is the set of strings _L_ = {_x_ ∈ {0,\n1}* : _A_(_x_) = 1}.\n\nBut _A_ need not halt on strings not in _L_. It could just never return an\nanswer. (The existence of problems like the Halting Problem necessitate\nconsidering this possibility.)\n\nA language is **decided** by an algorithm A if it accepts precisely those\nstrings in L _and_ rejects those not in L (i.e., _A_ is guaranteed to halt\nwith result 1 or 0).\n\nA language is **accepted in polynomial time** by A if it is accepted by A in\ntime O(_nk_) for any encoded input of length _n_ and some constant _k_.\nSimilarly, a language is **decided in polynomial time** by A if it is decided\nby A in time O(_nk_).\n\n###  Polynomial Time Verification\n\nA **complexity class** is a set of languages for which membership is\ndetermined by a complexity measure. (Presently we are interested in the\nrunning time required of any algorithm that decides _L_, but complexity\nclasses can also be defined by other measures, such as space required.) For\nexample, we can now define P more formally as:\n\n> **P** = {_L_ ⊆ {0, 1}* : ∃ algorithm _A_ that decides _L_ in polynomial\ntime}.\n\nA **verification algorithm** _A_(_x_, _y_) takes two arguments: an encoding\n_x_ of a problem and a **certificate** _y_ for a solution. _A_ returns 1 if\nthe solution is valid for the problem. (_A_ need not solve the problem; only\nverify the proposed solution.)\n\nThe **language verified** by a verification algorithm _A_ is\n\n> L = {_x_ ∈ {0, 1}*: ∃ _y_ ∈ {0, 1}* such that _A_(_x_, _y_) = 1}.\n\nWe can now define the complexity class **NP** as the class of languages that\ncan be verified by a polynomial time algorithm, or formally:\n\n> _L_ ∈ NP iff ∃ polynomial time algorithm _A_(_x_, _y_) and constant _c_ such\nthat:\n\n>\n\n>> _L_ = {_x_ ∈ {0,1}* : ∃ certificate _y_ with |_y_| = O(|_x_|_c_) such that\n_A_(_x_,_y_) = 1}.\n\n![](fig/Fig-34-2-Hamilton-Examples-a.jpg)\n\nThe constant _c_ ensures that the size of the certificate _y_ is polynomial in\nthe problem size, and also we require that _A_ runs in time polynomial in its\ninput, which therefore must be polynomial in both |_x_| and |_y_|.\n\nFor example, although only exponential algorithms are known for the\nHamiltonian Cycle problem, a proposed solution can be encoded as a sequence of\nvertices and verified in polynomial time.\n\n* * *\n\n* * *\n\n##  NP Completeness\n\nThe NP-Complete problems are the \"hardest\" problems in NP, in that if one of\nthem can be solved in polynomial time then every problem in NP can be solved\nin polynomial time. This relies on the concept of _reducibility_.\n\n### Reducibility\n\n![](fig/Fig-34-1-reduction-algorithm.jpg)\n\nA problem _A_ can be polynomially reduced to a problem _B_ if there exists a\npolynomial-time computable function _f_ that converts an instance α of _A_\ninto an instance β of _B_.\n\n![](fig/Fig-34-4-Polynomial-Reduction.jpg)\n\nStated in terms of formal languages, _L_1 is reducible to _L_2 if there exists\na polynomial-time computable function _f_ : {0, 1}* -> {0, 1}* such that:\n\n> _x_ ∈ _L_1 iff _f_(_x_) ∈ _L_2, ∀ _x_ ∈{0, 1}*.\n\n### NP Completeness Defined\n\nA language _L_ ⊆ {0, 1}* is **NP-Complete (in NPC) if**\n\n  1. **_L_ ∈ NP**, and \n  2. **Every _L'_ ∈ NP is polynomial reducible to _L_.**\n\nLanguages satisfying 2 but not 1 are said to be **NP-Hard**. (This includes\noptimization problems that can be converted to decision problems in NP.)\n\nThe major Theorem of this lecture is:\n\n> **If any NP-Complete problem is polynomial-time solvable, then P = NP. **\n\n![](fig/Fig-34-6-Complexity-Class-Relations.jpg)\n\n> Equivalently, **if any problem in NP is not polynomial-time solvable, then\nno NP-Complete problem is polynomial time solvable.**\n\nThe expected situation (but by no means proven) corresponds to the second\nstatement of the theorem, as depicted to the right.\n\n* * *\n\n* * *\n\n##  Constructing NPC\n\nIn order to construct the class NPC, we need to have one problem known to be\nin NPC. Then we can show that other problems are in NPC by reducibility proofs\n(reducing the other candidates to this known problem).\n\n### Circuit Satisfiability: An Initial Problem\n\nIn 1971, Cook defined the class NPC and proved that it is nonempty by proving\nthat the **Circuit Satisfiability _ (CIRCUIT-SAT)_** problem is NP-Complete.\n\n![](fig/Fig-34-10-CircuitSat-FormulaSat.jpg)\n\nThis problem asks: given an acyclic boolean combinatorial circuit composed of\nAND, OR and NOT gates, does there exist an assignment of values to the input\ngates that produces a \"1\" at a designated output wire? Such a circuit is said\nto be **satisfiable**.\n\nThe first part of the proof, that CIRCUIT-SAT is in NP, is straightforward.\n\n  * Given as certifiate an assignment of boolean values to each of the wires in a circuit, one can check that the assignment meets the logic conditions of each component, which clearly can be done in polynomial time in the size of the description of the circuit.\n  * Alternatively, one can give as certificate only the input values, and verify by simulating the circuit, propagating values through the gates and to the designated output wire, in polynomial time.\n\nThe second part of the proof, that CIRCUIT-SAT is NP-Hard, was complex. The\ngist was as follows.\n\n  * Given any language _L_ corresponding to a problem in NP, there must exist an algorithm _A_ that verifies _L_ in polynomial time.\n  \n\n  * We can represent the computation of _A_ as a sequence of configurations or states of a machine _M_ on which the algorithm runs. (A configuration represents the state of the computer, including program, storage and program counters.) \n  \n\n  * First, a boolean circuit corresponding to _M_ is constructed. \n  \n\n  * Then the reduction makes a copy of the boolean circuit for each possible state (computational configuration) of _M_, feeding the output of one configuration into the input of another configuration in such a manner that computer memory is no longer needed: all state is encoded in the circuit wiring. The result is a single combinatorial circuit.\n  \n\n  * Crucially, this transformation can be done in polynomial time, as _A_ verifies _L_ in polynomial time, so only needs to make a polynomial number of copies of the circuit for _M_. (Since state changes only with new input, the number of copies of _M_ is bounded by the input size _n_.)\n  \n\n  * Then an algorithm that solves CIRCUIT-SAT can be used to simulate _A_, deciding _L_.\n\n### NP Completeness Proofs by Reduction\n\nPolynomial reduction is transitive:\n\n> If _L_ is a language such that _L'_ reduces polynomially to _L_ for some\n_L'_ ∈ NPC, then _L_ is NP-Hard.\n\n> Furthermore, if _L_ ∈ NP, then _L_ ∈ NPC.\n\nTransitivity follows from the definitions and that the sum of two polynomials\nis itself polynomial.\n\nThis means that we can prove that other problems are in NPC without having to\nreduce every possible problem to them. The general procedure for proving that\n_L_ is in NPC is:\n\n  1. **Prove _L_ ∈ NP** (show one can check solutions in polynomial time).\n  2. **Prove _L_ is NP-Hard**:\n    1. Select a known language _L'_ in NPC\n    2. Describe an algorithm _A_ that computes function _f_ mapping _every_ instance _x_ ∈ {0, 1}* of _L'_ to _some appropriately constructed_ instance _f_(_x_) of _L_. \n    3. Prove that x ∈ _L'_ iff _f_(_x_) ∈ _L_, ∀ _x_ ∈ {0, 1}*. \n    4. Prove that _A_ runs in polynomial time.\n\n_Important:_ Why doesn't mapping _every_ instance of _L_ to _some_ instances\nof _L_' work?\n\nNow we can populate the class NPC, first by reducing some problems to CIRCUIT-\nSAT, and then other problems to these new problems. Literally thousands of\nproblems have been proven to be in NPC by this means. Let's look at a few.\n\n![](fig/Fig-34-13-Structure-of-NPC-Proofs.jpg)\n\n* * *\n\n##  NP Complete Problems\n\nThe text steps through reduction of problems as shown in the figure. We do not\nhave time to go through the proofs, and it is more important that you are\naware of the diversity of problems in NPC than that you are able to do NP-\nCompleteness proofs. So we just list them briefly.\n\n### Satisfiability (SAT)\n\nAn instance is a boolean formula φ composed of\n\n  * _n_ boolean variables _x_1 ... _x__n_, \n  * _m_ boolean connectives of two inputs and one output: ∧, ∨ ¬ -> ↔, and \n  * parentheses.\n\nA **truth assignment** is a set of values for the variables of φ and a **\nsatisfying assignment** is a truth assignment that evaluates to 1 (true).\n\n> SAT = {⟨φ⟩ : φ is a satisfiable boolean formula}\n\nThere are 2_n_ possible assignments, but a given assignment can be checked in\npolynomial time.\n\n![](fig/Fig-34-10-CircuitSat-FormulaSat.jpg)\n\nCIRCUIT-SAT is reduced to SAT in polynomial time through a construction that\nturns CIRCUIT-SAT gates into small logical formulas for SAT:\n\n  1. Define a variable _xi_ for each of the wires.\n  2. Write a formula φ that is the conjunction of the variable for the output wire plus a logical expression for each of the logic gates.\n\nThe resulting boolean formula is satisfied just when the circuit is satisfied.\n(You can verify that the formula shown is equivalent to the circuit.)\n\n> _x_10 ∧ (_x_4 ↔ ¬ _x_3) ∧ (_x_5 ↔ (_x_1 ∨ _x_2)) ∧ (_x_6 ↔ ¬ _x_4) ∧ (_x_7 ↔\n(_x_1 ∧ _x_2 ∧ _x_4)) ∧ (_x_8 ↔ (_x_5 ∨ _x_6)) ∧ (_x_9 ↔ (_x_6 ∨ _x_7)) ∧\n(_x_10 ↔ (_x_7 ∧ _x_8 ∧ _x_9))\n\n### 3-Conjunctive Normal Form Satisfiability (3-CNF-SAT)\n\nReduction proofs require that we handle any possible case of a known NPC\nproblem. It would be complicated to handle all the possible forms of SAT\nformulas, so it is useful to have a more restricted logical form for the\ntarget for reduction proofs. 3-CNF serves this purpose.\n\nA **literal** in a boolean formula is an occurrence of a variable or its\nnegation.\n\nA boolean formula is in **conjunctive normal form (CNF)** if it is a\nconjunction of **clauses**, each of which is the disjunction of one or more\nliterals.\n\nA boolean formula is in **3-conjunctive normal form (3-CNF)** if each clause\nhas exactly three distinct literals. For example:\n\n> (_x_1∨ ¬_x_1 ∨ ¬_x_2) ∧ (_x_3 ∨ _x_2 ∨ _x_4) ∧ (¬_x_1 ∨ ¬_x_3 ∨ ¬_x_4)\n\n3-CNF-SAT asks whether a boolean formula is satisfiable by an assignment of\ntruth values to the variables.\n\nThere are an exponential possible number of variable assignments, but a given\none can be checked in polynomial time merely by substituting and evaluating\nthe expression.\n\nSAT can be reduced to 3-CNF-SAT through a polynomial-time process of:\n\n  1. parsing the SAT expression into a binary tree with literals as leaves and connectives as internal nodes; \n  2. introducing a variable _yi_ for the output of each internal node; \n  3. rewriting as the conjunction of the root variable and a clause for each node of the binary tree (_yi_ ↔ the literal for its child nodes); \n  4. converting each clause to conjunctive normal form (see text), first by converting to disjunctive normal form and then applying DeMorgan's laws to convert to CNF; and then \n  5. supplying dummy variables as needed to convert clauses of 1 or 2 variables into 3-CNF.\n\n![](fig/Fig-34-11-Phi-Tree.jpg)\n\nFor ((_x_1 -> _x_2) ∨ ¬((¬_x_1 ↔ _x_3) ∨ _x_4)) ∧ ¬_x_2, the tree is shown to\nthe right and the expression resulting from the tree is shown below.\n\n> _y_1 ∧ (_y_1 ↔ (_y_2 ∧ ¬_x_2)) ∧ (_y_2 ↔ (_y_3 ∨ _x_4))  \n    ∧ (_y_3 ↔ (_x_1 -> _x_2)) ∧ (_y_4 ↔ ¬_y_5)   \n    ∧ (_y_5 ↔ (_y_6 ∨ _x_4)) ∧ (_y_6 ↔ (¬_x_1 ↔ _x_3)) \n\nThe remainder of the conversion uses DeMorgan's laws (see the text for the\nstep by step description):\n\n> ¬(_a_ ∧ _b_) ≡ ¬_a_ ∨ ¬_b_  \n¬(_a_ ∨ _b_) ≡ ¬_a_ ∧ ¬_b_\n\nresulting in:\n\n> (¬_y_1 ∨ ¬_y_2 ∨ ¬_x_2) ∧ (¬_y_1 ∨ _y_2 ∨ ¬_x_2) ∧ (¬_y_1 ∨ _y_2 ∨ _x_2) ∧\n(_y_1 ∨ ¬_y_2 ∨ _x_2).\n\n![](fig/cliques.jpg)\n\n### CLIQUE\n\nA **clique** in an undirected graph _G_ = (_V_, _E_) is a subset _V'_ ⊆ _V_,\neach pair of which is connected by an edge in _E_ (a complete subgraph of\n_G_). (Example cliques are shown.)\n\nThe **clique problem** is the problem of finding a clique of maximum size in\n_G_. This can be converted to a decision problem by asking whether a clique of\na given size _k_ exists in the graph:\n\n> CLIQUE = {⟨_G_, _k_⟩ : _G_ is a graph containing a clique of size _k_}\n\n![](fig/Fig-34-14-3-CNF-to-Clique.jpg)\n\nOne can check a solution in polynomial time.\n\n3-CNF-SAT is reduced to CLIQUE by a clever reduction illustrated in the\nfigure.\n\n  * There is a vertex for every literal\n  * There is an edge between vertices only if the corresponding literals are in different triples _and_ the literals are consistent (i.e., one is not the negation of the other).\n\nIf there are _k_ clauses we ask whether the graph has a _k_-clique. Such a\nclique exists if and only if there is a satisfying assignment.\n\n  * The fact that there is a _k_-clique means there are _k_ vertices that are all connected to each other. \n  * The fact that two vertices are connected to each other means that they can receive a consistent boolean assignment, _and_ that they are in different clauses.\n  * Since there are _k_ vertices then a literal from each of the _k_ clauses must be satisfied. \n\nAny arbitrary instance of 3-CNF-SAT can be converted to an instance of CLIQUE\nwith this particular structure. That means if we can solve CLIQUE we can solve\nany instance of 3-CNF-SAT. Mapping an arbitrary instance of CLIQUE to a\nspecialized instance of 3-CNF-SAT would not work.\n\n### Vertex Cover\n\nA vertex cover of an undirected graph _G_ = (_V_, _E_) is a subset _V'_ ⊆ _V_\nsuch that if (_u_, _v_) ∈ _E_ then _u_ ∈ _V'_ or _v_ ∈ _V'_ or both.\n\nEach vertex \"covers\" its incident edges, and a vertex cover for _G_ is a set\nof vertices that covers all the edges in _E_.\n\nThe **Vertex Cover Problem** is to find a vertex cover of minimum size in _G_.\nPhrased as a decision problem,\n\n> VERTEX-COVER = {⟨_G_, _k_⟩ : graph _G_ has a vertex cover of size _k_}\n\n![](fig/Fig-34-15-Clique-to-Vertex-Cover.jpg)\n\nThere is a straightforward reduction of CLIQUE to VERTEX-COVER, illustrated in\nthe figure. Given an instance _G_=(_V_,_E_) of CLIQUE, one computes the\ncomplement of _G_, which we will call _Gc_ = (_V_,_Ē_), where (_u_,_v_) ∈ _Ē_\niff (_u_,_v_) ∉ _E_. The graph _G_ has a clique of size _k_ iff the complement\ngraph has a vertex cover of size |_V_| - _k_.\n\n###  Hamiltonian Cycle (HAM-CYCLE)\n\n> HAM-CYCLE = {⟨_G_⟩ : _G_ is a Hamiltonian graph}\n\nThe Hamiltonian Cycle problem is shown to be in NPC by reduction of VERTEX-\nCOVER to HAM-CYCLE.\n\nThe construction converts edges of _G_ an instance of VERTEX-COVER into\nsubgraph \"widgets\" through which one can find a portion of a Hamiltonian Cycle\nonly if one or both of the vertices of the edge are in a covering set. There\nis one such widget per edge.\n\n![](fig/Fig-34-16-Vertex-Cover-Ham-Cycle.jpg)\n\nAny Hamiltonian cycle must include all the vertices in the widget (a), but\nthere are only three ways to pass through each widget (b, c, and d in the\nfigure). If only vertex _u_ is included in the cover, we will use path (b); if\nonly vertex _v_ then path (d); otherwise path (c) to include both.\n\nThe widgets are then wired together in sequences that chain all the widgets\nthat involve a given vertex, so if the vertex is selected all of the widgets\ncorresponding to its edges will be reached.\n\n![](fig/Fig-34-17-Vertex-Cover-Ham-Cycle.jpg)\n\nFinally, _k_ selector vertices are added, and wired such that each will select\nthe _k_th vertex in the cover of size _k_. I leave it to you to examine the\ndiscussion in the text, to see how clever these reductions can be!\n\n### Traveling Salesperson Problem (TSP)\n\nFinally, one of the more famous problems: Suppose you are a traveling\nsalesperson, and you want to visit _n_ cities exactly once in a Hamiltonian\ncycle, but choosing a **tour** with minimum cost.\n\n> TSP = {⟨_G_, _c_, _k_⟩ : _G_ = (_V_, _E_) is a complete graph,  \n              _c_ : _V_ x _V_ -> ℕ,   \n              _k_ ∈ ℕ, and   \n              _G_ has a traveling-salesperson tour with cost at most _k_} \n\n![](fig/travelling_salesman_problem.jpg)\n\nOnly exponential solutions have been found to date, although it is easy to\ncheck a solution in polynomial time.\n\nThe reduction represents a HAM-CYCLE problem as a TSP problem on a complete\ngraph, but with the cost of the edges in TSP being 0 if the edge is in the\nHAM-CYCLE problem, or 1 if not.\n\n### Subset-Sum Problem (SUBSET-SUM)\n\n![](fig/np_complete.jpg)\n\nMany NP-Complete problems are of a numerical nature. We already mentioned\ninteger linear programming. Another example is the subset-sum problem: given a\nfinite set _S_ of positive integers and an integer target _t_ > 0, does there\nexist a subset of _S_ that sums to _t_?\n\n> SUBSET-SUM = {⟨_S_, _t_ : ∃ subset _S'_ ⊆ _S_ such that _t_ = Σ_s_∈_S'__s_}\n\nThe proof reduces 3-CNF-SAT to SUBSET-SUM. Please see the text for the details\nof yet another clever reduction! Briefly:\n\n![](fig/Fig-34-19-3-CNF-Sat-to-Subset-Sum.jpg)\n\n  * It involves constructing two numbers for each variable _xi_ (one for the varaible and one for its negation), and two numbers for each clause _Cj_ (these will hold \"slack variables\" needed to meet the target sum). \n  * The digits of the numbers are arranged in columns. \n  * The numbers for literals (variables and their negations) have a \"1\" in the column that indicates which variable it corresponds to, and also a \"1\" in the columns for the clauses in which that literal occurs.\n  * The numbers for clauses have either a 1 or a 2 in the column corresponding to that clause.\n  * The target value _t_ has a 1 in each digit labeled by a variable and a 4 in each digit labeled by a clause. \n  * The only way one can achieve this target sum is to select those numbers corresponding to the literals that are satisfied in each clause, plus the required slack variables to reach 4. \n\nFor example, see how this clause maps to the table shown:\n\n> (_x_1 ∨ ¬_x_2 ∨ ¬_x_3) ∧ (¬_x_1 ∨ ¬_x_2 ∨ ¬_x_3) ∧ (¬_x_1 ∨ ¬_x_2 ∨ _x_3) ∧\n(_x_1 ∨ _x_2 ∨ _x_3).\n\n###  Other Problems\n\nOne can find large catalogs of other problems online, starting with those\nidentified in the book Garey & Johnson (1979), Computers and Intractability.\nSee for example [Wikipedia](http://en.wikipedia.org/wiki/List_of_NP-\ncomplete_problems). Following Garey & Johnson's outline they list problems in:\n\n  1. Graph theory \n    1. Covering and partitioning\n    2. Subgraphs and supergraphs\n    3. Vertex ordering\n    4. Iso- and other morphisms\n    5. Miscellaneous\n  2. Network design \n    1. Spanning trees\n    2. Cuts and connectivity\n    3. Routing problems\n    4. Flow problems\n    5. Miscellaneous\n    6. Graph Drawing\n  3. Sets and partitions \n    1. Covering, hitting, and splitting\n    2. Weighted set problems\n    3. Set partitions\n  4. Storage and retrieval \n    1. Data storage\n    2. Compression and representation\n    3. Database problems\n  5. Sequencing and scheduling \n    1. Sequencing on one processor\n    2. Multiprocessor scheduling\n    3. Shop scheduling\n    4. Miscellaneous\n  6. Mathematical programming\n  7. Algebra and number theory\n    1. Divisibility problems\n    2. Solvability of equations\n    3. Miscellaneous\n  8. Games and puzzles\n  9. Logic \n    1. Propositional logic\n    2. Miscellaneous\n  10. Automata and language theory \n    1. 1 Automata theory\n    2. 2 Formal languages\n  11. Computational geometry\n  12. Program optimization\n    1. 1 Code generation\n    2. 2 Programs and schemes\n  13. Miscellaneous\n\nThere are also problems for which their status is as of yet unknown. Much work\nto do!\n\n* * *\n\nDan Suthers Last modified: Mon Apr 21 11:46:18 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition; Garey & Johnson (1979), Computers and\nIntractability; and Weisstein, Eric W. \"Complete Graph.\" From MathWorld--A\nWolfram Web Resource. http://mathworld.wolfram.com/CompleteGraph.html,\nxkcd.com, and possibly other sites whom I thank for not suing me.  \n\n",
 "path"=>"morea//240.np-completeness/reading-notes.md"}
</pre>

<h2>/morea/240.np-completeness/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Introduction to NP-completeness",
 "published"=>true,
 "morea_id"=>"reading-screencast-24a",
 "morea_summary"=>"very informal",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=Cm_c_y2z0HY",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/240.np-completeness/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//240.np-completeness/reading-screencast-a.md"}
</pre>

<h2>/morea/240.np-completeness/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"An NP-complete problem",
 "published"=>true,
 "morea_id"=>"reading-screencast-24b",
 "morea_summary"=>"Show that circuit satisfiability is NP-Complete",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=H4OYXaVkLA4",
 "morea_labels"=>["Screencast", "Suthers", "12 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/240.np-completeness/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//240.np-completeness/reading-screencast-b.md"}
</pre>

<h2>/morea/240.np-completeness/reading-screencast-c.html</h2>

<pre>Hash
{"title"=>"NP Complete problems",
 "published"=>true,
 "morea_id"=>"reading-screencast-24c",
 "morea_summary"=>
  "Examples of problems from logic, graph theory, and arithmetic",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=J5l-crl0LgA",
 "morea_labels"=>["Screencast", "Suthers", "25 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/240.np-completeness/reading-screencast-c.html",
 "content"=>"",
 "path"=>"morea//240.np-completeness/reading-screencast-c.md"}
</pre>

<h2>/morea/250.approximation-algorithms/module.html</h2>

<pre>Hash
{"title"=>"Approximation algorithms",
 "published"=>true,
 "morea_id"=>"approximation algorithms",
 "morea_outcomes"=>["outcome-approximation-algorithms"],
 "morea_readings"=>
  ["reading-screencast-25a", "reading-screencast-25b", "reading-notes-25"],
 "morea_experiences"=>[],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/250.approximation-algorithms/logo.png",
 "morea_sort_order"=>250,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/250.approximation-algorithms/module.html",
 "content"=>
  "Vertex cover example, TSP example, randomization and linear programming strategies",
 "path"=>"morea//250.approximation-algorithms/module.md"}
</pre>

<h2>/modules/approximation algorithms/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module.md",
 "title"=>"Approximation algorithms",
 "url"=>"/modules/approximation algorithms/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      <p>\n        {% for label in outcome.morea_labels %}\n        <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      <p>\n        {% for label in assessment.morea_labels %}\n          <span class=\"badge\">{{ label }}</span>\n        {% endfor %}\n      </p>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/approximation algorithms/index.html"}
</pre>

<h2>/morea/250.approximation-algorithms/outcome.html</h2>

<pre>Hash
{"title"=>"Understand approximation algorithms",
 "published"=>true,
 "morea_id"=>"outcome-approximation-algorithms",
 "morea_type"=>"outcome",
 "morea_sort_order"=>250,
 "referencing_modules"=>[#Jekyll:Page @name="module.md"],
 "url"=>"/morea/250.approximation-algorithms/outcome.html",
 "content"=>"Understand approximation algorithms.",
 "path"=>"morea//250.approximation-algorithms/outcome.md"}
</pre>

<h2>/morea/250.approximation-algorithms/reading-cormen.html</h2>

<pre>Hash
{"title"=>"CLRS 35 - Approximation algorithms (35.1, 35.2, 35.4)",
 "published"=>true,
 "morea_id"=>"reading-cormen-35",
 "morea_summary"=>
  "Vertex cover algorithm, traveling salesman problem, randomization and linear programming",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/250.approximation-algorithms/reading-cormen.html",
 "content"=>"",
 "path"=>"morea//250.approximation-algorithms/reading-cormen.md"}
</pre>

<h2>/morea/250.approximation-algorithms/reading-notes.html</h2>

<pre>Hash
{"title"=>"Notes on approximation algorithms",
 "published"=>true,
 "morea_id"=>"reading-notes-25",
 "morea_summary"=>
  "Vertex cover example, TSP example, randomization and linear programming strategies",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/250.approximation-algorithms/reading-notes.html",
 "url"=>"/morea/250.approximation-algorithms/reading-notes.html",
 "content"=>
  "## Outline\n\n  1. Approximation Algorithms\n  2. Example: Vertex Cover \n  3. Example: TSP \n  4. Two Strategies: Randomization and Linear Programming \n\n\"Although this may seem a paradox, all exact science is dominated by the idea\nof approximation.\" _− Bertrand Russell_\n\n* * *\n\n##  Approximation Algorithms\n\n![](fig/garey-johnson-you-and-boss.jpg)\n\nWell, your boss understands it's a hard problem, but he still wants you to do\nsomething about it! After all, we can't abandon the lucrative iThingy market!\nIs there a way to configure iThingies to be \"good enough\" without using a huge\namount of computer time delaying the orders?\n\nThere are three broad approaches to handing NP-Complete or NP-Hard problems in\npractice:\n\n  1. **Stick with small problems,** where the total execution time for an optimal solution is not bad. Your boss rejects this as it would limit the configuration options the company offers.\n  \n\n  2. **Find special cases** of the problem that can be solved in polynomial time (e.g., 2-CNF rather than 3-CNF). It requires that we know more about the structure of the problem. We don't know much about iThingies, but we will use some restrictions to help with the third approach ... \n  \n\n  3. **Find near-optimal solutions** with ** approximation algorithms**. Your boss thinks it just might work: since the problem is hard, customers won't realize you haven't given them the optimal solution as long as a lot of their requests are met. This is the approach we'll examine today. \n\n### Definitions\n\nLet _C_ be the cost of a solution found for a problem of size _n_ and _C_* be\nthe optimal solution for that problem.\n\nThen we say an algorithm has an **approximation ratio of ρ(n)** _(that's\n\"rho\")_ if\n\n> _C_/_C_* ≤ ρ(n) for minimization problems: the factor by which the actual\nsolution obtained is larger than the optimal solution.\n\n> _C_*/_C_ ≤ ρ(n) for maximization problems: the factor by which the optimal\nsolution is larger than the solution obtained\n\n![](fig/equation-approximation-ratio.jpg)\n\nThe CLRS text says both of these at once in one expression shown to the right.\nThe ratio is never less than 1 (perfect performance).\n\nAn algorithm that has an approximation ratio of ρ(n) is called a\n**ρ(_n_)-approximation algorithm**.\n\nAn **approximation scheme** is a parameterized approximation algorithm that\ntakes an additional input ε > 0 and for any fixed ε is a (1+ε)-approximation\nalgorithm.\n\nAn approximation scheme is a **polynomial approximation scheme** if for any\nfixed ε > 0 the scheme runs in time polynomial in input size _n_. (We will not\nbe discussing approximation schemes today; just wanted you to be aware of the\nidea. See section 35.5)\n\n### A Question\n\nBy definition, if a problem A is NP-Complete then if we can solve A in O(f(n))\nthen we can solve any other problem B in NP in O(g(n)) where g(n) is\npolynomially related to f(n). (A polynomial time reduction of the other\nproblems to A exists.)\n\n_So, if we have a ρ(n)-approximation algorithm for the optimization version of\nA, does this mean we have a ρ(n)-approximation algorithm for the optimization\nversion of any problem B in NP? Can we just use the same polynomial time\nreduction, and solve A, to get a ρ(n)-approximation for B?_\n\nThat would be pretty powerful! Below we show we have a 2-approximation\nalgorithm for NP-Hard Vertex Cover: so is 2-approximation possible for the\noptimization version of _any_ problem in NP? (See problem 35.1-5.)\n\nWe examine two examples in detail before summarizing other approximation\nstrategies.\n\n* * *\n\n##  Vertex Cover Approximations\n\nRecall that a **vertex cover** of an undirected graph _G_ = (_V_, _E_) is a\nsubset _V'_ ⊆ _V_ such that if (_u_, _v_) ∈ _E_ then _u_ ∈ _V'_ or _v_ ∈ _V'_\nor both (there is a vertex in _V'_ \"covering\" every edge in _E_).\n\nThe optimization version of the **Vertex Cover Problem** is to find a vertex\ncover of minimum size in _G_.\n\nWe previously showed by reduction of CLIQUE to VERTEX-COVER that the\ncorresonding decision problem is NP-Complete, so the optimization problem is\nNP-Hard.\n\n### Approx-Vertex-Cover\n\nVertex Cover can be approximated by the following surprisingly simple\nalgorithm, which iterately chooses an edge that is not covered yet and covers\nit:\n\n![](fig/code-approx-vertex-cover.jpg)  \n![](fig/Fig-35-1-Approx-Vertex-Cover-a.jpg)\n\n#### Example\n\nSuppose we have this input graph:\n\nSuppose then that edge {_b_, _c_} is chosen. The two incident vertices are\nadded to the cover and all other incident edges are removed from\nconsideration:\n\n![](fig/Fig-35-1-Approx-Vertex-Cover-b.jpg)\n\nIterating now for edges {_e_, _f_} and then {_d_, _g_}:\n\n![](fig/Fig-35-1-Approx-Vertex-Cover-c.jpg)\n![](fig/Fig-35-1-Approx-Vertex-Cover-d.jpg)\n\nThe resulting vertex cover is shown on the left and the optimal vertex on the\nright:\n\n![](fig/Fig-35-1-Approx-Vertex-Cover-e.jpg)\n![](fig/Fig-35-1-Approx-Vertex-Cover-opt.jpg)\n\n_(Would the approximation bound be tighter if we always chose an edge with the\nhighest degree vertex remaining? Let's try it on this example. Would it be\ntighter in general? See 35.1-3.)_\n\n#### Analysis\n\nHow good is the approximation? We can show that the solution is within a\nfactor of 2 of optimal.\n\n_Theorem:_ **Approx-Vertex-Cover is a polynomial time 2-approximation\nalgorithm for Vertex Cover.**\n\n![](fig/code-approx-vertex-cover.jpg)\n\n_Proof:_ The algorithm is correct because it loops until every edge in _E_ has\nbeen covered.\n\nThe algorithm has O(|_E_|) iterations of the loop, and (using aggregate\nanalysis, [Topic\n15](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-15.html))\nacross all loop iterations, O(|V|) vertices are added to _C_. Therefore it is\nO(_E_ \\+ _V_), so is polynomial.\n\nIt remains to be shown that the solution is no more than twice the size of the\noptimal cover. We'll do so by finding a lower bound on the optimal solution\n_C_*.\n\nLet _A_ be the set of edges chosen in line 4 of the algorithm. Any vertex\ncover must cover at least one endpoint of every edge in _A_. No two edges in\n_A_ share a vertex (see algorithm), so in order to cover _A_, the optimal\nsolution _C_* must have at least as many vertices:\n\n> | _A_ |   ≤   | _C_* |\n\nSince each execution of line 4 picks an edge for which neither endpoint is yet\nin _C_ and adds these two vertices to _C_, then we know that\n\n> | _C_ |   =   2 | _A_ |\n\nTherefore:\n\n> | _C_ |   ≤   2 | _C_* |\n\nThat is, |_C_| cannot be larger than twice the optimal, so is a\n2-approximation algorithm for Vertex Cover.\n\nThis is a common strategy in approximation proofs: we don't know the size of\nthe optimal solution, but we can set a lower bound on the optimal solution and\nrelate the obtained solution to this lower bound.\n\n### Problems\n\n_Can you come up with an example of a graph for which Approx-Vertex-Cover\nalways gives a suboptimal solution?_\n\nSuppose we restrict our graphs to trees. _Can you give an efficient greedy\nalgorithm that always finds an optimal vertex cover for trees in linear time?_\n\n* * *\n\n##  TSP Approximations\n\n![](fig/tsp-cartoon.jpg)\n\nIn the **Traveling Salesperson Problem** (TSP) we are given a complete\nundirected graph _G_ = (_V_, _E_) (representing, for example, routes between\ncities) that has a nonnegative integer cost _c_(_u_, _v_) for each edge {_u_,\n_v_} (representing distances between cities), and must find a Hamiltonian\ncycle or tour with minimum cost. We define the cost of such a cycle _A_ to be\nthe sum of the costs of edges:\n\n![](fig/equation-TSP-cost.jpg)\n\nThe unrestricted TSP is very hard, so we'll start by looking at a common\nrestriction.\n\n### Triangle Inequality TSP\n\nIn many applications (e.g., Euclidean distances on two dimensional surfaces),\nthe TSP cost function satisfies the **triangle inequality**:\n\n![](fig/triangle-inequality.jpg)\n\n> _c_(_u_, _v_)   ≤   _c_(_u_, _w_) + _c_(_w_, _v_),     ∀ _u_, _v_, _w_ ∈\n_V_.\n\nEssentially this means that it is no more costly to go directly from _u_ to\n_v_ than it would be to go between them via a third point _w_.\n\n#### Approximate Tour for Triangle Inequality TSP\n\nThe triangle inequality TSP is still NP-Complete, but there is a\n2-approximation algorithm for it. The algorithm finds a minimum spanning tree\n([Topic\n17](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-17.html)),\nand then converts this to a low cost tour:\n\n![](fig/code-approx-TSP-tour.jpg)\n\n_(Another MST algorithm might also work.)_\n\n#### Example\n\nSuppose we are working on the graph shown below to the left. (Vertices are\nplaced on a grid so you can compute distances if you wish.) The MST starting\nwith vertex _a_ is shown to the right.\n\n![](fig/Fig-35-2-Approx-TSP-a.jpg)\n![](fig/Fig-35-2-Approx-TSP-b.jpg)\n\nRecall from early in the semester (or ICS 241) that a preorder walk of a tree\nvisits a vertex before visiting its children. Starting with vertex _a_, the\npreorder walk visits vertices in order _a_, _b_, _c_, _h_, _d_, _e_, _f_, _g_.\nThis is the basis for constructing the cycle in the center (cost 19.074). The\noptimal solution is shown to the right (cost 14.715).\n\n![](fig/Fig-35-2-Approx-TSP-c.jpg)\n![](fig/Fig-35-2-Approx-TSP-d.jpg)\n![](fig/Fig-35-2-Approx-TSP-e.jpg)\n\n#### Analysis of Approx-TSP-Tour\n\n_Theorem:_ **Approx-TSP-Tour is a polynomial time 2-approximation algorithm\nfor TSP with triangle inequality.**\n\n![](fig/code-approx-TSP-tour.jpg)\n\n_Proof:_ The algorithm is correct because it produces a Hamiltonian circuit.\n\nThe algorithm is polynomial time because the most expensive operation is `MST-\nPrim`, which can be computed in O(_E_ lg _V_) (see [Topic 17 notes](http://www\n2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-17.html)).\n\nFor the approximation result, let _T_ be the spanning tree found in line 2,\n_H_ be the tour found and _H_* be an optimal tour for a given problem.\n\nIf we delete any edge from _H_*, we get a spanning tree that can be no cheaper\nthan the _minimum_ spanning tree _T_, because _H_* has one more (nonegative\ncost) edge than _T_:\n\n![](fig/Fig-35-2-Approx-TSP-c.jpg)\n\n> _c_(_T_)   ≤   _c_(_H_*)\n\nConsider the cost of the **full walk** _W_ that traverses the edges of _T_\nexactly twice starting at the root. (For our example, _W_ is ⟨{_a_, _b_},\n{_b_, _c_}, {_c_, _b_}, {_b_, _h_}, {_h_, _b_}, {_b_, _a_}, {_a_, _d_}, ...\n{_d_, _a_}⟩.) Since each edge in _T_ is traversed twice in _W_:\n\n> _c_(_W_)   =   2 _c_(_T_)\n\n![](fig/Fig-35-2-Approx-TSP-d.jpg)\n\nThis walk _W_ is not a tour because it visits some vertices more than once,\nbut we can skip the redundant visits to vertices once we have visited them,\nproducing the same tour _H_ as in line 3. (For example, instead of ⟨{_a_,\n_b_}, {_b_, _c_}, {_c_, _b_}, {_b_, _h_}, ... ⟩, go direct: ⟨{_a_, _b_}, {_b_,\n_c_}, {_c_, _h_}, ... ⟩.)\n\nBy the triangle inequality, which says it can't cost any more to go direct\nbetween two vertices,\n\n> _c_(_H_)   ≤   _c_(_W_)\n\nNoting that _H_ is the tour constructed by Approx-TSP-Tour, and putting all of\nthese together:\n\n> _c_(_H_)   ≤   _c_(_W_)   =   2 _c_(_T_)   ≤   2 _c_(_H_*)\n\nSo, _c_(_H_) ≤ 2 _c_(_H_*), and thus `Approx-TSP-Tour` is a 2-approximation\nalgorithm for TSP. (The CLRS text notes that there are even better solutions,\nsuch as a 3/2-approximation algorithm.)\n\n### Closest Point Heuristic\n\nAnother algorithm that is a 2-approximation on the triangle inequality TSP is\nthe **closest point heuristic**, in which one starts with a trivial cycle\nincluding a single arbitrarily chosen vertex, and at each iteration adds the\nnext closest vertex not on the cycle until the cycle is complete.\n\n### The General TSP\n\nAbove we got our results using a restriction on the TSP. Unfortunately, the\ngeneral problem is harder ...\n\n_Theorem:_ **If P ≠ NP, then for any constant ρ ≥ 1 there is no polynomial\ntime approximation algorithm with ratio ρ for the general TSP.**\n\nThe proof by contradiction shows that if there were such an approximation one\ncan solve instances of Hamiltonian Cycle in polynomial time. Since Hamiltonian\nCycle is NP-Complete, then P = NP. The proof uses a reduction similar to that\nused in [Topic\n24](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-24.html),\nwhere edges for TSP graph _G'_ are given unit cost only if the corresponding\nedge is in the edge-set _E_ for the Hamiltonian Cycle problem graph _G_:\n\n![](fig/equation-TSP-Ham-Cycle.jpg)\n\nFor _any_ ρ(n) ≥ 1, a TSP approximation algorithm will choose the edges of\ncost 1 in _G'_ (because to include even one edge not in _E_ would exceed the\napproximation ratio), thereby finding a Hamiltonian Cycle in _G_ if such a\ncycle exists. (See text for details.)\n\n* * *\n\n## Hierarchy of Problem Difficulty\n\nWe have just seen that even within NP, some problems are harder than others in\nterms of whether they allow approximations.\n\nThe proof technique of reduction to NP-Complete problems has been used to\norganize the class NPC into problems that can be polynomially approximated and\nthose that cannot under the assumption that P ≠ NP. Further discussion can be\nfound in Garey and Johnson (1979).\n\nYou can probably guess that the answer to the question I raised in the\nbeginning concerning transfer of ρ(n)-approximation across problem reductions\nis negative, but _ why would that be the case? Why aren't approximation\nproperties carried across problem reductions?_\n\n* * *\n\n##  Two Strategies\n\nVarious reusable strategies for approximations have been found, two of which\nwe review briefly here.\n\n### Randomized Approximations\n\nThe approximation ratio ρ(_n_) of a randomized algorithm is based on its\n**expected cost _C_**. Otherwise the definition is the same.\n\nA randomized algorithm that achieves an expected cost within a factor ρ(_n_)\nof the optimal cost _C_* is called a ** randomized ρ(_n_)-approximation\nalgorithm**.\n\n#### Max-3-CNF Satisfiability\n\nRecall that 3-CNF-SAT ([Topic\n24](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-24.html))\nasks whether a boolean formula in 3-conjunctive normal form (3-CNF) is\nsatisfiable by an assignment of truth values to the variables.\n\nThe Max-3-CNF variation is an optimization problem that seeks to maximize the\nnumber of conjunctive clauses evaluating to 1. We assume that no clause\ncontains both a variable and its negation.\n\nAmazingly, a purely random solution is expected to be pretty good:\n\n_Theorem:_ **The randomized algorithm that independently sets each variable of\nMAX-3-CNF to 1 with probability 1/2 and to 0 with probabilty 1/2 is a\nrandomized 8/7-approximation algorithm.**\n\n_Proof:_ Given a MAX-3-CNF instance with _n_ variables _x_1 ... _x__n_ and _m_\nclauses, set each variable randomly to either 0 or 1 with probability 1/2 in\neach case. Define the _indicator random variable_ ([Topic\n5](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-05.html)):\n\n> Y_i_ = I{clause _i_ is satisfied}.\n\n![](fig/lemming.jpg)\n\nA clause is only unsatisfied if all three literals are 0, so Pr{clause _i_ is\nnot satisfied} = (1/2)3 = 1/8. Thus, Pr{clause _i_ is satisfied} = 7/8. By an\nimportant lemma from [Topic\n5](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-05.html),\nE[Y_i_] = 7/8.\n\nLet Y = Σ Y1 ... Y_m_ be the number of clauses satisified overall. Then:\n\n![](fig/equations-expected-value-Y.jpg)\n\nSince _m_ is the upper bound _C_* on the number of satisfied clauses, the\napproximation ratio _C_* / _C_ is\n\n> _m_ / (7_m_/8)   =   8/7.\n\nThe restriction on a variable and its negation can be lifted. This is just an\nexample: randomization can be applied to many different problems − but don't\nalways expect it to work out so well!\n\n### Linear Programming Approximations\n\nSometimes we can \"relax\" a problem to make it amenable to linear programming\n([Topic\n21](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-21.html)).\nFor example ...\n\n#### Minimum-Weight Vertex-Cover\n\nIn the **minimum-weight vertex-cover** problem, we are given an undirected\ngraph _G_ = (_V_, _E_), and a weight function _w_(_v_) ≥ 0 for _v_ ∈ _V_. We\ndefine the weight of a vertex cover _V'_ to be Σv∈_V'__w_(_v_) and seek to\nfind a vertex cover of minimum weight.\n\n#### Linear Programming Relaxation\n\nLet each vertex _v_ ∈ _V_ be associated with a variable _x_(_v_), which is 1\niff _v_ in the vertex cover and 0 otherwise.\n\nSince any edge (_u_, _v_) must be covered, _x_(_u_) + _x_(_v_) ≥ 1\\. This\nleads to the NP-Hard **0-1 integer linear program**:\n\n![](fig/equations-0-1-integer-programming.jpg)\n\nNow let's \"relax\" the formulation to allow _x_(_v_) to range over 0 ≤ _x_(_v_)\n≤ 1\\. Then the problem can be written as this **linear programming\nrelaxation**:\n\n![](fig/equations-linear-programming-relaxation.jpg)\n\nSince a solution to the 0-1 integer version of the problem is a legal solution\nto the relaxed version of the problem, the value of an optimal solution to\nthis latter relaxed program gives a lower bound on the value of an optimal\nsolution to the 0-1 integer problem.\n\nThe solution to the relaxed linear program can be converted to an\napproximation of the integer linear program with this algorithm:\n\n![](fig/code-approx-min-weight-vc.jpg)\n\nThis procedure essentially \"rounds\" the fractional values to 0 or 1.\n\n#### Analysis\n\n_Theorem:_ **`Approx-Min-Weight-VC` is a polynomial 2-approximation algorithm\nfor the mimimum-weight vertex-cover problem.**\n\n_Proof:_ There is a polynomial time algorithm for linear programming (line 2),\nand lines 3-5 are also polynomial in time. So, `Approx-Min-Weight-VC` is\npolynomial.\n\nThe result must be a vertex cover, since for any edge (_u_, _v_) the\nconstraint _x_(_u_) + _x_(_v_) ≥ 1 implies that at least one of the vertices\nmust have a value of 1/2, so is included in the vertex cover by lines 4-5 of\nthe algorithm, thereby covering the edge.\n\nTo show 2-approximation, let _C_* be an optimal solution and let _z_* be the\nvalue of the solution to the relaxed linear program shown above.\n\nAn optimal solution _C_* must be a feasible solution to the relaxed linear\nprogram for which _z_* is an optimal solution, so _z_* cannot be any worse\nthan _C_*:\n\n> _z_*   ≤   _w_(_C_*)\n\nWe've already established that every edge is covered. We bound the weight of\nthis cover from above by transforming the value of the optimal solution to the\nrelaxed problem:\n\n![](fig/equations-z-star.jpg)\n\nSo, _w_(_C_) ≤ 2 _z_*. This result with the prior result of _z_* ≤ _w_(_C_*)\ngives:\n\n> _w_(_C_)   ≤   2_z_*   ≤   2_w_(_C_*)\n\nThat is, _w_(_C_) ≤ 2_w_(_C_*), so we have 2-approximation.\n\n* * *\n\n## Other Examples\n\nIt is worth reading the other examples in the text.\n\nSection 35.3 shows how the Set Covering Problem, which has many applications,\ncan be approximated using a simple greedy algorithm with a logarithmic\napproximation ratio.\n\nSection 35.5 uses the Subset Sum problem to show how an exponential but\noptimal algorithm can be transformed into a fully polynomial time\napproximation scheme, meaning that we can give the algorithm a parameter\nspecifying the desired approximation ratio.\n\nMany more examples are suggested in the problem set for the chapter.\n\n* * *\n\n## Summary of Strategies\n\nFaced with an NP Hard optimization problem, your options include:\n\n  1. Use a known exponential algorithm and stick to small problems.\n  2. Figure out whether you can restrict your problem to a special case for which polynomial solutions are known.\n  3. Give up on optimality, and find or design an approximation algorithm that gives \"good enough\" results. Strategies include: \n    * Design a clever approximation using some heuristic (e.g., as for vertex cover and TSP in this lecture). \n    * Model the problem as an integer linear program and relax it to allow real valued solutions that are then used (e.g. by rounding) to determine an approximate integer solution. \n    * Get lucky and show that randomly choosing a solution is good enough!\n\n* * *\n\nDan Suthers Last modified: Mon Jan 13 19:12:25 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition, and from Garey & Johnson (1979), Computers and\nIntractability.  \n\n",
 "path"=>"morea//250.approximation-algorithms/reading-notes.md"}
</pre>

<h2>/morea/250.approximation-algorithms/reading-screencast-a.html</h2>

<pre>Hash
{"title"=>"Approximation algorithms",
 "published"=>true,
 "morea_id"=>"reading-screencast-25a",
 "morea_summary"=>"Heuristic approximations for NP-hard problems",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=hdch8ioLRqE",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/250.approximation-algorithms/reading-screencast-a.html",
 "content"=>"",
 "path"=>"morea//250.approximation-algorithms/reading-screencast-a.md"}
</pre>

<h2>/morea/250.approximation-algorithms/reading-screencast-b.html</h2>

<pre>Hash
{"title"=>"Approximation strategies",
 "published"=>true,
 "morea_id"=>"reading-screencast-25b",
 "morea_summary"=>"Randomization and relaxed linear programming",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=a0fkEdx_K3g",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/250.approximation-algorithms/reading-screencast-b.html",
 "content"=>"",
 "path"=>"morea//250.approximation-algorithms/reading-screencast-b.md"}
</pre>

<h2>/morea//footer.html</h2>

<pre>Hash
{"title"=>"Footer",
 "morea_id"=>"footer",
 "morea_type"=>"footer",
 "referencing_modules"=>[],
 "url"=>"/morea//footer.html",
 "content"=>
  "Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>\nsuthers@hawaii.edu\n",
 "path"=>"morea//footer.md"}
</pre>

<h2>/morea//home.html</h2>

<pre>Hash
{"title"=>"Home",
 "morea_id"=>"home",
 "morea_type"=>"home",
 "referencing_modules"=>[],
 "url"=>"/morea//home.html",
 "content"=>
  "## Welcome to ICS 311, Spring 2014\n\n**If you are an ICS 311, Spring 2014 student, do not use this site.**  Instead, refer to the [real site](http://www2.hawaii.edu/~suthers/courses/ics311s14/index.html).\n\nThis is a representation of a subset of the course material for ICS 311 Algorithms, Spring 2014, created to illustrate the use of the \nMorea Framework.\n\nAs of April, 2014, the site contains the Modules, Readings, and Experiences from ICS 311.  It does not include actual Outcome or Assessment entities. We plan to update the site with Outcomes and Assessments in the near future.",
 "path"=>"morea//home.md"}
</pre>

</div>



<div class="dark-blue-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a style="color: white" href="http://morea-framework.github.io/">Morea Framework</a><br>
       Last update on: <span>2014-04-23 15:59:05 -1000</span></p>
  </div>
</footer>
</div>
</body>
</html>

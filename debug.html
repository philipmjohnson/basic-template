<!DOCTYPE html>
<html>
<head>
  <title> Debug | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/3.1.0/cerulean/bootstrap.min.css">

  <!--  Load site-specific customizations after bootstrap. -->
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" href="/ics311s14/css/syntax.css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:normal,italic,bold">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        <li><a href="/ics311s14/readings/">Readings</a></li>
        <li><a href="/ics311s14/experiences/">Experiences</a></li>
        <li><a href="/ics311s14/assessments/">Assessments</a></li>
        <li><a href="/ics311s14/schedule/">Schedule</a></li>
        
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h1>Debugging</h1>

<h2>Site</h2>

<pre>Hash
{"source"=>"./master/src",
 "destination"=>"./gh-pages",
 "plugins"=>"_plugins",
 "layouts"=>"_layouts",
 "data_source"=>"_data",
 "keep_files"=>[".git", ".svn"],
 "gems"=>[],
 "timezone"=>nil,
 "encoding"=>nil,
 "safe"=>false,
 "detach"=>false,
 "show_drafts"=>nil,
 "limit_posts"=>0,
 "lsi"=>false,
 "future"=>true,
 "pygments"=>true,
 "relative_permalinks"=>true,
 "markdown"=>"redcarpet",
 "permalink"=>"date",
 "baseurl"=>"/ics311s14",
 "include"=>[".htaccess"],
 "exclude"=>["morea"],
 "paginate_path"=>"/page:num",
 "markdown_ext"=>"markdown,mkd,mkdn,md",
 "textile_ext"=>"textile",
 "port"=>"4000",
 "host"=>"0.0.0.0",
 "excerpt_separator"=>"\n\n",
 "maruku"=>
  {"fenced_code_blocks"=>true,
   "use_tex"=>false,
   "use_divs"=>false,
   "png_engine"=>"blahtex",
   "png_dir"=>"images/latex",
   "png_url"=>"/images/latex"},
 "rdiscount"=>{"extensions"=>[]},
 "redcarpet"=>{"extensions"=>[]},
 "kramdown"=>
  {"auto_ids"=>true,
   "footnote_nr"=>1,
   "entity_output"=>"as_char",
   "toc_levels"=>"1..6",
   "smart_quotes"=>"lsquo,rsquo,ldquo,rdquo",
   "use_coderay"=>false,
   "coderay"=>
    {"coderay_wrap"=>"div",
     "coderay_line_numbers"=>"inline",
     "coderay_line_number_start"=>1,
     "coderay_tab_width"=>4,
     "coderay_bold_every"=>10,
     "coderay_css"=>"style"}},
 "redcloth"=>{"hard_breaks"=>true},
 "name"=>"ICS 311 Spring 2014",
 "morea_debug"=>false,
 "morea_module_pages"=>
  [#Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="module-quicksort.md",
   #Jekyll:Page @name="module-balanced-trees.md",
   #Jekyll:Page @name="module-dynamic-programming.md",
   #Jekyll:Page @name="module-greedy-algorithms.md",
   #Jekyll:Page @name="module-graphs.md",
   #Jekyll:Page @name="module-amortized-analysis.md",
   #Jekyll:Page @name="module-disjoint-sets.md",
   #Jekyll:Page @name="module-minimum-spanning-tree.md"],
 "morea_outcome_pages"=>
  [#Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="outcome-quicksort.md",
   #Jekyll:Page @name="outcome-balanced-trees-algorithm.md",
   #Jekyll:Page @name="outcome-dynamic-programming.md",
   #Jekyll:Page @name="outcome-greedy-algorithms.md",
   #Jekyll:Page @name="outcome-graphs.md",
   #Jekyll:Page @name="outcome-amortized-analysis.md",
   #Jekyll:Page @name="outcome-disjoint-sets.md",
   #Jekyll:Page @name="outcome-minimum-spanning-tree.md"],
 "morea_reading_pages"=>
  [#Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-17a.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-screencast-14a.md",
   #Jekyll:Page @name="reading-screencast-16a.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-11a.md",
   #Jekyll:Page @name="reading-screencast-12a.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-13a.md",
   #Jekyll:Page @name="reading-screencast-15a.md",
   #Jekyll:Page @name="reading-screencast-15b.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-cormen-21.md",
   #Jekyll:Page @name="reading-screencast-11b.md",
   #Jekyll:Page @name="reading-screencast-12b.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-14b.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-13b.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-17b.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-12d.md",
   #Jekyll:Page @name="reading-screencast-13c.md",
   #Jekyll:Page @name="reading-screencast-14c.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-12c.md",
   #Jekyll:Page @name="reading-screencast-17c.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-notes-16.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-11c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-cormen-23.md",
   #Jekyll:Page @name="reading-cormen-17.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="reading-screencast-11d.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-screencast-14d.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="reading-sedgewick-31.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-screencast-14e.md",
   #Jekyll:Page @name="reading-sedgewick-15.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-screencast-14f.md",
   #Jekyll:Page @name="reading-notes-15.md",
   #Jekyll:Page @name="reading-notes-17.md",
   #Jekyll:Page @name="reading-cormen-15.md",
   #Jekyll:Page @name="reading-cormen-13.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-sedgewick-37.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-cormen-16.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="reading-notes-13.md",
   #Jekyll:Page @name="reading-notes-11.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="reading-notes-12.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-cormen-22.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-sedgewick-32.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-sedgewick-wayne-4.md",
   #Jekyll:Page @name="reading-goodrich-graphs.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-notes-10.md",
   #Jekyll:Page @name="reading-notes-14.md"],
 "morea_experience_pages"=>
  [#Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="experience-disjoint-sets.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="experience-project-1.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="experience-amortized-analysis.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="experience-graph-transpose.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-balanced-trees.md",
   #Jekyll:Page @name="experience-greedy-algorithms.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="experience-dynamic-programming.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-balanced-trees-2.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree-2.md",
   #Jekyll:Page @name="experience-graph-scc.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-dynamic-programming-2.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-graph-bfs-dfs.md",
   #Jekyll:Page @name="experience-3.md"],
 "morea_assessment_pages"=>[],
 "morea_home_page"=>#Jekyll:Page @name="home.md",
 "morea_footer_page"=>#Jekyll:Page @name="footer.md",
 "morea_page_table"=>
  {"intro"=>#Jekyll:Page @name="module-introduction.md",
   "outcome-311"=>#Jekyll:Page @name="outcome-311.md",
   "outcome-algorithm"=>#Jekyll:Page @name="outcome-algorithm.md",
   "reading-algorithms"=>#Jekyll:Page @name="reading-algorithms.md",
   "reading-assessment"=>#Jekyll:Page @name="reading-assessment.md",
   "reading-assignments"=>#Jekyll:Page @name="reading-assignments.md",
   "reading-cormen-1"=>#Jekyll:Page @name="reading-cormen-1.md",
   "reading-course-info"=>#Jekyll:Page @name="reading-course-info.md",
   "reading-format"=>#Jekyll:Page @name="reading-format.md",
   "reading-policies"=>#Jekyll:Page @name="reading-policies.md",
   "reading-topic-overview"=>#Jekyll:Page @name="reading-topic-overview.md",
   "experience-1"=>#Jekyll:Page @name="experience-1.md",
   "experience-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-3"=>#Jekyll:Page @name="experience-3.md",
   "examples-insertion-merge-sort"=>#Jekyll:Page @name="module-examples.md",
   "outcome-analysis-style"=>#Jekyll:Page @name="outcome-analysis-style.md",
   "reading-cormen-2"=>#Jekyll:Page @name="reading-cormen-2.md",
   "reading-notes-2"=>#Jekyll:Page @name="reading-notes-2.md",
   "reading-screencast-2a"=>#Jekyll:Page @name="reading-screencast-2A.md",
   "reading-screencast-2b"=>#Jekyll:Page @name="reading-screencast-2B.md",
   "reading-screencast-2c"=>#Jekyll:Page @name="reading-screencast-2C.md",
   "reading-screencast-2d"=>#Jekyll:Page @name="reading-screencast-2D.md",
   "reading-screencast-2e"=>#Jekyll:Page @name="reading-screencast-2E.md",
   "reading-screencast-mit-1"=>
    #Jekyll:Page @name="reading-screencast-mit-1.md",
   "experience-asymptotic-concepts"=>
    #Jekyll:Page @name="experience-asymptotic-concepts.md",
   "growth"=>#Jekyll:Page @name="module-growth.md",
   "outcome-growth"=>#Jekyll:Page @name="outcome-growth.md",
   "reading-cormen-3"=>#Jekyll:Page @name="reading-cormen-3.md",
   "reading-notes-3"=>#Jekyll:Page @name="reading-notes-3.md",
   "reading-screencast-3a"=>#Jekyll:Page @name="reading-screencast-3a.md",
   "reading-screencast-3b"=>#Jekyll:Page @name="reading-screencast-3b.md",
   "reading-screencast-3c"=>#Jekyll:Page @name="reading-screencast-3c.md",
   "reading-screencast-3d"=>#Jekyll:Page @name="reading-screencast-3d.md",
   "reading-screencast-mit-2"=>
    #Jekyll:Page @name="reading-screencast-mit-2.md",
   "experience-asymptotic-homework"=>
    #Jekyll:Page @name="experience-asymptotic-homework.md",
   "experience-asymptotic-basic-data-structures"=>
    #Jekyll:Page @name="experience-basic-data-structures.md",
   "experience-project-1"=>#Jekyll:Page @name="experience-project-1.md",
   "adt"=>#Jekyll:Page @name="module-adt.md",
   "outcome-adt"=>#Jekyll:Page @name="outcome-adt.md",
   "reading-cormen-10"=>#Jekyll:Page @name="reading-cormen-10.md",
   "reading-notes-4"=>#Jekyll:Page @name="reading-notes-4.md",
   "reading-screencast-4a"=>#Jekyll:Page @name="reading-screencast-4a.md",
   "reading-screencast-4b"=>#Jekyll:Page @name="reading-screencast-4b.md",
   "experience-indicator-random-variables"=>
    #Jekyll:Page @name="experience-indicator-random-variables.md",
   "probabilistic"=>#Jekyll:Page @name="module-probabilistic.md",
   "outcome-probabilistic"=>#Jekyll:Page @name="outcome-probabilistic.md",
   "reading-cormen-5"=>#Jekyll:Page @name="reading-cormen-5.md",
   "reading-goodrich"=>#Jekyll:Page @name="reading-goodrich.md",
   "reading-notes-5"=>#Jekyll:Page @name="reading-notes-5.md",
   "reading-screencast-5a"=>#Jekyll:Page @name="reading-screencast-5a.md",
   "reading-screencast-5b"=>#Jekyll:Page @name="reading-screencast-5b.md",
   "reading-screencast-5c"=>#Jekyll:Page @name="reading-screencast-5c.md",
   "reading-screencast-5d"=>#Jekyll:Page @name="reading-screencast-5d.md",
   "reading-screencast-mit-skip-lists"=>
    #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   "experience-data-structures-homework"=>
    #Jekyll:Page @name="experience-data-structures-homework.md",
   "experience-deletion"=>#Jekyll:Page @name="experience-deletion.md",
   "hash-tables"=>#Jekyll:Page @name="module-hash-tables.md",
   "outcome-hash-tables"=>#Jekyll:Page @name="outcome-hash-tables.md",
   "reading-cormen-11"=>#Jekyll:Page @name="reading-cormen-11.md",
   "reading-notes-6"=>#Jekyll:Page @name="reading-notes-6.md",
   "reading-screencast-6a"=>#Jekyll:Page @name="reading-screencast-6a.md",
   "reading-screencast-6b"=>#Jekyll:Page @name="reading-screencast-6b.md",
   "reading-screencast-6c"=>#Jekyll:Page @name="reading-screencast-6c.md",
   "reading-screencast-6d"=>#Jekyll:Page @name="reading-screencast-6d.md",
   "reading-screencast-mit-hash-tables-1"=>
    #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   "reading-screencast-mit-hash-tables-2"=>
    #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   "experience-master-method"=>
    #Jekyll:Page @name="experience-master-method.md",
   "experience-substitution"=>#Jekyll:Page @name="experience-substitution.md",
   "divide-conquer"=>#Jekyll:Page @name="module-divide-conquer.md",
   "outcome-divide-conquer-apply"=>
    #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   "outcome-divide-conquer-recognize"=>
    #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   "reading-cormen-4"=>#Jekyll:Page @name="reading-cormen-4.md",
   "reading-notes-7"=>#Jekyll:Page @name="reading-notes-7.md",
   "reading-screencast-7a"=>#Jekyll:Page @name="reading-screencast-7a.md",
   "reading-screencast-7b"=>#Jekyll:Page @name="reading-screencast-7b.md",
   "reading-screencast-7c"=>#Jekyll:Page @name="reading-screencast-7c.md",
   "reading-screencast-7d"=>#Jekyll:Page @name="reading-screencast-7d.md",
   "reading-screencast-mit-divide-conquer"=>
    #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   "experience-binary-search-trees-2"=>
    #Jekyll:Page @name="experience-binary-search-trees-2.md",
   "experience-binary-search-trees"=>
    #Jekyll:Page @name="experience-binary-search-trees.md",
   "binary-search-trees"=>#Jekyll:Page @name="module-binary-search-trees.md",
   "outcome-binary-search-trees"=>
    #Jekyll:Page @name="outcome-binary-search-trees.md",
   "reading-cormen-12"=>#Jekyll:Page @name="reading-cormen-12.md",
   "reading-notes-8"=>#Jekyll:Page @name="reading-notes-8.md",
   "reading-screencast-8a"=>#Jekyll:Page @name="reading-screencast-8a.md",
   "reading-screencast-8b"=>#Jekyll:Page @name="reading-screencast-8b.md",
   "reading-screencast-8c"=>#Jekyll:Page @name="reading-screencast-8c.md",
   "reading-screencast-8d"=>#Jekyll:Page @name="reading-screencast-8d.md",
   "experience-heaps-2"=>#Jekyll:Page @name="experience-heaps-2.md",
   "experience-heaps"=>#Jekyll:Page @name="experience-heaps.md",
   "heaps"=>#Jekyll:Page @name="module-heaps.md",
   "outcome-heaps"=>#Jekyll:Page @name="outcome-heaps.md",
   "reading-cormen-6"=>#Jekyll:Page @name="reading-cormen-6.md",
   "reading-notes-9"=>#Jekyll:Page @name="reading-notes-9.md",
   "reading-screencast-9a"=>#Jekyll:Page @name="reading-screencast-9a.md",
   "reading-screencast-9b"=>#Jekyll:Page @name="reading-screencast-9b.md",
   "reading-screencast-9c"=>#Jekyll:Page @name="reading-screencast-9c.md",
   "reading-screencast-9d"=>#Jekyll:Page @name="reading-screencast-9d.md",
   "experience-quicksort-2"=>#Jekyll:Page @name="experience-quicksort-2.md",
   "experience-quicksort"=>#Jekyll:Page @name="experience-quicksort.md",
   "quicksort"=>#Jekyll:Page @name="module-quicksort.md",
   "outcome-quicksort"=>#Jekyll:Page @name="outcome-quicksort.md",
   "reading-cormen-7"=>#Jekyll:Page @name="reading-cormen-7.md",
   "reading-cormen-8"=>#Jekyll:Page @name="reading-cormen-8.md",
   "reading-notes-10"=>#Jekyll:Page @name="reading-notes-10.md",
   "reading-screencast-10a"=>#Jekyll:Page @name="reading-screencast-10a.md",
   "reading-screencast-10b"=>#Jekyll:Page @name="reading-screencast-10b.md",
   "reading-screencast-10c"=>#Jekyll:Page @name="reading-screencast-10c.md",
   "experience-balanced-trees-2"=>
    #Jekyll:Page @name="experience-balanced-trees-2.md",
   "experience-balanced-trees"=>
    #Jekyll:Page @name="experience-balanced-trees.md",
   "balanced-trees"=>#Jekyll:Page @name="module-balanced-trees.md",
   "outcome-balanced-trees-algorithm"=>
    #Jekyll:Page @name="outcome-balanced-trees-algorithm.md",
   "reading-cormen-13"=>#Jekyll:Page @name="reading-cormen-13.md",
   "reading-notes-11"=>#Jekyll:Page @name="reading-notes-11.md",
   "reading-screencast-11a"=>#Jekyll:Page @name="reading-screencast-11a.md",
   "reading-screencast-11b"=>#Jekyll:Page @name="reading-screencast-11b.md",
   "reading-screencast-11c"=>#Jekyll:Page @name="reading-screencast-11c.md",
   "reading-screencast-11d"=>#Jekyll:Page @name="reading-screencast-11d.md",
   "reading-sedgewick-15"=>#Jekyll:Page @name="reading-sedgewick-15.md",
   "experience-dynamic-programming-2"=>
    #Jekyll:Page @name="experience-dynamic-programming-2.md",
   "experience-dynamic-programming"=>
    #Jekyll:Page @name="experience-dynamic-programming.md",
   "dynamic-programming"=>#Jekyll:Page @name="module-dynamic-programming.md",
   "outcome-dynamic-programming"=>
    #Jekyll:Page @name="outcome-dynamic-programming.md",
   "reading-cormen-15"=>#Jekyll:Page @name="reading-cormen-15.md",
   "reading-notes-12"=>#Jekyll:Page @name="reading-notes-12.md",
   "reading-screencast-12a"=>#Jekyll:Page @name="reading-screencast-12a.md",
   "reading-screencast-12b"=>#Jekyll:Page @name="reading-screencast-12b.md",
   "reading-screencast-12c"=>#Jekyll:Page @name="reading-screencast-12c.md",
   "reading-screencast-12d"=>#Jekyll:Page @name="reading-screencast-12d.md",
   "reading-sedgewick-37"=>#Jekyll:Page @name="reading-sedgewick-37.md",
   "experience-greedy-algorithms"=>
    #Jekyll:Page @name="experience-greedy-algorithms.md",
   "greedy-algorithms"=>#Jekyll:Page @name="module-greedy-algorithms.md",
   "outcome-greedy-algorithms"=>
    #Jekyll:Page @name="outcome-greedy-algorithms.md",
   "reading-cormen-16"=>#Jekyll:Page @name="reading-cormen-16.md",
   "reading-notes-13"=>#Jekyll:Page @name="reading-notes-13.md",
   "reading-screencast-13a"=>#Jekyll:Page @name="reading-screencast-13a.md",
   "reading-screencast-13b"=>#Jekyll:Page @name="reading-screencast-13b.md",
   "reading-screencast-13c"=>#Jekyll:Page @name="reading-screencast-13c.md",
   "experience-graph-bfs-dfs"=>
    #Jekyll:Page @name="experience-graph-bfs-dfs.md",
   "experience-graph-scc"=>#Jekyll:Page @name="experience-graph-scc.md",
   "experience-graph-transpose"=>
    #Jekyll:Page @name="experience-graph-transpose.md",
   "graphs"=>#Jekyll:Page @name="module-graphs.md",
   "outcome-graphs"=>#Jekyll:Page @name="outcome-graphs.md",
   "reading-cormen-22"=>#Jekyll:Page @name="reading-cormen-22.md",
   "reading-goodrich-graphs"=>#Jekyll:Page @name="reading-goodrich-graphs.md",
   "reading-notes-14"=>#Jekyll:Page @name="reading-notes-14.md",
   "reading-screencast-14a"=>#Jekyll:Page @name="reading-screencast-14a.md",
   "reading-screencast-14b"=>#Jekyll:Page @name="reading-screencast-14b.md",
   "reading-screencast-14c"=>#Jekyll:Page @name="reading-screencast-14c.md",
   "reading-screencast-14d"=>#Jekyll:Page @name="reading-screencast-14d.md",
   "reading-screencast-14e"=>#Jekyll:Page @name="reading-screencast-14e.md",
   "reading-screencast-14f"=>#Jekyll:Page @name="reading-screencast-14f.md",
   "reading-sedgewick-32"=>#Jekyll:Page @name="reading-sedgewick-32.md",
   "reading-sedgewick-wayne-4"=>
    #Jekyll:Page @name="reading-sedgewick-wayne-4.md",
   "experience-amortized-analysis"=>
    #Jekyll:Page @name="experience-amortized-analysis.md",
   "amortized-analysis"=>#Jekyll:Page @name="module-amortized-analysis.md",
   "outcome-amortized-analysis"=>
    #Jekyll:Page @name="outcome-amortized-analysis.md",
   "reading-cormen-17"=>#Jekyll:Page @name="reading-cormen-17.md",
   "reading-notes-15"=>#Jekyll:Page @name="reading-notes-15.md",
   "reading-screencast-15a"=>#Jekyll:Page @name="reading-screencast-15a.md",
   "reading-screencast-15b"=>#Jekyll:Page @name="reading-screencast-15b.md",
   "experience-disjoint-sets"=>
    #Jekyll:Page @name="experience-disjoint-sets.md",
   "disjoint-sets"=>#Jekyll:Page @name="module-disjoint-sets.md",
   "outcome-disjoint-sets"=>#Jekyll:Page @name="outcome-disjoint-sets.md",
   "reading-cormen-21"=>#Jekyll:Page @name="reading-cormen-21.md",
   "reading-notes-16"=>#Jekyll:Page @name="reading-notes-16.md",
   "reading-screencast-16a"=>#Jekyll:Page @name="reading-screencast-16a.md",
   "experience-minimum-spanning-tree-2"=>
    #Jekyll:Page @name="experience-minimum-spanning-tree-2.md",
   "experience-minimum-spanning-tree"=>
    #Jekyll:Page @name="experience-minimum-spanning-tree.md",
   "minimum-spanning-tree"=>
    #Jekyll:Page @name="module-minimum-spanning-tree.md",
   "outcome-minimum-spanning-tree"=>
    #Jekyll:Page @name="outcome-minimum-spanning-tree.md",
   "reading-cormen-23"=>#Jekyll:Page @name="reading-cormen-23.md",
   "reading-notes-17"=>#Jekyll:Page @name="reading-notes-17.md",
   "reading-screencast-17a"=>#Jekyll:Page @name="reading-screencast-17a.md",
   "reading-screencast-17b"=>#Jekyll:Page @name="reading-screencast-17b.md",
   "reading-screencast-17c"=>#Jekyll:Page @name="reading-screencast-17c.md",
   "reading-sedgewick-31"=>#Jekyll:Page @name="reading-sedgewick-31.md",
   "footer"=>#Jekyll:Page @name="footer.md",
   "home"=>#Jekyll:Page @name="home.md"},
 "morea_fatal_errors"=>false,
 "time"=>2014-04-18 16:12:25 -1000,
 "posts"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
 "pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="experience-project-1.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="module-quicksort.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-quicksort.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-notes-10.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="experience-balanced-trees-2.md",
   #Jekyll:Page @name="experience-balanced-trees.md",
   #Jekyll:Page @name="module-balanced-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-balanced-trees-algorithm.md",
   #Jekyll:Page @name="reading-cormen-13.md",
   #Jekyll:Page @name="reading-notes-11.md",
   #Jekyll:Page @name="reading-screencast-11a.md",
   #Jekyll:Page @name="reading-screencast-11b.md",
   #Jekyll:Page @name="reading-screencast-11c.md",
   #Jekyll:Page @name="reading-screencast-11d.md",
   #Jekyll:Page @name="reading-sedgewick-15.md",
   #Jekyll:Page @name="experience-dynamic-programming-2.md",
   #Jekyll:Page @name="experience-dynamic-programming.md",
   #Jekyll:Page @name="module-dynamic-programming.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-dynamic-programming.md",
   #Jekyll:Page @name="reading-cormen-15.md",
   #Jekyll:Page @name="reading-notes-12.md",
   #Jekyll:Page @name="reading-screencast-12a.md",
   #Jekyll:Page @name="reading-screencast-12b.md",
   #Jekyll:Page @name="reading-screencast-12c.md",
   #Jekyll:Page @name="reading-screencast-12d.md",
   #Jekyll:Page @name="reading-sedgewick-37.md",
   #Jekyll:Page @name="experience-greedy-algorithms.md",
   #Jekyll:Page @name="module-greedy-algorithms.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-greedy-algorithms.md",
   #Jekyll:Page @name="reading-cormen-16.md",
   #Jekyll:Page @name="reading-notes-13.md",
   #Jekyll:Page @name="reading-screencast-13a.md",
   #Jekyll:Page @name="reading-screencast-13b.md",
   #Jekyll:Page @name="reading-screencast-13c.md",
   #Jekyll:Page @name="experience-graph-bfs-dfs.md",
   #Jekyll:Page @name="experience-graph-scc.md",
   #Jekyll:Page @name="experience-graph-transpose.md",
   #Jekyll:Page @name="module-graphs.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-graphs.md",
   #Jekyll:Page @name="reading-cormen-22.md",
   #Jekyll:Page @name="reading-goodrich-graphs.md",
   #Jekyll:Page @name="reading-notes-14.md",
   #Jekyll:Page @name="reading-screencast-14a.md",
   #Jekyll:Page @name="reading-screencast-14b.md",
   #Jekyll:Page @name="reading-screencast-14c.md",
   #Jekyll:Page @name="reading-screencast-14d.md",
   #Jekyll:Page @name="reading-screencast-14e.md",
   #Jekyll:Page @name="reading-screencast-14f.md",
   #Jekyll:Page @name="reading-sedgewick-32.md",
   #Jekyll:Page @name="reading-sedgewick-wayne-4.md",
   #Jekyll:Page @name="experience-amortized-analysis.md",
   #Jekyll:Page @name="module-amortized-analysis.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-amortized-analysis.md",
   #Jekyll:Page @name="reading-cormen-17.md",
   #Jekyll:Page @name="reading-notes-15.md",
   #Jekyll:Page @name="reading-screencast-15a.md",
   #Jekyll:Page @name="reading-screencast-15b.md",
   #Jekyll:Page @name="experience-disjoint-sets.md",
   #Jekyll:Page @name="module-disjoint-sets.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-disjoint-sets.md",
   #Jekyll:Page @name="reading-cormen-21.md",
   #Jekyll:Page @name="reading-notes-16.md",
   #Jekyll:Page @name="reading-screencast-16a.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree-2.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree.md",
   #Jekyll:Page @name="module-minimum-spanning-tree.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-minimum-spanning-tree.md",
   #Jekyll:Page @name="reading-cormen-23.md",
   #Jekyll:Page @name="reading-notes-17.md",
   #Jekyll:Page @name="reading-screencast-17a.md",
   #Jekyll:Page @name="reading-screencast-17b.md",
   #Jekyll:Page @name="reading-screencast-17c.md",
   #Jekyll:Page @name="reading-sedgewick-31.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "html_pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="experience-project-1.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="module-quicksort.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-quicksort.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-notes-10.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="experience-balanced-trees-2.md",
   #Jekyll:Page @name="experience-balanced-trees.md",
   #Jekyll:Page @name="module-balanced-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-balanced-trees-algorithm.md",
   #Jekyll:Page @name="reading-cormen-13.md",
   #Jekyll:Page @name="reading-notes-11.md",
   #Jekyll:Page @name="reading-screencast-11a.md",
   #Jekyll:Page @name="reading-screencast-11b.md",
   #Jekyll:Page @name="reading-screencast-11c.md",
   #Jekyll:Page @name="reading-screencast-11d.md",
   #Jekyll:Page @name="reading-sedgewick-15.md",
   #Jekyll:Page @name="experience-dynamic-programming-2.md",
   #Jekyll:Page @name="experience-dynamic-programming.md",
   #Jekyll:Page @name="module-dynamic-programming.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-dynamic-programming.md",
   #Jekyll:Page @name="reading-cormen-15.md",
   #Jekyll:Page @name="reading-notes-12.md",
   #Jekyll:Page @name="reading-screencast-12a.md",
   #Jekyll:Page @name="reading-screencast-12b.md",
   #Jekyll:Page @name="reading-screencast-12c.md",
   #Jekyll:Page @name="reading-screencast-12d.md",
   #Jekyll:Page @name="reading-sedgewick-37.md",
   #Jekyll:Page @name="experience-greedy-algorithms.md",
   #Jekyll:Page @name="module-greedy-algorithms.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-greedy-algorithms.md",
   #Jekyll:Page @name="reading-cormen-16.md",
   #Jekyll:Page @name="reading-notes-13.md",
   #Jekyll:Page @name="reading-screencast-13a.md",
   #Jekyll:Page @name="reading-screencast-13b.md",
   #Jekyll:Page @name="reading-screencast-13c.md",
   #Jekyll:Page @name="experience-graph-bfs-dfs.md",
   #Jekyll:Page @name="experience-graph-scc.md",
   #Jekyll:Page @name="experience-graph-transpose.md",
   #Jekyll:Page @name="module-graphs.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-graphs.md",
   #Jekyll:Page @name="reading-cormen-22.md",
   #Jekyll:Page @name="reading-goodrich-graphs.md",
   #Jekyll:Page @name="reading-notes-14.md",
   #Jekyll:Page @name="reading-screencast-14a.md",
   #Jekyll:Page @name="reading-screencast-14b.md",
   #Jekyll:Page @name="reading-screencast-14c.md",
   #Jekyll:Page @name="reading-screencast-14d.md",
   #Jekyll:Page @name="reading-screencast-14e.md",
   #Jekyll:Page @name="reading-screencast-14f.md",
   #Jekyll:Page @name="reading-sedgewick-32.md",
   #Jekyll:Page @name="reading-sedgewick-wayne-4.md",
   #Jekyll:Page @name="experience-amortized-analysis.md",
   #Jekyll:Page @name="module-amortized-analysis.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-amortized-analysis.md",
   #Jekyll:Page @name="reading-cormen-17.md",
   #Jekyll:Page @name="reading-notes-15.md",
   #Jekyll:Page @name="reading-screencast-15a.md",
   #Jekyll:Page @name="reading-screencast-15b.md",
   #Jekyll:Page @name="experience-disjoint-sets.md",
   #Jekyll:Page @name="module-disjoint-sets.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-disjoint-sets.md",
   #Jekyll:Page @name="reading-cormen-21.md",
   #Jekyll:Page @name="reading-notes-16.md",
   #Jekyll:Page @name="reading-screencast-16a.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree-2.md",
   #Jekyll:Page @name="experience-minimum-spanning-tree.md",
   #Jekyll:Page @name="module-minimum-spanning-tree.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-minimum-spanning-tree.md",
   #Jekyll:Page @name="reading-cormen-23.md",
   #Jekyll:Page @name="reading-notes-17.md",
   #Jekyll:Page @name="reading-screencast-17a.md",
   #Jekyll:Page @name="reading-screencast-17b.md",
   #Jekyll:Page @name="reading-screencast-17c.md",
   #Jekyll:Page @name="reading-sedgewick-31.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "categories"=>
  {"jekyll"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
   "update"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"]},
 "tags"=>{},
 "data"=>{}}
</pre>

<h2>Pages</h2>

<h2>/assessments/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Assessments",
 "url"=>"/assessments/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Assessments <small>in module order</small></h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/intro/index.html\">Introduction</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/examples-insertion-merge-sort/index.html\">Analysis examples</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/growth/index.html\">Growth of functions</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/adt/index.html\">Abstract data types</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/probabilistic/index.html\">Probabilistic Analysis</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/hash-tables/index.html\">Hash Tables</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/divide-conquer/index.html\">Divide and conquer</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/binary-search-trees/index.html\">Binary Search Trees</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/heaps/index.html\">Heapsort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/quicksort/index.html\">Quicksort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/balanced-trees/index.html\">Balanced Trees</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/dynamic-programming/index.html\">Dynamic Programming</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/greedy-algorithms/index.html\">Greedy Algorithms</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/graphs/index.html\">Graphs</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/amortized-analysis/index.html\">Amortized analysis</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/disjoint-sets/index.html\">Disjoint sets</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/minimum-spanning-tree/index.html\">Minimum spanning tree</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n",
 "path"=>"assessments/index.md"}
</pre>

<h2>/debug.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Debug",
 "topdiv"=>"container",
 "url"=>"/debug.html",
 "content"=>
  "Debugging\n=========\n\nSite\n----\n\n{{ site | debug }}\n\nPages\n-----\n\n{% for page in site.pages %}\n{{ page.url }}\n--------------\n\n{{ page | debug }}\n{% endfor %}\n\n\n",
 "path"=>"debug.md"}
</pre>

<h2>/experiences/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Experiences",
 "url"=>"/experiences/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Experiential Learning <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_experiences %}\n      {% assign experience = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n             {{ experience.morea_summary | markdownify }}\n             <p>\n             {% for label in experience.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n       {% if forloop.index == 4 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 8 %}\n         </div><div class=\"row\">\n       {% endif %}\n      {% if forloop.index == 12 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 16 %}\n         </div><div class=\"row\">\n       {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"experiences/index.md"}
</pre>

<h2>/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Home",
 "topdiv"=>"container",
 "url"=>"/index.html",
 "content"=>
  "{% if site.morea_home_page %}\n  {{ site.morea_home_page.content | markdownify }}\n{% else %}\n  No home page content supplied.\n{% endif %}\n\n",
 "path"=>"index.md"}
</pre>

<h2>/modules/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Modules",
 "url"=>"/modules/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Modules <small>Topics covered in this class.</small></h1>\n  <div class=\"row\">\n     {% for module in site.morea_module_pages %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <img src=\"{{ site.baseurl }}{{ module.morea_icon_url }}\" width=\"100\" class=\"img-circle img-responsive\">\n            <div class=\"caption\">\n              <h3 style=\"text-align: center; margin-top: 0\">{{ forloop.index }}. {{ module.title }}</h3>\n              {{ module.content | markdownify }}\n              <p>\n              {% for label in module.morea_labels %}\n                <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n              </p>\n              {% if module.morea_coming_soon %}\n                <p class=\"text-center\"><a href=\"#\" class=\"btn btn-default\" role=\"button\">Coming soon...</a></p>\n              {% else %}\n                <p class=\"text-center\"><a href=\"{{ module.morea_id }}\" class=\"btn btn-primary\" role=\"button\">Learn more...</a></p>\n              {% endif %}\n            </div>\n          </div>\n        </div>\n       {% cycle '', '', '', '</div><div class=\"row\">' %}\n     {% endfor %}\n  </div>\n</div>\n\n\n",
 "path"=>"modules/index.md"}
</pre>

<h2>/outcomes/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Learning Outcomes",
 "url"=>"/outcomes/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Learning Outcomes</h1>\n</div>\n\n{% if site.morea_outcome_pages.size == 0 %}\n<p>No outcomes for this course.</p>\n{% endif %}\n\n\n{% for outcome in site.morea_outcome_pages %}\n\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Outcome:</small> {{ outcome.title }}</h2>\n    {{ outcome.content | markdownify }}\n    <p>\n    <em>Referencing modules:</em>\n    {% for module in outcome.referencing_modules %}\n      <a href=\"../modules/{{ module.morea_id }}\">{{ module.title }}</a>\n    {% endfor %}\n  </div>\n</div>\n\n{% endfor %}\n\n\n",
 "path"=>"outcomes/index.md"}
</pre>

<h2>/readings/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Readings",
 "url"=>"/readings/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Readings and other resources <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_readings %}\n      {% assign reading = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n             {{ reading.morea_summary | markdownify }}\n             <p>\n             {% for label in reading.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n        {% if forloop.index == 4 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 8 %}\n          </div><div class=\"row\">\n        {% endif %}\n       {% if forloop.index == 12 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 16 %}\n          </div><div class=\"row\">\n        {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"readings/index.md"}
</pre>

<h2>/schedule/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Schedule",
 "url"=>"/schedule/index.html",
 "content"=>
  "<!-- Load FullCalendar for schedule page. -->\n<!-- Documentation available at: http://arshaw.com/fullcalendar/docs/google_calendar/ -->\n<link rel=\"stylesheet\" href=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.css\">\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.min.js\"></script>\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/gcal.js\"></script>\n<div class=\"container\">\n  <h1>Schedule</h1>\n  <div id='calendar'></div>\n</div>\n<script>\n  $(document).ready(function () {\n\n    // page is now ready, initialize the calendar. See documentation page above.\n    // In brief: make the calendar public, and paste in the XML feed.\n    $('#calendar').fullCalendar({\n      events: 'http://www.google.com/calendar/feeds/hawaii.edu_h008qigs793hp1llpbindcun50%40group.calendar.google.com/public/basic'\n    })\n  });\n</script>\n",
 "path"=>"schedule/index.html"}
</pre>

<h2>/morea/010.introduction/module-introduction.html</h2>

<pre>Hash
{"title"=>"Introduction",
 "published"=>true,
 "morea_id"=>"intro",
 "morea_outcomes"=>["outcome-algorithm", "outcome-311"],
 "morea_readings"=>
  ["reading-course-info",
   "reading-assessment",
   "reading-assignments",
   "reading-format",
   "reading-policies",
   "reading-topic-overview",
   "reading-algorithms",
   "reading-cormen-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/010.introduction/module-introduction.jpg",
 "morea_sort_order"=>10,
 "referencing_modules"=>[],
 "morea_experiences"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/010.introduction/module-introduction.html",
 "content"=>
  "Information, assessment, format, assignments, policies, topics, and the role of algorithms in computing.\n\n\n\n",
 "path"=>"morea//010.introduction/module-introduction.md"}
</pre>

<h2>/modules/intro/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-introduction.md",
 "title"=>"Introduction",
 "url"=>"/modules/intro/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/intro/index.html"}
</pre>

<h2>/morea/010.introduction/outcome-311.html</h2>

<pre>Hash
{"title"=>"Understand the procedures for ICS 311.",
 "published"=>true,
 "morea_id"=>"outcome-311",
 "morea_type"=>"outcome",
 "morea_sort_order"=>11,
 "referencing_modules"=>[#Jekyll:Page @name="module-introduction.md"],
 "url"=>"/morea/010.introduction/outcome-311.html",
 "content"=>
  "Understand the policies, course format, assignments, and assessment mechanisms for ICS 311.",
 "path"=>"morea//010.introduction/outcome-311.md"}
</pre>

<h2>/morea/010.introduction/outcome-algorithm.html</h2>

<pre>Hash
{"title"=>"Understand what algorithm analysis is.",
 "published"=>true,
 "morea_id"=>"outcome-algorithm",
 "morea_type"=>"outcome",
 "morea_sort_order"=>10,
 "referencing_modules"=>[#Jekyll:Page @name="module-introduction.md"],
 "url"=>"/morea/010.introduction/outcome-algorithm.html",
 "content"=>"Understand the formal and informal definitions of \"algorithm.\"",
 "path"=>"morea//010.introduction/outcome-algorithm.md"}
</pre>

<h2>/morea/010.introduction/reading-algorithms.html</h2>

<pre>Hash
{"title"=>"Chapter 1 Notes",
 "published"=>true,
 "morea_id"=>"reading-algorithms",
 "morea_summary"=>
  "Overview of algorithms and why we study them in this course.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-algorithms.html",
 "url"=>"/morea/010.introduction/reading-algorithms.html",
 "content"=>
  "## Algorithms and Programs\n\nBy now you have a working idea of what a \"program\" is because you have written\nmany. Programs are particular instructions that work on specific machines.\n\nIn this course we work with an abstraction of programs: algorithms.\n\n### Algorithms\n\nInformally, an algorithm is a well-defined computational procedure that takes\nsome value(s) as input and produces some value(s) as output.\n\nSomewhat more formally, an algorithm is a finite sequence of instructions\nchosen from a finite, fixed set of instructions, where the sequence of\ninstructions satisfies the following criteria:\n\n  * **Input:** It has zero or more input parameters. \n  * **Output:** It produces at least one output parameter. \n  * **Definiteness:** Each instruction must be clear and unambiguous. \n  * **Finiteness:** For every input, it executes only a finite number of instructions (it eventually halts). \n  * **Effectiveness:** Every instruction must be sufficiently basic so that a machine can execute the instruction.\n\n_Discussion:_ What is the difference between an algorithm and a program?\n\n_Discussion:_ What kinds of algorithms do you think are needed when you use\nyour smartphone (or any mobile phone for that matter?)\n\n* * *\n\n## Algorithm Design & Analysis\n\nHere is how algorithm design is situated within the phases of problem solving\nin software development:\n\nPhase 1: **Formulation of the Problem** (Requirements Specification)\n\nTo understand a real problem, **model it mathematically**, and specify\ninput and output of the problem clearly.\n\n  \nPhase 2: **Design and Analysis of an Algorithm** for the Problem (our focus)\n\n* Step 1: **Specification** of an Algorithm - **what is it?**\n* Step 2: **Verification** of the Algorithm - **is it correct?**\n* Step 3: **Analysis** of the Algorithms - **what is its time and space complexity?**\n\nPhase 3: **Implementation of the Algorithm**\n\nDesign data structures and realize the algorithm as executable code for\na targeted platform (lower level abstraction).\n\nPhase 4: **Performance Evaluation of the Implementation** (Testing)\n\nThe predicted performance of the algorithm can be evaluated/verified by\nempirical testing.\n\nAlgorithms are often specified in **pseudocode**, a mixture of programming\nlanguage control structures, mathematical notation, and natural language\nwritten to be as unambiguous as possible. Examples will be given in the next\nlecture. In this course, we will use mostly the notation used in the book.\n\n* * *\n\n###  Why not just test programs?\n\nWhy not just run experimental studies on programs? We can implement the\nalgorithms of interest, run them on a modern computer on various input sizes,\nand compare the results. Why bother with all this math in the book?\n\nI find the math painful too, but there are three major limitations to\nexperimental studies:\n\n  * To run experiments, you have to implement and run the algorithm. Implementation takes time, and some of these runs may take a long time.\n  * Experiments can only be done on a limited set of test inputs. Are you sure your results generalize to all possible inputs? \n  * It is difficult to compare the efficency of tests run on one hardware and software environment to what will happen on others. Are you sure that your results generalize across platforms? \n\nFormal analysis of algorithms:\n\n  * Can be performed on a high-level description of the algorithm without implementation.\n  * Takes into account all possible inputs. \n  * Allows comparisons of algorithms independently of hardware and software.\n\nSo, there is a good reason ICS 311 is THE central course of the ICS\ncurriculum! Stick with us.\n\n* * *\n\n\n## Computational Complexity\n\n### Input Size\n\nThe computational complexity of an algorithm generally depends on the amount\nof information given as input to the algorithm.\n\nThis amount can be formally defined as the **number of bits** needed to\nrepresent the input information with a reasonable, non-redundant coding\nscheme.\n\nTo simplify things, we often analyze algorithms in terms of larger constant-\nsized **data units** (e.g., signed integer, floating point number, string of\nbounded length, or data record).\n\nThese units are a _constant factor_ larger than a single bit, and are operated\non as a unit, so the result of the analysis is the same.\n\n### Measures of Complexity\n\nThe choice of algorithms and data structures has a critical impact on the\nfollowing, both of which are used as measures of computational complexity:\n\n  * Run **time** to solve a problem of a given input size\n  * Storage **space** for data, including auxiliary structures\n\n###  Example (preview of next lecture)\n\nFor example, suppose you have an input size of n elements, such as n strings\nto be sorted in lexicographic order. Suppose further that you have two\nalgorithms at your disposal (these algorithms will be examined in detail in\nthe next lecture):\n\n**Insertion sort**:\n    \n\n  1. start with an empty list \n  2. take each item to be sorted and insert it in its proper location \n  \n**Merge sort**:\n    \n\n  1. if the list has only one item, return it \n  2. otherwise, split the list in half, sort each half with this procedure, and then merge the results \n\nWe will see that given _n_ items to be sorted (it does not matter what they\nare as long as they are bounded by a constant size and can be compared by an <\noperator),\n\n  * _Insertion sort_ takes time proportional to _c_1_n_2 steps, where _c_1 is a constant depending on the implementation, and requires space proportional to _n_.\n  \n\n  * _Merge sort_ takes _c_2_n_ lg(_n_) steps, where _c_2 is another constant depending on the merge sort implementation, and requires space proportional to 2_n_. \n\n_Exercise:_ This would be a good place for you to pause and do excercise\n1.2-2, page 14:\n\nSuppose we are comparing implementations of insertion sort and merge sort on\nthe same machine, where c1=8 and c2=16. For which values of n does insertion\nsort beat merge sort?\n\nConstants matter for small input sizes, but since constants don't grow we\nignore them when concerned with the time complexity of large inputs: it is the\ngrowth in terms of _n_ that matters.\n\nIn the example above, ignoring the constants and factoring out the common n in\neach term shows that the difference in growth rate is _n_ versus lg(_n_). For\none million items to sort, this would be a time factor of one million for\ninsertion sort, but about 20 for merge sort.\n\n### Models of Computation\n\nRather than bother with determining the constant factors for any given\nimplementation or computer, algorithms for a problem are analyzed by using an\nabstract machine called a **model of computation**.\n\nMany models of computation have been proposed, but they are essentially\nequivalent to each other (Church-Turing Thesis) as long as computation\nexecuted on them are _deterministic_ and _sequential_. Commonly used models\nare _Turing Machines_ and _Random Access Machines_ (see Section 2.2 of the\ntextbook).\n\n### Run Times for Different Complexities\n\nIn general, suppose that you have a computer of speed 107 steps per second.\nThe running time of algorithms of the given complexity (rows) as a function of\n_n_ would be:\n\n    \n    \n      --------------------------------------------------------------------\n      size n    10       20       30       50      100       1000    10000\n      --------------------------------------------------------------------\n      n         0.001ms  0.002ms  0.003ms  0.005ms  0.01ms   0.1ms     1ms\n      n lg n    0.003ms  0.008ms  0.015ms  0.03ms   0.07ms   1ms      13ms\n      n^2       0.01ms   0.04ms   0.09ms   0.25ms   1ms      100ms    10s\n      n^3       0.1ms    0.8ms    2.7ms    12.5ms   100ms    100s     28h\n    \n      ...................................................................\n    \n      2^n       0.1ms    0.1s     100s     3yr      3x10^13c  inf     inf\n      --------------------------------------------------------------------\n    \n\n_Discussion:_ What is the difference between analysis of an algorithm and\nanalysis of an implementation (a program)?\n\n_Discussion:_ What is the relationship between the efficiency of an algorithm\nand the difficulty of the problem to be solved by that algorithm? (We return\nto this in the last two topics of the semester, but see also below.)\n\nConsider the example above: the problem of sorting a list of items. We saw two\nalgorithms for solving the problem, one more efficient than the other. Is it\npossible to make a statement about the time efficiency of _any possible_\nalgorithm for the problem of sorting? (We address this question in Topic #10.)\n\n### Easy vs Hard Problems\n\nTheoretical computer science has made substantial progress on understanding\nthe intrinsic difficulty of _problems_ (across all possible algorithms),\nalthough there are still significant open questions (one in particular).\n\nFirst of all, there are problems that we cannot solve, i.e., problems for\nwhich there does not exist any algorithm. Those problems are called\n**unsolvable** (or **undecidable** or **incomputable**), and include the\n_Halting Problem_ (refer to Section 3.1 pp. 176-177 of the textbook for ICS141\n& 241).\n\nWithin the problems that can be solved, there is a hierarchy of **complexity\nclasses** according to how difficult they are. Difficulty is based on proofs\nof the minimum complexity of _any_ algorithm that solves the problem, and on\nproofs of equivalences between problems (translating one into another). Here\nis a graphic:\n\n![](Complexity-Hierarchy.jpg)\n\n(Although algorithms can be ranked by this hierarchy, the above figure refers\nto problem classes, not algorithms.)\n\nSometimes small differences in a problem specification can make a big\ndifference in complexity.\n\nFor example, suppose you use a graph of vertices representing cities and\nweighted edges between the vertices representing the distance via the best\nroad traveling directly between the cities.\n\n  * The **Single Pair Shortest Paths** problem: what is the shortest path between a single pair of vertices (from one start vertex to one destination vertex) in the weighted graph? \n  * The **Shortest Paths** problem: what is the shortest path from one vertex to all of the other vertices in the weighted graph? \n  * The **All Pairs Shortest Paths** problem: what is the shortest path between every pair of vertices in the weighted graph? \n  * The **Traveling Salesman** problem: what is the shortest path that starts at given vertex in a weighted graph and visits all (or a specified set of) other vertices once before returning to the start vertex?\n\n_Discussion:_ How do these problems differ from each other? Which are easier\nand which are harder? Which are tractable (e.g., can be computed in polynomial\ntime) and which are potentially intractable (e.g, require exponential time)?\n\nComplexity theory will be the topic of our second to last lecture.\n\n* * *\n\n##  Abstract Data Types\n\nAlgorithms and data structures go together. We often study algorithms in the\ncontext of Abstract Data Types (ADTs). But let's start with Data Structures.\n\n### Data Structures\n\nYou are already familiar with Data Structures. They are defined by:\n\n  * **Operations:** Specifications of external appearance of a data structure. \n  * **Storage Structures:** Organizations of data implemented in lower level data structures. (We are almost always building abstractions on layers of abstractions above the actual physical implementation.) \n  * **Algorithms:** Description of how to manipulate information in the storage structures to obtain the results defined for the operations \n\nThe definition of a data structure requires that you specify implementation\ndetails such as storage structures and algorithms. It would be better to hide\nthese details until we are ready to deal with them.\n\nAlso we may want to write a specification in terms of desired behavior\n(Operations) and then compare alternative storage structures and algorithms as\npossible solutions meeting those specifications. Data structures don't work\nbecause they already assume a given solution. Abstract Data Types or ADTs let\nus do this by abstracting the representations and algorithms.\n\n### Definition of Abstract Data Types (ADTs)\n\nAn ADT is a class of instances (i.e., data objects) with a set of the\noperations that can be applied to the data objects.\n\nAn ADT tells us _what to do_ instead of how to do it. This provides the\nspecifications againsts which we can design different algorithms: the _how_\npart.\n\nAn ADT is specified by\n\n  1. **the type(s) of data objects involved **\n  2. **a set of operations that can be applied to those objects**, and \n  3. **a set of properties (called axioms) that all the objects and operations must satisfy**.\n\n### Example: Stack ADT\n\nObjects:\n\n    Stack, and Elements (of arbitrary type) \n  \nOperations (categorized into three types):\n\n\nConstructor\n\n    `new()` creates the empty stack and returns it.\n\nAccessors\n\n     `empty(_s_)` returns whether stack _s_ is empty.  \n`top(_s_)` returns the element of stack _s_ that has been inserted into s\nlast.\n\nMutators (or Modifiers)\n\n     `push(_s_,_e_)` inserts an element _e_ into _s_.  \n`pop(_s_)` deletes the top element from stack _s_.\n\n  \nProperties:\n\n  * `top(push(_s_,_e_)) returns value _e_`\n  * `pop(push(_s_,_e_)) leaves _s_` in the same state \n  * `empty(new()) = true `\n  * `empty(push(_s_,_i_)) = false `\n  * `pop(new()) is an error `\n  * `top(new()) is an error `\n\n### Specification and Implementation\n\nADTs can be specified in different languages:\n\n  * formal languages (axiomatic, algebraic, functional, denotational semantics, etc.)\n  * natural language\n\nImplementation of an ADT requires\n\n  * defining the storage for the data structures\n  * implementing the algorithms for the operations\n\n### Advantages of ADTs\n\nModularity (Encapsulation)\n\nAbstract operations mean a program using an ADT are isolated from (need not know about or be affected by) the implementation of the ADT.\n\n  * Implementation of ADT can be changed without modifying programs using ADT.\n  * Makes a program smaller, simpler, and have less side effects   \n  * Helps to construct correct programs \n  \nHierarchical Specification\n\n     Supports Top-Down Design and Stepwise Refinement\n  \nImplementation\n\n    ADTs map well to Object-Oriented Programming Languages\n\n_Discussion:_ What is the difference between an ADT and a data structure?\n\n* * *\n\n_Some of the material in this page was adopted with permission (and\nsignificant editing) from Kazuo Sugihara's spring 2011 Lecture Notes #02._\n\n",
 "path"=>"morea//010.introduction/reading-algorithms.md"}
</pre>

<h2>/morea/010.introduction/reading-assessment.html</h2>

<pre>Hash
{"title"=>"Assessment",
 "published"=>true,
 "morea_id"=>"reading-assessment",
 "morea_summary"=>"Grading in ICS 311",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-assessment.html",
 "url"=>"/morea/010.introduction/reading-assessment.html",
 "content"=>
  "## Overview\n\nThe approach to assignments, exams, and relative weighting is intended to\nassess multiple aspects of your developing expertise in algorithm design,\nanalysis and implementation; while providing a little flexibility to recognize\nindividual strengths. In summary, the components and their default weights\n(percentages and points, where 10 points = 1% of your grade) include:\n\n  * **Quizzes** (120 points = 12%, 24 quizzes of 5 points each),\n  * **Problem Sets** (305 points = 30.5%, 8 sets of 30 points, 3 sets of 15 points, 1 set of 20 points; of which 105 is _in-class_ work and 200 _homework_)\n  * **Projects** (260 points = 26%, three at 60, 100, and 100 points, respectively)\n  * **Midterm Exams** (240 points = 24%, three at 80 points each)\n  * **Final Exam** (100 points = 10% of grade). \n\nThis sums to 1025 points: you can miss 25 points of work and still earn 1000\npoints.\n\nExtra credit (Variable), may be earned by peer assessment of group\nparticipation in class, by additional work that will be specified in some\nhomework problems and implementation assignments, and by having others use\nyour software in the third assigment. (There will be no separate extra credit\nproblems or projects, as this creates more grading work than we have TA time.)\n\n## Points, Percents and Letter Grades\n\nA \"point\" will be worth 0.1% of your grade. For example, an item worth 100\npoints is 10% of your grade, and a perfect score is 1000 points. At the end of\nthe semester, I add up all your points and divide by 10 to get your percentage\nof points earned, capping it at 100%.\n\nTo determine letter grades, I use a 4-percent spread per grade increment,\ni.e., 100-97=A+, 96-93=A, 92-89=A-, 88-85=B+, 84-81=B, 80-77=B-, 76-73=C+,\n72-69=C, 68-65=C-, 64-61=D+, 60-57=D, 56-53=D-, 52-0=F. I will set it up this\nway in Laulima. If upon inspection of the distribution of grades I feel that\ntoo many students who understand the material are not getting the grades they\ndeserve, I may then make adjustments in favor of students ... but don't rely\non this!\n\n## Components\n\n**Quizzes (12%, 120 points):**\n\nBefore the classes that have lecture material, a brief quiz will be due online. These quizzes will test basic understanding of the chapter on which the day's topic is based, such as whether you can simulate the operation of the data structure or algorithms or get the main point of the analyses of their relative merits. Most quizzes will not involve mathematical analysis or proofs: problems requiring deeper thought will be left for the classwork and homework problems. Quizzes will be given in Laulima, and will be automatically graded. Solutions will be given in class immediately after the quizzes are due, so _quizzes cannot be made up_.\n  \nI am expecting 24 quizzes worth 5 points each. There are 25 topics. We don't\ninclude Topic 1 or the two special topics, which leaves 22 topics with\nquizzes, but Topic 2 and Topic 14 have two quizzes.\n\n  \n**Problems (30.5%, 305 points):**\n\nThese will relatively frequent assignments intended to help assess how well you understand the algorithms and analytic concepts being presented. They will require more thought than the problems given on the quizzes. They will come in two parts: an in-class portion done, turned in, and graded in groups, and an after-class portion turned in individually the following Monday. I attempt to choose in-class problems that help expose conceptual issues in the material and prepare you to work on the take-home portion on your own. Thus, by working in a group in class you help each other understand the problem that you will then solve and turn in individually. See also \"Peer Evaluation of Participation\" under Extra Credit below for how everyone can earn extra credit through participating in the group sessions.\n  \nPoints are allocated as follows. Most topics are allocated 15 points, of which\n5 are allocated to the group work turned in at the end of class, and 10 to the\nindividual work turned in later. In 8 of the weeks, the problem set will\nconsist of two topics, for 30 points each week (10 being groupwork turned in\nover the two days and 20 turned in by the individual). In 3 of the weeks,\nthere is one topic, for 15 points. When we cover Topic 14 on Graphs, there\nwill be two in-class problems and one homework for 20 points. The TA will\ngrade problem sets. We aspire to a one week or less turn-around. If you have\nquestions about solutions after they are due or graded, post them in Laulima\nand I will discuss solutions in class after the exercises are due.\n\n  \n**Projects (Analysis, Implementation, Testing) (26%, 260 points):**\n\nThere will be three projects. These typically involve writing Java implementations of abstract data types and associated algorithms, and testing these on sample data. You will also provide instructions on how to compile and run the program, document your design and implementation (including complexith analysis), and present and discuss test results. The assignments will progressively give you more responsibility. For the first project, you will be told what to implement, and it will be weighted 6% (60 points). For the second assignment you will need to make some implementation choices. The second assignment will be weighted 10% (100 points). The third assignment will require some research and decision making on your part to solve the problem. It will be weighted 10% (100 points). The TA will grade the first two projects, and both the TA and the instructor will grade the third.\n  \n**Midterms (24%, 240 points):**\n\nThere will be three midterm exams taking one class period each. They will include problems similar to those on the quizzes (for the easiest problems), and class and homework problems (for the harder ones), covering both understanding of the algorithms and how to analyze them. They will cover the most recent set of lecture topics, but cumulative \"review\" questions may also be included. Exams are open-book, open-notes on paper, but no electronic devices allowed. Each midterm is 80 points; there are three for 240 points or 24%. The instructor will grade all midterm exams.\n  \n**Final (10%, 100 points):**\n\nThe final exam will take place at the time scheduled by the university and will be longer than a midterm exam. It will cover the final set of lecture topics, but also include review of the entire semester. Since the final is longer and is cumulative as well as covering recent material, it is weighted more (100 points). Grading may be shared between TA and instructor.\n\n## Extra Credit (Variable Points)\n\nYou may earn extra credit several ways. _The extra credit points will be\nrecorded in a separate field in Laulima and allocated where they are needed at\nthe end of the semester. Thus they do not appear in the grade estimate\ncalculated by Laulima during the semester._\n\n**Peer Evaluation of Participation**\n\nEach week in which there is a problem set, each individual in the group may assign points distributed across the other individuals in the group to assess how effectively they collaborated in the group. You should allocate the points according to how well the others worked as team members, including their role in team functioning (e.g., keeping the group focused and organized, or playing another important role), and how much they helped others understand the material (e.g., by explaining what they understood), as well as their contributions to the actual problem solution.\n  \nTo ensure that students in smaller groups are not penalized, we use this\nscheme, which distributes 12 points across each group:  \n\n  * If _three members_ besides yourself were present at some time, you have a total of _3 points_ to allocate across all members (NOT 3 points per member!).\n  * If only _two members_ besides yourself were present, you have a total of _4 points_ to allocate across all members.\n  * If only _one other member_ was present, you have a total of _6 points_ to allocate to that person. \n  * You need not allocate all the points available to you.\n  * _You cannot allocate these points to yourself!_ Points allocated to yourself will not be recorded.\n  * You will allocate these points when you turn in your assignment. To encourage you to do this, _you will be given one extra credit point for each assignment in which you assess your peers_.\n  \nSome example scenarios: if everyone else contributed equally, you might give 1\npoint to each person. If one person in the group was taking the majority of\nthe initiative in a helpful way and the other two were not so engaged, you\nmight assign the helpful person all three points If there was one person who\ndid slightly more, one who helped some, and one slacker, you might allocate 2,\n1, and 0 points. Or if you had to do everything yourself you can allocate 0\npoints to everyone.  \n  \nObviously, a person who is helpful in the groups can earn extra credit this\nway. The maximum possible (very unlikely) is that they get all the points from\neveryone in every group they participate in: 9*12=108, or about a 10% grade\nincrease. If everyone gave their three points equally to all group members,\nthe result would be 3*12=36 points, or about a 3.5% grade increase. So, just\nby participating in class you get a little boost to your grade.\n\n  \n**Classwork, Homework and Project Add-ons**\n\nExtra credit problems will be included in some class sessions, homework problem sets, and projects. For example, an extra problem may be provided in class for those groups who finish early; a more difficult problem may be included in a homework set; or a student may program more challenging graph manipulations for extra credit in the programming projects. Points will be specified when they are given.\n\n**Software Reuse**\n\nIn the third assignment, students will have the option of using other student's implementations of ADTs from prior assignments. If your software is chosen by another student, you will be awarded 5 points for each \"customer\" _in proportion to their grade and use of your software_. For example, if a student uses your Graph ADT from Project 2 in their Project 3 and gets 80%, you will get 4 points. Partial credit is given if only parts of your code are used. The \"customer\" student must acknowledge your code in their project documentation (Readme and Reference manual).\n\n## Flexibility\n\nWe each have our own strengths. If a student performs _significantly_ better\non one area than others, I may elect to put greater weight on the area that\ngives the student a better grade. I am more willing to do this with strong\nexam performance, as exams are not easy and are proctored so I know it's the\nstudent's work. However, excellent programming may be considered as well.\n\n",
 "path"=>"morea//010.introduction/reading-assessment.md"}
</pre>

<h2>/morea/010.introduction/reading-assignments.html</h2>

<pre>Hash
{"title"=>"Assignments",
 "published"=>true,
 "morea_id"=>"reading-assignments",
 "morea_summary"=>"Requirements for programming assignments.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-assignments.html",
 "url"=>"/morea/010.introduction/reading-assignments.html",
 "content"=>
  "Updated March 8, 2014 to discuss the use of GPL licenses.\n\nThis page discusses general procedures for implementation assignments and\nextra credit projects. See the individual assignment pages for details.\n\n## Implementation Assignments\n\n### Overview\n\nThere will be three implementation assignments. These will involve writing\nJava implementations of abstract data types and associated algorithms, and\ntesting the implementations on sample data. You will also provide instructions\non how to compile and run the program, document your design and implementation\n(including complexith analysis), and present and discuss test results. The\nassignments will progressively give you more responsibility. For the first\nproject, you will be told what to implement, and it will be weighted 6% (60\npoints). For the second assignment you will need to make some implementation\nchoices. The second assignment will be weighted 10% (100 points). The third\nassignment will require some research and decision making on your part to\nsolve the problem. It will be weighted 10% (100 points).\n\n### Software Requirements\n\nThe following requirements have been adopted from Dr. Sugihara:\n\n#### Programming Language\n\nAll software must be written in Java. Other programming languages may not be\nused except where specified by the assignment.\n\n**The software must be compilable on the default version of Java available on uhunix.hawaii.edu** at the time of the submission deadline. The reason for this is to ensure that there is a common reference environment against which we can resolve disputes. We can't grade projects on claims that \"it compiled on my machine\".\n\nUhunix is running [Solaris](http://www.oracle.com/technetwork/server-\nstorage/solaris/overview/index.html). At this writing, the Java version is:\n\n    \n    \n    java version \"1.7.0_51\"\n    Java(TM) SE Runtime Environment (build 1.7.0_51-b13)\n    Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)\n    \n\nProjects submitted with higher versions of Java (if they become available) are\nat your risk. Note that the instructor is presently running research projects\nin Java 1.6 in Snow Leopard on which Java 1.7 is not available. Some features\nof Java 1.7 do not compile in 1.6, so if you could stick to 1.6 features he\nwon't have to go to UH Unix to run your code.\n\n#### Program Type and Interface\n\nThe software should run as a Java application.\n\n  * Command line (console) applications are acceptable.\n  * GUI (graphical user interface versions) are also permissible.\n\n#### Source Code Requirements\n\nThe student shall **submit .java source files** (not class or jar files),\norganized in folders as required for your package structure, along with\ninstructions for compiling the program and other documentation discussed in\nthe next section.\n\n  * The Teaching Assistant will verify that the programs compile on the current version of Java on uhunix, as specified above.\n  * Submissions will only be evaluated for credit if they successfully compile.\n  * This procedure is intended to (1) enable us to verify that the software is based on the student's own source code and (2) provide an objective basis for evaluating whether code works..\n\nSource files should include the BSD License Header based on [ this\ntemplate,](../Resources/bsd_license_header.txt) with \"<year>\", \"<copyright\nholder>\" and \"<organization>\" replaced appropriately.\n\nOther open source licenses (e.g., [Apache](http://www.apache.org/licenses/) or\n[GNU](http://www.gnu.org/licenses/)) may be used with prior permission from\nthe instructor if the student has a specific reason for doing so and\nunderstands the consequences. See the discussion under Other Licenses two\nsections below.\n\nSource should include appropriate in-line comments documenting the software\ndesign. Comments should not document the obvious (e.g., \"this line adds 1 to\nthe index variable\"), but rather document functional intent and constraints\nsuch as loop invariants, explain something that would otherwise be obscure,\nmark places that need improvement, etc. Descriptive use of variable and method\nnames also constitutes internal documentation. See next section for external\ndocumentation requirements.\n\n#### Including Open Source Software\n\nEach assignment will specify where you are allowed to reuse source code of\nopen source software developed by other authors, and where you must write your\nown code for the assignment. Where allowed by the assignment, reuse of open\nsource code is allowed if the following conditions are also met:\n\n  * The software license of the reused open source code is compatible with the [ BSD license](http://www.opensource.org/licenses/bsd-license.php) (see discussion under Other Licenses below).\n  * The license header of reused source code is also inserted into the source files containing the reused source code.\n  * The `Readme.txt ` of your product clearly gives credit to the authors of the reused source code (i.e., including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\nWhen source code of a module is reused, add the name(s) of its original\nauthor(s) to an `@author` tag at the beginning of the reused module. If you\nmodified the source code for more than bug fixes, add your name as an author\nof a derivative from the original source code:\n\n    /**\n     *\n     * @author     Original Author     James Brown\n     * @author     Derivative Author   Robert Smith\n     *\n     */\n\n#### Other Licenses\n\nI am often asked whether one can use code under another license. You may use\nother open source licenses as long as (1) they give you the right to use the\ncode under conditions acceptable to you and (2) you document this as needed.\nAn example is the GPL license. You may use a [GPL compatible\nlicense](https://www.gnu.org/licenses/license-list.html), but you use this\nlicense, all the code you write _must_ also be under GPL. See\n<http://en.wikipedia.org/wiki/Free_software_license> for an overview, and read\nsome blogs about this hotly debated issue.\n\n### Documentation Requirements\n\nSoftware will be submitted with appropriate documentation, including the\nfollowing. (Think of your audience for this documentation as any potential\nusers, including your classmates as well as the TA and instructor.)\n\n**Readme.txt (plaintext file)**\n\nCritical information that a user should know about your product first, including:\n\n  * step-wise instructions for the user to reconstruct an application from your source code (including the version of JDK used),\n  * credits (acknowledgments to authors of open source reused at least in part for this product) including information such as the name of an author, the name of a reused product and a list of source file names of the reused product,\n  * revision history (a log of changes on the product), \n  * bug report (description of known bugs), and \n  * a listing of other documentation available (below).\n\n**Operation Manual (plaintext or PDF)**\n\nConcise, yet sufficient step-wise explanation about how to start and interact with the program, written for an end user who is concerned with using the program in an application domain (not with the code).\n\n**Reference Manuals (plaintext or PDF, and Javadoc HTML)**\n\nRequirements and design specifications; organization of modules; algorithms and data structures used; functionality of each class or method; etc. A reference manual is written for experienced users and/or programmers who perform various maintenance activities for correction, enhancement, adaptation, etc. Javadoc pages may also be included, and should be included if you intend to have others use your code.\n\n**Testing Document (plaintext or PDF)**\n\nTest plan describing objective(s) of testing, method(s) used for testing, assumption(s) of testing, and hardware/software environment in testing; test case specification describing classification of test cases; test data and I/O of test runs; and whatever else is useful to convince other people about the correctness and good features of your program. **For ICS 311 assignments the testing document will include your conclusions related to the purpose of the assignment.**\n\n### Submission Requirements\n\n  1. Place the files and folders required (as discussed above under Software and Documentation Requirements) in a folder titled using the scheme Last-First-A#, for example, Suthers-Dan-A1 for assignment 1, Suthers-Dan-A2 for assignment 2, etc. Extra credit projects should be submitted with extentions -E1, etc.\n  2. Zip (.zip) or gzip (.gz) this folder using commands by those names on uhunix, or appropriate equivalents on your platform.\n  3. It is suggested that you test unzipping, compiling and running the software per the instructions you gave before submitting the assignment.\n  4. Upload the zip file to the Laulima area for the given assignment.\n  5. You should receive email confirmation of your submission at the address registered in Laulima.\n  6. Unlimited resubmissions are allowed up to the assignment deadline. Extra credit for early submission, if any, will be based on the date of the last submission, not the first!\n\n### Evaluation Criteria for Implementation Assignments\n\n### Warning: these will be revised for spring 2014\n\nThese are our default grading criteria. Some adjustments may be made when we\nsee where the greatest effort is required.\n\nProgram: 60%\n\n    \n\n  * 50% if all operations and all ADTs are implemented.\n  * 5% for following instructions for input and output (although many more points will be deducted if the interface is so bad we can't verify that the operations and ADTs are implemented). \n  * 5% for adequate error handling. \nAnalysis and Documentation 40%\n\n    \n\n  * 30% for adequate analysis of the results\n  * 10% for Readme, Operation, Reference manuals\n\n## Use of Software by Other Students\n\nIf others elect to use your software in a subsequent assignment (e.g., using\nyour graph ADT implementation in the third assignment), we will give extra\ncredit. See discussion in [Assessement](Assessment.html) page. Use should be\ncredited in the Readme and Reference Manual.\n\n",
 "path"=>"morea//010.introduction/reading-assignments.md"}
</pre>

<h2>/morea/010.introduction/reading-cormen-1.html</h2>

<pre>Hash
{"title"=>"CLRS 1 - Role of algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-1",
 "morea_summary"=>"The role of algorithms in computing",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/010.introduction/reading-cormen-1.html",
 "content"=>"",
 "path"=>"morea//010.introduction/reading-cormen-1.md"}
</pre>

<h2>/morea/010.introduction/reading-course-info.html</h2>

<pre>Hash
{"title"=>"Course information",
 "published"=>true,
 "morea_id"=>"reading-course-info",
 "morea_summary"=>
  "Student learning outcomes, textbook, instructor information.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-course-info.html",
 "url"=>"/morea/010.introduction/reading-course-info.html",
 "content"=>
  "## Catalog Description\n\nICS 311 Algorithms (3 credits) Design and correctness of algorithms, including\ndivide-and-conquer, greedy and dynamic programming methods. Complexity\nanalyses using recurrence relations, probabilistic methods, and NP-\ncompleteness. Applications to order statistics, disjoint sets, B-trees and\nbalanced trees, graphs, network flows, and string matching. Pre: 211 and 241,\nor consent.\n\n## SLOs (Student Learning Outcomes)\n\n  * Students are aware of fundamental algorithms of computer science, and their associated data structures and problem solving techniques. \n  * Students can compose a problem formulation of a real-world problem mathematically.\n  * Students can decide whether given pseudocode is correct for a given problem formulation; construct a counterexample if the given pseudocode is incorrect; and outline a proof for its correctness otherwise.\n  * Students can design a correct algorithm for a given problem and describe the algorithm as pseudocode in a given pseudocode syntax.\n  * Students can analyze the worst-case and best-case space and time complexities of a given algorithm.\n  * Students can create a software program for accurately implementing an algorithm specified in pseudocode. Students can implement software objects meeting Abstract Data Type specifications.\n  * Students can produce a software product including documentation for given requirements & design specifications.\n\n_Comment:_ On the fall 2013 final exam one student wrote about a problem:\n\n> \"This question is too hard! We shouldn't have to know implementations we have not used before!\"\n\nI wrote back to thank the student for concisely expressing (the negation of)\nexactly what this course _ is_ intended to teach! You may not \"know\" an\nimplementation you have not encountered before, but this course should prepare\nyou with the tools to analyze and make informed decisions about new\nimplementations.\n\nDo not approach this course solely as a memorization task, where you can only\ndo algorithms you are trained to do, like a circus animal. We do want you to\nlearn a \"catalog\" of algorithms, but you should be understanding their\nanalyses as examples that enable you to analyze unexpected algorithms in the\nfuture. This is essential for being successful in a fast changing field where\n_you_ are expected to figure out whether a new idea will work, as _ you _ are\nthe computer scientist hired to do this.\n\n## Textbook\n\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein,\n[Introduction to Algorithms, Third\nEdition,](http://mitpress.mit.edu/algorithms/) The MIT Press, 2009.\n\nStudents are advised to purchase the textbook, as this book will serve as a\nlifelong reference (it is the second most cited publication in computer\nscience). Also, the exams will be open book but with no electronics permitted,\nso the PDF version of the book won't help you there.\n\nStudents are also advised to keep their ICS 241 (Discrete Mathematics for\nComputer Science) textbooks for reference.\n\n## Instructor\n\n[Daniel D. Suthers](http://www2.hawaii.edu/~suthers/)  \nProfessor of ICS\n\n  * Office: POST 309B\n  * Office Telephone: (808) 956-3890\n  * Email: [suthers@hawaii.edu](mailto:suthers@hawaii.edu) (Put \"ICS 311\" in the subject line. Use Laulima for questions that may also apply to other students.)\n  * Office Hours: Mondays 3:00-4:00, or by appointment. (It's best to let me know in advance if you plan to come to office hour. Please don't try to talk to me while I am setting up for class. Brief consultations at the end of class are OK unless I indicate that I need to leave.\n\n## Teaching Assistant\n\nRobert Ward  \nM.S./Ph.D. Student in ICS\n\n  * Office: POST 303 cubicle\n  * Email: [rward@hawaii.edu](mailto:rward@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Office Hours: TuTh 1-2:30 or by appointment.\n\n## Assistant Teaching Assistant\n\nAnthony Sarria  \nICS Undergrad and ICS 311 Survivor\n\n  * Lab: POST 318A\n  * Email: [aksarria@hawaii.edu](mailto:aksarria@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Lab Hours: Mondays 3:15-6:15.\n\n## Communications\n\n**Questions about Course Content**:   In general, questions about course content such as concepts, clarifications of assignments, etc. should be posted to the Laulima discussion forum of the week. This is because (1) other students can see our responses there, and thus also benefit; and (2) other students may notice the question and answer before the instructor or TA notices it (though we will check daily). If you email us a question, we will post the reply in Laulima unless personal information is involved.\n\n**Personal Topics**: For topics that are not of interest to other students or are personal, you may email us, or stop by office hours. (Of course you may also use office hours for course content questions.) If using email, put \"ICS 311\" in the subject line.\n\n**Communication with other students (e.g., group members)**: You can send email to other students in the course using the Laulima \"Mailtool\". You don't need to know their real email address to do this.\n\n##  Online Media\n\nWe use the **course website** [www2.hawaii.edu/~suthers/courses/ics311s14/](ww\nw2.hawaii.edu/~suthers/courses/ics311s14/) for posting schedules and notes.\n\nWe use **[Laulima**](https://laulima.hawaii.edu/portal) for all other online\nrequired course functions such as podcasts, quizzes, discussions and\nsubmitting assignments. Please see [this document on everything Laulima users\nshould\nknow](http://www.hawaii.edu/talent/laulima_tab/tabs/laulima_essentials.html).\n\nWe will use **Google Docs** for in-class problem solving, as it supports\nsimultaneous editing.\n\nScreencasts (videos) of lectures are available on **YouTube** and **iTunesU**\nas well as Laulima (your choice). They are linked from the individual Notes\npages (Topic-XX.html).\n\n",
 "path"=>"morea//010.introduction/reading-course-info.md"}
</pre>

<h2>/morea/010.introduction/reading-format.html</h2>

<pre>Hash
{"title"=>"Format",
 "published"=>true,
 "morea_id"=>"reading-format",
 "morea_summary"=>"Exam cycles, weekly routine, studying, and group work.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-format.html",
 "url"=>"/morea/010.introduction/reading-format.html",
 "content"=>
  "By \"routines\" I mean our periodic patterns of activity: what happens in a\ntypical class, a typical week, and a typical exam and assignment cycle. This\npage tells you what I will do and what you will be expected to do on a\nrecurring basis.\n\n## Exam Cycles\n\nThere are three midterm exams covering the core material. The first two are\ngiven one full week after the last problem set on the exam topics was due, so\nyou have time to get feedback on homework problems. Due to the university\ncutoff for midterm exams, the third is held the week after the relevant topics\nare covered. The first exam is at least half review of 211 and 241: results\nare returned before the withdrawal date so you can assess whether you are\nready for ICS 311.\n\n## Weekly Routine\n\nOn most weeks we cover one book chapter/topic in each of the two classes (two\nchapters per week). The exceptions are the first two weeks when we are getting\nour bearings and covering material that must be understood to comprehend the\nrest of the semester; the weeks we have midterms; and the chapter that\nintroduces graphs (we take a full week for it).\n\nThe pace is intense: set aside time almost every day for ICS 311. The basic\npattern is as follows:\n\n**Saturdays:**\n\n  * Instructor posts any updates to screencasts and notes for the material of the following week by Saturday night, and possibly earlier.\n\n**Sundays:**\n\n  * Projects will be due midnight* on a Sunday night.\n\n**Mondays:**\n\n  * Problem sets from the previous week are due midnight* the Monday after they are given.\n  * Read or review material to understand Tuesday's topic (your choice of my podcasts, my web notes, the CLRS textbook chapter, other readings offered, and/or the MIT lecture videos).\n  * Post any questions you have about the material in the appropriate Laulima discussion forum (instructor will review these after midnight when preparing for Tuesday's class). \n\n**Tuesdays:**\n\n  * The online quiz for Tuesday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n\n**Wednesdays:**\n\n  * Read or review material to understand Thursday's topic.\n  * Post any questions you have about the material in the appropriate Laulima discussion forum.\n\n**Thursdays:**\n\n  * The online quiz for Thursday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n  * Problem sets for the weekend are usually posted Thursday evening.\n\n**Fridays through the weekend:**\n\n  * Use to do problem sets and/or projects in preparation for Sunday deadlines, and to watch screencasts and read book chapters. \n\n  * \"Midnight\": The deadline in Laulima will be set to 23:55 (11:55 pm) on the due date, to avoid any ambiguity of which day \"00:00\" refers to. Upload your solutions before midnight. There will be a 5 minute grace period just in case of network delays. The three projects can be submitted late for 1% off per hour up to 50% off, but the homework assignments cannot be late.\n\nThe Assistant Teaching Assistant will have lab hours on Mondays. This will be\nyour last chance for clarifications before assignments are due. Instructor\noffice hour will be Wednesdays (with some availability Monday by appointment).\nPlan in advance for programming projects due Sundays.\n\n## Class Routine\n\nThe focus of our 75 minute class will be student problem solving in groups,\nwith opportunites to get help. For at least the first part of the semester,\nthe groups will be formed anew _randomly_ each week. Each day you will solve a\nseries of conceptual problems and turn them in as a group for a group grade.\nThese problems prepare you to take on more substantial problems that you turn\nin individually on Sunday. At that time you will also allocate points to group\nmembers. (See [ Assessment](Assessment.html) for explanations of grading.)\n\nHere are typical schedules for 75 minute classes: Adjustments to the class\nroutine will likely be made to meet current needs.\n\n**Tuesdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Icebreaker for new groups (groups rotate weekly)\n  * 5: Solution to previous weekend's homework (if applicable) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n\n**Thursdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Discussion of Tuesday's problem (if needed) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n  * 5: Discussion of homework problems \n\nPlan to **bring your laptop or tablet to classes** held in Webster 101 (but\nnot to exams in BusAd A101). You'll need a VGA connector if you want to be\nable to project your laptop to your working group. Apple iPads can connect via\nApple TV. I'm not aware of another method. Of course, groups can function with\nonly one or two members having a projectable laptop, but it's better for you\nif you can be an active participant. At least one person in each group should\nhave the textbook handy in class as well.\n\n##  Other Comments\n\n### Inverted Classroom\n\nThis class is \"inverted\" in the sense that lectures are recorded and made\navailable outside of class, and classroom time is used for what can only be\ndone in person: collaboration and helping each other.\n\nLectures have their advantages, but they have problems too. For most students\nlistening to lectures is too passive an activity. The temptation to daydream\nor check Facebook may be too great, and it takes effort to keep your mind on\nthe material. Actual problem solving is more effective for learning. Also,\nlectures are a form of \"distance learning\": though we are all in the same room\nwe might as well be at a distance, as there is little interaction. When I ask\nworking professionals what skills they want our students to have, being able\nto collaborate in teams is ALWAYS mentioned on the first breath.\n\nFor these reasons, the inverted classroom puts lectures online so that\nstudents who benefit from them can have them, and even review them repeatedly;\nand uses the classroom time in ways that engage students more actively and\ntakes advantage of the unique opportunity provided by being in the same room.\n\n### Studying Before Class and Quizzes\n\nThe online quizzes before class are of course intended to motivate students to\nreview the material before class. There is another motivation: If you don't\nprepare in advance, you risk looking foolish in front of your peers, who may\nbe annoyed at you for being unprepared to help, and you'll miss a learning\nopportunity. You don't want to get a reputation for being the person who is\nnot prepared. It's a small world: someday your peers may be able to influence\nhiring decisions.\n\n### Groups\n\nMuch has been published by researchers and practitioners on how to organize\ngroups for collaborative learning. My approach is based on this research and\nmy experience with this course.\n\nDuring the first month, students will be assigned randomly to groups, rotating\nto new groups each week to help you get to know each other. (A survey of\nstudents fall 2013 indicated that many liked this format as it was a rare\nopportunity to get to know other ICS students.) Also it helps prevent reliance\non dysfunctional relationships (e.g., freeloading and \"the sucker effect\"): a\nstudent can't plan on being with someone who will do the work for him or her,\nand after a while people figure out who to avoid when forming groups.\n\nAfter the Midterm 1 exam, you may form voluntary groups. Students who form\nvoluntary groups will stay in them until the next midterm, at which time they\nmay elect to continue or disband. Other students will continue to be assigned\nrandomly.\n\nRegardless of whether you are in random or voluntary groups, this is an\nimportant opportunity to develop group collaboration skills and also to\ndevelop a good reputation with your peers. It may affect whether you are\nselected to be part of a good group in ICS 311, and I have seen some students\nafter graduation get hired while others fail to get a job because of the\nreputations they had with their peers.\n\n",
 "path"=>"morea//010.introduction/reading-format.md"}
</pre>

<h2>/morea/010.introduction/reading-policies.html</h2>

<pre>Hash
{"title"=>"Policies",
 "published"=>true,
 "morea_id"=>"reading-policies",
 "morea_summary"=>
  "Cheating, reuse of open source, abuse of facilities, makeups, and deadlines",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-policies.html",
 "url"=>"/morea/010.introduction/reading-policies.html",
 "content"=>
  "_These policies were adopted (with changes) from Kazuo Sugihara._\n\nStudents in this course are subject to all the policies of ICS Department and\nthe University of Hawaii at Manoa, including but not limited to the following:\n\n## Cheating\n\nAny form of cheating (such as plagiarism and unauthorized collaboration on an\nassignment) results in the grade of F and will be reported to ICS Department\nfor further actions. In general, I encourage students to help each other but\nyou should each produce your own final version of the homework assignments to\nturn in, except where use of another student's code is explicitly allowed.\nCheck with me if you are not sure.\n\nIn a technical report, a student may quote a few sentences written by another\nauthor with an explicit citation to its original source. Writing sentences or\nideas of the other author without giving credit to the original author is\nplagiarism. Copying content of online documents is also regarded as\nplagiarism, irrespective of how much the original content is modified, if the\nstudent fails to write an accurate citation to the original source. Avoid so-\ncalled \"patchwork plagiarism.\" Informative suggestions are given in [Avoiding\nPlagiarism](http://emedia.leeward.hawaii.edu/resources/plagiarism/05avoid.htm)\nby Professors Marilyn Bauer and Jacie Moriyama of Leeward Community College.\n\n## Reuse of Open Source\n\nIn a software product for an implementation assignment, a student is allowed\nto reuse open source if ALL of the following are met.\n\n  1. The software does not cover functionality that the assignment specifies that the student will write.\n  2. The software license of the open source reused gives permission for this reuse.\n  3. The license header of reused source code is copied into a source file containing the reused source code.\n  4. Documentation (e.g., Readme, User manual) of the software product clearly gives credits to authors of the reused source code (i.e., acknowledgments to the authors including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\n## Abuse of Facilities\n\nAny form of abuse of computing resources of ICS Department or the University\nof Hawaii will not be tolerated. It results in termination of your account on\ntheir servers any time the abuse is detected, will lead to the grade F, and\nwill be reported to ICS Department for further actions.\n\nPlease keep in your mind that access to and usage of our computing resources\nare your privilege, but not your right.\n\nInappropriate content that may be of an offensive nature to other students\nshould not be displayed on laptops or group workstations during class.\n\n## Makeup\n\nIf a student missed an exam due to illness or injury, a makeup exam will be\ngiven to a student only when the student has a doctor's note dated that day\nand contacts the instructor by email within 3 days after the exam date or the\ndate that the doctor's note suggests the student to recover enough to contact\nthe instructor. However, an ordinary doctor's appointment (scheduled by a\nstudent in advance) is not an acceptable reason for makeup unless it is\ninevitable to conflict with an exam beyond control of the student. The makeup\nexam must be completed before exam solutions are reviewed in class.\n\nFor an exceptional case other than illness or injury, a student must submit an\nofficial document to the instructor providing sufficiently convincing evidence\nof the fact that the cause for missing an exam was beyond control of the\nstudent (e.g., in case of a traffic accident on a way to a class, a police\nrecord of the accident should be furnished).\n\n## Deadlines\n\n**If Laulima or uhunix.hawaii.edu becomes unexpectedly unavailable** in the last several hours before a quiz, homework or project deadline, **_email your solutions to the instructor or TA_** once you are sure that you will not be able to upload it but before the deadline. Please note that some quiz questions use random ordering for multiple choice: specify the content of your response, not just the letter.\n\nThe deadline of every assignment is firm. No late submission will be accepted\n(unless explicitly stated otherwise, possibly with penalties). No extension\nwill be given except the following cases.\n\n  * If Laulima is unexpectedly down for an extended period before the deadline preventing most students from uploading, the deadline will be extended for the downtime. Please notify the instructor of any unexpected downtime when you notice it so that a decision can be made and announced to all students in a timely manner. On the other hand, no extension will be given for scheduled downtime.\n  * A student has a doctor's note dated on the day of a deadline of the assignment and contacts the instructor by email within 3 days after the deadline or the date that the doctor's note suggests that the student has recovered enough to email.\n  * For any other exceptional circumstance other than illness or injury, a student is required to submit to the instructor an official document that is a sufficiently convincing evidence of the fact that the cause for missing a deadline was beyond control of the student.\n\n## Data Backup\n\nEach student is responsible for taking a periodical backup of her/his work and\ncomputer resources needed to meet course requirements. Especially, files for\nassignments should frequently be saved on an external storage device. An\nunexpected failure of the computer is not an acceptable excuse for a late\nsubmission. CS major students should exercise automatic, periodical backup\nprocedures. If you do not practice periodical cloning and backup, start to do\nit from now.\n\nA simple solution is to put all your coursework in a cloud environment such as\nDropbox, but you still need to ensure that you have backup of a working\ncomputer to access this data.\n\nMy own procedure (in OS X) uses two external drives. External drives make it\neasy to move to another computer if yours becomes inoperable. One external\ndrive is about the same size as my internal drive, and I use SuperDuper to\nmake a bootable clone about once a week. This does not save intermediate or\nprevious versions. The other drive is twice the size of the internal drive,\nand I use Time Machine to automatically record previous versions of documents.\nTime Machine does not produce a bootable disk. If my computer failed, I could\nboot off the SuperDuper clone disk and use the Time Machine disk to restore to\nmy most recent versions.\n\nThis is a good procedure for one computer, but the frequency of Time Machine\nruns (once per hour) may be insufficient in time-critical situations. You\nmight manually initiate back up on a more frequent basis. I use four computers\nand a server, so I also use Interarchy to synchronize (via disk mirroring) all\nof my document folders with my server and across the four computers. I run\nsynchronization to my server after each major piece of work.\n\n",
 "path"=>"morea//010.introduction/reading-policies.md"}
</pre>

<h2>/morea/010.introduction/reading-topic-overview.html</h2>

<pre>Hash
{"title"=>"Topics",
 "published"=>true,
 "morea_id"=>"reading-topic-overview",
 "morea_summary"=>
  "Conceptual overview of how topics are grouped and sequenced.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/010.introduction/reading-topic-overview.html",
 "url"=>"/morea/010.introduction/reading-topic-overview.html",
 "content"=>
  "## _Part I: Introduction to Analysis_\n\nThis section introduces algorithms as abstractions of programs, and motivates\nwhy we need to do analysis of algorithms rather than just run empirical tests\nof programs. It then introduces some mathematical and conceptual tools for\ndoing analysis. Two useful sorting algorithms are used for illustration; we'll\nreturn to sorting later.\n\n  * **#1** \\- Chapter 1: Introduction to Course Format and to Algorithms \n  * **#2** \\- Chapter 2: Examples of Analysis with Insertion and Merge Sort\n  * **#3** \\- Chapter 3: Growth of Functions and Asymptotic Concepts \n\n## _Part II: Data Structures for Dictionaries & Sets_\n\nIn this section we introduce problem solving and analysis methods (chapters\n3-5), some of which have been covered in ICS 241, and also review review basic\ndata structures and algorithms (chapters 10-12) that were introduced in ICS\n211. The chapters from the text are interleaved and paired up in a manner that\nuses the basic dictionary and set data structures and algorithms to illustrate\nthe problem solving and analysis methods. Hopefully this is mostly review of\nthe two prerequisite courses with some added depth.\n\n  * **#4** \\- Chapter 10: Stacks, Queues, Lists and Trees (Review)\n  * **#5** \\- Chapter 5 and Goodrich & Tamassia section: Probabilistic Analysis and Randomized Algorithms; Skip list example\n  * **#6** \\- Chapter 11: Hash Tables\n  * **#7** \\- Chapter 4: Divide & Conquer and Associated Analysis Methods\n  * **#8** \\- Chapter 12: Binary Search Trees\n\n## _Part III: Sorting and Balanced Trees_\n\nWe continue into more advanced applications of trees (chapters 6 and 13), also\nproviding the basis for another efficient sorting algorithm. We compare\nadditional sorting algorithms to those from Chapter 2. Sorting is one of the\nmost fundamental and common applications of computers, so efficiency is very\nimportant. We consider the broader question of how fast _any_ sort algorithm\ncan be. This is an example of a powerful method of computer science: reasoning\nabout sets of possible algorithms rather than specific algorithms.\n\n  * **#9** \\- Chapter 6: Heapsort and Priority Queues\n  * **#10** \\- Chapters 7 & 8.1: Quicksort, Theoretical Limits, and O(n) sorts \n  * **#11** \\- Chapter 13 & Sedgewick Chapter 15: Balanced Trees \n\n## _Part IV: Problem Solving and Analysis Methods_\n\nThis part introduces two further problem solving methods, _dynamic\nprogramming_ and _greedy algorithms_, with example applications. We then cover\nanother important analytic method, _amortization _, with examples in the\nanalysis of the union-find representation of sets. (Chronologically, #14\ngraphs will be introduced in this sequence to help you get started on a\nprogramming assignment, but conceptually they belong in the next section.)\n\n  * **#12** \\- Chapter 15: Dynamic Programming\n  * **#13** \\- Chapter 16: Greedy Algorithms & Huffman Codes\n  * **#15** \\- Chapter 17 - Amortization\n  * **#16** \\- Chapter 21 - Sets and Union-Find\n\n## _Part V: Graphs_\n\nGraphs are a very flexible data structure for which many applications exist.\nEquipped with various problem solving and analytic tools, we examine the most\nimportant algorithms on graphs -- including some of the most classic work in\ncomputer science -- and their applications.\n\n  * **#14** \\- Chapter 22: Graph Representations and Basic Algorithms _(covered earlier)_\n  * **#17** \\- Chapter 23: Minimum Spanning Trees \n  * **#18** \\- Chapter 24: Single-Source Shortest Paths \n  * **#19** \\- Chapter 25: All Pairs Shortest Paths \n  * **#20** \\- Chapter 26: Maximum Flow\n\n## _Part VI: Selected Topics_\n\nThere are a number of other important or common application areas that have\ntheir own specialized algorithms of interest: multithreading (parallel\nalgorithms), matrix operations, linear programming, operations on polynomials,\nnumber-theory, string matching, and computational geometry. These are covered\nin the text, but we can't fit them all in. Linear Programming will be examined\nbecause it fills out an area not covered well above (numeric algorithms), and\nalso has interesting connections to previous material (e.g., you can solve\nflow problems with a linear program). Two other topics will be covered more\nlightly in class (no homework).\n\n  * **#21** \\- Sedgewick Chapters 5 and 38; Chapter 29: Linear Programming \n  * **#22** \\- Chapter 27: Multithreading \n  * **#23** \\- Chapter 32: String Matching \n\n## _Part VII: Complexity Theory and NP-Completeness_\n\nFinally, abstracting further from algorithms to problems, we deal with the\nimportant question of whether an efficient algorithm is known (or even\npossible) for a given problem, and what to do if none are known. We encounter\nthe most important open problem in theoretical computer science (indeed in all\nof mathematics), which is of _practical_ interest because awareness of\n\"intractable\" problems and approximation algorithms could save you\nconsiderable trouble if you encounter one of these very common problems on the\njob! The seminal book on this topic is the most cited reference in computer\nscience.\n\n  * **#24** \\- Chapter 34 - Complexity Theory & NP-Completeness\n  * **#25** \\- Chapter 35 - Approximation Algorithms \n\n",
 "path"=>"morea//010.introduction/reading-topic-overview.md"}
</pre>

<h2>/morea/020.examples/experience-1.html</h2>

<pre>Hash
{"title"=>"Linear Search",
 "published"=>true,
 "morea_id"=>"experience-1",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze the linear search algorithm",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-1.html",
 "url"=>"/morea/020.examples/experience-1.html",
 "content"=>
  "## In Class: Linear Search\n\n#### 5 points\n\n1. Consider the **searching problem**:\n\nInput:\n\n     A sequence of _n_ numbers _A_ = ⟨ _a_1, _a_2, .... _a__n_ ⟩ (in _no particular order_), and a value _v_.\n\nOutput:\n\n     An index _i_ such that _v_ = _A_[_i_] or the special value NIL if _v_ does not appear in _A_.\n\n**a. (1 pt):** Write pseudocode for `LinearSearch(A,v)`, an algorithm that scans through sequence _A_ looking for _v_, and provides the desired output (_i_ or NIL). Use Java style 0-based indexing, A[_i_] to access elements, and A.length to get _n_. \n\n**b. (2 pts):** Let **_p_** be the number of array positions checked. (_p_ will be replaced with a function of _n_. For example, when the element is not present, _p_ = _n_.) Let _c_1 be the cost of executing line 1, _c_2 the cost of executing line 2, etc. As was done in the lecture notes: \n\n  * Write the expression for the cost to execute each line in terms of _p_ and the _c__i_. \n  * Sum these to get the total cost _T_(_n_). \n  * Then collect the constants and rename them \"a\" and \"b\".\n\n**c. (1 pt):** What is _p_ for the _worst case_: precisely how many elements of the input sequence need to be checked? \n\n  * Rewrite your last expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ(____).) \n\n**d. (1pt):** What is _p_ for the _average case_: precisely how many elements of the input sequence need to be checked on average, assuming that the element being searched for is present and equally likely to be any element in the array? \n\n  * Rewrite your expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ (____).) \n\nJustify all your answers!\n\n",
 "path"=>"morea//020.examples/experience-1.md"}
</pre>

<h2>/morea/020.examples/experience-2.html</h2>

<pre>Hash
{"title"=>"Binary Search",
 "published"=>true,
 "morea_id"=>"experience-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze binary search",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-2.html",
 "url"=>"/morea/020.examples/experience-2.html",
 "content"=>
  "## In Class: Mixing and Matching with `BinarySearch`\n\n####  5 points\n\n**1\\. (1 pt):** The while loop of `InsertionSort` uses a Θ(_n_) linear search to scan backwards through the sorted subarray A[1 .. _j_-1]. Since this subarray is sorted, can we use `BinarySearch` for this search to improve the overall worst case running time of InsertionSort to Θ(_n_ lg _n_)? If yes, argue why the hybrid algorithm would be Θ(_n_ lg _n_). If no, explain why it won't work. \n\n**2\\. (4 pts):** We know from Tuesday's class work that `LinearSearch` is Θ(_n_), so one might think it is always better to use the Θ(lg _n_) `BinarySearch`. However, in order to apply `BinarySearch` we have to sort the data, which (we will soon learn) requires Θ(_n_ lg _n_) time for the best known algorithms (e.g., `MergeSort`). This question explores when it is worth paying this extra cost of sorting the data in order to get fast search.\n\nSuppose you will be **searching a list of _n_ items _m_ times**. When is _m_\nbig enough relative to _n_ to make it worth sorting and using binary search\nrather than just using linear search?\n\nWe have two alternatives. Simply using `LinearSearch` _m_ times on _n_ items\nhas an expected (average) cost of Θ(_mn_). The second alternative is (a)\nbelow, and (b)-(d) explore the tradeoffs.\n\n**a.** What is the expected cost to apply `MergeSort` once to sort _n_ items, and then apply `BinarySearch` _m_ times to _n_ items? \n\n**b.** Suppose _m_ = 1: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**c.** Suppose _n_ = _m_: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**d.** What is the cutoff point in terms of _m_ expressed as a function of _n_ between when it is faster to just apply the linear search and when it is faster to apply MergeSort? (Set up an equation and solve for m.) \n\n(Comment: a more accurate analysis would take into account the different\nconstants associated with each algorithm: `MergeSort` has a higher constant.\nBut it can be hard to know the numerical value of the constant to be used in\nthe analysis. So, if you are really concerned about performance, once the\nanalysis gives you the ballpark tradeoff, empirical studies can be used to\ndecide the best cutoff for a given implementation.)\n\n\n",
 "path"=>"morea//020.examples/experience-2.md"}
</pre>

<h2>/morea/020.examples/experience-3.html</h2>

<pre>Hash
{"title"=>"More on linear and binary search",
 "published"=>true,
 "morea_id"=>"experience-3",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze linear and binary search (homework)",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/experience-3.html",
 "url"=>"/morea/020.examples/experience-3.html",
 "content"=>
  "## #1. Peer Credit Assignment\n\n### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## #2. Correctness of `LinearSearch`\n\n### 5 points\n\n**(a)** Show the pseudocode for `LinearSearch` that you will be analyzing. It should be code that you understand and believe is correct, so you may revise your group's solution if you wish. Give each line a number for reference in your analysis.\n\n**(b)** Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties (page 19).\n\n_(This will be a little tricky because the loop can exit for two reasons.\nRather than complicate your invariant trying to cover the two cases, you might\nwrite a simpler one that gets part of the work done, and reason about the two\nexit conditions when you show how the invariant _helps_ to prove\ncorrectness.)_\n\n## #3. Runtime of `BinarySearch`\n\n### 5 points\n\nThis problem steps you through a recursion tree analysis of BinarySearch (the\nalgorithm for searching a sorted array that was reviewed in class) to show\nthat it is Θ(lg _n_) in the worst case (that is, O(lg _n_) in general). Θ and\n\"big-O\" are concepts introduced in Topic 3: if they are not familiar to you,\njust think of this as meaning the longest possible execution on input of size\n_n_ will take time proportional to lg _n_.\n\n**(a)** Write the recurrence relation for `BinarySearch`, using the formula T(_n_) = _a_T(_n_/_b_) + D(_n_) + C(_n_). (We'll assume T(1) = some constant _c_, and you can use _c_ to represent other constants as well, since we can choose _c_ to be large enough to work as an upper bound everywhere it is used.)\n\n**(b)** Draw the recursion tree for `BinarySearch`, in the style shown in podcast 2E and in Figure 2.5. (Don't just copy the example for `MergeSort`: it will be incorrect. Make use of the recurrence relation you just wrote!)\n\n**(c)** Using a format similar to the counting argument in Figure 2.5 of the text or of podcast 2E, use the tree to show that `BinarySearch` is Θ(lg _n_) in the worst case. Specifically,\n\n  1. show what the row totals are,\n  2. write an expression for the tree height (justifying it), and\n  3. use this information to determine the total computation represented by the tree.\n\nSince this problem involves mathematical expressions and diagrams, you may\nwant to do your work on paper and digitize it (please compress images) and\nsubmit as jpg or pdf. Alternatively, use a drawing program.\n\n\n\n",
 "path"=>"morea//020.examples/experience-3.md"}
</pre>

<h2>/morea/020.examples/module-examples.html</h2>

<pre>Hash
{"title"=>"Analysis examples",
 "published"=>true,
 "morea_id"=>"examples-insertion-merge-sort",
 "morea_outcomes"=>["outcome-analysis-style"],
 "morea_readings"=>
  ["reading-screencast-2a",
   "reading-screencast-2b",
   "reading-screencast-2c",
   "reading-screencast-2d",
   "reading-screencast-2e",
   "reading-cormen-2",
   "reading-screencast-mit-1",
   "reading-notes-2"],
 "morea_experiences"=>["experience-1", "experience-2", "experience-3"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/020.examples/module-examples.gif",
 "morea_sort_order"=>20,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/020.examples/module-examples.html",
 "content"=>
  "Insertion sort, merge sort, asymptotic analysis, recurrences, loop invariants, linear search, binary search.\n",
 "path"=>"morea//020.examples/module-examples.md"}
</pre>

<h2>/modules/examples-insertion-merge-sort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-examples.md",
 "title"=>"Analysis examples",
 "url"=>"/modules/examples-insertion-merge-sort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/examples-insertion-merge-sort/index.html"}
</pre>

<h2>/morea/020.examples/outcome-analysis-style.html</h2>

<pre>Hash
{"title"=>"Understand analysis of algorithm styles.",
 "published"=>true,
 "morea_id"=>"outcome-analysis-style",
 "morea_type"=>"outcome",
 "morea_sort_order"=>20,
 "referencing_modules"=>[#Jekyll:Page @name="module-examples.md"],
 "url"=>"/morea/020.examples/outcome-analysis-style.html",
 "content"=>
  "By viewing examples, become familiar with the style of analysis used in ICS 311.",
 "path"=>"morea//020.examples/outcome-analysis-style.md"}
</pre>

<h2>/morea/020.examples/reading-cormen-2.html</h2>

<pre>Hash
{"title"=>"CLRS 2 - Getting started",
 "published"=>true,
 "morea_id"=>"reading-cormen-2",
 "morea_summary"=>"Getting started with analysis of algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "26 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-cormen-2.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-cormen-2.md"}
</pre>

<h2>/morea/020.examples/reading-notes-2.html</h2>

<pre>Hash
{"title"=>"Chapter 2 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-2",
 "morea_summary"=>"Modeling a problem, loop invariants, analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/020.examples/reading-notes-2.html",
 "url"=>"/morea/020.examples/reading-notes-2.html",
 "content"=>
  "## Outline\n\n  1. The Sorting Problem\n  2. Insertion Sort: An Incremental Strategy\n  3. Loop Invariants and Correctness of Insertion Sort\n  4. RAM Model; What do we count?\n  5. Analysis of Insertion Sort: Best and Worst Cases\n  6. Worst Case Rate of Growth and Θ (Theta)\n  7. Merge Sort: A Divide & Conquer Strategy\n  8. Brief Comment on Merge Sort Correctness\n  9. Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n## Modeling a Problem: The Sorting Problem\n\n### Problem Formulation\n\nClear and unambiguous definition of what to be solved in terms of:\n\n  * Input of the problem\n  * Output of the problem\n  * Assumptions in the problem\n\nDescriptions in a problem formulation must be declarative (not procedural).\nAll assumptions concerning input and output must be explicit. The problem\nformulation provides the requirements for an algorithm.\n\n### Problem Formulation for Sorting\n\nInput:\n\n    A sequence σ of n real numbers xi (1 ≤ i ≤ n)\nAssumptions:\n\n  1. n is a positive integer.\n  2. The real numbers xi (1 ≤ i ≤ n) are not necessarily distinct.\nOutput:\n\n    A permutation π = x'1 x'2  x'n of the given sequence σ such that x'j ≤ x'j+1 for every j (1 ≤ j < n)\n\nThe numbers are referred to as **keys**.\n\nAdditional information known as **satellite data** may be associated with each\nkey.\n\nSorting is hugely important in most applications of computers. We will cover\nseveral ways to solve this problem in this course.\n\n* * *\n\n## Insertion Sort: An Incremental Strategy\n\n![](fig/sorting-cards.jpg)\n\nInsertion sort takes an **incremental strategy** of problem solving: pick off\none element of the problem at a time and deal with it. Our first example of\nthe text's pseudocode:\n\n![](fig/code-insertion-sort.jpg)\n\nHere's a step by step example:\n\n![](fig/fig-2-2-insertion-sort-example.jpg)\n\n_Is the strategy clear? For fun, see the visualization at\n<http://youtu.be/ROalU379l3U>_\n\n* * *\n\n## Loop Invariants and Correctness of Insertion Sort\n\n### Loop Invariants\n\nA loop invariant is a formal property that is (claimed to be) true at the\nstart of each iteration. We can use loop invariants to prove the correctness\nof iteration in programs, by showing three things about the loop invariant:\n\n**Initialization:**\n    It is true prior to the first iteration.\n**Maintenance:**\n    If it is true prior to a given iteration, then it remains true before the next iteration.\n**Termination:**\n    When the loop terminates, the invariant (and the conditions of termination) gives us a useful property that helps to show that the algorithm is correct.\n\nNotice the similarity to mathematical induction, but here we have a\ntermination condition.\n\n### Correctness of Insertion Sort\n\n![](fig/code-insertion-sort.jpg)\n\n**Loop Invariant:**\n    At the start of each iteration of the outer `for` loop at line 1, the subarray A[1 .. _j_-1] consists of the elements originally in A[1 .. _j_-1] but in sorted order. \n**Initialization:**\n    We start with _j_=2. The subarray A[1 .. _j_-1] is the single element A[1], which is the element originally in A[1] and is trivially sorted.\n**Maintenance:**\n    A precise analysis would state and prove another loop invariant for the `while` loop. For simplicity, we'll note informally that at each iteration the elements A[_j_-1], A[_j_-2], A[_j_-3], etc. are shifted to the right (so they remain in the sequence in proper order) until the proper place for _key_ (the former occupant of A[_j_]) is found. Thus at the next iteration, the subarray A[1 .. _j_] has the same elements but in sorted order.\n**Termination:**\n    The outer `for` loop ends when _j_=_n_+1. Therefore _j_-1=_n_. Plugging _n_ into the loop invariant, the subarray A[1 .. _n_] (which is the entire array) consists of the elements originally in A[1 .. _n_] but in sorted order.\n\n_Convinced? Questions? Could you do it with another problem?_\n\n* * *\n\n## RAM Model: What do we count?\n\nIf we are going to tally up time (and space) requirements, we need to know\nwhat counts as a unit of time (and space). Since computers differ from each\nother in details, it is helpful to have a common abstract model.\n\n### Random Access Machine (RAM) Model\n\nThe RAM model is based on the design of typical von Neumann architecture\ncomputers that are most widely in use. For example:\n\n  * Instructions are executed one after the other (no concurrent operations).\n  * Instructions operate on a small number (one or two) of data \"words\" at a time.\n  * Data words are of a limited, constant size (cannot get arbitrarily large computation done in one operation by putting the data in an arbitrarily large word).\n\n### Categories of Primitive Operations\n\nWe identify the primitive operations that count as \"one step\" of computation.\nThey may differ in actual time taken, but all can be bounded by the same\nconstant, so we can simplify things greatly by counting them as equal.\n\n#### Data Manipulation\n\n  * Arithmetic operation: +, -, *, /, remainder, floor, ceiling, left/right shift\n  * Comparison: <, =, >, ≤, ≥\n  * Logical operation: ∧, ∨, ¬\n\n> _These assume bounded size data objects being manipulated, such as integers\nthat can be represented in a constant number of bits (e.g, a 64-bit word),\nbounded precision floating numbers, or boolean strings that are bounded in\nsize. Arbitrarily large integers, arbitrarily large floating point precision,\nand arbitrarily long strings can lead to nonconstant growth in computation\ntime._\n\n#### Flow Control\n\n  * Branch: case, if, etc.\n  * Loop; while, for   __   <-   ___   to   ___ \n\n> _Here we are stating that the time to execute the machinery of the\nconditional loop controllers are constant time. However, if the language\nallows one to call arbitrary methods as part of the boolean expressions\ninvolved, the overall execution may not be constant time._\n\n#### Miscellaneous\n\n  * Assignment: <-\n  * Subscription: [ ]\n  * Reference\n  * Setting up a procedure or function call (see below)\n  * Setting up an I/O operation (see below) \n\n> _The time to set up a procedure call is constant, but the time to execute\nthe procedure may not be. Count that separately. Similarly, the time to set up\nan I/O operation is constant, but the time to actually read or write the data\nmay be a function of the size of the data. Treat I/O as constant only if you\nknow that the data size is bounded by a constant, e.g., reading one line from\na file with fixed data formats._\n\n###  Input Size\n\nTime taken is a function of input size. How do we measure input size?\n\n  * It is often most convenient to use the number of items in the input, such as the number of numbers being sorted. \n  * For some algorithms we need to measure the size of data, such as the number of bits in two integers being multiplied. \n  * For other algorithms we need more than one number, such as the number of vertices _and_ edges in a graph.\n\n* * *\n\n## Analysis of Insertion Sort: Best and Worst Cases\n\nWe now undertake an exhaustive quantitative analysis of insertion sort. We do\nthis analysis in greater detail than would normally be done, to illustrate why\nthis level of detail is not necessary!!!\n\nFor each line, what does it cost, and how many times is it executed?\n\nWe don't know the actual cost (e.g., in milliseconds) as this varies across\nsoftware and hardware implementations. A useful strategy when you do not know\na quantity is to just give it a name ...\n\n![](fig/analysis-insertion-sort.jpg)\n\nThe _ci_ are the unknown but constant costs for each step. The _tj_ are the\nnumbers of times that line 5 is executed for a given _j_. These quantities\ndepend on the data, so again we just give them names.\n\nLet T(_n_) be the running time of insertion sort. We can compute T(_n_) by\nmultiplying each cost by the number of times it is incurred (on each line) and\nsumming across all of the lines of code:\n\n![](fig/equation-insertion-total-time.jpg)\n\n### Best Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is already sorted, we always find that A[_i_] ≤ _key_ the first\ntime the `while` loop is run; so all _tj_ are 1 and _tj-1_ are 0. Substituting\nthese values into the above:\n\n![](fig/equation-insertion-best.jpg)\n\nAs shown in the second line, this is the same as _a__n_ \\+ _b_ for suitable\nconstants _a_ and _b_. Thus the running time is a **linear function of n.**\n\n### Worst Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is in reverse sorted order, we always find that A[_i_] > _key_\nin the while loop, and will need to compare _key_ to all of the (growing) list\nof elements to the left of _j_. There are _j_-1 elements to compare to, and\none additional test for loop exit. Thus, _tj=j_.\n\n![](fig/equation-insertion-worst-1.jpg) ![](fig\n/equation-insertion-worst-2.jpg)\n\nPlugging those values into our equation:\n\n![](fig/equation-insertion-total-time.jpg)\n\nWe get the worst case running time, which we simplify to gather constants:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _an2 \\+ bn + c_ for some _a, b, c_: _T(n)_ is a\n**quadratic function of n**.\n\nSo we can draw these conclusions purely from mathematical analysis, with _ no\nimplementation or testing needed_: Insertion sort is very quick (linear) on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. But the worst case is slow for reverse sorted data.\n\n* * *\n\n## Worst Case Rate of Growth and Θ (Theta)\n\nFrom the above example we introduce two key ideas and a notation that will be\nelaborated on later.\n\n###  Worst Case Analysis\n\nAbove, both best and worst case scenarios were analyzed. We usually\nconcentrate on the worst-case running times for algorithms, because:\n\n  * This gives us a guaranteed upper bound.\n  * For some algorithms, the worst case occurs often (such as failing to find an item in a search). \n  * The average is often almost as bad as the worst case.\n\n_How long does it take on average to successfully find an item in an unsorted\nlist of n items?  \nHow long does it take in the worst case, when the item is not in the list?  \nWhat is the difference between the two?_\n\n###  Rate of Growth\n\nIn the above example, we kept track of unknown but named constant values for\nthe time required to execute each line once. In the end, we argued that these\nconstants don't matter!\n\n  * Their specific values don't matter because they all add up to summary constants in the equations (e.g., _a_ and _b_).\n  * Even their presence does not matter, because it is the growth of the function of _n_ that dominates the time taken to run the algorithm.\n\nThis is good news, because it means that all of that excruciating detail is\nnot needed!\n\nFurthermore, only the fastest growing term matters. In _an2 \\+ bn + c_, the\ngrowth of _n2_ dominates all the other terms (including _bn_) in its growth.\n\n###  Theta: Θ\n\nWe will use Θ notation to concentrate on the fastest growing term and ignore\nconstants.\n\nIf we conclude that an algorithm requires _an2 \\+ bn + c_ steps to run, we\nwill dispense with the constants and lower order terms and say that its growth\nrate (the growth of how long it takes as _n_ grows) is Θ(_n_2).\n\nIf we see _bn + c_ we will write Θ(_n_).\n\nA simple constant _c_ will be Θ(1), since it grows the same as the constant 1.\n\nWhen we combine Θ terms, we similarly attend only to the dominant term. For\nexample, suppose an analysis shows that the first part of an algorithm\nrequires Θ(_n_2) timeand the second part requires Θ(_n_) time. Since the\nformer term dominates, we need not write Θ(_n_2 \\+ _n_): the overall algorithm\nis Θ(_n_2).\n\nFormal definitions next week!\n\n\n\n* * *\n\n## Merge Sort: A Divide & Conquer Strategy\n\nAnother strategy is to **Divide and Conquer**:\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\nMerge Sort takes this strategy:\n\n**Divide:**\n    Given A[_p .. r_], split the given array into two subarrays A[_p .. q_] and A[_q+1 .. r_] where _q_ is the halfway point of A[_p .. r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure `Merge` that iterates over the subarrays from the smallest element up to copy the next smallest element into a result array.   \n(This is like taking two decks of sorted cards and picking the next smallest\none off to place face-down in a new pile to make one sorted deck.)\n\nThe strategy can be written simply and elegantly in recursive code ...\n\n![](fig/code-merge-sort.jpg)\n\nHere are examples when the input is a power of two, and another example when\nit is not a power of two:\n\n![](fig/example-merge-sort-1.jpg) ![](fig/example-merge-sort-2.jpg)\n\nNow let's look in detail at the merge procedure, implemented using ∞ as\n**sentinels** _(what do lines 1-2 do? lines 3-9 ? lines 10-17?)_:\n\n![](fig/code-merge-procedure.jpg)\n\nHere's an example of how the final pass of `MERGE(9, 12, 16)` happens in an\narray, starting at line 12. Entries with slashes have had their values copied\nto either L or R and have not had a value copied back in yet. Entries in L and\nR with slashes have been copied back into A.\n\n![](fig/example-merge-sort-3.jpg)\n\nWe can also dance this one: <http://youtu.be/XaqR3G_NVoo>\n\n* * *\n\n## Merge Sort Correctness\n\nA loop invariant is used in the book to establish correctness of the Merge\nprocedure. Since the loop is rather straightforward, we will leave it to the\nabove example. Once correctness of Merge is established, induction can be used\nto show that Merge-Sort is correct for any N.\n\n* * *\n\n## Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n![](fig/code-merge-procedure-small.jpg)\n\nMerge Sort provides us with our first example of using recurrence relations\nand recursion trees for analysis. We will go into more detail on these methods\nwhen we cover Chapter 4.\n\n### Analysis of Merge\n\nAnalysis of the Merge procedure is straightforward. The first two `for` loops\n(lines 4 and 6) take Θ(_n1+n2_) = Θ(_n_) time, where _n_1+_n_2 = _n_. The last\n`for` loop (line 12) makes _n_ iterations, each taking constant time, for\nΘ(_n_) time. Thus total time is Θ(_n_).\n\n### Analyzing Divide-and-Conquer Algorithms\n\n**Recurrence equations** are used to describe the run time of Divide & Conquer algorithms. Let _T(n)_ be the running time on a problem of size _n_. \n\n  * If _n_ is below some constant (or often, _n=1_), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each _1/b_ size of the original. Often, as in Merge Sort, _a = b = 2_.\n  * We pay cost **_D(n)_** to divide the problems and **_C(n)_** to combine the solutions. \n  * We also pay cost **_aT(n/b)_** solving subproblems. \n\nThen the total time to solve a problem of size _n_ by dividing into _a_\nproblems of size _n_/_b_can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\n### Recurrence Analysis of Merge Sort\n\n![](fig/code-merge-sort.jpg)\n\nMerge-Sort is called with _p=1_ and _r=n_. For simplicity, assume that _n_ is\na power of 2. (We can always raise a given _n_ to the next power of 2, which\ngives us an upper bound on a tighter Θ analysis.) When _n≥2_, the time\nrequired is:\n\n  * **Divide** (line 2): Θ(1) is required to compute _q_ as the average of _p_ and _r_.\n  * **Conquer** (lines 3 and 4): 2_T_(_n_/2) is required to recursively solve two subproblems, each of size _n/2_.\n  * **Combine** (line 5): Merging an n-element subarray takes Θ(_n_) (this term absorbs the Θ(1) term for Divide). \n![](fig/recurrence-mergesort-theta.jpg)\n\nIn Chapter 4 we'll learn some methods for solving this, such as the Master\nTheorem, by which we can show that it has the solution T(_n_) = Θ(_n_\nlg(_n_)). Thus, Merge Sort is faster than Insertion Sort in proportion to the\ndifference in growth of lg(_n_) versus _n_.\n\n### Recursion Tree Analysis\n\nRecursion trees provide an intuitive understanding of the above result. In\ngeneral, recursion trees can be used to plan out a formal analysis, or even\nconstitute a formal analysis if applied carefully.\n\nLet's choose a constant _c_ that is the largest of all the constant costs in\nthe algorithm (the base case and the divide steps). Then the recurrence can be\nwritten:\n\n![](fig/recurrence-mergesort-c.jpg)\n\nIt costs _cn_ to divide the original problem in half and then to merge the\nresults. We then have to pay cost _T_(_n_/2) twice to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-1.jpg)\n\nFor each of the two subproblems, _n_/2 is playing the role of _n_ in the\nrecurrence. So, it costs _cn_/2 to divide and then merge the _n_/2 elements,\nand _T_(_n_/4) to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-2.jpg)\n\nIf we continue in this manner we eventually bottom out at problems of size 1:\n\n![](fig/recurrence-tree-mergesort-3.jpg)\n\nNotice that if we sum across the rows each level has cost _cn_. So, all we\nhave to do is multiply this by the number of levels. Cool, huh?\n\n_But how many levels are there?_ A little thought (or a more formal inductive\nproof you'll find in the book) shows that there are about (allowing for the\nfact that n may not be a power of 2) lg(_n_)+1 levels of the tree. This is\nbecause you can only divide a power of two in half as many times as that power\nbefore you reach 1, and _n_ = 2lg(_n_). The 1 counts the root note before we\nstart dividing: there is always at least one level.\n\n_Questions? Does it make sense, or is it totally mysterious?_\n\n### One more Animation\n\nRecapitulating our conclusions, we have seen that Insertion sort is quick on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. Due to its simplicity it is a good choice when the sequence to\nsort will always be small. But for large inputs Merge Sort will be faster than\nInsertion Sort, as _n_2 grows much faster than _n_lg(_n_). Each sort algorithm\nhas different strengths and weaknesses, and performance depends on the data.\nSome of these points are made in the following visualizations (also watch for\npatterns that help you understand the strategies):\n\n> <http://www.sorting-algorithms.com/> (set to 50 elements)\n\n* * *\n\n## Next\n\nNext week we cover Chapter 3: Growth of Functions and Asymptotic Concepts.\nProblems will be posted for my students in Laulima.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:09:15 HST 2014\n\nImages are from the instructor's manual for Cormen et al.\n\n",
 "path"=>"morea//020.examples/reading-notes-2.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-2A.html</h2>

<pre>Hash
{"title"=>"Insertion Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2a",
 "morea_summary"=>"Insertion sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-2A.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-2A.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-2B.html</h2>

<pre>Hash
{"title"=>"Loop Invariant",
 "published"=>true,
 "morea_id"=>"reading-screencast-2b",
 "morea_summary"=>"Loop invariant example",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://youtu.be/t1ranlQmofQ",
 "morea_labels"=>["Screencast", "Suthers", "6 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-2B.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-2B.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-2C.html</h2>

<pre>Hash
{"title"=>"Insertion sort analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2c",
 "morea_summary"=>"Insertion sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://youtu.be/UtEMLcKHcGc",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-2C.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-2C.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-2D.html</h2>

<pre>Hash
{"title"=>"Merge Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2d",
 "morea_summary"=>"Merge sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://youtu.be/9BI0Lw1kzkE",
 "morea_labels"=>["Screencast", "Suthers", "11 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-2D.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-2D.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-2E.html</h2>

<pre>Hash
{"title"=>"Merge Sort Analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2e",
 "morea_summary"=>"Merge sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://youtu.be/1JbqqmK7e5s",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-2E.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-2E.md"}
</pre>

<h2>/morea/020.examples/reading-screencast-mit-1.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 1",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-1",
 "morea_summary"=>
  "Analysis of algorithms-insertion sort, asymptotic analysis, merge sort, recurrences",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec01/",
 "morea_labels"=>["Screencast", "Leiserson", "80 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/020.examples/reading-screencast-mit-1.html",
 "content"=>"",
 "path"=>"morea//020.examples/reading-screencast-mit-1.md"}
</pre>

<h2>/morea/030.growth/experience-asymptotic-concepts.html</h2>

<pre>Hash
{"title"=>"Asymptotic concepts",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-concepts",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of functions with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/030.growth/experience-asymptotic-concepts.html",
 "url"=>"/morea/030.growth/experience-asymptotic-concepts.html",
 "content"=>
  "## Asymptotic Concepts\n\n#### 5 points\n\n**1\\. (1 pt)** We can extend asymptotic notation to the case of two parameters n and m that can go to infinity independently at different rates. For example, we denote by O(g(n,m)) the set of functions:\n\n> O(_g_(_n_,_m_)) = {_f_(_n_,_m_) : there exists positive constants _c_, _n_0\nand _m_0 such that 0 ≤ _f_(_n_,_m_) ≤ _c__g_(_n_,_m_) for all _n_ ≥ _ _n0 or\n_m_ ≥ _m_0}\n\nGive a corresponding definition for Θ(_g_(_n_,_m_)).\n\n**2\\. (4 pts)** Indicate, for each pair of expressions (_f_(_n_), _g_(_n_)) in the table below, whether _f_(_n_) = ___(_g_(_n_)), where the ___ may be O, o, Ω, ω or Θ. Assume that k ≥ 1, _c_ > 1, and d > 0 are constants and we are analyzing growth rates in terms of the variable _n_. To respond, write \"Yes\" or \"No\" in each box. Grading will be based on these entries first, but if you give your justifications below we can give better feedback and possibly partial credit in case of wrong answers. \n\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\"><i>f</i>(<i>n</i>)</th>\n    <th scope=\"col\"><i>g</i>(<i>n</i>)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">a.</th>\n    <td>n<sup>lg <i>c</i></sup></td>\n    <td>c<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">b.</th>\n    <td>lg<sup><i>k</i></sup><i>n</i></td>\n    <td>n<sup><i>d</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">c.</th>\n    <td>2<sup><i>n</i></sup></td>\n    <td>2<sup><i>n</i>/2</sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">d.</th>\n    <td>lg(<i>n</i>!)</td>\n    <td>lg(<i>n</i><sup><i>n</i></sup>)</td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n",
 "path"=>"morea//030.growth/experience-asymptotic-concepts.md"}
</pre>

<h2>/morea/030.growth/module-growth.html</h2>

<pre>Hash
{"title"=>"Growth of functions",
 "published"=>true,
 "morea_id"=>"growth",
 "morea_outcomes"=>["outcome-growth"],
 "morea_readings"=>
  ["reading-screencast-3a",
   "reading-screencast-3b",
   "reading-screencast-3c",
   "reading-screencast-3d",
   "reading-cormen-3",
   "reading-notes-3",
   "reading-screencast-mit-2"],
 "morea_experiences"=>["experience-asymptotic-concepts"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/030.growth/module-growth.png",
 "morea_sort_order"=>30,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/030.growth/module-growth.html",
 "content"=>
  "Asymptotic notations, omega, theta, recurrences, substitution, master method.\n",
 "path"=>"morea//030.growth/module-growth.md"}
</pre>

<h2>/modules/growth/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-growth.md",
 "title"=>"Growth of functions",
 "url"=>"/modules/growth/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/growth/index.html"}
</pre>

<h2>/morea/030.growth/outcome-growth.html</h2>

<pre>Hash
{"title"=>"Understand analysis of asymptotic growth.",
 "published"=>true,
 "morea_id"=>"outcome-growth",
 "morea_type"=>"outcome",
 "morea_sort_order"=>30,
 "referencing_modules"=>[#Jekyll:Page @name="module-growth.md"],
 "url"=>"/morea/030.growth/outcome-growth.html",
 "content"=>
  "Understand how to characterize the behavior of functions in terms of their limiting, or asymptotic behavior. ",
 "path"=>"morea//030.growth/outcome-growth.md"}
</pre>

<h2>/morea/030.growth/reading-cormen-3.html</h2>

<pre>Hash
{"title"=>"CLRS 3 - Growth of functions",
 "published"=>true,
 "morea_id"=>"reading-cormen-3",
 "morea_summary"=>
  "Asymptotic notation, standard notation, and common functions.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-cormen-3.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-cormen-3.md"}
</pre>

<h2>/morea/030.growth/reading-notes-3.html</h2>

<pre>Hash
{"title"=>"Chapter 3 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-3",
 "morea_summary"=>"Introduction to asymptotic analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/030.growth/reading-notes-3.html",
 "url"=>"/morea/030.growth/reading-notes-3.html",
 "content"=>
  "## Outline\n\n  1. Intro to Asymptotic Analysis\n  2. Big-O\n  3. Ω (Omega)\n  4. Θ (Theta)\n  5. Asymptotic Notation in Equations\n  6. Asymptotic Inequality\n  7. Properties of Asymptotic Sets\n  8. Common Functions\n\n* * *\n\n## Intro to Asymptotic Analysis\n\nThe notations discussed today are ways to describe behaviors of _functions,_\nparticularly _in the limit_, or _asymptotic_ behavior.\n\nThe functions need not necessarily be about algorithms, and indeed asymptotic\nanalysis is used for many other applications.\n\nAsymptotic analysis of algorithms requires:\n\n  1. Identifying ** what aspect of an algorithm we care about**, such as:    \n\n    * runtime;\n    * use of space;\n    * possibly other attributes such as communication bandwidth;\n  \n\n  2. Identifying **a function that characterizes that aspect; ** and \n  \n\n  3. Identifying **the asymptotic class of functions that this function belongs to**, where classes are defined in terms of bounds on growth rate. \n\nThe different asymptotic bounds we use are analogous to equality and\ninequality relations:\n\n  * O   ≈   ≤\n  * Ω   ≈   ≥\n  * Θ   ≈   =\n  * o   ≈   <\n  * ω   ≈   >\n\nIn practice, most of our analyses will be concerned with run time. Analyses\nmay examine:\n\n  * Worst case\n  * Best case\n  * Average case (according to some probability distribution across all possible inputs)\n\n* * *\n\n## Big-O (asymptotic ≤)\n\nOur first question about an algorithm's run time is often \"how bad can it\nget?\" We want a guarantee that a given algorithm will complete within a\nreasonable amount of time for typical n expected. This requires an\n**asymptotic upper bound**: the \"worst case\".\n\nBig-O is commonly used for worst case analyses, because it gives an upper\nbound on growth rate. Its definition in terms of set notation is:\n\n> O(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/graph-big-O.jpg)\n\nThis definition means that as _n_ increases, afer a point _f_(_n_) grows no\nfaster than _g_(_n_) (as illustrated in the figure): _g_(_n_) is an\n_asymptotic upper bound_ for _f_(_n_).\n\nSince O(_g_(_n_)) is a set, it would be natural to write _f_(_n_) ∈\nO(_g_(_n_)) for any given _f_(_n_) and _g_(_n_) meeting the definition above,\nfor example, _f_ ∈ O(_n_2).\n\nBut the algorithms literature has adopted the convention of using = instead of\n∈, for example, writing _f_(_n_) = O(_g_(_n_)). This \"abuse of notation\" makes\nsome manipulations possible that would be more tedious if done strictly in\nterms of set notation. (We do _not_ write O(_g_(_n_))=_f_(_n_); will return to\nthis point).\n\nUsing the = notation, we often see definitions of big-O in in terms of truth\nconditions as follows:\n\n> _f_(_n_) = O(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0.\n\nWe assume that all functions involved are asymptotically non-negative. Other\nauthors don't make this assumption, so may use |_f_(_n_)| etc. to cover\nnegative values. This assumption is reflected in the condition 0 ≤ _f_(_n_).\n\n### Examples\n\nShow that 2_n_2 is O(_n_2).\n\nTo do this we need to show that there exists some _c_ and _n_0 such that\n(letting 2_n_2 play the role of _f_(_n_) and _n_2 play the role of _g_(_n_) in\nthe definition):\n\n> 0 ≤ 2_n_2 ≤ _c__n_2 for all _n_ ≥ _n_0.\n\nIt works with _c_ = 2, since this makes the _f_ and _g_ terms equivalent for\nall _n_ ≥ _n_0 = 0. (We'll do a harder example under Θ.)\n\n#### What's in and what's out\n\nThese are all O(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_\n  * _n_1.99999\n  * _n_\n\n  * _n_3\n  * _n_2.00001\n  * _n_2 lg _n_\n\n#### Insertion Sort Example\n\nRecall that we did a tedious analysis of the worst case of insertion sort,\nending with this formula:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _pn2 \\+ _q__n_ \\- r_ for suitable _p, q, r_ (_p_ =\n(_c_5/2 + _c_6/2 + _c_7/2), etc.).\n\nThe textbook (page 46) sketches a proof that __f_(_n_) = _a__n_2 \\+ _b__n_ \\+\n_c__ is Θ(_n_2), and we'll see shortly that Θ(_n_2) -> O(_n_2). This is\ngeneralized to all polynomials in Problem 3-1. So, any polynomial with highest\norder term _a__n__d_ (i.e., a polynomial in _n_ of degree _d_) will be\nO(_n__d_).\n\nThis suggests that the worst case for insertion sort _T_(_n_) ∈ O(_n_2). An\nupper bound on the worst case is also an upper bound on all other cases, so we\nhave already covered those cases.\n\nNotice that the definition of big-O would also work for __g_(_n_) = n3_,\n__g_(_n_) = 2n_, etc., so we can also say that _T_(_n_) (the worst case for\ninsertion sort) is O(_n_3), O(2_n_), etc. However, these loose bounds are not\nvery useful! We'll deal with this when we get to Θ (Theta).\n\n* * *\n\n## Ω (Omega, asymptotic ≥)\n\nWe might also want to know what the best we can expect is. In the last lecture\nwe derived this formula for insertion sort:\n\n![](fig/equation-insertion-best.jpg)\n\nWe could prove that this best-case version of T(n) is big-O of something, but\nthat would only tell us that the best case is no worse than that something.\nWhat if we want to know what is \"as good as it gets\": a lower bound below\nwhich the algorithm will never be any faster?\n\nWe must both pick an appropriate function to measure the property of interest,\nand pick an appropriate asymptotic class or comparison to match it to. We've\ndone the former with _T_(_n_), but what should it be compared to?\n\nIt makes more sense to determine the **asymptotic lower bound** of growth for\na function describing the best case run-time. In other words, what's the\nfastest we can ever expect, in the best case?\n\n![](fig/graph-Omega.jpg)\n\n**Ω (Omega)** provides what we are looking for. Its set and truth condition definitions are simple revisions of those for big-O:\n\n> Ω(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_cg_(_n_) ≤ _f_(_n_) ∀ _n_ ≥ _n_0}.  \n_[The _f_(_n_) and _cg_(_n_) have swapped places.]_\n\n> _f_(_n_) = Ω(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that\n_f_(_n_) ≥ _cg_(_n_) ∀ _n_ ≥ _n_0.  \n_[≤ has been replaced with ≥.]_\n\nThe semantics of Ω is: as _n_ increases after a point, _f_(_n_) grows no\nslower than _g_(_n_) (see illustration).\n\n### Examples\n\nSqrt(_n_) is Ω(lg _n_) with _c_=1 and _n_0=16.  \n_(At n=16 the two functions are equal; try at n=64 to see the growth, or graph\nit.)_\n\n####  What's In and What's Out\n\nThese are all Ω(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000n   _(It's also O(_n_2)!)_\n_\n\n  * 1000_n_2 \\+ 1000_n_\n  * 1000_n_2 \\- 1000_n_\n  * _n_3\n  * _n_2.00001\n__ _\n\n  * _n_1.99999\n  * _n_\n  * lg _n_\n\n#### Insertion Sort Example\n\nWe can show that insertion will take at least Ω(_n_) time in the best case\n(i.e., it won't get any better than this) using the above formula and\ndefinition.\n\n![](fig/equation-insertion-best.jpg)\n\n_T_(_n_) can be expressed as _pn - q_ for suitable _p, q_ (e.g., _q_ = _c_2 \\+\n_c_4 \\+ _c_5 \\+ _c_8, etc.). (In this case, _p_ and _q_ are positive.) This\nsuggests that _T_(_n_) ∈ Ω(_n_), that is, ∃ _c, n0_ s.t. _pn - q ≥ cn,_ ∀_n ≥\nn0_. This follows from the generalized proof for polynomials.\n\n* * *\n\n## Θ (Theta, asymptotic =)\n\nWe noted that there are _ loose _ bounds, such as _f_(_n_) = _n_2 is O(_n_3),\netc., but this is an overly pessimistic assessment. It is more useful to have\nan **asymptotically tight bound** on the growth of a function. In terms of\nalgorithms, we would like to be able to say (when it's true) that a given\ncharacteristic such as run time grows _no better and no worse_ than a given\nfunction. That is, we want to simultaneoulsy bound from above and below.\nCombining the definitions for O and Ω:\n\n![](fig/graph-Theta.jpg)\n\n> Θ(_g_(_n_)) = {_f_(_n_) : ∃ positive constants **_c_1, _c_2**, and _n_0 such\nthat 0 ≤ **_c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)**, ∀ _n_ ≥ _n_0}.\n\nAs illustrated, _g_(_n_) is an asymptotically tight bound for _f_(_n_): after\na point, _f_(_n_) grows no faster and no slower than _g_(_n_).\n\nThe book suggests the proof of this theorem as an easy exercise (just combine\nthe two definitions):\n\n> _f_(_n_) = Θ(_g_(_n_)) iff _f_(_n_) = Ω(_g_(_n_)) ∧ _f_(_n_) = O(_g_(_n_)).\n\nYou may have noticed that some of the functions in the list of examples for\nbig-O are also in the list for Ω. This indicates that the set Θ is not empty.\n\n### Examples\n\n> _Reminder:_ _f_(_n_) = Θ(_g_(_n_)) iff ∃ positive constants _c_1, _c_2, and\n_n_0 such that 0 ≤ _c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)∀ _n_ ≥ _n_0.\n\n_n_2 \\- 2_n_ is Θ(_n_2),   with _c_1 = 1/2; _c_2 = 1, and _n_0 = 4,   since:\n\n> _n_2/2   ≤   _n_2 \\- 2_n_   ≤   _n_2   for _n_ ≥ _n_0 = 4.\n\n#### Find an asymptotically tight bound (Θ) for\n\n  * 4_n_3\n  * 4_n_3 \\+ 2_n_. \n\nPlease try these before you [find solutions\nhere](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-03\n/Example-Analysis.html).\n\n#### What's in and what's out\n\nThese are all Θ(n2):\n\nThese are not\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_ \\+ 32,700\n  * 1000_n_2 \\- 1000_n_ \\- 1,048,315\n\n  * _n_3\n  * _n_2.00001\n  * _n_1.99999\n  * _n_ lg _n_\n\n* * *\n\n## Asymptotic Inequality\n\nThe O and Ω bounds may or may not be asymptotically tight. The next two\nnotations are for upper bounds that are strictly _not_ asymptotically tight.\nThere is an _analogy_ to inequality relationships:\n\n  * \"≤\" is to \"<\" as big-O (may or may not be tight) is to little-o (strictly not equal) \n  * \"≥\" is to \">\" as Ω (may or may not be tight) is to little-ω (strictly not equal). \n\n### o-notation (\"little o\", asymptotic <)\n\n> o(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _f_(_n_) **<** _cg_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/o-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _insignificant_ relative to _g_(_n_) as _n_\napproaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically smaller** than _g_(_n_) if _f_(_n_) =\no(_g_(_n_))\n\n  * _n_1.99999 ∈ o(_n_2)\n  * _n_2/lg _n_ ∈ o(_n_2)\n  * _n_2 ∉ o(_n_2) (similarly, 2 is not less than 2)\n  * _n_2/1000 ∉ o(_n_2) \n\n### ω-notation (\"little omega\", asymptotic >)\n\n> ω(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _cg_(_n_) **<** _f_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/omega-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _ arbitrarily large _ relative to _g_(_n_) as\n_n_ approaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically larger** than _g_(_n_) if _f_(_n_) =\nω(_g_(_n_))\n\n  * _n_2.00001 ∈ ω(_n_2)\n  * _n_2lg _n_ ∈ ω(_n_2)\n  * _n_2 ∉ ω(_n_2)\n\nThe two are related:   **_f_(_n_) ∈ ω(_g_(_n_))   iff   _g_(_n_) ∈\no(_f_(_n_)).**\n\n* * *\n\n## Asymptotic Notation in Equations\n\nWe already noted that while asymptotic categories such as Θ(_n_2) are sets, we\nusually use \"=\" instead of \"∈\" and write (for example) _f_(_n_) = Θ(_n_2) to\nindicate that _f_ is in this set.\n\nPutting asymptotic notation in equations lets us do shorthand manipulations\nduring analysis.\n\n### Asymptotic Notation on Right Hand Side: ∃\n\nO(_g_(_x_)) on the right hand side stands for _some_ anonymous function in the\nset O(_g_(_x_)).\n\n> 2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ Θ(_n_)     means:  \n2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ _f_(_n_)       for _some_ __f_(_n_) ∈ Θ(_n_)_\n(in particular, _f_(_n_) = 3_n_ \\+ 1).\n\n### Asymptotic Notation on Left Hand Side: ∀\n\nThe notation is only used on the left hand side when it is also on the right\nhand side.\n\nSemantics: No matter how the anonymous functions are chosen on the left hand\nside, there is a way to choose the functions on the right hand side to make\nthe equation valid.\n\n> 2_n_2 \\+ Θ(_n_) = Θ(_n_2)     means   for _all_ _f_(_n_) ∈ Θ(_n_), there _\nexists_ _g_(_n_) ∈ Θ(_n_2) such that  \n2_n_2 \\+ _f_(_n_) = _g_(_n_).\n\n### Combining Terms\n\nWe can do basic algebra such as:\n\n> 2_n_2 \\+ 3_n_ \\+ 1   =   2_n_2 \\+ Θ(_n_)   =   Θ(_n_2)\n\n* * *\n\n## Properties\n\nIf we keep in mind the analogy to inequality, many of these make sense, but\nsee the end for a caution concerning this analogy.\n\n### Relational Properties\n\n**Transitivity**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_)) and _g_(_n_) = Θ(h(n))   ⇒   _f_(_n_) = Θ(h(n)).\n  * _f_(_n_) = O(_g_(_n_)) and _g_(_n_) = O(h(n))   ⇒   _f_(_n_) = O(h(n)).\n  * _f_(_n_) = Ω(_g_(_n_)) and _g_(_n_) = Ω(h(n))   ⇒   _f_(_n_) = Ω(h(n)).\n  * _f_(_n_) = o(_g_(_n_)) and _g_(_n_) = o(h(n))   ⇒   _f_(_n_) = o(h(n)).\n  * _f_(_n_) = ω(_g_(_n_)) and _g_(_n_) = ω(h(n))   ⇒   _f_(_n_) = ω(h(n)).\n**Reflexivity**:\n    \n\n  * _f_(_n_) = Θ(_f_(_n_))\n  * _f_(_n_) = O(_f_(_n_))\n  * _f_(_n_) = Ω(_f_(_n_))\n  * _What about o and ω?_\n**Symmetry**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_))   iff   _g_(_n_) = Θ(_f_(_n_)) \n  * _Should any others be here? Why or why not?_\n**Transpose Symmetry**:\n    \n\n  * _f_(_n_) = O(_g_(_n_))   iff   _g_(_n_) = Ω(_f_(_n_)) \n  * _f_(_n_) = o(_g_(_n_))   iff   _g_(_n_) = ω(_f_(_n_)) \n\n### Incomparability\n\nHere is where the analogy to numeric (in)equality breaks down: There is no\ntrichotomy. Unlike with constant numbers, we can't assume that one of <, =, >\nhold. Some functions may be incomparable.\n\nExample: _n_1 + _sin n_ is incomparable to _n_ since _sin n_ oscillates\nbetween -1 and 1, so 1 + _sin n_ oscillates between 0 and 2. _(Try graphing\nit.)_\n\n* * *\n\n## Common Functions and Useful Facts\n\nVarious classes of functions and their associated notations and identities are\nreviewed in the end of the chapter: please review the chapter and refer to ICS\n241 if needed. Here we highlight some useful facts:\n\n### Monotonicity\n\n  * _f_(_n_) is **monotonically increasing**   if   _m_ ≤ _n_   ⇒   _f_(_m_) ≤ _f_(_n_).\n  * _f_(_n_) is **monotonically decreasing**   if   _m_ ≥ _n_   ⇒   _f_(_m_) ≥ _f_(_n_).\n  * _f_(_n_) is **strictly increasing**   if   _m_ < _n_   ⇒   _f_(_m_) < _f_(_n_).\n  * _f_(_n_) is **strictly decreasing**   if   _m_ > _n_   ⇒   _f_(_m_) > _f_(_n_).\n\n### Polynomials\n\n  * _p_(_n_) = Θ(_n__d_), for asymptoptically positive polynomials in _n_ of degree _d_\n\n### Exponentials\n\n  * _n__b_ = o(_a__n_) for all real constants _a_ and _b_ such that _a_ > 1: **_Any exponential function with a base greater than 1 grows faster than any polynomial function._**\n  \n\n  * Useful identities: \n    * _a_-1 = 1/_a_\n    * (_a__m_)_n_ = _a__mn_\n    * _a__m__a__n_ = _a__m_ \\+ _n_\n\n### Logarithms\n\n  * (lg _n_)_b_ = lg_b__n_ = o(_n__a_), for a > 0: **_any positive polynomial function grows faster than any polylogarithmic function._**\n  \n\n  * Useful identities: \n    * _a_ = _b_log_b__a_   _(Definition of logs.)_\n    * log_a__n_ = log_b__n_/log_b__a_     \n    _(Base change. If _n_ is variable and _a_ and _b_ are constant, the denominator is constant: this is why asymptotic analysis can ignore the base.)_\n    * log_c_(_ab_) = log_c__a_ \\+ log_c__b_   _(Ask your slide rule!)_\n    * log_b__a_n__ = _n_ log_b__a_\n    * log_b_(1/_a_) = −log_b__a_\n    * log_b__a_ = 1 / log_a__b_\n    * _a_log_b__c_ = _c_log_b__a_   _(Useful for getting the variable where you want it.)_\n\n### Factorials\n\n  * _n_! = ω(2_n_):   _**factorials grow faster than exponentials** (but it could be worse):_\n  * _n_! = o(_n__n_)\n  * lg(_n_!) = Θ(_n_ lg _n_)\n  * See also the more complex **Stirling's approximation** from which these are derived.\n\n### Iterated Functions\n\n  * Definition: _f_(_i_)(_n_) is _f_ applied _i_ times to the initial value _n_. \n  * Iterated Logarithm: lg*_n_ = min{_i_ ≥ 0: lg(_i_)_n_ ≤ 1}   _(The iteration at which lg(_i_)_n_ is less than 1: a very slowly growing function.)_\n\n### Fibonacci Numbers\n\n  * Definition: _F_0 = 0; _F_1 = 1; and for _i_ > 1 _F__i_ = _F__i_-1 \\+ _F__i_-2. \n  * **_Fibonacci numbers grow exponentially._**\n\n* * *\n\nDan Suthers Last modified: Sat Jan 25 03:51:57 HST 2014  \nImages are from the instructor's manual for Cormen et al.  \n\n",
 "path"=>"morea//030.growth/reading-notes-3.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-3a.html</h2>

<pre>Hash
{"title"=>"Asymptotic notations",
 "published"=>true,
 "morea_id"=>"reading-screencast-3a",
 "morea_summary"=>"Notations for this analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=y86z2OrIYQQ",
 "morea_labels"=>["Screencast", "Suthers", "12 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-3a.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-3a.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-3b.html</h2>

<pre>Hash
{"title"=>"Omega and Theta",
 "published"=>true,
 "morea_id"=>"reading-screencast-3b",
 "morea_summary"=>"The omega and theta notations",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-3b.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-3b.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-3c.html</h2>

<pre>Hash
{"title"=>"little-o and omega",
 "published"=>true,
 "morea_id"=>"reading-screencast-3c",
 "morea_summary"=>"The little guys, properties, and use in equations",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=uaqLI449XQw",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-3c.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-3c.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-3d.html</h2>

<pre>Hash
{"title"=>"Common Functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-3d",
 "morea_summary"=>"Common functions and useful identities",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=f2czg61AtQw",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-3d.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-3d.md"}
</pre>

<h2>/morea/030.growth/reading-screencast-mit-2.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 2",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-2",
 "morea_summary"=>
  "Asymptotic notation, recurrences, substitution, master method (start at 16 min)",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec02/",
 "morea_labels"=>["Screencast", "Demaine", "71 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/030.growth/reading-screencast-mit-2.html",
 "content"=>"",
 "path"=>"morea//030.growth/reading-screencast-mit-2.md"}
</pre>

<h2>/morea/040.adt/experience-asymptotic-homework.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis: Homework",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Practice asymptotic analysis.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-asymptotic-homework.html",
 "url"=>"/morea/040.adt/experience-asymptotic-homework.html",
 "content"=>
  "## Homework Problems\n\n_There are 6 problems for a total of 20 points. Most of them are easier than\nthey look at first._\n\n_Please consider:_ The homework is really \"worth\" a lot more than 20 points\nbecause exams have similar problems. If you have not practiced with the\nhomeworks, you'll do worse on the exams. So, if you think these problems are\nnot worth the time for the points, think again.\n\n### #1. Peer Credit Assignment\n\n#### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### #2. Proving asymptotic complexity\n\n#### 4 points\n\nUsing the truth-condition definition of big-O, prove that 3_n_2 \\+ 9 =\nO(_n_2).\n\n> The definition is the one that starts with \"_f_(_n_) = O(_g_(_n_)) iff ...\".\nYou will have to choose suitable _c_ and _n_0, plug them into the definition\nin \"...\", and argue that the condition is met.  \nThe 4 points are: (1) identification of _c_ and _n_0 that work; (1) writing\nout the definition, and (2) demonstrating that it is satsified as _n_ grows\nbeyond _n_0 (not just for _n_ = _n_0).\n\n* * *\n\n### #3. Relative growth rates of functions\n\n#### 3 points\n\nContinuing in the style of Tuesday's class exercise, fill in the table for\nthese pairs of functions with \"Yes\" or \"No\" in each empty box. Then, for each\nrow, justify your choice, preferably by showing mathematical relationships\n(e.g., transforming one expression into another, or into expressions that are\nmore easily compared).\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">f(n)</th>\n    <th scope=\"col\">g(n)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">e.</th>\n    <td>4<i>n</i><sup>2</sup></td>\n    <td>4<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">f.</th>\n    <td>2<sup>lg <i>n</i></sup></td>\n    <td>lg<sup>2</sup><i>n</i></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">g.</th>\n    <td>&#8730;<i>n</i></td>\n    <td>n<sup>sin <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n### #4. Complexity of Dynamic Set Operations in List Implementations\n\n#### 2 points\n\nFor each of the four types of lists in the following table, what is the\nasymptotic worst-case running time for `predecessor` and `maximum`?\n\nAssume that _k_ is the key, _d_ the data associated with the key, and _p_ a\nposition in the data structure. This version of the ADT is similar to the\nbook's, but abstracts list elements _x_ to positions (returned by search).\n`predecessor` and `maximum` are with respect to ordering of keys in the set\nunder \"<\", NOT ordering of the data structure. Sorted lists are sorted in\nascending order. This continues your in-class work, which you may want to\nreview for correctness first.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  (continued)\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th colspan=\"5\" scope=\"row\"><i>... others done in class here ... </i></th>\n  </tr>\n  <tr>\n    <th scope=\"row\">predecessor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">maximum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n\n### #5. Tree Traversals\n\n#### 4 points\n\n_In class you wrote a recursive procedure for traversal of a binary tree in\nO(n) time, printing out the keys of the nodes. Here you write two other tree\ntraversal procedures. The first is a variation of what you wrote in class; the\nsecond is on a different kind of tree that you read about pages 248-249 and in\nmy lecture notes and screencast._\n\n**(a)** Write an O(_n_)-time **_non-recursive_** procedure that, given an _n_-node binary tree, prints out the key of each node of the tree in whatever order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your procedure takes a `TreeNode` as its argument (the root of the tree). **_Use a stack as an auxiliary data structure._**\n\n>     printBinaryTreeNodes(TreeNode root) {\n\n\n**(b)** Write an O(_n_)-time procedure that, given an _n_-node rooted tree with **_arbitrary_** number of children using the **_left-child, right-sibling representation_**, prints out the key of each node of the tree in whatever order you wish. Assume that for economy of code we are re-using our `TreeNode` class. The instance variable `left` points to the left child as before, but now `right` points to the right sibling instead of the right child (which is no longer unique). Your procedure takes a `TreeNode` as its argument (the root of the tree). You may choose to use either the recursive or non-recursive approach.\n\n>     printLCRSTreeNodes(TreeNode root) {\n\n\n### #6. A Hybrid Merge/Insertion Sort Algorithm\n\n#### 7 points\n\nAlthough MergeSort runs in Θ(_n_ lg _n_) worst-case time and InsertionSort\nruns in Θ(_n_2) worst-case time, the constant factors in insertion sort\n(including that fact that it can sort in-place) can make it faster in practice\nfor small problem sizes on many machines. Thus, it makes sense to\n**_coarsen_** the leaves of the MergeSort recursion tree by using\nInsertionSort within MergeSort when subproblems become sufficiently small.\n\nConsider a modification to MergeSort in which _n_/_k_ sublists of length _k_\nare sorted using InsertionSort and are then merged using the standard merging\nmechanism, where _k_ is a value to be determined in this problem. In the first\ntwo parts of the problem, we get expressions for the contributions of\nInsertionSort and MergeSort to the total runtime as a function of the input\nsize _n_ and the cutoff point between the algorithms _k_.\n\n**(a - 1pt)** Show that InsertionSort can sort the _n_/_k_ sublists, each of length _k_, in Θ(_nk_) worst-case time. To do this:\n\n  1. write the cost for sorting _k_ items with InsertionSort,\n  2. multiply by how many times you have to do it, and \n  3. show that the expression you get simplifies to Θ(_nk_).\n\n**(b - 3pts)** Show that MergeSort can merge the _n_/_k_ sublists of size _k_ in Θ(_n_ lg (_n_/_k_)) worst-case time. To do this: \n\n  1. draw the recursion tree for the merge (a modification of figure 2.5), \n  2. determine how many elements are merged at each level, \n  3. determine the height of the recursion tree from the _n_/_k_ lists that InsertionSort had already taken care of up to the single list that results at the end, and \n  4. show how you get the final expression Θ(_n_ lg (_n_/_k_)) from these two values. \n\n_**Putting it together:**_ The asymptotic runtime of the hybrid algorithm is\nthe sum of the two expressions above: the cost to sort the _n_/_k_ sublists of\nsize _k_, and the cost to divide and merge them. You have just shown this to\nbe\n\n> Θ(_n__k_ \\+ _n_ lg (_n_/_k_))\n\nIn the second two parts of the question we explore what _k_ can be.\n\n**(c - 2pts)** The bigger we make _k_ the bigger lists InsertionSort has to sort. At some point, its Θ(_n_2) growth will overcome the advantage it has over MergeSort in lower constant overhead. How big can _k_ get before InsertionSort starts slowing things down? Derive a theoretical answer by proving the largest value of _k_ for which the hybrid sort has the same Θ runtime as a standard Θ(_n_ lg _n_) MergeSort. This will be an upper bound on _k_. To do this: \n\n  1. Looking at the expression for the hybrid algorithm runtime Θ(_n__k_ \\+ _n_ lg (_n_/_k_)), identify the upper bound on _k_ expressed as a function of _n_, above which Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) would grow faster than Θ(_n_ lg _n_). _Give the _f_ for _k_ = Θ(_f_(_n_)) and argue for why it is correct._\n  2. Show that this value for _k_ works by substituting it into Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) and showing that the resulting expression simplifies to Θ(_n_ lg _n_). \n\n**(d - 1pt)** Now suppose we have implementations of InsertionSort and MergeSort. How should we choose the optimal value of _k_ to use for these given implementations in practice? \n\n",
 "path"=>"morea//040.adt/experience-asymptotic-homework.md"}
</pre>

<h2>/morea/040.adt/experience-basic-data-structures.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis of basic data structures",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-basic-data-structures",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of basic data structures with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-basic-data-structures.html",
 "url"=>"/morea/040.adt/experience-basic-data-structures.html",
 "content"=>
  "## Basic Data Structures\n\n####  5 points\n\n**1\\. (4 pts)** For each of the four types of lists in the following table, what is the _asymptotic worst-case running time_ for each dynamic set operation listed, given a list of length _n_? \n\n  * Assume that __k_ is the key_, _d_ the data associated with the key, and __p_ a position_ in the data structure.\n  * As explained in class, positions abstract and provide encapsulated access to elements of a data structure. We don't have to search for the item if we have a position p for it.\n  * Sorted lists are sorted in _ascending order_.\n  * Predecessor, successor, minimum, and maximum are _with respect to ordering of keys in the set under \"<\"_, not necessarily ordering of the list data structure.\n  * The cells filled in are from the quiz.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">insert(k, d)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(1)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">search(k)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(n)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">delete(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">successor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">minimum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n  \n\n**2\\. (1 pt)** Write a Θ(_n_)-time recursive procedure that, _given an _n_-node binary tree, prints out the key of each node of the tree_ in any order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your recursive procedure takes a `TreeNode` as its argument (the root of the tree or subtree being considered by the recursive call).\n    \n    \n    **\n    printTreeNodes(TreeNode root)\n        if (root != null) {\n    \n    \n    **\n\n\n",
 "path"=>"morea//040.adt/experience-basic-data-structures.md"}
</pre>

<h2>/morea/040.adt/experience-project-1.html</h2>

<pre>Hash
{"title"=>"Battle of the Dynamic Sets",
 "published"=>true,
 "morea_id"=>"experience-project-1",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Provide four implementations of the Dynamic Set ADT and compare their performance.",
 "morea_sort_order"=>1,
 "morea_labels"=>["Programming"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/experience-project-1.html",
 "url"=>"/morea/040.adt/experience-project-1.html",
 "content"=>
  "# Battle of the Dynamic Sets\n\nLate submissions with **penalty of 1% per hour** accepted until 2AM Saturday\nMarch 15th, at which time the deduction reaches 50%. (If you used all that\ntime and turned in a perfect \"A+\" project, you would get a \"C\".)\n\n\n### Change Log\n\nMar 10, 2014\n\n    Assignment extended three days. Corrected the late deadline on March 11th.\nMar 1, 2014\n\n    Changed the interface to Robert Ward's version with generics and comparable.\nFeb 7, 2014\n\n    Added delete to operations to be tested. \nFeb 6, 2014\n\n    First released version. This differs from the 2012 and 2013 versions. \n\n## Overview\n\nYou will provide four implementations of the Dynamic Set ADT (see below,\nmodified from [Topic 4 Notes](/morea/040.adt/reading-notes-4.html)). You will then analyze their expected\nperformance (this has been largely done in the lectures and textbook), and\nthen test them empirically to see how your test data compares to expected\nperformance. You will write a report on the results. The project is worth 60\npoints.\n\nYou may either use open source implementations of the following data\nstructures (see the [Assignments Page](/morea/010.introduction/reading-assignments.html) Section \"Including Open Source Software\"\nfor requirements), or implement them yourself:\n\n  * **Sorted Doubly Linked Lists** (Chapter 10 or [Notes/Topic-04](morea/040.adt/reading-notes-4.html)) \n  * **Skip Lists** ([Notes/Topic-05](morea/050.probabilistic/reading-notes-5.html) or ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf) \n  * **Binary Search Trees** (Chapter 12 or [ Notes/Topic-08](morea/080.binary-search-trees/reading-notes-8.html)) \n  * **Red-Black trees** (Chapter 13 or [Notes/Topic-11]morea/110.balanced-trees/reading-notes-11.html)): for this, use Java TreeMap. \n\nThere are several advantages to implementing at least some of these\nyourself:\n\n  * Others' implementations may have incompatible assumptions.\n  * You will understand the data structures and algorithms better\n  * You can then offer your implementation to other students for extra credit in Implementation Project 2.\n\nYou will then write your own code, including:\n\n  * Implementations of Dynamic Sets that use these data structures;\n  * A main method to read in a file of strings, insert these in the dynamic set as keys, and run tests on all four implementations as specified below.\n\nWe will provide you with data in the form of files of keywords (automatically\ngenerated names). There will be ten data files, three _unsorted_ and three\n_sorted_ of size 100, 1000, 10,000, 100,000 and 1,000,000. (See discussion\nbelow concerning the largest file.) We may use other data files in our\ntesting, so make your code robust!\n\nYour accompanying report will provide the **minimum**, **average** and\n**maximum** times in **nanoseconds** required for: `insert`, `retrieve`,\n`successor`, and `predecessor`, and the actual time for `minimum` and\n`maximum`, for each of the input files. You will compare these results to the\ntheoretical analyses for the algorithms. While `delete` is not included in the\ntiming tests, we will test that deletion works.\n\nDetails follow. Questions and requests for clarification are welcome and\nshould be submitted early.\n\n## Dynamic Set ADT\n\nNote that this **ADT has been changed**: it is not the same as the one in the\nlecture notes, and it is not the same as the ones used in my 2012 or 2013\nclasses. Also, it was changed to use generics.\n\n{% highlight java linenos %}\n    package ics311;\n    import java.util.Map.Entry;\n\n    \n    /**\n     * This interface is to be used for all of the Dynamic Sets you create for this assignment.\n     * \n     * @author      Robert Ward\n     * @version     1.0       \n     * @since       2014-02-01\n     */\n    public interface DynamicSet<Key extends Comparable<Key> , Value> {\n        \n       /**\n        * Returns the name of Data Structure this set uses. \n        *           \n        * @return  the name of Data Structure this set uses. \n        */\n        public String setDataStructure();\n\n       /**\n        * Returns the number of key-value mappings in this set. \n        * This method returns zero if the set is empty\n        *           \n        * @return  the number of key-value mappings in this set. \n        */\n        public int size();\n       \n       /**\n        * Associates the specified value with the specified key in this map.\n        * If the map previously contained a mapping for the key, the old value is replaced. \n        * If there is no current mapping for this key return null,\n        * otherwise the previous value associated with key. \n        * \n        * @param  key - key with which the specified value is to be associated\n        * @param  value - value to be associated with the specified key\n        * \n        * @return the previous value associated with key, or null if there was no mapping for key. \n        *  (A null return can also indicate that the map previously associated null with key.)  \n        */\n        public Value insert(Key key, Value value);\n        \n       /**\n        * Removes the mapping for this key from this set if present.\n        *\n        * @param key - key for which mapping should be removed\n        * \n        * @return the previous value associated with key, or null if there was no mapping for key. \n        * (A null return can also indicate that the map previously associated null with key.) \n        */\n        public Value delete(Key key);\n      \n    \n       /**\n        * Returns the value to which the specified key is mapped, or null if this set contains no\n        * mapping for the key. \n        * \n        * @param  key The key under which this data is stored. \n        *   \n        * @return the Value of element stored in the set under the Key key.\n        */\n        public Value retrieve(Key key);\n       \n       \n       /**\n        * Returns a key-value mapping associated with the least key in this map, or null if the set\n        * is empty. \n        * IMPORTANT: This operation only applies when there is a total ordering on the Key \n        * Returns null if the set is empty. If there is not total ordering on the Key returns null.  \n        * \n        * @return an entry with the least key, or null if this map is empty\n        */\n        public Entry<Key, Value>  minimum( );\n       \n    \n       /**\n        * Returns a key-value mapping associated with the greatest key in this map, or null if the\n        * map is empty. \n        * IMPORTANT: This operation only applies when there is a total ordering on the Key \n        * Returns null if the set is empty. If there is not total ordering on the key returns null. \n        * \n        * @return an entry with the greatest key, or null if this map is empty.\n        */\n        public Entry<Key, Value>  maximum( );\n       \n        \n       /**\n        * Returns a key-value mapping associated with the least key strictly greater than the given\n        * key, or null if there is no such key. \n        * IMPORTANT: This operation only applies when there is a total ordering on the key \n        * Returns null if the set is empty or the key is not found. \n        * Returns null if the key is the maximum element. \n        * If there is not total ordering on the key for the set returns null. \n        * @param key - the key\n        * \n        * @return an entry with the greatest key less than key, or null if there is no such key\n        */\n        public Entry<Key, Value>  successor(Key key);\n        \n       /**\n        * Returns a key-value mapping associated with the greatest key strictly less than the given\n        * key, or null if there is no such key. \n        * IMPORTANT: This operation only applies when there is a total ordering on the key \n        * Returns null if the set is empty or the key is not found. \n        * Returns null if the key is the minimum element. \n        * If there is not total ordering on the key for the set returns null. \n        * @param key - the key\n        * \n        * @return an entry with the greatest key less than key, or null if there is no such key\n        */\n        public Entry<Key, Value>  predecessor(Key key);\n    \n    }\n{% endhighlight %}\n\n### Comments on ADT\n\nIf you wish, to simplify things you may change KeyType to String. The data\n`Object d` stored under a key is not important for our testing: just give it\nthe key `k`.\n\nIn a real implementation, returning a key in `minimum`, `maximum`, `successor`\nand `predecessor` would be inefficient if the client then has to use\n`retrieve` to get the data. This could be solved by using the position\nabstraction, or by returning multiple values (key + data). But for our\npurposes we only need the key to test runtime.\n\n### Implementing the ADT\n\nYou will have four classes implementing this interface, named to reflect the\nimplementation:\n\n  * **`DLLDynamicSet`**: Doubly Linked List implementation, using OSS \n  * **`SkipListDynamicSet`**: Skip List implementation, using OSS\n  * **`BSTDynamicSet`**: Binary Search Tree implementation, using OSS\n  * **`RedBlackDynamicSet`**: Red Black Tree implementation, _using Java TreeMap_\n\nTo implement the ADT you need to write the methods that are specified by the\ninterface but map to the underlying implementation. As a simple example, in\nRedBlackDynamicSet the interface method   `retrieve (KeyType k)` will be\nimplemented by calling TreeMap's `get(Object key)`. This is trivial code, but\nit accomplishes an important objective: hiding the details of the\nimplementation behind the interface.\n\nWe suggest that you implement **`RedBlackDynamicSet`** first and get the rest\nof the program running on it, as you already have this implementation. Then\neither write or look for OSS of the other three and add them to the testing.\n\nIn general, you are free to make your own implementation decisions and use the\nbest practices you are capable of. This includes whether or not you use\nComparable or generics, whether you use locator abstractions (such as Goodrich\n& Tamassia's \"position\") rather than returning pointers to data stored, how\nmuch error checking is done, etc. The important thing is that you have\nimplemented the set operations in a general way, and in particular can run the\ntests.\n\nWe suggest that you organize your code in packages, but this is not required.\n\n* * *\n\n## Test Data\n\nFiles including from 100 to 1,000,000 keys, sorted and unsorted, are available\nin [this directory](http://www2.hawaii.edu/~suthers/courses/ics311s14/Projects\n/Project-1/). Each file has one name on each line. There are no quotations or\nother delimiters other than newline. For example:\n    \n    Hugh Moreno \n    Traci Obrien \n    Doug Moore \n    Sammy West \n    Anne Reid \n    Deanna Zimmerman \n    Marcus Waters \n    Clyde Walton \n    Matthew Rios \n    Jacqueline Robertson\n    ...\n    \n\nYour program will take the input file name as an argument at the command line\n(i.e., read into String args). It will read the file into an array until end\nof file is reached. We need to read into an array because we will be randomly\nselecting elements for test runs. You do not need to test the input for any\nproperties: just read each line from the file as if it is a string.\n\n* * *\n\n## Input\n\nYour program will read an argument from the command line: the name of the file\nto be loaded in this test run.\n\nRead the string keys in the specified file into an array. Count the keys as\nyou read them so you know what your _n_ is. You will then use this array to\ndrive a test of each of the Dynamic Set implementations, as discused in the\nnext section.  \n\nScreencasts are available showing how to read in data from a file if you have\nforgotten.\n\n* * *\n\n## Tests\n\nEach `runtest` run will use `System.nanoTime()` (we have found that\n`System.currentTimeMillis()` is not precise enough for O(lg n) algorithms) to\ntime the following operations and report **minimum**, **average** and\n**maximum** times required:\n\n  * `insert(k):` min, avg and max across all keys. (You have all the keys in the array. Insert all of them into the data structure, timing the time it takes for each insertion and keeping track of min, max and sum of times as you go so you can compute average at the end.) \n  \n\n  * `retrieve(k):` Use the `Random` class to produce 10 integers in the range of the data array. Then search for the 10 keys (name strings) indexed by these random numbers, timing each search. Report min, avg and max across these 10 searches.\n  \n\n  * `successor(k):` run 10 trials on the same keys you used in the above retrieval tests. Report min, avg and max. \n  \n\n  * `predecessor(k):` run 10 trials on the same keys you used in the above retrieval tests. Report min, avg and max.\n  \n\n  * `minimum:` Report time to run one call (since there is only one minimum). \n  \n\n  * `maximum:` Report time to run one call (since there is only one maximum). \n  \n\n  * `delete(k):` Last, delete the 10 randomly chosen keys you used above, timing the deletion. Report min, avg and max.\n  \n\n(_Note: _ 10 has been specified as the number of trials because the smallest\ndata set has only 100 elements. 10 trials will give you meaningful results,\nbut in real world testing you'd run many more trials for more reliable and\nconvincing averages. Compared to the cost of programming this test, computer\ntrials are cheap! One approach is to use a percentage, e.g., 10% of the keys\nwould give you 100 trials on the data set of 1000.)\n\n#### Memory:\n\nIf you run out of memory, increase the heap size available to Java using the\n-Xmx argument (or set the corresponding preference in your IDE). Try 1024M, or\nhigher if you have the disk space to spare. (For one of my Java-based research\nprojects, I set this value to 32G, which is _twice_ the physical memory on my\n16G machine: Java will use virtual memory.)\n\nAlso try loading the data directly into one Dynamic Set implementation at a\ntime (skipping the array and the interactive interface), and then destroying\nall references to each dynamic set after you test it so it can be garbage\ncollected before loading into the next one from the file.\n\nIf you are unable to run the test file of one million, turn in up to 100,000\nalong with an explanation.\n\n\n## Output\n\nEach `runtest` run will generate an output table as follows (this layout is\ndesigned to enable you to print the table out incrementally as you go).\n\n    \n    \n    Size: N\n    --------------------------------------------------------------------------------------\n                | Insert   | Retrieve | Pred     | Succ     | Min    | Max    | Delete   |\n    --------------------------------------------------------------------------------------\n    Linked List |          |          |          |          |        |        |          |  \n    --------------------------------------------------------------------------------------\n    Skip List   |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    Binary Tree |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    RB Tree     |          |          |          |          |        |        |          |\n    --------------------------------------------------------------------------------------\n    \n\nThe entry in each cell of this table will be of form\n\n    \n    \n             -------------------\n             | min / avg / max |\n             -------------------\n    \n\n(except `minimum` and `maximum`), each being in nanoseconds. (Resize the table\nas needed to fit.)\n\nYou will need to run the program once for each data set: 100, 1000, 10,000,\n100,0000, and, if possible, 1,000,000; each sorted and unsorted. Thus you will\nhave at least 8 runs (8 tables) if you go up to 100,000, or 10 runs (10\ntables) if you are able to include the 1,000,000. Your reports will be based\non these tests, in comparison to the Θ, big-O, and Ω analyses for the\nalgorithms.\n\nThe TA may use other data sets, which may include reverse sorted items,\nduplicates, etc.\n\n* * *\n\n## Documentation and Report\n\nRequirements stated on the [ Assignments Page](/morea/010.introduction/reading-assignments.html) are included in the requirements\nfor this project. Read that first. Further comments are below.\n\n###  Readme.txt\n\nThe Readme file should be clear and enable the TA to compile and run your\nprogram.\n\n### Operation and Reference Manuals\n\nFor this assignment, the Operation manual is not required _ as long as you put\nthe instructions on how to run the program in your Readme.txt._ The Reference\nmanual can be brief, as the program as a whole is not intended to be\ndistributed for general use. Describe the organization of your program and the\nsource of the implemenatations: anything that you as a programmer maintaining\nthe code would want to know.\n\n### Testing Document (30% of grade)\n\nThe Testing Document will include a summary of your empirical test results and\na discussion of how these results compare to the asymptotic analyses of the\nunderlying data structures.\n\nPut the tables output by your program in the appendix of your testing\ndocument. Then, in the beginning of your testing document discuss whether the\ntimes you got fit the theoretical analyses for the various data structures.\nDid implementations behave as expected as _n_ gets bigger? Does the different\nbetween sorted and unsorted data make sense? How do they compare to each\nother?\n\n**This document is 30% of your grade:** be thorough (showing you have thought carefully about the results in relation to the theoretical analyses and your implementation), yet concise (don't put in a lot of blather to make it look bigger). \n\n* * *\n\n## Grading\n\n### Program: 60%\n\n  * 5% for correct handling of input as specified. This includes reading command line arguments and error handling. \n  * 40% = 10% x 4 for each correct implementation of the Dynamic Set ADT. \n  * 10% for proper organization of the tests (reading data into array; running one ADT at a time and recording values; able to run up to one million). \n  * 5% for providing output table as specified. \n\n### Analysis and Documentation: 40%\n\n  * 30% for adequate analysis of the results (in the Testing Document)\n  * 10% for Readme and Reference manuals\n\n* * *\n\nDan Suthers Last modified: Tue Mar 11 03:27:03 HST 2014\n\n",
 "path"=>"morea//040.adt/experience-project-1.md"}
</pre>

<h2>/morea/040.adt/module-adt.html</h2>

<pre>Hash
{"title"=>"Abstract data types",
 "published"=>true,
 "morea_id"=>"adt",
 "morea_outcomes"=>["outcome-adt"],
 "morea_readings"=>
  ["reading-screencast-4a",
   "reading-screencast-4b",
   "reading-cormen-10",
   "reading-notes-4"],
 "morea_experiences"=>
  ["experience-asymptotic-basic-data-structures",
   "experience-asymptotic-homework",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/040.adt/module-adt.png",
 "morea_sort_order"=>40,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/040.adt/module-adt.html",
 "content"=>
  "Stacks, queues, lists, trees, dynamic sets, pointers and objects, rooted trees, asymptotic analysis.\n",
 "path"=>"morea//040.adt/module-adt.md"}
</pre>

<h2>/modules/adt/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-adt.md",
 "title"=>"Abstract data types",
 "url"=>"/modules/adt/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/adt/index.html"}
</pre>

<h2>/morea/040.adt/outcome-adt.html</h2>

<pre>Hash
{"title"=>
  "Understand definition, implementation, and behavior of abstract data types.",
 "published"=>true,
 "morea_id"=>"outcome-adt",
 "morea_type"=>"outcome",
 "morea_sort_order"=>40,
 "referencing_modules"=>[#Jekyll:Page @name="module-adt.md"],
 "url"=>"/morea/040.adt/outcome-adt.html",
 "content"=>
  "Gain further practice in algorithm analysis through examination of stacks, queues, lists, and trees.",
 "path"=>"morea//040.adt/outcome-adt.md"}
</pre>

<h2>/morea/040.adt/reading-cormen-10.html</h2>

<pre>Hash
{"title"=>"CLRS 10 - Elementary Data Structures",
 "published"=>true,
 "morea_id"=>"reading-cormen-10",
 "morea_summary"=>
  "Stacks, queues, linked lists, pointers and objects, rooted trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "21 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-cormen-10.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-cormen-10.md"}
</pre>

<h2>/morea/040.adt/reading-notes-4.html</h2>

<pre>Hash
{"title"=>"Chapter 4 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-4",
 "morea_summary"=>"Introduction to abstract data types",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/040.adt/reading-notes-4.html",
 "url"=>"/morea/040.adt/reading-notes-4.html",
 "content"=>
  "## Outline\n\n  1. Stacks \n  2. Queues \n  3. Lists \n  4. First peek at Trees \n  5. Dynamic Set ADT\n\nHere we review some basic Abstract Data Types that organize information in\nuseful ways. This should be review, so will be covered briefly, although some\nnuances of implementation are discussed and we will also do asymptotic\nanalyses of the main operations of implementations.\n\n## Stacks\n\nStacks follow the **Last In, First Out (LIFO)** principle. They are useful\nwhen a problem has goal-subgoal structure, and we need to keep track of higher\nlevel goals or processes when we set them aside to pursue subgoals or sub-\nprocesses (e.g., the run-time stack of a computer operating system, or keeping\ntrack of neighbor vertices yet to be visited when searching a graph).\n\n### Stack ADT\n\nWe start by specifying the desired behavior of stacks before looking at\nimplementations. Here's the Stack ADT written as a simple Java interface:\n\n{% highlight java %}    \n      // ADT that stores and retrieves Objects in a LIFO manner\n      public interface Stack {\n    \n         public Stack( ); \n         // Create an instance of ADT Stack and initialize it to the empty stack.\n    \n         public void push(Object o); \n         // Insert object o at the top of the stack.\n    \n         public Object pop( ); \n         // Remove and return the top (most recently pushed) object on the stack.\n         // Error occurs if the stack is empty. \n     \n         public int size( ); \n         // Return the number of objects in the stack.\n    \n         public boolean isEmpty( ); \n         // Return a boolean indicating whether the stack is empty.\n    \n         public Object top( ); \n         // Return the top (most recently pushed) object on the stack, without \n         // removing it. Error occurs if the stack is empty.\n      }\n\n{% endhighlight %}\n    \n\n**Properties**, given `s` a stack instance:\n\n  1. { `push(_s_,_e_); _s_.top()` } returns value `_e_`\n  2. { `push(_s_,_e_); _s_.pop()` } returns value `_e_` and leaves `_s_` in the same state \n  3. { `_s_ = new(); _s_.isEmpty() } returns true `\n  4. { `push(_s_,_i_); _s_.isEmpty() } returns false `\n  5. if `s.isEmpty()` then `s.top()` is an error, and does not change `s`\n  6. if `s.isEmpty()` then `s.pop()` is an error, and does not change `s`\n  7. if `s.isEmpty()` then `s.size() == 0`\n  8. if `s.size() == _n_` then after ` s.push(o), s.size() == _n_+1`\n  9. if `¬s.isEmpty()` and `s.size() == _n_` then after `s.pop(o), s.size() == _n_-1.`\n\n_What is the relationship of stacks to method execution in the Java Virtual\nMachine?_\n\n_What is the relationship of stacks to recursion?_\n\n### Array Implementation\n\nAssume instance variables (fields) of object array `S` and `int top`. The\nthree essential operations follow. (I am modifying the book's pseudocode\nslightly.)\n\n![](fig/Fig-10-1-a-Stack.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if top == 0 \n      2       return TRUE\n      3     else\n      4       return FALSE\n    \n      void **push**(Object o)\n      1     top = top + 1\n      2     S[top] = o                // what might happen here?\n    \n      Object **pop**( )\n      1     if isEmpty()\n      2       error \"stack underflow\" // or throw new StackException (...) \n      3     else\n      4       top = top - 1\n      5       return S[top+1]         // we comment on this later \n    \n\n_What is the asymptotic complexity of these operations?_\n\nThe potential error in `push` is an implementation concern outside of the\nscope of the _logical_ definition of the stack ADT. How might it be handled?\n\n#### Example\n\nLet's start with this stack:\n\n![](fig/Fig-10-1-a-Stack.jpg)  \n  \n\nPush 17, and then 3:\n\nPop once:\n\n![](fig/Fig-10-1-b-Stack.jpg)\n\n![](fig/Fig-10-1-c-Stack.jpg)\n\n_What is the status of S[top+1] after pop returns? Why might that be a\nproblem?_\n\n####  An Improvement\n\n    \n    \n      Object **pop**( )  // version that dereferences objects for garbage collection\n      1     if isEmpty()\n      2       error \"stack underflow\" \n      3     else\n      4       o = S[top]\n      5       S[top] = null  // don't keep references to objects not really there \n      6       top = top - 1\n      7       return o \n    \n\n* * *\n\n## Queues\n\nQueues operate in a **First In, First Out (FIFO)**, like what the British call\na \"queue\" at the post office or bank. They are also very useful for managing\nprioritization of tasks in computing.\n\n### Queue ADT\n\nAgain, expressed as a simple Java interface:\n\n    \n    \n      public interface **Queue**{\n      // ADT that stores and retrieves Objects in a FIFO manner\n    \n        public **Queue**( ); \n        // Create an instance of ADT Queue and initialize it to the empty queue.\n    \n        public void **enqueue**(Object o); \n        // Insert object o at the rear of the queue.\n    \n        public Object **dequeue**( );\n        // Remove and return the frontmost (least recently queued) object from the queue. \n        // queue. Error occurs if the queue is empty.\n    \n        public int **size**( ); \n        // Return the number of objects in the queue.\n    \n        public boolean **isEmpty**( ); \n        // Return a boolean indicating whether the queue is empty.\n    \n        public Object **front**( ); \n        // Return the front (least recently queued) object in the queue, without \n        // removing it. Error occurs if the queue is empty.\n      }\n    \n\n**Properties** (given `q` a queue instance): are very similar to those for Stack, except for operations where ordering matters (FIFO rather than LIFO). Replace the first two properties for Stack with:\n\n  1. if `q.enqueue(o1) ` occurs before `q.enqueue(o2)` then successive `q.dequeue()` returns `o1` before `o2`\n  2. `q.front() ` returns the least recently enqueued element that has not been dequeued.\n\nThen rewrite the other properties with substitution `{enqueue/push,\ndequeue/pop, front/top}`.\n\n###  Array Implementation\n\nAssume three instance variables (fields): object array `Q`; `int head`\nindexing the next element to dequeue; and `int tail` indexing the next place a\nnew element may be placed.\n\n![](fig/Fig-10-2-a-Queue.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if head == tail\n      2       return TRUE\n      3     else\n      4       return FALSE\n      \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     if tail == length  \n      3       tail = 1           // wrap around\n      4     else\n      5       tail = tail + 1\n      \n      Object **dequeue**( )     \n      1     o = Q[head]\n      2     if head == length\n      3       head = 1\n      4     else \n      5       head = head + 1\n      6     return o\n    \n\nThe queue is full when `head == tail + 1`; an error results if enqueue is\ncalled (again, this is an implementation concern outside the logical\ndefinition of the ADT).\n\n#### Example\n\nBeginning with this Queue:\n\n![](fig/Fig-10-2-a-Queue.jpg)  \n  \n\nEnqueue 17, 3 and 5 (notice wrap-around):\n\nDequeue once:\n\n![](fig/Fig-10-2-b-Queue.jpg)\n\n![](fig/Fig-10-2-c-Queue.jpg)\n\nThe same issue concerning object dereferencing applies.\n\n#### Variation using modular arithmetic\n\nThis version handles dereferencing but does not check for overflow or\nunderflow. It assumes that the array index starts with 0, but can be changed\nfor 1-based indexing.\n\n    \n    \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     tail = (tail + 1) mod length // mod is % in Java \n      \n      Object **dequeue**( )\n      1     o = Q[head]\n      2     Q[head] = null               // allow garbage collection!\n      3     head = (head + 1) mod length \n      4     return o\n    \n\n_What is the asymptotic complexity of these operations?_\n\n### Deques\n\nOne can combine the stack and queue concepts into a double-ended queue (deque)\nthat allows insertion and deletion at both ends. O(1) procedures are possible\nfor all insertion and deletion algorithms.\n\n\n\n* * *\n\n## Lists\n\nLists store objects in linear order. We will assume that list elements have a\n`key` and may have other satellite data.\n\nIn an **unsorted** list, we assume no particular order to the elements (the\norder is arbitrary). In a **sorted** list or set, the elements are ordered by\nkey.\n\nA suitable ADT for lists will be given later, in the form of `DynamicSet`.\n\n### Linked Lists\n\n**Linked lists** use list element objects to hold the data (here in the form of a `key`), and record the linear order using `next` pointers. **Doubly linked lists** also have `prev` pointers.\n\n  * `L.head` points to the first element in the list.\n  * If `x.next == nil` then x is the last element of the list.\n  * If `x.prev == nil` then x is the first element of the list.\n\n_What are the advantages of adding `prev` pointers?_\n\nOur examples will assume List instance variables for `head` and `tail`, and\nListElement instance variables `key`, `next`, and `prev`. (Note: public\ninterfaces for ADTs would probably not expose listElement: see discussion\nunder Dynamic Sets later.)\n\n### Searching\n\nThe procedure for seaching is the same for singly and doubly linked lists:\n\n    \n    \n      ListElement **listSearch**(Key k)\n      1     e = head\n      2     while e ≠ null and e.key ≠ k\n      3       e = e.next \n      4     return e\n    \n\n![](fig/Fig-10-3-a-DLL.jpg)\n\n_What is returned if `k` is not in the list?_\n\n_What is the worst case complexity of this algorithm?_\n\n### Inserting and Deleting\n\nSince you are familiar with singularly linked lists from your previous\nstudies, we'll go direct to doubly linked lists, but recall that with singly\nlinked lists you had to be careful to keep track of the tail end of the list\nthat you had \"snipped off\" during an insertion or deletion. The same applies\nhere, but we also have to manage prev pointers.\n\n    \n    \n      void **listInsert**(ListElement e) // inserts at beginning of list\n      1     e.next = head\n      2     if head ≠ null\n      3       head.prev = e \n      4     head = e\n      5     e.prev = null\n    \n\nInserting 25:  \n![](fig/Fig-10-3-b-DLL.jpg)\n\n    \n    \n      void **listDelete**(ListElement e) // removes from list, wherever it is \n      1     if e.prev ≠ null\n      2       e.prev.next = e.next\n      3     else \n      4       head = e.next \n      5     if e.next ≠ null\n      6       e.next.prev = e.prev\n    \n\nDeleting the element keyed by 4:  \n![](fig/Fig-10-3-c-DLL.jpg)\n\n_What is the worst case complexity of these algorithms?_\n\n_What about garbage collection in listDelete? Same problem as for pop and\ndequeue?_\n\n### Circular DLLs with Sentinels\n\nCLRS discuss adding an extra **sentinel** element that marks the beginning of\nthe list and making the linked list circular so that we don't have to check\nfor null (falling off the end of the list). It also enables us to get to the\nend of the list quickly\n\nSentinels remove the need for a conditional test, but this only speeds up\noperations a small constant, at the cost of an extra listElement object per\nevery list. Their use is more compelling if you often need to go to the end of\nthe list.\n\nFor example, here is the above list as a circular doubly linked list. (`L.nil`\nreferences the sentinel.)\n\n![](fig/Fig-10-4-b-DLL-Sentinel.jpg)\n\n    \n    \n     \n      void **listInsert**(ListElement e) // Sentinel version \n      1     e.next = nil.next           \n      2     nil.next.prev = e \n      3     nil.next = e \n      5     e.prev = nil\n    \n\nInsert 25: ![](fig/Fig-10-4-c-DLL-Sentinel.jpg)\n\n_Let's insert something into the empty list ..._  \n![](fig/Fig-10-4-a-DLL-Sentinel.jpg)\n\n(Left for you to try.)\n\nYou might check your understanding by doing exercises 10.2-1, 10.2-2 and\n10.2-3.\n\n* * *\n\n## Array Representations of Lists\n\nWe generally do not need to be concerned with the topic of this section in\nmodern programming languages, but if you ever have to program in FORTRAN, the\nsection shows how to store objects such as listElement in arrays:\n\n![](fig/Fig-10-5-DLL-Array.jpg)\n\n... and how to manage your own **free list** of available listElements\n(languages like Java and LISP do this automatically, but (cue old fart voice)\n\"when I was your age ...\"). Here is an array with both a DLL and a free list\nembedded in it:\n\n![](fig/Fig-10-7-a-Allocate-Free.jpg)\n\nAfter allocating one free cell to add 7 to the front of the list:\n\nAfter deleting list item 2 at array position 5:\n\n![](fig/Fig-10-7-b-Allocate-Free.jpg)\n\n![](fig/Fig-10-7-c-Allocate-Free.jpg)\n\nOf course, someone has to implement the memory management, and there is a\nlarge literature on methods of **garbage collection**.\n\n* * *\n\n## Binary Trees (A First Look)\n\nTrees in general and binary trees in particular are _hugely_ important data\nstructures in computer science. There are many ways to represent them. A\nlinked represention provides great flexibility and is widely used. In a few\nweeks we'll also see how trees can be embedded in arrays.\n\nAssume that class `BinaryTree` has instance variable `root`, and it consists\nof vertices of class `TreeNode` with instance variables `parent`, `left` and\n`right`, as well as possibly other data.\n\n![](fig/Fig-10-9-Binary-Tree.jpg)\n\nIn a few weeks we will study methods for search, insertion and deletion in\nspecial types of tree, **heaps** and **binary search trees**.\n\n_Do you have any thoughts on what insertion and deletion might involve, in\ngeneral?_\n\n_Exercises:_  \n10.4-2: write an O(n) recursive procedure to visit (e.g., print out) the nodes\nof the tree.  \n10.4-3: write an O(n) non-recursive procedure to visit the nodes of the tree.\nUse a stack.\n\n* * *\n\n## N-ary Trees\n\nWe can represent n-ary trees by providing each node with a fixed number _n_\nchild fields (child1, child2, child3 ... childn). An equivalent approach is\nused for **b-trees,** which are used for efficient disk access.\n\nBut a fixed _n_ is only viable if we can bound the number of children, and can\nbe wasteful of memory if many nodes do not have _n_ children.\n\nAn alternative representation allows each TreeNode to have an arbitrary number\nof children while still using O(n) space.\n\n### Left-Child Right-Sibling Representation\n\nThis implementation has instance variable `root`, but consists of vertices\nthat are instances of a class we'll call LCRSTreeNode with instance variables\n`parent`, `left-child` and `right-sibling`, as well as possibly other data.\n(Alternatively, we can just use TreeNode, but understand `left` to refer to\nthe left-child and `right` to refer to the right sibling.)\n\n![](fig/Fig-10-10-LC-RS-Tree.jpg)\n\nA good practice problem is to write a procedure for visiting (printing out)\nall the nodes of these kinds of trees.\n\n\n\n* * *\n\n## Dynamic Set ADT\n\nAbove we have been reviewing basic data structures for keeping track of\nobjects under specific organizational schemes (e.g., FIFO, LIFO, sequential,\nand hierarchical).\n\nAnother organizational scheme is the **set** or **ordered set**. We often need\nto keep track of a set of objects, query it for membership, and possibly\nmodify the set dynamically. Other operations are also possible if the elements\nof the set are ordered.\n\nThese capabilities can be implemented in different ways. The Dynamic Set ADT\ncaptures the requirements that implementations must meet. Many of the ADTs\n(and their implementations as data structures and algorithms) we will study\ncan be seen as specializations of the Dynamic Set ADT.\n\n### Text's Dynamic Set ADT\n\nThe introduction to Part III of the textbook, page 230, gives this\nspecification:\n\nSEARCH(S; k)\n\n    A query that, given a set S and a key value k, returns a pointer x to an element in S such that x.key = k, or NIL if no such element belongs to S.\n  \nINSERT(S; x)\n\n    A modifying operation that augments the set S with the element pointed to by x. We usually assume that any attributes in element x needed by the set implementation have already been initialized.\n  \nDELETE(S; x)\n\n    A modifying operation that, given a pointer x to an element in the set S, removes x from S. (Note that this operation takes a pointer to an element x, not a key value.)\n  \nMINIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the smallest key.\n  \nMAXIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the largest key.\n  \nSUCCESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next larger element in S, or NIL if x is the maximum element.\n  \nPREDECESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next smaller element in S, or NIL if x is the minimum element.\n\nThere are some issues with this specification, particularly in the use of x.\n\n  * The specification seems to require that the client know about the the internal implementation of the set (\"We usually assume that any attributes in element x needed by the set implementation have already been initialized\").\n  * Alternatively, if the elements are client objects, the set implementation would have to know how to access these to get the key. \n\nA safer specification would give INSERT and DELETE the key k rather than the\nelement x, hiding implementation details and reducing dependencies between\nclient and ADT. This in turn leads to a performance problem, dicussed below,\nbut it can be resolved.\n\n### Encapsulated Dynamic Set ADT\n\nAn encapsulated version of the ADT is given as a Java interface below. It\ncommunicates with clients primarily through keys and associated elements that\nonly the client need understand.\n\n    \n    \n      public interface **DynamicSet** {\n      // ADT that stores and retrieves Objects according to keys of type KeyType\n     \n         public **DynamicSet**( ); \n         // Creates an instance of ADT DynamicSet and initializes it to the empty set.   \n     \n         public void **insert**(KeyType k; Object e); \n         // Inserts element e in the set under key k.\n     \n         public void **delete**(KeyType k); \n         // Given a key k, removes elements indexed by k from the set.\n     \n         public Object **search**(KeyType k); \n         // Finds an Object with key k and returns a pointer to it,\n         // or null if not found. \n     \n         // The following operations apply when there is a total ordering on KeyType   \n     \n         public Object **minimum**( ); \n         // Finds an Object that has the smallest key, and returns a pointer to it,\n         // or null if the set is empty. \n     \n         public Object **maximum**( ); \n         // Finds an Object that has the largest key, and returns a pointer to it,\n         // or null if the set is empty.\n     \n         public Object **successor**(KeyType k); \n         // Finds an Object that has the next larger key in the set above k, \n         // and returns a pointer to it, or null if k is the maximum element.\n     \n         public Object **predecessor**(KeyType k); \n         // Finds an Object that has the next smaller key in the set below k,\n         // and returns a pointer to it, or null if k is the minimum element.\n     }\n    \n\nAs hinted above, we may pay a cost for proper encapsulation. For example,\nsuppose an application must frequently pair `search` and `delete` operations\nto find elements we want to remove. If `search` cannot communicate the\nlocation found in the underlying datastructure to `delete`, then `delete` will\nhave to search again to find what to operate on.\n\nThis inefficiency could be eliminated by abstracting the concept of a\n**position** in a data structure, and passing around position objects that\nhide implementation details. This solution is not discussed here as it is more\nof a software engineering rather than algorithm design and analysis concern:\nsee Goodrich & Tamassia's Algorithms textbook for one approach.\n\n### Alternative Dynamic Set Implementations\n\nLinked lists can be used to support a viable Dynamic Set implementation for\nsmall sets, for example using `listInsert` and `listSearch` to implement\n`insert` and `search`, respectively.\n\nFuture Topics will present Hash Tables, Binary Search Trees, and Red-Black\nTrees as alternative implementations of DynamicSet. You will use some of these\nin your assignments (and often as a working professional), so need to\nunderstand them well.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:30:23 HST 2014  \nImages are from Cormen et al. Introduction to Algorithms, Third Edition.  \n\n",
 "path"=>"morea//040.adt/reading-notes-4.md"}
</pre>

<h2>/morea/040.adt/reading-screencast-4a.html</h2>

<pre>Hash
{"title"=>"Stacks, queues, and lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-4a",
 "morea_summary"=>"Basic abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=86QY8mBX7Ks",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-screencast-4a.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-screencast-4a.md"}
</pre>

<h2>/morea/040.adt/reading-screencast-4b.html</h2>

<pre>Hash
{"title"=>"Trees and dynamic sets",
 "published"=>true,
 "morea_id"=>"reading-screencast-4b",
 "morea_summary"=>"More on abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=eECZ_lKXsHs",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/040.adt/reading-screencast-4b.html",
 "content"=>"",
 "path"=>"morea//040.adt/reading-screencast-4b.md"}
</pre>

<h2>/morea/050.probabilistic/experience-indicator-random-variables.html</h2>

<pre>Hash
{"title"=>"Indicator random variables: Homework",
 "published"=>true,
 "morea_id"=>"experience-indicator-random-variables",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about indicator random variables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/050.probabilistic/experience-indicator-random-variables.html",
 "url"=>"/morea/050.probabilistic/experience-indicator-random-variables.html",
 "content"=>
  "# Indicator Random Variable Analysis\n\nUse indicator random variables to compute the expected value of the sum of _n_\nrolls of a fair dice that has _s_ sides. A fair dice can have values from 1 to\n_s_ with equal probability. Do it in these steps, and answer the numbered\nquestions:\n\nDefine the _indicator_ random variable _X__i_ = I {the event of a dice coming\nup with value _i_}, for each _i_ = {1, 2, ... _s_}.\n\n**1.** What is Pr{_X__i_ = 1} for each i? \n\n**2.** What is E[_X__i_]? In other words, the expected value of _X__i_? \n\nDefine the _regular_ random variable _X_ to be the value of a single roll of a\ndice with _s_ sides.\n\n**3.** Write an equation expressing _X_ in terms of _X__i_.   _(Keep in mind that indicator random variables take on values 0 or 1.)_\n\n**4.** Take the expectation of both sides of this equation and solve for E[_X_], the expected value of _X_.   _(Show all steps, like was done in the derivation of the expected number of inversions.)_\n\n**5.** Use the result to write an expression for the expected value of _n_ rolls of an _s_-sided fair dice. \n\n# Additional Activity\n\nIf you finish the above early, this will get you started on future work.\n\nSuppose I assigned the _n_ students in a class randomly to groups, with no\nconstraint on group size, but I decided in advance to have _n_/4 groups.\n\n**6.** Let's pick two students from our class. Call them Michael Jackson and Bruno Mars. What is the probability that Michael and Bruno end up in the same group? Express as a function of _n_. \n\n\n",
 "path"=>"morea//050.probabilistic/experience-indicator-random-variables.md"}
</pre>

<h2>/morea/050.probabilistic/module-probabilistic.html</h2>

<pre>Hash
{"title"=>"Probabilistic Analysis",
 "published"=>true,
 "morea_id"=>"probabilistic",
 "morea_outcomes"=>["outcome-probabilistic"],
 "morea_readings"=>
  ["reading-screencast-5a",
   "reading-screencast-5b",
   "reading-screencast-5c",
   "reading-screencast-5d",
   "reading-screencast-mit-skip-lists",
   "reading-cormen-5",
   "reading-goodrich",
   "reading-notes-5"],
 "morea_experiences"=>
  ["experience-indicator-random-variables", "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/050.probabilistic/module-probabilistic.png",
 "morea_sort_order"=>50,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/050.probabilistic/module-probabilistic.html",
 "content"=>
  "Indicator random variables, inversions, randomized algorithms, skip lists, the hiring problem.\n",
 "path"=>"morea//050.probabilistic/module-probabilistic.md"}
</pre>

<h2>/modules/probabilistic/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-probabilistic.md",
 "title"=>"Probabilistic Analysis",
 "url"=>"/modules/probabilistic/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/probabilistic/index.html"}
</pre>

<h2>/morea/050.probabilistic/outcome-probabilistic.html</h2>

<pre>Hash
{"title"=>"Understand probabilistic analysis.",
 "published"=>true,
 "morea_id"=>"outcome-probabilistic",
 "morea_type"=>"outcome",
 "morea_sort_order"=>50,
 "referencing_modules"=>[#Jekyll:Page @name="module-probabilistic.md"],
 "url"=>"/morea/050.probabilistic/outcome-probabilistic.html",
 "content"=>
  "Understand when and how to analyze an algorithm based on a distribution of the probability of each case.",
 "path"=>"morea//050.probabilistic/outcome-probabilistic.md"}
</pre>

<h2>/morea/050.probabilistic/reading-cormen-5.html</h2>

<pre>Hash
{"title"=>"CLRS 5 - Probabilistic Analysis and Randomized Algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-5",
 "morea_summary"=>
  "The hiring problem, indicator random variable, randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "16 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-cormen-5.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-cormen-5.md"}
</pre>

<h2>/morea/050.probabilistic/reading-goodrich.html</h2>

<pre>Hash
{"title"=>"Skip Lists in Java",
 "published"=>true,
 "morea_id"=>"reading-goodrich",
 "morea_summary"=>
  "Skip lists, from Goodrich and Tamassia's Data Structures and Algorithms in Java",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>
  "https://laulima.hawaii.edu/portal/tool/a8c355d6-b3af-4db8-a856-1713858f8720?panel=Main#",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-goodrich.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-goodrich.md"}
</pre>

<h2>/morea/050.probabilistic/reading-notes-5.html</h2>

<pre>Hash
{"title"=>"Chapter 5 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-5",
 "morea_summary"=>"Probabilistic analysis and randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/050.probabilistic/reading-notes-5.html",
 "url"=>"/morea/050.probabilistic/reading-notes-5.html",
 "content"=>
  "## Outline\n\n  1. Probabilistic Analysis\n  2. Randomized Algorithms\n  3. Skip Lists \n\n## Motivations and Preview\n\nInstead of limiting analysis to best case or worst case, analyze all cases\nbased on a distribution of the probability of each case.\n\nWe implicitly used probabilistic analysis when we said that _given random\ninput_ it takes n/2 comparisons _on average_ to find an item in a linked list\nof n items.\n\n### Hiring Problem and Cost\n\nThe book's example is a little strange but illustrates the points well.\nSuppose you are using an employment agency to hire an office assistant.\n\n  * The agency sends you one candidate per day: interview and decide.\n  * Cost to interview is _c__i_ per candidate (fee to agency). \n  * Cost to hire is _c__h_ per candidate (includes firing prior assistant and fee to agency).\n  * _ch_ > _ci_\n  * You always hire the best candidate seen so far.\n    \n    \n      Hire-Assistant(n)\n      1  best = 0                // fictional least qualified candidate\n      2  for i = 1 to n\n      3    interview candidate i // paying cost  _ci_\n      4    if candidate i is better than candidate best\n      5      best = i\n      6      hire candidate i    // paying cost _ch_\n    \n\nWhat is the cost of this strategy?\n\n  * If we interview _n_ candidates and hire _m_ of them, cost is O(_cin_ \\+ _chm_)\n  * We interview all _n_ and _ci_ is small, so we focus on _chm_.\n  * _chm_ varies with each run and depends on interview order\n  * This is a common paradigm: finding the maximum or minimum in a sequence by examining each element, and changing the winner _m_ times.\n\n#### Best Case\n\nIf each candidate is worse than all who came before, we hire one candidate:  \n    O(_cin_ \\+ _ch_) = O(_cin_)\n\n#### Worst Case\n\nIf each candidate is better than all who came before, we hire all _n_ (_m_ =\n_n_):  \n    O(_cin_ \\+ _chn_) = O(_chn_) since _ch_ > _ci_  \nBut this is pessimistic. What happens in the average case?\n\n### Probabilistic Analysis\n\n  * We must know or make assumptions about the distribution of inputs.\n  * The expected cost is over this distribution.\n  * The analysis will give us **_average case_** running time.\n\nWe don't have this information for the Hiring Problem, but suppose we could\nassume that candidates come in random order. Then the analysis can be done by\ncounting permutations:\n\n  * Each ordering of candidates (relative to some reference ordering such as a ranking of the candidates) is equally likely to be any of the n! permutations of the candidates. \n  * In how many do we hire once? twice? three times? ... _n_−1 times? _n_ times?\n  * It depends on how many permutations have zero, one two ... _n_−2 or _n_−1 candidates that come before a better candidate.\n  * This is complicated!\n  * Instead, we can do this analysis with indicator variables (next section)\n\n### Randomized Algorithms\n\nWe might not know the distribution of inputs or be able to model it.\n\nInstead we _randomize_ within the algorithm to _impose_ a distribution on the\ninputs.\n\nAn algorithm is **randomized** if its behavior is determined in parts by\nvalues provided by a random number generator.\n\nThis requires a change in the hiring problem scenario:\n\n  * The employment agency sends us a list of _n_ candidates in advance and lets us choose the interview order.\n  * We choose randomly.\n\nThus we _take control_ of the question of whether the input is randomly\nordered: we _enforce_ random order, so the average case becomes the **_\nexpected value_**.\n\n* * *\n\n## Probabilistic Analysis with Indicator Random Variables\n\nHere we introduce technique for computing the expected value of a random\nvariable, even when there is dependence between variables. Two informal\ndefinitions will get us started:\n\nA **random variable** (e.g., _X_) is a variable that takes on any of a range\nof values according to a probability distribution.\n\nThe **expected value** of a random variable (e.g., E[_X_]) is the average\nvalue we would observe if we sampled the random variable repeatedly.\n\n###  Indicator Random Variables\n\nGiven sample space _S_ and event _A_ in _S_, define the **indicator random\nvariable**\n\n![](fig/indicator-random-variable.jpg)\n\nWe will see that indicator random variables simplify analysis by letting us\nwork with the probability of the values of a random variable separately.\n\n![](fig/lemming.jpg)\n\n#### Lemma 1\n\nFor an event _A_, let _XA_ = I{_A_}. Then the expected value **E[_XA_] =\nPr{_A_}** (the probability of event _A_).\n\n_Proof:_ Let ¬_A_ be the complement of _A_. Then\n\n> E[_XA_] = E[I{_A_}]   (by definition)  \n    = 1*Pr{_A_} + 0*Pr{¬_A_}   (definition of expected value)  \n    = Pr{_A_}. \n\n### Simple Example\n\nWhat is the expected number of heads when flipping a fair coin once?\n\n  * Sample space _S_ is {H, T}\n  * Pr{H} = Pr{T} = 1/2\n  * Define indicator random variable _X_H= I{H}, which counts the number of heads in one flip.\n  * Since Pr{H} = 1/2, Lemma 1 says that E[_X_H] = 1/2. \n\n### Less Simple Example\n\nWhat is the expected number of heads when we flip a fair coin _n_ times?\n\nLet _X_ be a random variable for the number of heads in _n_ flips.\n\nWe could compute E[_X_] = ∑_i_=0,_n__i_ Pr{_X_=_i_} \\-- that is, compute and\nadd the probability of there being 0 heads total, 1 head total, 2 heads total\n... n heads total, as is done in C.37 in the appendix and in my screencast\nlecture [5A](http://youtu.be/MgnvWTZgqcA) \\-- but it's messy!\n\nInstead use indicator random variables to count something we _do_ know the\nprobability for: the probability of getting heads when flipping the coin once:\n\n  * For _i = 1, 2, ... n_ define _Xi_ = I{the _i_th flip results in event H}.\n  * Then _X_ = ∑_i_=1,_n__Xi_.   _ (That is, count the flips individually and add them up.)_\n  * Lemma 1 says that E[_Xi_] = Pr{H} = 1/2 for _i = 1, 2, ... n_.\n  * Expected number of heads is E[_X_] = E[∑_i_=1,_n__Xi_]\n  * _Problem:_ We don't have ∑_i_=1,_n__Xi_; we only have E[_X_1], E[_X_2], ... E[_Xn_].\n  * _Solution:_ **Linearity of expectation** (appendix C): _**expectation of sum equals sum of expectations.**_ Therefore:   \n![](fig/expected-value-n-flips.jpg)\n\nThe key idea: if it's hard to count one way, use indicator random variables to\ncount an easier way!\n\n### Hiring Problem Revisited\n\nAssume that the candidates arrive in random order.\n\nLet _X_ be the random variable for the number of times we hire a new office\nassistant.\n\nDefine indicator random variables _X_1, _X_2, ... _Xn_ where _Xi_ =\nI{candidate _i_ is hired}.\n\nWe will rely on these properties:\n\n  * _X_ = _X_1 \\+ _X_2 \\+ ... + _Xn_   _(The total number of hires is the sum of whether we did each individual hire (1) or not (0).)_\n  * Lemma 1 implies that E[_Xi_] = Pr{candidate _i_ is hired}.\n\nWe need to compute Pr{candidate _i_ is hired}:\n\n  * Candidate _i_ is hired iff candidate _i_ is better than candidates 1, 2, ..., _i_−1\n  * Assumption of random order of arrival means any of the first _i_ candidates are equally likely to be the best one so far. \n  * Thus, Pr{candidate _i_ is the best so far} = 1/i.   \n_(Intuitively, as you add more candidates each candidate is less and less\nlikely to be better than all the ones prior.)_\n\nBy Lemma 1, E[Xi] = _1/i_, a fact that lets us compute E[X]:  \n![](fig/expected-value-hiring-problem.jpg)\n\nThe sum is a harmonic series. From formula A7 in appendix A, the _n_th\n**harmonic number** is:  \n![](fig/A7-harmonic-number.jpg)\n\nThus, the expected hiring cost is O(_ch_ ln _n_), much better than worst case\nO(_chn_)! (ln is the natural log. Formula 3.15 of the text can be used to show\nthat ln _n_ = O(lg _n_.)\n\nWe will see this kind of analysis repeatedly. Its strengths are that it lets\nus count in ways for which we have probabilities (compare to C.37), and that\nit works even when there are dependencies between variables.\n\n### Expected Number of Inversions\n\nThis is Exercise 5.2-5 page 122, for which there is a publicly posted\nsolution. This example shows the great utility of random variables.\n\nLet A[1.. _n_] be an array of _n_ distinct numbers. If _i < j_ and A[_i_] >\nA[_j_], then the pair (_i_, _j_) is called an **inversion** of A (they are\n\"out of order\" with respect to each other). Suppose that the elements of A\nform a uniform random permutation of ⟨1, 2, ... _n_⟩.\n\nWe want to find the expected number of inversions. This has obvious\napplications to analysis of sorting algorithms, as it is a measure of how much\na sequence is \"out of order\". In fact, each iteration of the `while` loop in\ninsertion sort corresponds to the elimination of one inversion (see the posted\nsolution to problem 2-4c).\n\n_If we had to count in terms of whole permutations, figuring out how many\npermutations had 0 inversions, how many had 1, ... etc. (sound familiar? :),\nthat would be a real pain, as there are _n_! permutations of n items. Can\nindicator random variables save us this pain by letting us count something\neasier? _\n\nWe will count the number of inversions directly, without worrying about what\npermutations they occur in:\n\nLet _Xij_, _i < j_, be an indicator random variable for the event where A[_i_] > A[_j_] (they are inverted).\n\nMore precisely, define: X_ij_= I{A[_i_] > A[_j_]} for 1 ≤ _i_ < _j_ ≤ _n_.\n\nPr{X_ij_ = 1} = 1/2 because given two distinct random numbers the probability\nthat the first is bigger than the second is 1/2. _(We don't care where they\nare in a permutation; just that we can easily identify the probabililty that\nthey are out of order. Brilliant in its simplicity!)_\n\nBy Lemma 1, E[X_ij_] = 1/2, and now we are ready to count.\n\nLet X be the random variable denoting the total number of inverted pairs in\nthe array. X is the sum of all X_ij_ that meet the constraint 1 ≤ _i_ < _j_ ≤\n_n_:  \n![](fig/inversions-random-var.jpg)\n\nWe want the expected number of inverted pairs, so take the expectation of both\nsides:  \n![](fig/inversions-expected.jpg)\n\nUsing linearity of expectation, we can simplify this far:  \n![](fig/inversions-solution-a.jpg)\n\nThe fact that our nested summation is choosing 2 things out of _n_ lets us\nwrite this as:  \n![](fig/inversions-solution-b.jpg)\n\nWe can use formula C.2 from the appendix:  \n![](fig/C2-n-choose-k.jpg)\n\nIn screencast [5A](http://youtu.be/MgnvWTZgqcA) I show how to simplify this to\n(_n_(_n_−1))/2, resulting in:\n\n![](fig/inversions-solution-c.jpg)\n\nTherefore the expected number of inverted pairs is _n_(_n_ − 1)/4, or O(_n_2).\n\n* * *\n\n## Randomized Algorithms\n\n![](fig/badguy.jpg)\n\nAbove, we had to _assume_ a distribution of inputs, but we may not have\ncontrol over inputs.\n\nAn \"adversary\" can always mess up our assumptions by giving us worst case\ninputs. (This can be a fictional adversary in making analytic arguments, or it\ncan be a real one ...)\n\nRandomized algorithms foil the adversary by _imposing_ a distribution of\ninputs.\n\nThe modifiation to HIRE-ASSISTANT is trivial: add a line at the beginning that\nrandomizes the list of candidates.\n\n  * The randomization is now in the algorithm, not the input distribution. \n  * Whereas before the algorithm was deterministic, and we could predict the hiring cost for a given input, now we can no longer say what the hiring cost will be.\n  * But our payoff is that no particular input elicits worst-case behavior, even what was worst-case for the deterministic version!\n  * Bad behavior occurs only if we get \"unlucky\" numbers. \n\nHaving done so, the above analysis applies to give us _expected value_ rather\nthan average case.\n\n_Discuss:_ Summarize the difference between probabilistic analysis and\nrandomized algorithms.\n\n####  Randomization Strategies\n\nThere are different ways to randomize algorithms. One way is to randomize the\nordering of the input before we apply the original algorithm (as was suggested\nfor HIRE-ASSISTANT above). A procedure for randomizing an array:\n\n    \n    \n      Randomize-In-Place(A)\n      1  _n_ = A.length\n      2  for _i_ = 1 to _n_\n      3      swap A[_i_] with A[Random(_i_,_n_)]  \n    \n\nThe text offers a proof that this produces a uniform random permutation. It is\nobviously O(_n_).\n\nAnother approach to randomization is to randomize choices made within the\nalgorithm. This is the approach taken by Skip Lists ...\n\n\n\n* * *\n\n## Skip Lists\n\nThis is additional material, not found in your textbook. I introduce Skip\nLists here for three reasons:\n\n  1. They are a natural extension of the linked list implementation of Dynamic Sets, which we covered recently.\n  2. They are a good example of a randomized algorithm, where randomization is used to _improve_ asymptotic behavior from O(_n_) to O(lg _n_).\n  3. They are one candidate implementation to be tested in your homework, the Battle of the Dynamic Sets!\n\nMotivation: Why do we have to search the entire linked list one item at a\ntime? Can't we be more efficient by diving into the middle somewhere?\n\nSkip lists were first described by William Pugh. 1990. Skip lists: a\nprobabilistic alternative to balanced trees. Commun. ACM 33, 6 (June 1990),\n668-676. DOI=10.1145/78973.78977 <http://doi.acm.org/10.1145/78973.78977> or\n<ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf> (actually he had a\nconference paper the year before, but the CACM verion is more accessible).\n\nMy discussion below follows Goodrich & Tamassia (1998), _Data Structures and\nAlgorithms in Java_, first edition, and uses images from their slides. Some\ndetails differ from the edition 4 version of the text.\n\nAn animated applet may be found at\n<http://iamwww.unibe.ch/~wenger/DA/SkipList/>.\n\n### Definition of Skip List\n\nGiven a set _S_ of items with distinct keys, a **skip list** is a series of\nlists _S_0, _S_1, ... _Sh_ (as we shall see, _h_ is the height) such that:\n\n  * Each _S__i_ contains the special keys −∞ and +∞\n  * List _S__h_ contains only −∞ and +∞\n  * List _S_0 contains all of the keys of _S_ in nondecreasing order. \n  * Each list is a subsequence of the previous one: _S_0 ⊇ _S_1 ⊇ ... ⊇ _Sh_. \n![](fig/skip-list.jpg)\n\nWe can implement skip lists with nodes that have `above` and `below` fields as\nwell as the more familiar `prev` and `next`:\n\n![](fig/skip-list-node.jpg)\n\n### Searching a Skip List\n\nAn algorithm for searching for a key _k_ in a skip list as follows:\n\n    \n    \n     SkipSearch(k)\n       Input: search key k\n       Output: Position p in S such that the item at p has the largest key ≤ k.\n       Let p be the topmost-left position of S // _which has at least -∞ and +∞_\n       while below(p) ≠ null do\n           p = below(p)                       // _drop down_\n           while key (next(p)) ≤ k do\n               p = next(p)                    // _scan forward _\n       return p. \n    \n\nExample: Search for 78:\n\n![](fig/skip-list-search.jpg)\n\n### Insertion and Randomization\n\nConstruction of a skip list is randomized:\n\n  * Begin by inserting the new item where it belongs in S0\n  * After inserting an item at level Si, flip a coin to decide whether to also insert it at Si+1.\n  * If Si+1 does not exist, the height of the Skip lists can be increased.   \n_(Alternatively, some policy can be used to limit growth as a function of n,\nbut the probability of a run of \"heads\" diminishes greatly as the number of\nflips increases.)._\n\nThe psuedocode provided by Goodrich & Tamassia uses a helper procedure\n`InsertAfterAbove(p1, p2, k, d)` (left as exercise), which inserts key `k` and\ndata `d` after `p1` and above `p2`. (The following omits code for returning\n\"elements\" not relevant here.)\n\n    \n    \n     SkipInsert(k,d)\n       Input: search key k and data d\n       Instance Variables: s is the start node of the skip list,\n         h is the height of the skip list, and n the number of entries \n       Output: None (list is modified to store d under k)\n       p = SkipSearch(k)\n       q = InsertAfterAbove(p, null, k, d)    // _we are at the bottom level_\n       l = 0                                  // _keeps track of level we are at_ \n       while random(0,1) ≤ 1/2 do\n           l = l + 1\n           if l ≥ h then                      // need to add a level\n               h = h + 1\n               t = next(s)\n               s = insertAfterAbove(null, s, −∞, null)\n               insertAfterAbove(s, t +∞, null) \n           while above(p) == null do\n               p = prev(p)                    // _scan backwards to find tower_\n           p = above(p)                       // _jump higher_\n           q = insertAfterAbove(p, q, k, d)   // _add new item to top of tower_\n       n = n + 1.\n    \n\nFor example, inserting key 15, when the randomization gave two \"heads\",\nforcing growth of _h_ (for simplicity the figure does not include the above\nand below pointers):\n\n![](fig/skip-list-insert.jpg)\n\nDeletion requires finding and removing all occurrences, and removing all but\none empty list if needed. Example for removing key 34:\n\n![](fig/skip-list-delete.jpg)\n\n### Analysis\n\nThe **worst case** performance of skip lists is very bad, but highly unlikely.\nSuppose `random(0,1)` is always less than 1/2. If there were no bound on the\nheight of the data structure, `SkipInsert` would never exit! But this is as\nlikely as an unending sequence of \"heads\" when flipping a fair coin.\n\nIf we do impose a bound _h_ on the height of the list (_h_ can be a function\nof _n_), the worst case is that every item is inserted at every level. Then\nsearching, insertion and deletion is O(_n+h_): you not only have to search a\nlist S0 of _n_ items, as with conventional linked lists; you also have to go\ndown _h_ levels.\n\nBut the probabilistic analysis shows that the expected time is much better.\nThis requires that we find the expected value of the height _h_:\n\n  * Probability that item is stored at level _i_ is the probability of getting _i_ consecutive heads: 1/2_i_.\n  * Probability P_i_ that level _i_ has at least one item: P_i_ ≤ n/2_i_   _(We had n tries at getting i consecutive heads.)_\n  * Probablity that _h_ is larger than _i_ is no more than P_i_.\n  * G&T show that given a constant _c_ > 1, the probability that _h_ is larger than _c_ lg _n_ is at most 1/_n__c_−1 (also worked out in screencast [5A](http://youtu.be/MgnvWTZgqcA)).\n  * For example, for _c_ = 3, the probability that _h_ is larger than 3 lg _n_ is at most 1/_n_2, which gets very small as n grows (e.g., p = .000001 = 1/1000000 for a list of length 1000).\n  * They conclude that the height _h_ is O(lg _n_).\n\nThe search time is proportional to the number of drop-down steps plus the\nnumber of scan-forward steps. The number of drop-down steps is the same as _h_\nor O(lg _n_). So, we need the number of scan-forward steps.\n\nIn their textbook (1998), G&T provide this argument: Let _Xi_ be the number of\nkeys examined scanning forward at level _i_.\n\n![](fig/code-SkipSearch.jpg)\n\n  * After the starting position, each key examined at level _i_ cannot also belong to level _i+1_. _(Why?)_\n  * Thus the probability that any key is counted in _Xi_ is 1/2. _(Why??)_\n  * Therefore the expected value of _Xi_ is the expected number of times we must flip a coin before it comes up heads: 2.\n  * Hence the expected amount of time scanning forward at each level is O(1). _(Wow!)_\n  * Since there are O(lg _n_) levels, the expected search time is O(lg _n_). \n\nIn their slides (2002), they provide this alternative analysis of the number\nof scan-forwards needed. The reasoning is very similar, but based on the odds\nof the list we encounter being constructed:\n\n  * When we scan forward in a list, the destination key does not belong to a higher list.\n  * Therefore, a scan forward is associated with a former coin toss that gave tails (otherwise it would be in the higher list).\n  * The expected number of coin tosses in order to get tails is 2.\n  * Therefore the expected number of scan-forward steps at each level is 2.\n  * Thus the total number of expected scan forward steps (summing across all _h_ or O(lg _n_) levels) is O(lg _n_). \n\nA similar analysis can be applied to insertion and deletion. Thus, skip lists\nare far superior to linked lists in performance.\n\nG&T also show that the expected space requirement is O(n). They leave as an\nexercise the elimination of `above` and `prev` fields: if random(0,1) is\ncalled up to _h_ times in advance of the insertion search, then one can insert\nthe item \"on the way down\" as specified by the results.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:40:33 HST 2014  \nImages of mathematical expressions are from the instructor's material for\nCormen et al. Introduction to Algorithms, Third Edition. Images of skip lists\nare from lecture slides provided by M. Goodrich & R. Tamassia.  \n\n",
 "path"=>"morea//050.probabilistic/reading-notes-5.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-5a.html</h2>

<pre>Hash
{"title"=>"Indicator random variables",
 "published"=>true,
 "morea_id"=>"reading-screencast-5a",
 "morea_summary"=>"Indicator random variables.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=MgnvWTZgqcA",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-5a.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-5a.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-5b.html</h2>

<pre>Hash
{"title"=>"Example analysis: inversions",
 "published"=>true,
 "morea_id"=>"reading-screencast-5b",
 "morea_summary"=>"Inversions",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=k-jusEhrRik",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-5b.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-5b.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-5c.html</h2>

<pre>Hash
{"title"=>"Randomized algorithms and skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5c",
 "morea_summary"=>"Randomized algorithms and skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=iaKu6jaKPFw",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-5c.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-5c.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-5d.html</h2>

<pre>Hash
{"title"=>"Analysis of skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5d",
 "morea_summary"=>"Analysis of skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=oW2VnviRh5M",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-5d.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-5d.md"}
</pre>

<h2>/morea/050.probabilistic/reading-screencast-mit-skip-lists.html</h2>

<pre>Hash
{"title"=>"Skip Lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-skip-lists",
 "morea_summary"=>"Skip Lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec12/",
 "morea_labels"=>["Screencast", "Demaine", "85 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/050.probabilistic/reading-screencast-mit-skip-lists.html",
 "content"=>"",
 "path"=>"morea//050.probabilistic/reading-screencast-mit-skip-lists.md"}
</pre>

<h2>/morea/060.hash-tables/experience-data-structures-homework.html</h2>

<pre>Hash
{"title"=>"Analysis of data structures",
 "published"=>true,
 "morea_id"=>"experience-data-structures-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Consolidate your understanding of data structures",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/060.hash-tables/experience-data-structures-homework.html",
 "url"=>"/morea/060.hash-tables/experience-data-structures-homework.html",
 "content"=>
  "# Analysis of data structures\n\nThis week's problems focus on ensuring you understand the operations of the\nmain data structures. They are not conceptually difficult but require\ndilligence in execution. Don't be careless or just go on intuition: you should\nactually follow the algorithms and hash functions precisely, or you will go\nwrong.\n\n## Peer Credit Assignment\n\n**(1)** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate across all members (_NOT_ 3 points per member!).\n  * If two members besides yourself were present, you have a total of 4 points to allocate across all members.\n  * If only one other member was present, you have a total of 6 points to allocate across all members.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## Skip Lists\n\n#### 11 points\n\nPlease read carefully; this has multiple parts. Answer the lettered parts in\nboldface.\n\nHere is a skip list, including instance variables **s** (the starting\nposition), **h** (the height: we assume that `h` starts counting from 0), and\n**n** (the number of keys currently stored in the skip list). We won't bother\nto show the data associated with the keys. The double lines in the graphic are\nmeant to remind you that these are doubly linked lists in both the horizontal\nand vertical directions, but you need not draw double lines in your responses.\n\n![](fig/starting-skip-list.jpg)\n\n**(2)** _Trace the path that `SkipSearch(36)` takes, by circling every node that p is assigned to as the `SkipSearch` algorithm executes, starting with s. _\n\nIn the remaining questions, you will how what the skip list shown above looks\nlike after the cumulative operations indicated below, using the pseudocode for\n`SkipInsert` and `SkipSearch` in the lecture notes, and your understanding of\nhow `SkipDelete` works from the class activity.\n\nSince this is a random algorithm and we want everyone to have the same answer\nto facilitate grading, I also give you sequences of random numbers (not all of\nwhich will be used, as I am testing your understanding of when and how the\nrandom numbers are used). Note that the insertion code says ` while\nrandom(0,1) ≤ 1/2 do`...\n\nThe operations are **cumulative:** each step builds on the result of the\nprevious one. Redraw the entire data structure after each operation, and also\nupdate instance variables **s**, **h** and **n** as needed.\n\n**(3)** _Redraw after `SkipInsert(19,data)`_ where `random(0,1)` returns .70, .94, .14, .11, .89, ... \n\n**(4)** _Redraw after `SkipInsert(53,data)`_ where `random(0,1)` returns .14, .51, .22, .68, .45, ... \n\n**(5)** _Redraw after `SkipInsert(32,data)`_ where `random(0,1)` returns .25, .39, .18, .97, .02, ... \n\n**(6)** _ Redraw after `SkipDelete(SkipSearch(15))`. _\n\n_Something to think about (but not graded): What should the list look like if\nwe now deleted 32? There is a choice to be made here that we have not\ndiscussed! _\n\n**(7)** _Now draw what an _empty_ skip list would look like, including s, h and n._\n\n## Hashing\n\n#### 9 points\n\n###  Hashing with Chaining\n\n**(8)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **chaining** and **_h_(_k_) = _k_ mod 11.**_ _Draw this one with a vertical table indexed from 0 to 10, and linked lists going off to the right, as shown._\n\n![](fig/hash-chaining-template.jpg)\n\n###  Open Addressing with Linear Probing\n\n**(9)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **linear probing**_ and\n\n> h'(k) = k mod 11  \nh(k,i) = (h'(k) + i) mod 11\n\n_Draw this and the next result as horizontal arrays indexed from 0 to 10 as\nshown below. Show your work to justify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(10)** _How many re-hashes after collision are required for this set of keys?_ _ Show your work here so we can give partial credit or feedback if warranted._\n\n### Open Addressing with Double Hashing\n\n**(11)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size _m_ = 11 with **double hashing**_ and\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _i__h_2(_k_)) mod 11  \n_h_1(_k_) = _k_ mod 11  \n_h_2(_k_) = 1 + (_k_ mod 7)\n\n_Refer to the code in the book for how i is incremented. Show your work to\njustify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(12)** _How many re-hashes after collision are required for this set of keys?_ _Show your work here so we can give partial credit or feedback if warranted._\n\n**(13)** Open addressing insertion is like an unsuccessful search, as you need to find an empty cell, i.e., to _not_ find the key you are looking for! If the open addressing hash functions above were uniform hashing, _ what is the expected number of probes at the time that the last key (40) was inserted?_ _Hints: At that point, 7 keys are in the table. Use the theorem for unsuccessful search in open addressing._ _Answer with a specific number, not O or Theta._\n\n    \n",
 "path"=>"morea//060.hash-tables/experience-data-structures-homework.md"}
</pre>

<h2>/morea/060.hash-tables/experience-deletion.html</h2>

<pre>Hash
{"title"=>"Hash tables: understanding deletion",
 "published"=>true,
 "morea_id"=>"experience-deletion",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about deletion in hash tables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/060.hash-tables/experience-deletion.html",
 "url"=>"/morea/060.hash-tables/experience-deletion.html",
 "content"=>
  "# Deletion under Open Addressing\n\nFollowing the directions below, write pseudocode for `HASH-DELETE` to delete\nby writing a special `DELETED` value, and modify `HASH-INSERT` to handle the\n`DELETED` value. You will write your response in the exact same form as the\nbook's pseudocode (shown).\n\n**1.** Write `HASH-DELETE` by renaming `HASH-SEARCH` and adding or changing ONE line in the body.\n    \n    \n    Hash-Search (T,k)  // rename to Hash-Delete and remove this line \n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == k\n               return j\n           i = i + 1\n       until T[j] == NIL or i == m\n      return NIL\n    \n\n**2.** Write the new `HASH-INSERT` by changing only ONE line in the following. \n    \n    \n    Hash-Insert (T,k)\n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == NIL\n               T[j] = k\n               return j\n           else i = i + 1\n       until i == m\n       error \"hash table overflow\" \n    \n\n**3.** What is the Θ runtime complexity of the worst case for the modified `HASH-INSERT` and `HASH-DELETE` in terms of _n_ (number of elements stored) and _m_ (table size)? (_ Describe the worst possible situation. Express its runtime with Θ. _) \n\n### Deletion from Skip Lists\n\n![](fig/SgkipList-Small.jpg)\n\n`SkipSearch(k)` returns a pointer `p` to the bottom most element of the tower\nyou want to delete. Suppose this were passed to a method `SkipDelete(p)` −\nnotice it takes `p` as argument, not `k`. In this problem you analyze the\ncomplexity of `SkipDelete`. Before you start the analysis, you should discuss\nhow it works! Then, if you have time, you can write pseudocode for it for\nextra credit.\n\n  * Assume that a skip list node has fields `p.next`, `p.prev`, `p.above` and `p.below`.\n  * Assume that `SkipInsert(k,d)` has built the skip list using `random(0,1)` with cutoff of 0.5. \n  * The delete procedure climbs the tower of linked lists above `p`, doing repeated deletion from each doubly linked list that the element occurs in. \n\n**4.** Assuming a uniform distribution of keys stored in random order, what is the Θ expected case performance of `SkipDelete` in terms of _n_, the number of keys stored in the skiplist?\n\n**5.** What is the probability that a given call to `SkipDelete` would have to _delete at least _k_ nodes_? (_Hint: Think of the probability that SkipInsert builds a \"tower\" of _k_ nodes for a given key._) \n\n#### Extra Credit\n\n**6.** If you finish early, write the recursive pseudocode for `SkipDelete(p)`. \n\nHint:\n    \n    SkipDelete(p)\n    if p ≠ null {\n    // splice out of this doubly linked list\n    // recurse to splice out of list above\n    }\n    \n",
 "path"=>"morea//060.hash-tables/experience-deletion.md"}
</pre>

<h2>/morea/060.hash-tables/module-hash-tables.html</h2>

<pre>Hash
{"title"=>"Hash Tables",
 "published"=>true,
 "morea_id"=>"hash-tables",
 "morea_outcomes"=>["outcome-hash-tables"],
 "morea_readings"=>
  ["reading-screencast-6a",
   "reading-screencast-6b",
   "reading-screencast-6c",
   "reading-screencast-6d",
   "reading-cormen-11",
   "reading-notes-6",
   "reading-screencast-mit-hash-tables-1",
   "reading-screencast-mit-hash-tables-2"],
 "morea_experiences"=>
  ["experience-deletion", "experience-data-structures-homework"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/060.hash-tables/module-hash-tables.png",
 "morea_sort_order"=>60,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/060.hash-tables/module-hash-tables.html",
 "content"=>
  "Analysis of chaining, universal chaining, open addressing, direct address tables, hash functions.\n",
 "path"=>"morea//060.hash-tables/module-hash-tables.md"}
</pre>

<h2>/modules/hash-tables/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-hash-tables.md",
 "title"=>"Hash Tables",
 "url"=>"/modules/hash-tables/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/hash-tables/index.html"}
</pre>

<h2>/morea/060.hash-tables/outcome-hash-tables.html</h2>

<pre>Hash
{"title"=>"Understand hash tables.",
 "published"=>true,
 "morea_id"=>"outcome-hash-tables",
 "morea_type"=>"outcome",
 "morea_sort_order"=>60,
 "referencing_modules"=>[#Jekyll:Page @name="module-hash-tables.md"],
 "url"=>"/morea/060.hash-tables/outcome-hash-tables.html",
 "content"=>
  "Understand the design and run-time characteristics of hash tables and how they compare to related data structures. ",
 "path"=>"morea//060.hash-tables/outcome-hash-tables.md"}
</pre>

<h2>/morea/060.hash-tables/reading-cormen-11.html</h2>

<pre>Hash
{"title"=>"CLRS 11 - Hash tables",
 "published"=>true,
 "morea_id"=>"reading-cormen-11",
 "morea_summary"=>
  "Direct address tables, hash tables, hash functions, and open addressing",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-cormen-11.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-cormen-11.md"}
</pre>

<h2>/morea/060.hash-tables/reading-notes-6.html</h2>

<pre>Hash
{"title"=>"Chapter 6 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-6",
 "morea_summary"=>"Notes on hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/060.hash-tables/reading-notes-6.html",
 "url"=>"/morea/060.hash-tables/reading-notes-6.html",
 "content"=>
  "## Outline\n\n  1. Motivations and Introduction\n  2. Hash Tables with Chaining \n  3. Hash Functions and Universal Hashing\n  4. Open Addressing Strategies\n\n## Motivations and Introduction\n\nMany applications only need the insert, search and delete operations of a\n[dynamic set](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-04\n.html#dynamicsetadt). Example: symbol table in a compiler.\n\nHash tables are an effective approach. Under reasonable assumptions, they have\nO(1) operations, but they can be Θ(n) worst case\n\n### Direct Addressing\n\nHash tables generalize arrays. Let's look at the idea with arrays first. Given\na key _k_ from a universe _U_ of possible keys, a **direct address table**\nstores and retrieves the element in position _k_ of the array.\n\n![](fig/Fig-11-1-direct-address.jpg)\n\nDirect addressing is applicable when we can allocate an array with one element\nfor every key (i.e., of size |_U_|). It is trivial to implement:\n\n![](fig/pseudocode-direct-address.jpg)\n\nHowever, often the space of possible keys is much larger than the number of\nactual keys we expect, so it would be wasteful of space (and sometimes not\npossible) to allocate an array of size |_U_|.\n\n### Hash Tables and Functions\n\n**Hash tables** are also arrays, but typically of size proportional to the number of keys expected to be stored (rather than to the number of keys). \n\nIf the expected keys K ⊂ U, the Universe of keys, and |K| is substantially\nsmaller than |U|, then hash tables can reduce storage requirements to Θ(|K|).\n\nA **hash function** _h(k)_ maps the larger universe U of external keys to\nindices into the array. Given a table of size _m_ with zero-based indexing (we\nshall see why this is useful):\n\n  * _h_ : U -> {0, 1, ..., _m_-1}.\n  * We say that _k_ **hashes** to slot _h(k)_. \n\n###  Collisions\n\nThe major issue to deal with in designing and implementing hash tables is what\nto do when the hash function maps multiple keys to the same table entry.\n\n![](fig/Fig-11-2-collisions.jpg)\n\nCollisions may or may not happen when |K| ≤ _m_, but definitely happens when\n|K| > _m_. _(Is there any way to avoid this?)_\n\nThere are two major approaches: Chaining (the preferred method) and Open\nAddressing. We'll look at these and also hash function design.\n\n* * *\n\n## Hash Tables with Chaining\n\nA simple resolution: Put elements that hash to the same slot into a linked\nlist. This is called _chaining_ because we chain elements off the slot of the\nhash table.\n\n  * Slot _j_ points to the head of a list of all stored elements that hash to _j_, or to NIL if there are no such elements.\n  * Doubly linked lists may be used when deletions are expected to be frequent.\n  * Sentinels can also be used to simplify the code.\n\n![](fig/Fig-11-3-chaining.jpg)\n\n### Pseudocode for Chaining\n\nImplementation is simple if you already have implemented linked lists:\n\n![](fig/pseudocode-chained-hashing.jpg)\n\n_What are the running times for these algorithms? Which can we state directly,\nand what do we need to know to determine the others?_\n\n### Analysis of Hashing with Chaining\n\nHow long does it take to find an element with a given key, or to determine\nthat there is no such element?\n\n  * Analysis is in terms of the **load factor _α = n/m_**, where \n    * _n_ = number of elements in the table \n    * _m_ = number of slots in the table = number of (possibly empty) linked lists\n  * The load factor α is the average number of elements per linked list. \n  * Can have α < 1; α = 1; or α > 1\\. \n  * Worst case is when all _n_ keys hash to the same slot.   \n_Why? What happens? Θ(_____?)_\n\n  * Average case depends on how well the hash function distributes the keys among the slots. \n\nLet's analyze averge-case performance under the assumption of **simple uniform\nhashing:** any given element is equally likely to hash into any of the _m_\nslots:\n\n  * For _j_ = 0, 1, ..., _m_-1, denote the length of list T[_j_] by _nj_.\n  * Then _n_ = _n0_ \\+ _n1_ \\+ ... + _nm-1_. \n  * Average value of _nj_ is E[_nj_] = α = _n/m_. \n  * Assuming _h(k)_ computed in O(1), so time to search for _k_ depends on length _nh(k)_ of the list T[_h(k)_]. \n\nConsider two cases: Unsuccessful and Successful search. The former analysis is\nsimpler because you always search to the end, but for successful search it\ndepends on where in T[_h(k)_] the element with key _k_ will be found.\n\n#### Unsuccessful Search\n\nSimple uniform hashing means that any key not in the table is equally likely\nto hash to any of the _m_ slots.\n\nWe need to search to end of the list T[_h(k)_]. It has expected length\nE[_nh(k)_] = α = _n/m_.\n\nAdding the time to compute the hash function gives **Θ(1 + α)**. (We leave in\nthe \"1\" term for the initial computation of _h_ since α can be 0, and we don't\nwant to say that the computation takes Θ(0) time).\n\n#### Successful Search\n\nWe assume that the element _x_ being searched for is equally likely to be any\nof the _n_ elements stored in the table.\n\nThe number of elements examined during a successful search for _x_ is 1 more\nthan the number of elements that appear before _x_ in _x_'s list (because we\nhave to search them, and then examine _x_).\n\nThese are the elements inserted _after x_ was inserted (because we insert at\nthe head of the list).\n\nNeed to find on average, over the _n_ elements _x_ in the table, how many\nelements were inserted into _x_'s list after _x_ was inserted. _Lucky we just\nstudied indicator random variables!_\n\nFor _i_ = 1, 2, ..., _n_, let _xi_ be the _i_th element inserted into the\ntable, and let _ki_ = _key_[_xi_].\n\nFor all _i_ and _j_, define the indicator random variable:\n\n> _Xij_ = I{_h(ki)_ = _h(kj)_}.     _(The event that keys _ki_ and _kj_ hash\nto the same slot.)_\n\n![](fig/lemming.jpg)\n\nSimple uniform hashing implies that Pr{_h(ki)_ = _h(kj)_} = 1/_m_ _(Why?)_\n\nTherefore, E[_Xij_] = 1/_m_ by Lemma 1 ([Topic #5](http://www2.hawaii.edu/~sut\nhers/courses/ics311s14/Notes/Topic-05.html#lemma1)).\n\nThe expected number of elements examined in a successful search is those\nelements _j_ that are inserted after the element _i_ of interest _and_ that\nend up in the same linked list (_Xij_):\n\n![](fig/analysis-chaining-1.jpg)\n\n  * The innermost summation is adding up, for all _j_ inserted after _i_ (_j_=_i_+1), those that are in the same hash table (when _Xij_ = 1).\n  * The outermost summation runs this over all _n_ of the keys inserted (indexed by _i_), and finds the average by dividing by _n_.\n\nI fill in some of the implicit steps in the rest of the text's analysis.\nFirst, by linearity of expectation we can move the E in:\n\n![](fig/analysis-chaining-2.jpg)\n\nThat is the crucial move: instead of analyzing the probability of complex\nevents, use indicator random variables to break them down into simple events\nthat we know the probabilities for. In this case we know E[_Xi,j_] (if _you_\ndon't know, ask the lemming above):\n\n![](fig/analysis-chaining-3.jpg)\n\nMultiplying 1/_n_ by the terms inside the summation,\n\n  * For the first term, we get Σ_i_=1,_n_1/_n_, which is just _n_/_n_ or 1\n  * Move 1/_m_ outside the summation of the second term to get 1/_nm_. This leaves Σ_i_=1,_n_(Σ_j_=_i_+1,_n_1), which simplifies as shown below (if you added 1 _n_ times, you would overshoot by _i_).\n![](fig/analysis-chaining-4.jpg)\n\nSplitting the two terms being summed, the first is clearly _n_2, and the\nsecond is the familiar sum of the first _n_ numbers:\n\n![](fig/analysis-chaining-5.jpg)  \n\n![](fig/analysis-chaining-6.jpg)\n\nDistributing the 1/_nm_, we get 1 + (_n_2/_nm_ \\- _n_(_n_+1)/2_nm_   =   1 +\n_n_/_m_ \\- (_n_+1)/2_m_   =   1 + 2_n_/2_m_ \\- (_n_+1)/2_m_, and now we can\ncombine the two fractions:\n\n![](fig/analysis-chaining-7.jpg)\n\nNow we can turn two instances of _n_/_m_ into α with this preparation: 1 +\n(_n_ \\- 1)/2_m_   =   1 + _n_/2_m_ \\- 1/2_m_   =   1 + α/2 - n/2_mn_   =  \n\n![](fig/analysis-chaining-8.jpg)\n\nAdding the time (1) for computing the hash function, the expected total time\nfor a successful search is:\n\n> Θ(2 + α/2 - α/2_n_) = **Θ(1 + α).**\n\nsince the third term vanishes in significance as _n_ grows, and the constants\n2 and 1/2 have Θ(1) growth rate.\n\nThus, **search is an average of Θ(1 + α) in either case.**\n\nIf the number of elements stored _n_ is bounded within a constant factor of\nthe number of slots _m_, i.e., _n_ = O(_m_), then α is a constant, and search\nis O(1) on average.\n\nSince insertion takes O(1) worst case and deletion takes O(1) worst case when\ndoubly linked lists are used, all three operations for hash tables are O(1) on\naverage.\n\n_(I went through that analysis in detail to show again the utility of\nindicator random variables and to demonstrate what is possibly the most\ncrucial fact of this chapter, but we won't do the other analyses in detail.\nWith perserverence you can similarly unpack the other analyses.)_\n\n* * *\n\n## Hash Functions and Universal Hashing\n\nIdeally a hash function satisfies the assumptions of simple uniform hashing.\n\nThis is not possible in practice, since we don't know in advance the\nprobability distribution of the keys, and they may not be drawn independently.\n\nInstead, we use heuristics based on what we know about the domain of the keys\nto create a hash function that performs well.\n\n### Keys as natural numbers\n\nHash functions assume that the keys are natural numbers. When they are not, a\nconversion is needed. Some options:\n\n  * Floating point numbers: If an integer is required, sum the mantissa and exponent, treating them as integers.\n  * Character string: Sum the ASCII or Unicode values of the characters of the string. \n  * Character string: Interpret the string as an integer expressed in some radix notation. (This gives very large integers.) \n\n### Division method\n\nA common hash function: **_h(k)_ = _k_ mod _m_**.  \n_(Why does this potentially produce all legal values, and only legal values?)_\n\n_Advantage:_ Fast, since just one division operation required.\n\n_Disadvantage:_ Need to avoid certain values of _m_, for example:\n\n  * Powers of 2. If _m_ = 2_p_ for integer _p_ then _h(k)_ is the least significant _p_ bits of _k_.   \n(There may be a domain pattern that makes the keys clump together).\n\n  * If character strings are interpreted in radix 2_p_ then _m_ = 2_p_ \\- 1 is a bad choice: permutations of characters hash the same. \n\nA prime number not too close to an exact power of 2 is a good choice for _m_.\n\n### Multiplication method\n\n**_h(k)_ = Floor(_m_(_k_ A mod 1))**, where _k_ A mod 1 = fractional part of _k_A. \n\n  1. Choose a constant A in range 0 < A < 1\\. \n  2. Multiply _k_ by A\n  3. Extract the fractional part of _k_A\n  4. Multiply the fractional part by _m_\n  5. Take the floor of the result. \n\n_Disadvantage:_ Slower than division.\n\n_Advantage:_ The value of _m_ is not critical.\n\nThe book discusses an implementation that we won't get into ...\n\n![](fig/Fig-11-4-multiplication-hashing.jpg)\n\n### Universal Hashing \n\n![](fig/badguy.jpg)\n\nOur malicious adversary is back! He's choosing keys that all hash to the same\nslot, giving worst case behavior and gumming up our servers! What to do?\n\nRandom algorithms to the rescue: randomly choose a different hash function\neach time you construct and use a new hash table.\n\nBut it has to be a good one. Can we define a family of good candidates?\n\nConsider a finite collection _Η_ of hash functions that map universe U of keys\ninto {0, 1, ..., _m_-1}.\n\n_Η_ is **universal** if for each pair of keys _k, l_ ∈ U, where _k ≠ l_, the\nnumber of hash functions _h ∈ Η_ for which _h(k) = h(l)_ is less than or equal\nto _|Η|/m_ (that's the size of _Η_ divided by _m_).\n\nIn other words, with a hash function _h_ chosen randomly from _Η_, the\nprobability of collision between two different keys is no more than _1/m_, the\nchance of a collision when choosing two slots randomly and independently.\n\nUniversal hash functions are good because (proven as Theorem 11.3 in text):\n\n  * If _k_ is not in the table, the expected length E[_nh(k)_] of the list that _k_ hashes to is less than or equal to α. \n  * If _k_ is in the table, the expected length E[_nh(k)_] of the list that holds _k_ is less than or equal to 1 + α. \n\nTherefore, the expected time for search is O(1).\n\nOne candidate for a collection _Η_ of hash functions is:\n\n> _Η_ = {_hab_(_k_) : **_hab_(_k_) = ((_ak + b_) mod _p_) mod _m_)},** where\n_a_ ∈ {1, 2, ..., _p_-1} and _b_ ∈ {0, 1, ..., _p_-1}, where _p_ is prime and\nlarger than the largest key.\n\nDetails in text, including proof that this provides a universal set of hash\nfunctions. Java built in hash functions take care of much of this for you:\nread the Java documentation for details.\n\n* * *\n\n## Open Addressing Strategies\n\nOpen Addressing seeks to avoid the extra storage of linked lists by putting\nall the keys in the hash table itself.\n\nOf course, we need a way to deal with collisions. If a slot is already\noccupied we will apply a systematic strategy for searching for alternative\nslots. This same strategy is used in both insertion and search.\n\n###  Probes and _h_(_k_,_i_)\n\nExamining a slot is called a **probe**. We need to extend the hash function\n_h_ to take the probe number as a second argument, so that _h_ can try\nsomething different on subsequent probes. We count probes from 0 to _m_-1\n(you'll see why later), so the second argument takes on the same values as the\nresult of the function:\n\n> **_h_ : _U_ x {0, 1, ... _m_-1} -> {0, 1, ... _m_-1}**  \n\nWe require that the **probe sequence**\n\n> ⟨ _h_(_k_,0),   _h_(_k_,1)   ...   _h_(_k_,_m_-1) ⟩\n\nbe a permutation of ⟨ 0, 1, ... _m_-1 ⟩. Another way to state this requirement\nis that all the positions are visited.\n\nThere are three possible outcomes to a probe: _k_ is in the slot probed\n(successful search); the slot contains NIL (unsuccessful search); or some\nother key is in the slot (need to continue search).\n\nThe strategy for this continuation is the crux of the problem, but first let's\nlook at the general pseudocode.\n\n### Pseudocode\n\n**Insertion** returns the index of the slot it put the element in _k_, or throws an error if the table is full:\n\n![](fig/pseudocode-open-hash-insert.jpg)\n\n**Search** returns either the index of the slot containing element of key _k_, or NIL if the search is unsuccessful:\n\n![](fig/pseudocode-open-hash-search.jpg)\n\n**Deletion** is a bit complicated. We can't just write NIL into the slot we want to delete. _(Why?)_\n\nInstead, we write a special value DELETED. During search, we treat it as if it\nwere a non-matching key, but insertion treats it as empty and reuses the slot.\n\n_Problem:_ the search time is no longer dependent on α. _(Why?)_\n\nThe ideal is to have **uniform hashing**, where each key is equally likely to\nhave any of the _m_! permutations of ⟨0, 1, ... _m_-1⟩ as its probe sequence.\nBut this is hard to implement: we try to guarantee that the probe sequence is\n_some_ permutation of ⟨0, 1, ... _m_-1⟩.\n\nWe will define the hash functions in terms of ** auxiliary hash functions**\nthat do the initial mapping, and define the primary function in terms of its\n_i_th iterations, where 0 ≤ _i_ < _m_.\n\n### Linear Probing\n\nGiven an **auxiliary hash function _h'_**, the probe sequence starts at\n_h'_(_k_), and continues sequentially through the table:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _i_) mod _m_\n\n_Problem:_ **primary clustering**: sequences of keys with the same _h'_ value\nbuild up long runs of occupied sequences.\n\n### Quadratic Probing\n\nQuadratic probing is attempt to fix this ... instead of reprobing linearly, QP\n\"jumps\" around the table according to a quadratic function of the probe, for\nexample:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _c_1_i_ \\+ _c_2_i_2) mod _m_,  \nwhere _c_1 and _c_2 are constants.\n\n_Problem:_ **secondary clustering**: although primary clusters across\nsequential runs of table positions don't occur, two keys with the same _h'_\nmay still have the same probe sequence, creating clusters that are broken\nacross the same sequence of \"jumps\".\n\n### Double Hashing\n\nA better approach: use two auxiliary hash functions _h1_ and _h_2, where _h_1\ngives the initial probe and _h_2 gives the remaining probes (here you can see\nthat having _i_=0 initially drops out the second hash until it is needed):\n![](fig/Fig-11-5-double-hashing.jpg)\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _ih_2(_k_)) mod _m_.\n\n_h_2(_k_) must be relatively prime to _m_ (relatively prime means they have no\nfactors in common other than 1) to guarantee that the probe sequence is a full\npermutation of ⟨0, 1, ... _m_-1⟩. Two approaches:\n\n  * Choose _m_ to be a power of 2 and _h_2 to always produce an odd number > 1.\n  * Let _m_ be prime and have 1 < _h_2(_k_) < _m_.   \n(The example figure is _h_1(_k_) = _k_ mod 13, and _h_2(_k_) = 1 + (_k_ mod\n11).)\n\nThere are Θ(_m_2) different probe sequences, since each possible combination\nof _h_1(_k_) and _h_2(_k_) gives a different probe sequence. This is an\nimprovement over linear or quadratic hashing.\n\n### Analysis of Open Addressing\n\nThe textbook develops two theorems you will use to compute the expected number\nof probes for unsuccessful and successful search. (These theorems require α <\n1 because an expression 1/1−α is derived and we don't want to divide by 0.)\n\n> **Theorem 11.6:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in an _**unsuccessful**_ search is\nat most **1/(1 − α)**, assuming uniform hashing.\n\n> **Theorem 11.8:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in a _**successful**_ search is at\nmost **(1/α) ln (1/(1 − α))**, assuming uniform hashing and assuming that each\nkey in the table is equally likely to be searched for.\n\nWe leave the proofs for the textbook, but note particularly the \"intuitive\ninterpretation\" in the proof of 11.6 of the **_expected number of probes_** on\npage 275:\n\n> E[_X_]   =   1/(1-α)   =   1   \\+   α   \\+   α2   \\+   α3   \\+   ...\n\nWe always make the first probe (1). With probability α < 1, the first probe\nfinds an occupied slot, so we need to probe a second time (α). With\nprobability α2, the first two slots are occupied, so we need to make a third\nprobe ...\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:14:59 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//060.hash-tables/reading-notes-6.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-6a.html</h2>

<pre>Hash
{"title"=>"Hash tables: introduction and chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6a",
 "morea_summary"=>"Introduction to hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=NMm1BKomO_Y",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-6a.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-6a.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-6b.html</h2>

<pre>Hash
{"title"=>"Hash tables: analysis of chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6b",
 "morea_summary"=>"Analysis of chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=ei7T9Y97u0M",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-6b.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-6b.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-6c.html</h2>

<pre>Hash
{"title"=>"Hash tables: Hash functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-6c",
 "morea_summary"=>"Examples of hash functions and universal chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=jW4wCfz3DwE",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-6c.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-6c.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-6d.html</h2>

<pre>Hash
{"title"=>"Hash table: open addressing",
 "published"=>true,
 "morea_id"=>"reading-screencast-6d",
 "morea_summary"=>
  "Using open addressing to avoid the overhead of linked lists.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=SGGP_HJNUts",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-6d.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-6d.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-mit-hash-tables-1.html</h2>

<pre>Hash
{"title"=>"Hash tables I",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-1",
 "morea_summary"=>"Hash tables and the symbol table problem",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec07/",
 "morea_labels"=>["Screencast", "Leiserson", "77 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-mit-hash-tables-1.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-mit-hash-tables-1.md"}
</pre>

<h2>/morea/060.hash-tables/reading-screencast-mit-hash-tables-2.html</h2>

<pre>Hash
{"title"=>"Hash tables II",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-2",
 "morea_summary"=>"Universal hashing",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec08/",
 "morea_labels"=>["Screencast", "Leiserson", "79 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/060.hash-tables/reading-screencast-mit-hash-tables-2.html",
 "content"=>"",
 "path"=>"morea//060.hash-tables/reading-screencast-mit-hash-tables-2.md"}
</pre>

<h2>/morea/070.divide-conquer/experience-master-method.html</h2>

<pre>Hash
{"title"=>"Understanding master method and substitution",
 "published"=>true,
 "morea_id"=>"experience-master-method",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Apply your knowledge of the master method and substitution.",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/070.divide-conquer/experience-master-method.html",
 "url"=>"/morea/070.divide-conquer/experience-master-method.html",
 "content"=>
  "### Peer Credit Assignment\n\n**1.** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### Master Method Practice\n\n**2\\. ** (6 pts) Use the Master Method to give tight Θ bounds for the following recurrence relations. Show _a_, _b_, and _f_(_n_). Then explain why it fits one of the cases, if any. If it fits a case, write and _ simplify _ the final Θ result \n\n**a.**   _T_(_n_) = 2_T_(_n_/4) + √_n_  \n\n**b.**   _T_(_n_) = 2_T_(_n_/4) + _n_  \n\n**c.**   _T_(_n_) = 4_T_(_n_/3) + _n_\n\n\n\n### Substitution Method\n\n**3.** (7 pts) Use substitution _as directed below_ to solve \n\n> _T_(_n_) = 4_T_(_n_/3) + _n_\n\nIt is strongly recommended that you read page 85-86 \"Subtleties\" before trying\nthis!\n\n**a.**   First, use the result from the Master Method in 2c as your \"guess\" and inductive assumption. We will do this without Θ and _c_: just use the algebraic portion. Take the proof up to where it fails and say where and why it fails. (See steps below.) \n\n**b.**   Redo the proof, but subtracting _d__n_ from the guess to construct a new guess. This time it should succeed. \n\nAs a reminder, to do a proof by substitution you:\n\n  1. Write the definition _T_(_n_) = 4_T_(_n_/3) + _n_\n  2. Replace the _T_(_n_/3) with your \"guess\" instantiated for _n_/3 (you can do that by the inductive hypothesis because it's smaller than _n_). \n  3. Operating _only_ on the right hand side of the equation, transform that side into the _exact_ form of your \"guess\".\n  4. Determine any constraints on the constants involved. \n  5. Show the base case holds. \n\n\n",
 "path"=>"morea//070.divide-conquer/experience-master-method.md"}
</pre>

<h2>/morea/070.divide-conquer/experience-substitution.html</h2>

<pre>Hash
{"title"=>"Understanding substitution and induction",
 "published"=>true,
 "morea_id"=>"experience-substitution",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about substitution and induction.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/070.divide-conquer/experience-substitution.html",
 "url"=>"/morea/070.divide-conquer/experience-substitution.html",
 "content"=>
  "## Solving _T_(_n_) = _T_(_n_ − 1) + _n_ with Substitution\n\n### 5 points\n\nUsing substitution and induction, show that the solution of _T_(_n_) = _T_(_n_\n− 1) + _n_ is O(_n_2). In the terminology of CLRS, this is our \"guess\" at the\nsolution to the recurrence relation.\n\n**a.**   Convert the \"guess\" to an equivalent algebraic inequality according to the definition of Big-O (removing the Big-O and adding the implied constant _c_): \n\n> _T_(_n_) =\n\nMake the inductive assumption that what you wrote in (a) holds for all _m_ <\n_n_.\n\nNow you need to use induction and substitution to show that the definition\n_T_(_n_) = _T_(_n_ − 1) + _n_ implies the inequality that you wrote in (a). In\nthe process you will determine the constraints on _c_. We'll do the base case\nlast for your chosen _c_.\n\n**b.**   Write out the definition of T(_n_), and operating _ only_ on the right hand side, substitute in the inductive assumption where appropriate and simplify to isolate the expression (from a) to be proven from the lower order terms:\n\n> _T_(_n_) =\n\n**c.** Use ≤ to get rid of the lower order terms (effectively claiming that what you had above is ≤ _c__n_2), and determine the values of _c_ and _n_ for which the inequality is true:  \n\n> The above is true for all _c_ ≥ ____ and _n_ ≥ ____ because ...\n\n**d.** For the base case, assuming that _T_(0) = 0, show that _T_(1) = _c__n_2 for your choice of _c_:  \n\n> _T_(1) =\n\n### If you finish early:###\n\nTry T(_n_) = 4T(_n_/3) + _n_\n    \n",
 "path"=>"morea//070.divide-conquer/experience-substitution.md"}
</pre>

<h2>/morea/070.divide-conquer/module-divide-conquer.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"divide-conquer",
 "morea_outcomes"=>
  ["outcome-divide-conquer-recognize", "outcome-divide-conquer-apply"],
 "morea_readings"=>
  ["reading-screencast-7a",
   "reading-screencast-7b",
   "reading-screencast-7c",
   "reading-screencast-7d",
   "reading-cormen-4",
   "reading-notes-7",
   "reading-screencast-mit-divide-conquer"],
 "morea_experiences"=>["experience-substitution", "experience-master-method"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/070.divide-conquer/module-divide-conquer.gif",
 "morea_sort_order"=>70,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/070.divide-conquer/module-divide-conquer.html",
 "content"=>
  "Substitution, master method, recurrence relations, induction, maximum subarray problem, Strassen's algorithm. \n",
 "path"=>"morea//070.divide-conquer/module-divide-conquer.md"}
</pre>

<h2>/modules/divide-conquer/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-divide-conquer.md",
 "title"=>"Divide and conquer",
 "url"=>"/modules/divide-conquer/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/divide-conquer/index.html"}
</pre>

<h2>/morea/070.divide-conquer/outcome-divide-conquer-apply.html</h2>

<pre>Hash
{"title"=>"Design divide and conquer algorithms",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-apply",
 "morea_type"=>"outcome",
 "morea_sort_order"=>71,
 "referencing_modules"=>[#Jekyll:Page @name="module-divide-conquer.md"],
 "url"=>"/morea/070.divide-conquer/outcome-divide-conquer-apply.html",
 "content"=>
  "Successfully design and implement divide and conquer algorithms to solve specific programming problems.\n",
 "path"=>"morea//070.divide-conquer/outcome-divide-conquer-apply.md"}
</pre>

<h2>/morea/070.divide-conquer/outcome-divide-conquer-recognize.html</h2>

<pre>Hash
{"title"=>"Recognize when divide and conquer is appropriate",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-recognize",
 "morea_type"=>"outcome",
 "morea_sort_order"=>70,
 "referencing_modules"=>[#Jekyll:Page @name="module-divide-conquer.md"],
 "url"=>"/morea/070.divide-conquer/outcome-divide-conquer-recognize.html",
 "content"=>
  "Be able to recognize when the divide and conquer algorithm is an appropriate algorithm to apply to a programming problem.\n",
 "path"=>"morea//070.divide-conquer/outcome-divide-conquer-recognize.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-cormen-4.html</h2>

<pre>Hash
{"title"=>"CLRS 4 - Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-cormen-4",
 "morea_summary"=>
  "The maximum subarray problem, strassen's algorithm for matrix multiplication, substitution method, recursion tree method, and the master method.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "30 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-cormen-4.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-cormen-4.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-notes-7.html</h2>

<pre>Hash
{"title"=>"Chapter 7 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-7",
 "morea_summary"=>"Notes on divide and conquer",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/070.divide-conquer/reading-notes-7.html",
 "url"=>"/morea/070.divide-conquer/reading-notes-7.html",
 "content"=>
  "# Outline\n\n  1. Divide & Conquer and Recurrences\n  2. Substitution Method\n  3. Recursion Trees\n  4. Master Theorem & Method\n\n## Divide & Conquer Strategy\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\n## Recurrences\n\nThe recursive nature of D&C leads to _recurrences_, or functions defined in\nterms of:\n\n  * one or more base cases, and \n  * itself, with smaller arguments.\n\nReviewing from [Topic #2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort), a common (but not the only) form of recurrence\nis as follows. Let _T_(_n_) be the running time on a problem of size _n_.\n\n  * If _n_ is below some constant (often, _n_=1), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each 1/_b_ size of the original. \n  * We pay cost _D_(_n_) to divide the problems and _C_(_n_) to combine the solutions. \n  * We also pay cost _aT_(_n_/_b_) solving subproblems. \n\nThen the total time to solve a problem of size _n_ can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\nSome technical points should be made:\n\n  * Subproblems are not constrained to being a constant fraction of the original problem size, for example, you can have T(_n_) = T(_n-1_) + Θ(1).   _(What's an example algorithm that this describes?)_\n  * There can be other forms, such as multiple ways of dividing the problem. The book gives an example page 91 that divides the problem into 1/3 and 2/3 parts, requiring terms for T(_n/3_) and T(_2n/3_)\n  * Floors and ceilings can easily be removed and don't affect the solution to the recurrence.\n  * Boundary conditions (the smaller order terms that result from base cases) are usually Θ(1) and are omitted from asymptotic analyses, though they do matter for exact solutions.\n  * Recurrences can be inequalities. We use Big-O or Ω as appropriate. \n\nToday we cover three approaches to solving such relations: substitution,\nrecursion tree, and the master method. But first, we look at two examples, one\nof which we have already seen ...\n\n### Merge Sort\n\nSort an array A[_p_ .. _r_] of comparable elements recursivly by divide and\nconquer:\n\n**Divide:**\n    Given A[_p_ .. _r_], split the given array into two subarrays A[_p_ .. _q_] and A[_q_+1 .. _r_] where _q_ is the halfway point of A[_p_ .. _r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure Merge ... \n![](fig/code-merge-sort.jpg)\n\nWe have seen in [Topic 2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort) that this has the following recurrence (please\nreview Topic 2 if you don't see why):\n\n![](fig/recurrence-merge-subarray.jpg)\n\n### Recursive Solution to Maximum Subarray\n\nSuppose you have an array of numbers and need to find the subarray with the\nmaximum sum of elements in the subarray. (The problem is trival unless there\nare negative numbers involved.)\n\n![](fig/Fig-4-3-Maximum-Subarray.jpg)\n\nThe book provides a not very convincing application: there are applications to\ngraphics (2D version: finding the brightest spot in an image).\n\nThe following algorithm is not the fastest known (a linear solution exists),\nbut it illustrates divide and conquer. The solution strategy, given an array\nA[_low_ .. _high_], is:\n\n**Divide**\n     the subarray into two subarrays of equal size as possible by finding the midpoint _mid_ of the subarrays. \n**Conquer**\n    by finding a maximum subarray of A[_low_ .. _mid_] and A[_mid_+1 .. _high_].\n**Combine**\n    by also finding a maximum subarray that crosses the midpoint, and using the best solution of the three (the subarray crossing the midpoint and the best of the solutions in the Conquer step).\n\nThe strategy works because any subarray must lie in one of these three\npositions:\n\n![](fig/Fig-4-4-a-Subarrays.jpg)\n\n####  Pseudocode\n\nRecursion will handle the lower and upper halves. The algorithm relies on a\nhelper to find the crossing subarray. Any maximum subarray crossing the\nmidpoint must include arrays ending at A[_mid_] and starting at A[_mid_+1]:\n\n![](fig/Fig-4-4-b-Crossing.jpg)\n\nTherefore the pseudocode finds the maximum array on each side and adds them\nup:\n\n![](fig/find-max-crossing-subarray.jpg)\n\nIt should be clear that the above is Θ(n). The recursive solution follows.\n\n![](fig/find-maximum-subarray.jpg)\n\n_Check your understanding: Where is the work done? What adds up the values in\nthe left and right subarrays?_\n\n#### Analysis\n\nThe analysis relies on the simplifying assumption that the problem size is a\npower of 2 (the same assumption for merge sort). Let T(_n_) denote the running\ntime of FIND-MAXIMUM-SUBARRAY on a subarray of _n_ elements.\n\n**Base case:**\n    Occurs when _high_ equals _low_, so that _n=1_: it just returns in Θ(1) time. \n  \n**Recursive Case** (when _n_>1):\n    \n\n  * Dividing takes Θ(1) time. \n  * Conquering solves two subproblems, each on an array of n/2 elements: 2T(_n_/2). \n  * Combining calls FIND-MAX-CROSSING-SUBARRAY, which takes Θ(_n_), and some constant tests: Θ(_n_) + Θ(1). \nT(_n_)   =   Θ(1) + 2T(_n_/2) + Θ(_n_) + Θ(1)   =   2T(_n_/2) + Θ(_n_).\n\nThe resulting recurrence is the same as for merge sort:\n\n![](fig/recurrence-merge-subarray.jpg)\n\nSo how do we solve these? We have three methods: Substitution, Recursion\nTrees, and the Master Method.\n\n* * *\n\n##  Substitution Method\n\nDon't you love it when a \"solution method\" starts with ...\n\n  1. Guess the solution!\n  2. Use induction to find any unspecified constants and show that the solution works.\n\nRecursion trees (next section) are one way to guess solutions. Experience\nhelps too. For example, if a problem is divided in half we may expect to see\nlg _n_ behavior.\n\nAs an example, let's solve the recurrence for merge sort and maximum subarray.\nWe'll start with an exact rather than asymptotic version:\n\n![](fig/recurrence-merge-subarray-exact.jpg)\n\n  1. **Guess:**   T(_n_) = _n_ lg _n_ \\+ _n_.  _(Why this guess?)_\n  \n\n  2. **Induction:**\n\n**_Basis:_**\n    _n_ = 1   ⇒   _n_ lg _n_ \\+ _n_   =   1 lg 1 + 1   =   1   =   T(_n_). \n  \n**_Inductive Step:_**\n    Inductive hypothesis is that T(_k_) = _k_ lg _k_ \\+ _k_ for all _k < n_. We'll use _k = n/2_, and show that this implies that T(_n_) = _n_ lg _n_ \\+ _n_. First we start with the definition of T(_n_); then we substitute ...   \n![](fig/proof-merge-subarray-exact.jpg)\n\nInduction would require that we show our solution holds for the boundary\nconditions. This is discussed in the textbook.\n\nNormally we use asymptotic notation rather than exact forms:\n\n  * writing T(_n_) = 2T(_n/2_) + O(_n_),\n  * assuming T(_n_) = O(1) for sufficiently small _n_,\n  * not worrying about boundary or base cases, and\n  * writing solutions in asymptotic notation, e.g., T(_n_) = O(_n_ lg _n_).\n\nIf we want Θ, sometimes we can prove big-O and Ω separately \"squeezing\" the Θ\nresult.\n\nBut be careful when using asymptotic notation. For example, suppose you have\nthe case where _a_=4 and _b_=4 and want to prove T(_n_) = O(_n_) by guessing\nthat T(_n_) ≤ _cn_ and writing:\n\n![](fig/false-proof.jpg)\n\nOne must prove the _exact form_ of the inductive hypothesis, T(_n_) ≤ _cn_.\n\nSee the text for other strategies and pitfalls.\n\nProblems 4.3-1 and 4.3-2 are good practice problems.\n\n* * *\n\n##  Recursion Trees\n\nAlthough recursion trees can be considered a proof format, they are normally\nused to generate guesses that are verified by substitution.\n\n  * Each node represents the cost of a single subproblem in the set of recursive invocations\n  * Sum the costs with each level of the tree to obtain per-level costs\n  * Sum the costs across levels for the total cost.\n\n### A Familiar Example\n\nWe have already seen recursion trees when analyzing the recurrence relations\nfor Merge Sort:\n\n![](fig/recurrence-mergesort-c.jpg)  \n![](fig/recurrence-tree-mergesort-3.jpg)\n\nThe subproblems are of size _n_/20, _n_/21, _n_/22, .... The tree ends when\n_n_/2_p_ = _n_/_n_ = 1, the trivial subproblem of size 1.\n\nThus the height of the tree is the power _p_ to which we have to raise 2\nbefore it becomes _n_, i.e., _p_ = lg _n_. Since we start at 20 there are lg\n_n_ \\+ 1 levels. Multiplying by the work _cn_ at each level, we get _cn_ lg\n_n_ \\+ _cn_ for the total time.\n\n###  A More Complex Example\n\nA more complex example is developed in the textbook for\n\n> T(_n_) = 3T(_n_/4) + Θ(_n_2)\n\nwhich is rewritten (making the implied constant explicit) as\n\n> T(_n_) = 3T(_n_/4)+ _cn_2\n\n![](fig/Fig-4-5-Recursion-Tree-a.jpg) node, T(_n_) = 3T(_n_/4)\n+_cn_2.\n\nWe can develop the recursion tree in steps, as follows. First, we begin the\ntree with its root ![](fig/Fig-4-5-Recursion-Tree-b.jpg)\n\nNow let's branch the tree for the three recursive terms 3T(_n_/4). There are\nthree children nodes with T(_n_/4) as their cost, and we leave the cost _cn_2\nbehind at the root node.\n\nWe repeat this for the subtrees rooted at each of the nodes for T(_n/4_):\nSince each of these costs 3T((_n_/4)/4) +_c_(_n_/4)2, we make three branches,\neach costing T((_n_/4)/4) = T(_n_/16), and leave the _c_(_n_/4)2 terms behind\nat their roots.\n\n![](fig/Fig-4-5-Recursion-Tree-c.jpg)\n\nContinuing this way until we reach the leaf nodes where the recursion ends at\ntrivial subproblems T(1), the tree looks like this:\n\n![](fig/Fig-4-5-Recursion-Tree-d.jpg)\n\nSubproblem size for a node at depth _i_ is _n_/4_i_, so the subproblem size\nreaches _n_ = 1 when (assuming _n_ a power of 4) _n_/4_i_ = 1, or when _i_ =\nlog4_n_.  \nIncluding _i_ = 0, there are log4_n_ \\+ 1 levels. Each level has 3_i_ nodes.  \nSubstituting _i_ = log4_n_ into 3_i_, there are 3log4_n_ nodes in the bottom\nlevel.  \nUsing alogbc = clogba, there are _n_log43 in the bottom level (_not_ _n_, as\nin the previous problem).\n\nAdding up the levels, we get:  \n![](fig/solution-recursion-tree-1.jpg)\n\nIt is easier to solve this summation if we change the equation to an\ninequality and let the summation go to infinity (the terms are decreasing\ngeometrically), allowing us to apply equation A.6 (∑_k_=0,∞_xk_ = 1/1-_x_):  \n![](fig/gsolution-recursion-tree-2.jpg)\n\nAdditional observation: since the root contributes _cn2_, the root dominates\nthe cost of the tree, and the recurrence must also be Ω(_n_2), so we have\nΘ(_n_2).\n\nPlease see the text for an example involving unequal subtrees. For practice,\nexercises 4.4-6 and 4.4-9 have solutions posted on the book's web site.\n\n* * *\n\n##  Master Theorem & Method\n\nIf we have a divide and conquer recurrence of the form\n\n> T(_n_) = _a_T(_n/b_) + _f(n)_  \n  \nwhere _a ≥ 1_, _b > 1_, and _f(n) > 0_ is asymptotically positive,\n\nthen we can apply the **master method**, which is based on the **master\ntheorem**. We compare _f(n)_ to _nlogba_ under asymptotic (in)equality:\n\n**Case 1: _f(n)_ = O(_nlogba - ε_)** for some constant _ε_ > 0.  \n    (That is, _f(n)_ is polynomially smaller than _nlogba_.)  \n    **_Solution:_** T(_n_) = **Θ(_nlogba_).**   \n    Intuitively: the cost is dominated by the leaves.\n\n**Case 2: _f(n)_ = Θ(_nlogba_)**, or more generally (exercise 4.6-2): _f(n)_ = Θ(_nlogba_lg_k__n_), where _k_ ≥ 0.  \n    (That is, _f(n)_ is within a polylog factor of _nlogba_, but not smaller.)  \n    _**Solution:**_ T(_n_) = **Θ(_nlogba_lg_n_),** or T(_n_) = Θ(_nlogba_lg_k+1__n_) in the more general case.  \n    Intuitively: the cost is _nlogba_lg_k_ at each level and there are Θ(lg_n_) levels.\n\n**Case 3: _f(n)_= Ω(_nlogba + ε_)** for some constant _ε_ > 0, and _f(n)_ satisfies the regularity condition _af(n/b) ≤ cf(n)_ for some constant _c<1_ and all sufficiently large _n_.   \n    (That is, _f(n)_ is polynomially greater than _nlogba_.)  \n    _**Solution:**_ T(_n_) = **Θ(_f(n)_)**,  \n    Intuitively: the cost is dominated by the root.\n\nImportant: there are functions that fall between the cases!\n\n### Examples\n\n**T(_n_) = 5T(_n_/2) + Θ(_n_2)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_2\n  * Compare   _n_2   to   _n_log_b__a_ = _n_log25. \n  * log25 - ε = 2 for some constant ε > 0\\. \n  * Case 1: T(_n_) = Θ(_n_lg 5). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 lg _n_\n  * Compare   _n_3 lg _n_   to   _n_log327 = _n_3\n  * Case 2 with _k_ = 1: T(_n_) = Θ(_n_3 lg2 _n_). \n\n**T(_n_) = 5T(_n_/2) + Θ(_n_3)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_3\n  * Compare   _n_3   to   _n_log25\n  * log25 + ε = 3 for some constant ε > 0\\. \n  * Check regularity condition (not necessary since _f_(_n_) is polynomial:  \n_a__f_(_n_/_b_) = 5(_n_/2)3 = 5_n_3/8 ≤ _cn_3 for _c_ = 5/8 < 1\\.\n\n  * Case 3: T(_n_) = Θ(_n_3). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 / lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 / lg _n_\n  * Compare   _n_3/lg _n_   to   _n_log327 = _n_3\n  * Cases 1 and 3 won't work as no ε can adjust the exponent of 3 to account for the 1/lg_n_ = lg−1_n_ factor. Only hope is Case 2. \n  * But _n_3/lg _n_ = _n_3 lg−1_n_ ≠ Θ(_n_3 lg_k_ _n_) for any _k_ ≥ 0\\. \n  * Cannot use master method. \n  * Could try substitution, which requires a guess. Drawing the full recursion tree would be tedious, but perhaps visualizing its general form would help with the guess. \n\n* * *\n\n## Next\n\nChapter 12, Binary Search Trees (entire chapter), to which we can apply divide\n& conquer and use recurrence relations.\n\n* * *\n\nDan Suthers Last modified: Sat Feb 8 02:42:12 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//070.divide-conquer/reading-notes-7.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-7a.html</h2>

<pre>Hash
{"title"=>"Divide and conquer and recurrence relations",
 "published"=>true,
 "morea_id"=>"reading-screencast-7a",
 "morea_summary"=>"Introduction to the divide and conquer algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=W7rChliGE5M",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-7a.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-7a.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-7b.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: substitution",
 "published"=>true,
 "morea_id"=>"reading-screencast-7b",
 "morea_summary"=>"How to perform substitution.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=X2D80jsS3sY",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-7b.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-7b.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-7c.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: recursion trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-7c",
 "morea_summary"=>
  "How to generate a guess for the form of the solution to the recurrence.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8F2OvQIlGiU",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-7c.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-7c.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-7d.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: master method",
 "published"=>true,
 "morea_id"=>"reading-screencast-7d",
 "morea_summary"=>
  "Find solutions to recurrence relations of form T(n) = aT(n/b) + h(n), where a and b are constants, a ≥ 1 and b > 1",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=h4Avr0byu1g",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-7d.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-7d.md"}
</pre>

<h2>/morea/070.divide-conquer/reading-screencast-mit-divide-conquer.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-divide-conquer",
 "morea_summary"=>
  "Divide and conquer: binary search, powering a number, fibonacci numbers, matrix multiplication",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec03/",
 "morea_labels"=>["Screencast", "Demaine", "68 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/070.divide-conquer/reading-screencast-mit-divide-conquer.html",
 "content"=>"",
 "path"=>"morea//070.divide-conquer/reading-screencast-mit-divide-conquer.md"}
</pre>

<h2>/morea/080.binary-search-trees/experience-binary-search-trees-2.html</h2>

<pre>Hash
{"title"=>"More reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees some more.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/080.binary-search-trees/experience-binary-search-trees-2.html",
 "url"=>"/morea/080.binary-search-trees/experience-binary-search-trees-2.html",
 "content"=>
  "### Binary Search Trees\n\n**4.** (4 pts) Suppose you have some data keys sorted in an array and you want to construct a _**balanced binary search tree**_ from them. Assume a tree node representation `TreeNode` that includes instance variables `key`, `left`, and `right`.\n\n**a.**  Write pseudocode (or Java if you wish) for an algorithm that constructs the tree and returns the root node. (We won't worry about making the enclosing `BinaryTree` class instance.) You will need to use methods for making a new `TreeNode`, and for setting its left and right children.\n\n_Hints: First, identify the array location of the key that would have to be\nthe root of the balanced BST. Now think about how BinarySearch works on the\narray. Which item does it access first in any given subarray it is called\nwith? Using a similar strategy a simple recursive algorithm is possible._\n\n**b.**   What is the Θ cost to construct the tree? How does the expected runtime of BinarySearch on the array compare to the expected runtime of search in the tree you just constructed? \n\n\n\n**5.** (3 pts) In `Tree-Delete` (page 298 or as shown in the web notes), when node _z_ has two children, we arbitrarily decide to replace it with its successor. We could just as well replace it with its predecessor. \n\n**a.**   Rewrite `Tree-Delete` to use the predecessor rather than the successor. Modify this code just as you need to.\n    \n    \n      TREE-DELETE(T, z)\n      1  if z.left == NIL\n      2      TRANSPLANT(T, z, z.right)\n      3  elseif z.right == NIL\n      4     TRANSPLANT(T, z, z.left)\n      5  else y = TREE-MINIMUM(z.right)  // successor\n      6      if y.p != z\n      7          TRANSPLANT(T, y, y.right) \n      8          y.right = z.right\n      9          y.right.p = y\n      10      TRANSPLANT(T, z, y)\n      11      y.left = z.left\n      12      y.left.p = y\n    \n\n**b.**   Some computer scientists have argued that if equal priority were given to replacing the successor and the predecessor to not skew deletions on one side, better performance might result. How might `Tree-Delete` be modified to implement such a strategy? (_Hint:_ think about last week's topics.)\n",
 "path"=>"morea//080.binary-search-trees/experience-binary-search-trees-2.md"}
</pre>

<h2>/morea/080.binary-search-trees/experience-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/080.binary-search-trees/experience-binary-search-trees.html",
 "url"=>"/morea/080.binary-search-trees/experience-binary-search-trees.html",
 "content"=>
  "# Reasoning about binary search trees\n\n**1.** Show that if a node in a binary search tree has two children, then its successor Y has no left child and its predecessor has no right child. (_The proofs are symmetric. Hints: Rule out where the successor cannot be to narrow down to where it must be. Draw Pictures!!!_) \n\n> **(a)** Prove by contradiction that the successor Y cannot be an ancestor of\nX, so Y must be in a subtree.  \n**(b)** Identify and prove the subtree of X that successor Y must be in.   \n**(c)** Show by contradiction that successor Y cannot have a left child.  \n**(d)** Indicate how this proof would be changed for predecessor. \n\n**2\\. ** Delete the nodes with keys 10 and 27 from this Binary Search Tree, indicating for each case what \"if/elseif\" block is executed. (_You will need to apply the cases carefully to get this right: refer to the text or web notes. \"Eyeballing\" it may lead to a legal tree that would not result from the code._) \n\n![](fig/BST-for-Class-Problem-small.jpg)\n\n> **(a)** Lines executed in deletion of 10:  \n**(b)** Lines executed in deletion of 27: \n\n![](fig/pseudocode-tree-delete.jpg)",
 "path"=>"morea//080.binary-search-trees/experience-binary-search-trees.md"}
</pre>

<h2>/morea/080.binary-search-trees/module-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Binary Search Trees",
 "published"=>true,
 "morea_id"=>"binary-search-trees",
 "morea_outcomes"=>["outcome-binary-search-trees"],
 "morea_readings"=>
  ["reading-screencast-8a",
   "reading-screencast-8b",
   "reading-screencast-8c",
   "reading-screencast-8d",
   "reading-cormen-12",
   "reading-notes-8"],
 "morea_experiences"=>
  ["experience-binary-search-trees",
   "experience-binary-search-trees-2",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>
  "/morea/080.binary-search-trees/module-binary-search-trees.svg",
 "morea_sort_order"=>80,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/080.binary-search-trees/module-binary-search-trees.html",
 "content"=>
  "Queries, insertion, deletion, modification, height, performance. \n",
 "path"=>"morea//080.binary-search-trees/module-binary-search-trees.md"}
</pre>

<h2>/modules/binary-search-trees/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-binary-search-trees.md",
 "title"=>"Binary Search Trees",
 "url"=>"/modules/binary-search-trees/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/binary-search-trees/index.html"}
</pre>

<h2>/morea/080.binary-search-trees/outcome-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Understand binary search trees",
 "published"=>true,
 "morea_id"=>"outcome-binary-search-trees",
 "morea_type"=>"outcome",
 "morea_sort_order"=>80,
 "referencing_modules"=>[#Jekyll:Page @name="module-binary-search-trees.md"],
 "url"=>"/morea/080.binary-search-trees/outcome-binary-search-trees.html",
 "content"=>
  "Understand the properties of binary search trees and how to apply them. \n",
 "path"=>"morea//080.binary-search-trees/outcome-binary-search-trees.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-cormen-12.html</h2>

<pre>Hash
{"title"=>"CLRS 12 - Binary search trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-12",
 "morea_summary"=>
  "What is a binary search tree; querying, insertion, and deletion.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "13 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-cormen-12.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-cormen-12.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-notes-8.html</h2>

<pre>Hash
{"title"=>"Chapter 8 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-8",
 "morea_summary"=>"Notes on binary search trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/080.binary-search-trees/reading-notes-8.html",
 "url"=>"/morea/080.binary-search-trees/reading-notes-8.html",
 "content"=>
  "## Outline\n\n  1. Trees, Binary Trees, Binary Search Trees \n  2. Querying BSTs\n  3. Modifying BSTs (Insertion and Deletion)\n  4. Performance of BSTs \n\n##  Trees, Binary Trees, Binary Search Trees\n\nFirst, a preliminary look at trees. (This should be review. Some of this\nmaterial is taken from Thomas Standish Data Structure Techniques (1980) and\nGoodrich & Tamassia (1998) as well as the Cormen appendix, but is widely\npublished.)\n\n###  Fundamental Theorem of Free Trees\n\nIf _G_=(_V_,_E_) is a finite graph with _v > 1_ vertices, the following\nproperties are equivalent definitions of a generalized or **free tree**:\n\n  1. _G_ is connected and has no simple cycles. \n  2. _G_ has no simple cycles and has _v-1_ edges (|_E_| = |_V_| - 1)\n  3. _G_ is connected and has _v-1_ edges.\n  4. _G_ is acyclic, and if an edge is added that joins two nonadjacent vertices, exactly one cycle is formed.\n  5. _G_ is connected, but if an edge is deleted, _G_ becomes disconnected. \n  6. Every pair of vertices is connected by exactly one path. \n\nAlthough this is a definition, the theorem is that these definitions are\nequivalent. A classic exercise in basic graph theory is to prove each of these\nstatements using the one before it, and #1 from #6.\n\n#### Comments\n\nWhen we use the term \"tree\" without qualification, we will assume that we mean\na free tree unless the context makes it clear otherwise (e.g., when we are\ndiscussing binary trees).\n\nIn some contexts, _G_=({},{}) and _G_=({_v_},{}) are also treated as trees.\nThese are obvious base cases for recursive algorithms.\n\nA **forest** is a (possibly disconnected) graph, each of whose connected\ncomponents is a tree.\n\nAn **oriented tree** is a directed graph having a designated vertex _r_,\ncalled the **root**, and having exacly one oriented path between the root and\nany vertex _v_ distinct from the root, in which _r_ is the origin of the path\nand _v_ the terminus.\n\nIn some fields (such as social network analysis), the word \"node\" is used\ninterchangeably with \"vertex\". I use \"vertex\" in these notes but may slip into\n\"node\" in my recorded lectures or in class.\n\n![](fig/diagram-tree-heights-tall.jpg)\n\n### Binary Trees\n\nA **binary tree** is a finite set of vertices that is either empty or consists\nof a vertex called the root, together with two binary subtrees that are\ndisjoint from each other and from the root and are called the **left** and\n**right subtrees**.\n\nA **full binary tree ** is a binary tree in which each vertex either is a leaf\nor has exactly two nonempty descendants. In a full binary tree of height _h_:\n\n  1. number of leaves = (number internal vertices) + 1.\n  2. number leaves is at least _h_+1 _(first example figure)_ and at most 2_h_ _(second example figure)_.\n\n![](fig/diagram-tree-heights-wide.jpg)\n\n  3. number internal vertices is at least _h_ _(first example)_ and at most 2_h_-1 _(second example)_.\n  4. Total number of vertices (summing the last two results) is at least 2_h_+1 _(first example)_ and at most 2_h+1_-1 _(second example)_.\n  5. Height _h_ is at least lg(_n_+1)-1 _(second example)_ and at most (_n_-1)/2 _(first example)_\n\nA **complete binary tree ** is full binary tree in which all leaves have the\nsame depth and all internal vertices have degree 2 _(e.g., second example\nabove)_.\n\n(_Note:_ some earlier texts allow the last level of a \"complete\" tree to be\nincomplete! They are defined as binary trees with leaves on at most two\nadjacent levels _l-1_ and _l_ and in which the leaves at the bottommost level\n_l_ lie in the leftmost positions of _l_.)\n\n### Binary Search Trees (BSTs)\n\nA **binary search tree** (BST) is a binary tree that satisfies the **binary\nsearch tree property:**\n\n  * if _y_ is in the left subtree of _x_ then _y.key ≤ x.key_. \n  * if _y_ is in the right subtree of _x_ then _y.key ≥ x.key_. \n\nBSTs provide a useful implementation of the Dynamic Set ADT, as they support\nmost of the operations efficiently (as will be seen).\n\nTwo examples on the same data:  \n![](fig/Fig-12-1a-balanced.jpg) ![](fig/Fig-12-1b-\nunbalanced.jpg)\n\n_Could we just just say \"if y is the **left child** of x then y.key ≤ x.key,\netc., and rely on transitivity? What would go wrong?_\n\nImplementations of BSTs include a _root_ instance variable. Implementations of\nBST vertices usually include fields for the _key_, _left_ and _right_\nchildren, and the _parent_.\n\n* * *\n\n## Querying Binary Search Trees\n\nNote that all of the algorithms described here are given a tree vertex as a\nstarting point. Thus, they can be applied to any subtree of the tree as well\nas the full tree.\n\n### Traversing Trees\n\nTraversals of the tree \"visit\" (e.g., print or otherwise operate on) each\nvertex of the tree exactly once, in some systematic order. This order can be\n**Inorder**, **Preorder**, or **Postorder**, according to when a vertex is\nvisited relative to its children. Here is the code for inorder:\n\n![](fig/pseudocode-inorder-tree-walk.jpg)\n\n_Quick exercise: Do INORDER-TREE-WALK on this tree ... in what order are the\nkeys printed?_\n\n![](fig/example-BST-simple.jpg)\n\n_Quick exercise: How would you define Preorder traversal? Postorder\ntraversal?_\n\nTraversals can be done on any tree, not just binary search trees. For example,\ntraversal of an expression tree will produce preorder, inorder or postorder\nversions of the expressions.\n\n#### Time to Traverse a BST\n\n**Time:** Traversals (INORDER-TREE-WALK and its preorder and postorder variations) take _T_(_n_) = Θ(_n_) time for a tree with _n_ vertices, because we visit and print each vertex once, with constant cost associated with moving between vertices and printing them. More formally, we can prove as follows:\n\n_T_(_n_) = Ω(_n_) since these traversals must visit all _n_ vertices of the\ntree.\n\n_T_(_n_) = O(_n_) can be shown by substitution. First the base case of the\nrecurrence relation captures the work done for the test _x_ ≠ NIL:\n\n> _T_(0) = _c_ for some constant c > 0\n\nTo obtain the recurrence relation for _n_ > 0, suppose the traversal is called\non a vertex _x_ with _k_ vertices in the left subtree and _n_−_k_−1 vertices\nin the right subtree, and that it takes constant time _d_ > 0 to execute the\nbody of the traversal exclusive of recursive calls. Then the time is bounded\nby\n\n> _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_.\n\nWe now need to \"guess\" the inductive hypothesis to prove. The \"guess\" that\nCLRS use is _T_(_n_) ≤ (_c_ \\+ _d_)_n_ \\+ _c_, which is clearly O(_n_). It's\nless clear how they got this guess. As discussed in Chapter 4, section 4\n(especially subsection \"Subtleties\" page 85-86), one must prove the exact form\nof the inductive hypothesis, and sometimes you can get a better guess by\nobserving how your original attempt at the proof fails. Perhaps this is what\nthey did. We'll skip the failure part and go directly to proving their\nhypothesis by substitution (showing two steps skipped over in the book):\n\n> **_Inductive hypothesis:_** Suppose that _T_(_m_) ≤ (_c_ \\+ _d_)_m_ \\+ _c_\nfor all _m_ < _n_  \n  \n**_Base Case:_** (_c_ \\+ _d_)0 + _c_ = _c_ = _T_(0) as defined above.  \n  \n**_Inductive Proof:_**  \n   _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_\n_by definition_  \n           = ((_c_ \\+ _d_)_k_ \\+ _c_) + ((_c_ \\+ _d_)(_n_−_k_−1) + _c_) + _d_    _substiting inductive hypothesis for values < n_   \n            = ((_c_ \\+ _d_)(_k_ \\+ _n_ − _k_ − 1) + _c_ \\+ _c_ \\+ _d_             _collecting factors _   \n            = ((_c_ \\+ _d_)(_n_ − 1) + _c_ \\+ _c_ \\+ _d_                         _simplifying _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_ − (_c_ \\+ _d_) + _c_ \\+ _d_                   _multiplying out _n_−1 and rearranging _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_.                                            _the last terms cancel._\n\n### Searching for an Element in a BST\n\nHere are two implementations of the dynamic set operation `search`:\n\n![](fig/pseudocode-recursive-tree-search.jpg) ![](fig\n/pseudocode-iterative-tree-search.jpg)\n\n_Quick exercise: Do TREE-SEARCH for D and C on this tree ... _\n\n![](fig/example-BST-simple.jpg)\n\nFor now, we will characterize the run time of the remaining algorithms in\nterms of _h_, the height of the tree. Then we will consider what _h_ can be as\na function of _n_, the number of vertices in the tree.\n\n**Time:** Both of the algorithms visit vertices on a downwards path from the root to the vertex sought. In the worst case, the leaf with the longest path from the root is reached, examining _h_+1 vertices (_h_ is the height of the tree, so traversing the longest path must traverse _h_ edges, and _h_ edges connect _h_+1 vertices). Comparisons and movements to the chosen child vertex are O(1), so the algorithm is O(_h_). (_Why don't we say Θ?_) \n\n### Finding the Minimum and Maximum Element\n\nThe BST property guarantees that:\n\n  * The minimum key of a BST is located at the leftmost vertex.\n  * The maximum key of a BST is located at the rightmost vertex.\n\n_(Why?)_ This leads to simple implementations:\n\n![](fig/pseudocode-tree-min-max.jpg) ![](fig/example-\nBST-simple.jpg)\n\n**Time:** Both procedures visit vertices on a path from the root to a leaf. Visits are O(1), so again this algorithm is O(_h_).\n\n###  Finding the Successor or Predecessor of an Element\n\nAssuming that all keys are distinct, the successor of a vertex _x_ is the\nvertex _y_ such that _y.key_ is the smallest _key_ > _x.key_. If _x_ has the\nlargest key in the BST, we define the successor to be NIL.\n\nWe can find _x_'s successor based entirely on the tree structure (no key\ncomparison is needed). There are two cases:\n\n  1. **If vertex _x_ has a non-empty right subtree, then _x_'s successor is the minimum in its right subtree.** _(Why?)_\n  2. **If vertex _x_ has an empty right subtree, then _y_ is the lowest ancestor of _x_ whose left child is also an ancestor of _x_.**   _To see this, consider these facts: _\n    * If _y_ is the successor of _x_ then _x_ is the predecessor of _y_, so _x_ is the maximum in _y_'s left subtree _(flip the reasoning of your answer to the last question)_.\n    * Moving from _x_ to the left up the tree (up through right children) reaches vertices with smaller keys, which must also be in this left subtree. \n![](fig/pseudocode-tree-successor.jpg)\n\n_Exercise: Write the pseudocode for TREE-PREDECESSOR_\n\nLet's trace the min, max, successor (15, 13, 6, 4), and predecessor (6)\noperations:\n\n![](fig/Fig-12-2-example-BST.jpg)\n\n**Time:** The algorithms visit notes on a path down or up the tree, with O(1) operations at each visit and a maximum of _h+1_ visitations. Thus these algorithms are O(_h_). \n\n_Exercise: Show that if a vertex in a BST has two children, then its succesor\nhas no left child and its predecessor has no right child._\n\n* * *\n\n##  Modifying Binary Search Trees\n\nThe key point is that the BST property must be sustained. This is more\nstraightforward with insertion (as we can add a vertex at a leaf position)\nthan with deletion (where an internal vertex may be deleted).\n\n###  Insertion\n\nThe algorithm assumes that the vertex _z_ to be inserted has been initialized\nwith _z.key_ = _v_ and _z.left_ = _z.right_ = NIL.\n\nThe strategy is to conduct a search (as in tree search) with pointer _x_, but\nto sustain a **trailing pointer** _y_ to keep track of the parent of _x_. When\n_x_ drops off the bottom of the tree (becomes NIL), it will be appropriate to\ninsert _z_ as a child of _y_.\n\nComment on variable naming: I would have preferred that they call _x_\nsomething like `leading` and _y_ `trailing`.\n\n![](fig/pseudocode-tree-insert.jpg)\n\nTry `TREE-INSERT(T,C)`:\n\n![](fig/example-BST-simple.jpg)\n\n**Time:** The same as TREE-SEARCH, as there are just a few additional lines of O(1) pointer manipulation at the end.\n\n_Discuss: How would you use TREE-INSERT and INORDER-TREE-WALK to sort a set of\nnumbers?_  \n_Think about at home: How would you prove its time complexity?_\n\n###  Deletion\n\nDeletion is more complex, as the vertex _z_ to be deleted may or may not have\nchildren. We can think of this in terms of three cases:\n\n  1. If _z_ has no children, we can just remove it (by setting _z_'s parent's pointer to NIL). \n  2. If _z_ has just one child _c_, then make _c_ take _z_'s position in the tree, updating _z_'s parent to point to _c_ and \"dragging\" _c_'s subtree along.\n  3. If _z_ has two children, find _z_'s successor _y_ and replace _z_ by _y_ in the tree (noting that _y_ has no left child): \n    * If _y_ is _z_'s right child, then replace _z_ by _y_ (including updating _z_'s parent to point to _y_, and _y_ to point to _z_'s left child) and we are done. \n    * Otherwise _y_ is further down in _z_'s right subtree (and again has no left child): \n      1. Replace _y_ with its own right child. \n      2. The rest of _z_'s right subtree becomes _y_'s new right subtree.\n      3. _z_'s left subtree becomes _y_'s new left subtree.\n      4. Make _z_'s parent point to _y_.\n\nThe code organizes the cases differently to simplify testing and make use of a\ncommon procedure for moving subtrees around. This procedure replaces the\nsubtree rooted at _u_ with the subtree rooted at _v_.\n\n  * It makes _u_'s parent become _v_'s parent (lines 6-7), unless _u_ is the root, in which case it makes _v_ the root (lines 1-2).\n  * _v_ replaces _u_ as _u_'s parent's left or right child (lines 3-5).\n  * It does not update _v.left_ or _v.right_, leaving that up to the caller. \n![](fig/pseudocode-transplant.jpg)\n\n_(If we have time, draw a few examples.)_\n\nHere are the four actual cases used in the main algorithm TREE-DELETE(T,_z_):\n\n![](fig/Fig-12-4-a-no-left-child.jpg)\n\n#### No left child (and possibly no children):\n\nIf _z_ has no left child, replace _z_ by its right child (which may or may not\nbe NIL). This handles case 1 and half of case 2 in the conceptual breakdown\nabove. (Lines 1-2 of final algorithm.)\n\n![](fig/Fig-12-4-b-no-right-child.jpg)\n\n#### No right child (and has left child):\n\nIf _z_ has just one child, and that is its left child, then replace _z_ by its\nleft child. This handles the rest of case 2 in the conceptual breakdown above.\n(Lines 3-4.)\n\nNow we just have to deal with the case where both children are present. Find\n_z_'s successor (line 5), which must lie in _z_'s right subtree and have no\nleft child (_why?_). Handling depends on whether or not the successor is\nimmediately referenced by _z_:\n\n![](fig/Fig-12-4-c-successor-is-child.jpg)\n\n#### Successor is child:\n\nIf successor _y_ is _z_'s right child (line 6), replace _z_ by _y_, \"pulling\nup\" _y_'s right subtree. The left subtree of _y_ is empty so we can make _z_'s\nformer left subtree _l_ be _y_'s new left subtree. (Lines 10-12.)\n\n#### Successor is not child:\n\nOtherwise, _y_ is within _z_'s right subtree rooted at _r _but is not the root\nof this subtree (_y≠r_).\n\n  1. Replace _y_ by its own right child _x_. (Line 7.)\n  2. Set _y_ to be _r_'s parent. (Line 8-9.)\n  3. Then let _y_ take _z_'s place with respect to _z_'s parent __ and left child _l_. (Lines 10-12.)\n![](fig/Fig-12-4-d-successor-not-child.jpg)\n\nNow we are ready for the full algorithm:\n\n![](fig/pseudocode-tree-delete.jpg)\n\nThe last three lines excecute whenever _z_ has two children (the last two\ncases above).\n\nLet's try `TREE-DELETE(T,_x_)` on _x=_ I, G, K, and B:\n\n![](fig/example-BST-to-delete.jpg)\n\n**Time:** Everything is O(1) except for a call to TREE-MINIMUM, which is O(_h_), so TREE-DELETE is O(_h_) on a tree of height _h_. \n\nThe above algorithm fixes a problem with some published algorithms, including\nthe first two editions of the book. Those versions copy data from one vertex\nto another to avoid a tree manipulation. If other program components maintain\npointers to tree vertices (or their positions in Goodrich & Tamassia's\napproach), this could invalidate their pointers. The present version\nguarantees that a call to TREE-DELETE(T, _z_) deletes exactly and only vertex\n_z_.\n\nAn animation is available at\n<http://www.csc.liv.ac.uk/~ullrich/COMP102/applets/bstree/> (The code shown\nprobably has the flaw discussed above.)\n\n* * *\n\n##  Performance of Binary Search Trees\n\nWe have been saying that the asympotic runtime of the various BST operations\n(except traversal) are all O(lg _h_), where _h_ is the height of the tree. But\n_h_ is usually hidden from the user of the ADT implementation and we are more\nconcerned with the runtime as a function of _n_, our input size. So, what is\n_h_ as a function of _n_?\n\nWe know that in the worst case, _h_ = O(_n_) (when the tree degenerates to a\nlinear chain). Is this the expected case? Can we do anything to guarantee\nbetter performance? These two questions are addressed below.\n\n###  Expected height of randomly built binary search trees\n\nThe textbook has a proof in section 12.4 that **the expected height of a\nrandomly build binary search tree on _n_ distinct keys is O(lg _n_).**\n\nWe are not covering the proof (and you are not expected to know it), but I\nrecommend reading it, as the proof elegantly combines many of the ideas we\nhave been developing, including indicator random variables and recurrences.\n(They take a huge step at the end: can you figure out how the log of the last\npolynomial expression simplifies to O(lg _n_)?)\n\nAn alternative proof provided by Knuth (Art of Computer Programming Vol. III,\n1973, p 247), and also summarized by Standish, is based on average path\nlengths in the tree. It shows that about 1.386 lg _n_ comparisons are needed:\n**the average tree is about 38.6% worse than the best possible tree in number\nof comparisons required for average search**.\n\nSurprisingly, _analysts have not yet been able to get clear results when\nrandom deletions are also included_.\n\n### Balanced Trees\n\nGiven the full set of keys in advance, it is possible to build an optimally\nbalanced BST for those keys (guaranteed to be lg _n_ height). See section 15.5\nof the Cormen et al. text.\n\nIf we don't know the keys in advance, many clever methods exist to keep trees\nbalanced, or balanced within a constant factor of optimal, by performing\nmanipulations to re-balance after insertions (AVL trees, Red-Black Trees), or\nafter all operations (in the case of splay trees). We cover Red-Black Trees in\ntwo weeks ([Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html)),\nafter a diversion to heaps (which have tree-like structure) and sorting.\n\n* * *\n\n## Next\n\nIn [Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html) we\nlook at how a special kind of tree, a Heap, can be embedded in an array and\nused to implement a sorting algorithm and priority queues.\n\nAfter a brief diversion to look at other sorting algorithms, we will return to\nother kinds of trees, in particular special kinds of binary search trees that\nare kept balanced to guarantee O(lg _n_) performance, in [Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:15:30 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//080.binary-search-trees/reading-notes-8.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-8a.html</h2>

<pre>Hash
{"title"=>"Introduction to binary search trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-8a",
 "morea_summary"=>"Basic qualitative facts about BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=bAxzRuu3Uy4",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-8a.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-8a.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-8b.html</h2>

<pre>Hash
{"title"=>"BST Queries",
 "published"=>true,
 "morea_id"=>"reading-screencast-8b",
 "morea_summary"=>"How to perform queries on BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=LDncFcNOr_I",
 "morea_labels"=>["Screencast", "Suthers", "22 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-8b.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-8b.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-8c.html</h2>

<pre>Hash
{"title"=>"Modifying BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8c",
 "morea_summary"=>"How to modify binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=qJ7TyaKSf_0",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-8c.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-8c.md"}
</pre>

<h2>/morea/080.binary-search-trees/reading-screencast-8d.html</h2>

<pre>Hash
{"title"=>"Analyzing BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8d",
 "morea_summary"=>"Determine the height of binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=vx0uWYHIRes",
 "morea_labels"=>["Screencast", "Suthers", "5 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/080.binary-search-trees/reading-screencast-8d.html",
 "content"=>"",
 "path"=>"morea//080.binary-search-trees/reading-screencast-8d.md"}
</pre>

<h2>/morea/090.heaps/experience-heaps-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps (again)",
 "published"=>true,
 "morea_id"=>"experience-heaps-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps (at home).",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/experience-heaps-2.html",
 "url"=>"/morea/090.heaps/experience-heaps-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### Heaps\n\n#### 8 points\n\n**2\\. ** Illustrate `Build-Max-Heap` on this data in a 1-based indexing array:\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5],\n    \n\nRewrite the array as it exists after each execution of line 3 (the call to\n`Max-Heapify`). A template is provided below. Please use a plain text editor\nwith fixed width font and replace each underscore character \"_\" with the\ncorrect value.\n\n    \n    \n      index        1  2  3  4  5  6  7  8  9\n      Start:  A = [1, 6, 2, 8, 3, 9, 4, 7, 5]\n    \n      i = 4:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 3:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 2:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 1:  A = [_, _, _, _, _, _, _, _, _]\n    \n\nYou may want to draw the heap in tree form and do the operations on the tree.\nYou are encouraged to include these trees in your response to help Robert\n\"debug\" any problems, but grading will initially be done on the above\ntemplate.\n\n**3\\. ** Illustrate `Heap-Extract-Max` on the heap you constructed above. Show the array representation:\n\n**(a)** After line 5 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**(b)** After line 6 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**4\\. ** Consider now min-heaps rather than max-heaps. Write pseudocode for `Min-Heapify` and `Heap-Decrease-Key`, by copying the textbook's code for the max versions and changing only what you need to change. To make grading easier, please highlight, boldface or circle your changes.\n    \n    \n      MAX-HEAPIFY (A, i) // Change to **MIN**-HEAPIFY \n      1   l = LEFT(i)\n      2   r = RIGHT(i)\n      3   if l <= A.heap-size and A[l] > A[i]\n      4       largest = l\n      5   else largest = i\n      6   if r <= A.heap-size and A[r] > A[largest]\n      7       largest = r\n      8   if largest != i\n      9       exchange A[i] with A[largest]\n      10      MAX-HEAPIFY (A, largest)\n    \n      HEAP-INCREASE-KEY (A, i, key) // Change to HEAP-**DECREASE**-KEY\n      1  if key < A[i]\n      2      error \"new key is smaller than current key\"\n      3  A[i] = key\n      4  while i > 1 and A[PARENT(i)] < A[i]\n      5      exchange A[i] with A[PARENT(i)]\n      6      i = PARENT(i)\n    \n\n\n",
 "path"=>"morea//090.heaps/experience-heaps-2.md"}
</pre>

<h2>/morea/090.heaps/experience-heaps.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps",
 "published"=>true,
 "morea_id"=>"experience-heaps",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/experience-heaps.html",
 "url"=>"/morea/090.heaps/experience-heaps.html",
 "content"=>
  "### 1\\. Heap-Delete(A, i)\n\n#### 2 points\n\nProcedure `Heap-Delete(_A_, _i_)` deletes the node at index _i_ in heap _A_\n(represented as an array). Give an implementation of `Heap-Delete` that runs\nin O(lg _n_) time for a heap of size _n_ = `A.heapSize`. You may use instance\nvariable `A.heapSize` and any of the other procedures already defined in the\ntext.\n\n_This is very similar to an existing procedure._\n\n>\n\n>     Heap-Delete(A,i)\n\n>  \n\n### 2\\. Heapsort on Sorted Data\n\n#### 3 points\n\n**(a)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_increasing_** order_?\n\n**(b)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_decreasing_** order_?\n\n**(c)** For which of these cases would `Heapsort` make _more swaps of elements in the array_, or are they the same?\n\n_Give your reasoning to help grading feedback. You might start working an\nexample for each case, but don't get bogged down in details: return to high\nlevel asymptotic reasoning as soon as you see what is going on. _Refer to line\nnumbers in code _ when discussing your analyses. _\n\n\n",
 "path"=>"morea//090.heaps/experience-heaps.md"}
</pre>

<h2>/morea/090.heaps/module-heaps.html</h2>

<pre>Hash
{"title"=>"Heapsort",
 "published"=>true,
 "morea_id"=>"heaps",
 "morea_outcomes"=>["outcome-heaps"],
 "morea_readings"=>
  ["reading-screencast-9a",
   "reading-screencast-9b",
   "reading-screencast-9c",
   "reading-screencast-9d",
   "reading-cormen-6",
   "reading-notes-9"],
 "morea_experiences"=>["experience-heaps", "experience-heaps-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/090.heaps/module-heaps.jpg",
 "morea_sort_order"=>90,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/090.heaps/module-heaps.html",
 "content"=>
  "Heaps, correctness, run-time analysis, priority queues, application to sorting.\n",
 "path"=>"morea//090.heaps/module-heaps.md"}
</pre>

<h2>/modules/heaps/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-heaps.md",
 "title"=>"Heapsort",
 "url"=>"/modules/heaps/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/heaps/index.html"}
</pre>

<h2>/morea/090.heaps/outcome-heaps.html</h2>

<pre>Hash
{"title"=>"Understand heaps, heapsort, and priority queues",
 "published"=>true,
 "morea_id"=>"outcome-heaps",
 "morea_type"=>"outcome",
 "morea_sort_order"=>90,
 "referencing_modules"=>[#Jekyll:Page @name="module-heaps.md"],
 "url"=>"/morea/090.heaps/outcome-heaps.html",
 "content"=>"Understand how to manipulate heaps and their benefits. \n",
 "path"=>"morea//090.heaps/outcome-heaps.md"}
</pre>

<h2>/morea/090.heaps/reading-cormen-6.html</h2>

<pre>Hash
{"title"=>"CLRS 6 - Heapsort",
 "published"=>true,
 "morea_id"=>"reading-cormen-6",
 "morea_summary"=>
  "Heapsort, heaps, maintaining the heap property, building a heap, priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "19 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-cormen-6.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-cormen-6.md"}
</pre>

<h2>/morea/090.heaps/reading-notes-9.html</h2>

<pre>Hash
{"title"=>"Chapter 9 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-9",
 "morea_summary"=>"Notes on heaps and heapsort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/090.heaps/reading-notes-9.html",
 "url"=>"/morea/090.heaps/reading-notes-9.html",
 "content"=>
  "# Heaps, Heapsort, and Priority Queues\n\n## Outline\n\n  1. Heaps and their Properties\n  2. Building and Maintaining Heaps\n  3. Application to Sorting\n  4. Application to Priority Queues\n\n##  Heaps and their Properties\n\nHeaps are a useful data structure with applications to sorting and priority\nqueues.\n\nThey are _nearly complete binary trees_ that satisfy a _heap property_ that\norganizes data under a partial ordering of their keys, enabling access to\nelements with maximum (or minimum) keys without having to pay the cost of\nfully sorting the keys.\n\nHeaps are not to be confused with garbage collected storage (a heap of\ngarbage)!\n\n### Heaps as Nearly Complete Binary Trees\n\nConceptually, heaps are **nearly complete binary trees**: they have leaves on\nat most two adjacent levels _l-1_ and _l_ and in which the leaves at the\nbottommost level _l_ lie in the leftmost positions of _l_:\n\n![](fig/Fig-6-1-max-heap-tree.jpg)\n\nThese quantitative properties concerning full and nearly complete binary trees\nwill be useful:\n\n#### Number of elements in nearly complete binary trees of height _h_ (6.1-1)\n\n![](fig/diagram-tree-heights-wide.jpg)\n\nAs discussed in [Topic 8](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-08.html#binarytrees), a **complete binary tree** has at most 2_h_+1\n− 1 nodes (vertices). We can see this by adding up the number of elements at\neach level: 20 \\+ 21 \\+ ... + 2h for a complete binary tree of height _h_.\nThen apply formula A.5 with _x_=2 and _n_=_h_:\n\n![](fig/formula-A-5.jpg)\n\nYou get (2_h_+1 − 1) / (2 − 1) = 2_h_+1 − 1\\.\n\nSo, a _nearly_ complete binary tree has _at most_ 2_h_+1 − 1 elements (if it\nis complete, as analyzed above). The _fewest_ number of elements it can have\nat height _h_ is when the last level has just 1 element and the level before\nit is complete. So do the math for a complete binary tree of height _h_−1:\nthere are exactly 2_h_ − 1 elements in levels 1 to _l_−1 and one more element\nin the _l_th level, for a total of 2_h_ elements.\n\n#### Height of an _n_-element nearly complete binary tree (6.1-2)\n\nGiven an _n_-element nearly complete binary tree of height _h_, from 6.1-1:\n\n> 2_h_   ≤   _n_   ≤   2_h+1_ − 1   <   2_h+1_\n\nTaking the log of the first, second and last terms,\n\n> _h_   ≤   lg _n_   <   _h_ \\+ 1\n\nSince _h_ is an integer, _h_ = ⌊lg _n_⌋     _(Notice the \"floor\" notation.)_\n\n#### Number of leaves\n\nAn _n_-element nearly complete binary tree has ⌈n/2⌉ leaves.     _(Notice the\n\"ceiling\" notation. Left as exercise.)_\n\n####  Nodes of height _h_ in a nearly complete binary tree (6.3-3)\n\nThere are at most ⌈n/2h+1⌉ nodes of height _h_ in a nearly complete binary\ntree. (A proof by contradiction is possible.)\n\n### The Heap Property\n\nDepending on whether it is a _max heap_ or a _min heap_, to be a heap the\nbinary tree must also satisfy a heap property:\n\n**Max Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≥ key(_i_).  \n  \nBy induction and transitivity of ≥, the max heap property guarantees that the\nmaximum element of a max-heap is at the root.\n\n  \n**Min Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≤ key(_i_).  \n  \nBy induction and transitivity of ≤, the min heap property guarantees that the\nminimum element of a min heap is at the root.\n\n### Array Representation\n\nHeaps are usually represented using arrays, following the mapping shown by the\nindices in the tree:\n\n![](fig/Fig-6-1-max-heap-tree-array-indices.jpg)\n![](fig/Fig-6-1-max-heap-array.jpg)  \n\nThe fact that we can see a heap both as a binary tree and as an array is an\nexample of a powerful idea in computer science: mapping between an\nimplementation representation that has efficient computational properties and\na conceptual representation that fits how we think about a problem.\n\n![](fig/code-parent-children.jpg)\n\nIf a heap is stored in array `A`, then movement in the tree is easy:\n\n  * Root of the tree is `A[1]`\n  * Parent of `A[_i_]` is `A[⌊_i_/2⌋]`     _(Notice we are taking the floor of _i_/2)_.\n  * Left Child of `A[_i_]` is `A[_2i_]`\n  * Right Child of `A[_i_]` is `A[_2i+1_]`\n  * Index operations are fast in binary (left and right shifts and setting the low order bit).\n\n#### Indices of leaves (6.1-7)\n\nBy the number of leaves fact, when an _n_-element heap is stored in the array\nrepresentation, the leaves are the nodes indexed by ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2,\n..., _n_. (Left as exercise.)\n\nThis fact will be used in algorithms that only need to process either the\nleaves or the internal nodes of a heap.\n\n* * *\n\n##  Building and Maintaining Heaps\n\n### Maintaining the Heap Property\n\nMAX-HEAPIFY is used to maintain the max-heap property by addressing a possible\nviolation at node `A[_i_]`:\n\n  * MAX-HEAPIFY assumes that the left and right subtrees of _i_ are max-heaps.\n  * When called, `A[_i_]` may (or may not) be smaller than its children, violating the max-heap property if it is.\n  * After MAX-HEAPIFY, the subtree rooted at _i_ will be a heap. \n![](fig/code-max-heapify.jpg)\n\nIt works by comparing `A[_i_]` with its left and right children (lines 3-7),\nand if necessary swapping `A[_i_]` with the larger of the two children to\npreserve the heap property (lines 8-9). _Tail recursion_ after the swap\npropagates this change until the subtree is a heap (line 10).\n\n#### Example\n\nMax-Heapify from the node at index 2 (containing 4):\n\n![](fig/Fig-6-2-max-heapify.jpg)\n\n#### Analysis\n\nIt is easy to see that the body of each call before recursion is O(1), and the\nrecursion repeats this for at most O(lg _n_) nodes on the path from the root\nto the leaves.\n\nMore formally, the worst case is when the bottom level is exactly half full,\nand in this case, the _children's subtrees_ can have at most 2_n_/3 nodes. So,\nadding the cost to recurse on these subtrees plus Θ(1) cost for comparisons at\na given node, we get the recurrence relation:\n\n> _T_(_n_)   ≤   _T_(2_n_/3) + Θ(1).\n\nThis fits case 2 of the Master Theorem (_a_ = 1, _b_ = 3/2 since 1/(3/2) =\n2/3, and _f_(_n_) = 1 = O(_n_log3/21) = O(_n_0)), giving Θ(lg _n_).\n\n### Building a Heap\n\nSuppose we load some keys into an array in arbitrary order from left to right,\ncreating an almost complete binary tree that may not satisfy the heap\nproperty.\n\nEach leaf of the corresponding tree is trivially a heap. If we call MAX-\nHEAPIFY on the parents of the leaves, the assumption that the right and left\nsubtrees are heaps is met. Once MAX-HEAPIFY returns, the parents are roots of\nheaps too, so we call it on _their_ parents.\n\nUsing the previously established result that the leaves begin in the array at\nindex ⌊n/2⌋ \\+ 1, so the last non-leaf node is at ⌊n/2⌋, the implementation is\ntrivial:\n\n![](fig/code-build-max-heap.jpg)\n\n#### Example\n\n![](fig/Fig-6-3-build-max-heap-array.jpg)\n\nLet's trace this on an array of size 10, for _i_ = 5 downto 1:\n\n![](fig/Fig-6-3-build-max-heap-ab.jpg)\n\n(a) The heap rooted at vertex or array index 5 is already a max heap: no\nchange is made.\n\n(b) The heap rooted at index 4 is not a max heap: the value 2 is smaller than\nits children. We restore the max heap property by swapping 2 with the larger\nchild key, 14 (see next figure for result). If we had swapped with 8, it would\nnot be a max heap: this is why we always swap with the larger child.\n\n![](fig/Fig-6-3-build-max-heap-cd.jpg)\n\n(c) Decrementing _i_ to 3, there is another violation of the max heap\nproperty, and we swap value 3 at index 3 with value 10 at index 7 (the larger\nchild).\n\n(d) The heap at index 2 violates the max heap property: we must propagate the\nvalue 1 down by swapping with 16, and then with 7 in a recursive call to Max-\nHeapify (see next figure).\n\n![](fig/Fig-6-3-build-max-heap-ef.jpg)\n\n(e) Finally, checking the value at index 1 (value 4) against its children, we\nfind we need to swap it with value 16 at index 2, and then with value 14 at\nindex 4 and value 8 at index 9 in two recursive calls to Max-Heapify. (f)\nshows the resulting max heap.\n\n#### Correctness\n\n![](fig/code-build-max-heap.jpg)\n\n_**Loop Invariant:**_\n\n    At the start of every iteration of the `for` loop, each node _i_+1, _i_+2, ..., _n_ is a root of a max heap.\n  \n_**Initialization:**_\n\n    By Exercise 6.1-7, each node ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2, ..., _n_ is a leaf, which is the root of a trivial max-heap. Since _i_ = ⌊n/2⌋ before the first iteration of the `for` loop, the invariant is initially true. \n  \n_**Maintenance:**_\n\n    Children of node _i_ are indexed higher than _i_, so by the loop invariant, they are both roots of max-heaps. Thus the assumption of MAX-HEAPIFY is met, enabling it to make node _i_ a max-heap root. Decrementing _i_ reestablishes the loop invariant at each iteration.\n  \n_**Termination:**_\n\n    The loop terminates when _i_ = 0. By the loop invariant, each node including the root indexed by 1 is the root of a max-heap.\n\n#### Analysis\n\nSometimes a good approach is to prove an easy bound and then tighten it.\n\nIt is easy to see that there are O(_n_) (about _n_/2) calls to MAX-HEAPIFY,\nand we already know that MAX-HEAPIFY on a tree of height O(lg _n_) is O(lg\n_n_). Therefore an upper bound is O(_n_ lg _n_).\n\nHowever, only the root node and those near it are at height O(lg _n_). Many\nnodes are close to the leaves and we don't even process half of them. So let's\ntry for a tighter bound ...\n\nThere are no more than ⌈n/2h+1⌉ nodes of height _h_ (Exercise 6.3-3), and the\nheap is ⌊lg_n_⌋ high (Exercise 6.1-2). MAX-HEAPIFY called on a node of height\n_h_ is O(_h_), so we need to sum this cost times the number of nodes at each\n_h_ for all relevant _h_:\n\n![](fig/analysis-build-max-heap-1.jpg)\n\nWe can simplify this as follows:\n\n  1. Wrap big-O around the whole thing, leaving h behind.\n  2. Remove the ceiling (which does not affect big-O analysis).\n  3. Rewrite the resulting _nh_/2_h_+1 as (_n_/2)(h/2_h_).\n  4. Move _n_/2 out of the summation, as it does not involve _h_.\n  5. Eliminate the constant 1/2, as we are inside the magical world of big-O!\n\nTricky, huh? Now maybe you can see why the text authors write that as:\n\n![](fig/analysis-build-max-heap-2.jpg)\n\nThe above summation runs up to ⌊lg_n_⌋, but we would like to use a convenient\nformula A-8, shown below, which runs up to ∞:\n\n![](fig/formula-A-8.jpg)\n\nSince big-O implies an inequality (upper bound), we can go ahead and run the\nsummation to ∞ instead of ⌊lg_n_⌋, because all of the additional terms are\npositive (and also very small), so the inequality will be maintained. Then, if\nwe let _x_ = 1/2 (since _h_/2_h_ = _h_(1_h_/2_h_) can be written as h(1/2)h),\nwe get:\n\n![](fig/analysis-build-max-heap-3.jpg)\n\nThus our big-O expression simplifies to O(_n_*2) = O(_n_), which is a tighter\nbound than O(_n_ lg _n_). The same analysis appliles to the min-heap version.\n\n(You might wonder why we can build a heap in O(_n_) time when sorting takes\nO(_n_ lg _n_), as will be proven in [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html).\nThis is because a heap is only a partial order, so less work needs to be done\nto guarantee the heap property.)\n\n* * *\n\n##  Application to Sorting\n\nSuppose `A[1.._n_]` contains keys to be sorted. If we call BUILD-MAX-HEAP on\nthis array, the maximum element will be at `A[1]`. We can swap it with the\nitem at `A[_n_]`, then repeat on `A[1.._n_-1]` (reducing the size of the heap\nby 1 each iteration) until this reaches size 1.\n\n![](fig/code-heapsort.jpg)\n\n#### Analysis:\n\nBUILD-MAX-HEAP is O(_n_) (by analysis above). The `for` loop executes _n_-1\ntimes, with O(1) exchange each iteration and a call to O(lg _n_) MAX-HEAPIFY.\nThus heapsort is O(_n_ lg _n_).\n\n#### Example:\n\nSuppose we have an array A with five integers. First, BUILD-MAX-HEAP is called\non it, resulting in the array A = [7, 4, 3, 1, 2] shown as the tree in (a)\nbelow.\n\n![](fig/Fig-6-4-heapsort-alt-ab.jpg)\n\nThen the loop of HEAPSORT successively takes out the maximum from the first\nindex by swapping it with the last element in the heap, and calls MAX-HEAPIFY.\nSo, 7 is swapped with 2, and then the heap (now one smaller) is reconstructed,\nresulting in the heap shown in (b): A = [4, 2, 3, 1, 7], with the first four\nelements being the heap.\n\n![](fig/Fig-6-4-heapsort-alt-cd.jpg)\n\nThe maximum element 4 (from b) was swapped with the minimum element 1\n(removing 4 from the heap) and the heap restored, resulting in (c) A = [3, 2,\n1, 4, 7] with the first three elements being the heap. Then in (d), the max\nelement 3 was swapped with 1 and the heap restored by percolating 1 down: A =\n[2, 1, 3, 4, 7] with the heap being the first two elements.\n\n![](fig/Fig-6-4-heapsort-alt-e.jpg)\n\n(e) Finally, the maximum element 2 is removed by swapping with the only\nremaining element 1, resulting in the sorted array shown.\n\nHere is a [playing card\ndemonstration](http://www.youtube.com/watch?v=WYII2Oau_VY) of heap sort, in\ncase it helps. This demonstration is using a _min-heap_ to sort the cards with\nthe card of _maximum_ value ending up at the top of the stack of cards.\n\n* * *\n\n##  Application to Priority Queues\n\nAn important application of heaps is implementing **priority queues**. There\nare _min_ and _max_ versions.\n\nA **max-priority queue** is an ADT with the following operations:\n\nINSERT(S,_x_)\n\n    S <- S ∪ {_x_}\n  \nMAXIMUM(S)\n\n    Returns the element of S with the largest key.\n  \nEXTRACT-MAX(S)\n\n    Removes and returns the element of S with the largest key.\n  \nINCREASE-KEY(S,_x_,_k_)\n\n    Increases the value of _x_'s key to the new value _k_ ≥ current key(_x_).\n\nA **min-priority queue** has corresponding operations MINIMUM, EXTRACT-MIN,\nand DECREASE-KEY.\n\nMax-priority queues can be used in job scheduling: the highest priority job is\nalways run next, but job priority can be increased as a job ages in the queue,\nor for other reasons.\n\nMin-priority queues will be very important in graph algorithms we cover later\nin the semester: efficient implementations of EXTRACT-MIN and DECREASE-KEY\nwill be especially important.\n\nMin-priority queues also used in event-driven simulations, where an event may\ngenerate future events, and we need to simulate the events in chronological\norder.\n\n#### Accessing Maximums\n\nIn the array representation, MAXIMUM is trival to implement in O(1) by\nreturning the first element of the array. However, if we EXTRACT-MAX we need\nto restore the heap property afterwards.\n\nHEAP-EXTRACT-MAX takes the root out, replaces it with the last element in the\nheap _(stop and think: why this element?)_, and then uses MAX-HEAPIFY to\npropagate that element (which probably has a small key) down to its proper\nplace:\n\n![](fig/code-heap-extract-max.jpg)\n\nHEAP-EXTRACT-MAX is O(lg _n_) since there is only constant work added to the\nO(lg _n_) MAX-HEAPIFY.\n\n#### Increasing keys\n\nAn increase to the key may require propagating the element _up_ the tree (the\nopposite direction as compared to MAX-HEAPIFY):\n\n![](fig/code-heap-increase-key.jpg)\n\nThis is clearly O(lg _n_) due to following a simple path up the tree. Let's\nwork this example, where the element at _i_ has its key increased from 4 to\n15, and then it is propagated up:\n\n![](fig/Fig-6-5-heap-increase-key.jpg)\n\nThis propagation follows the \"Peter Principle\": the claim that persons in a\nhierarchical organization are promoted through the ranks of management until\nthey reach their level of incompetency!!!\n\n#### Inserting New Elements\n\nWhen inserting, we are going to have to make the heap bigger, so let's add the\nelement at the end and propagate it up to where it belongs.\n\nHEAP-INCREASE-KEY already has the code for this propagation, so if we set the\nkey to the smallest possible value and then try to increase it with HEAP-\nINCREASE-KEY, it will end up in the right place:\n\n![](fig/code-max-heap-insert.jpg)\n\nAgain, this is O(lg _n_).\n\n* * *\n\n## Next\n\nIn [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html) we\nwrap up our examination of sort algorithms with Quicksort, a practical sort\nthat performs well in practice and also illustrates the value of probabilistic\nanalysis and random algorithms.\n\nWe will return to other kinds of trees, in particular special kinds of binary\nsearch trees that are kept balanced to guarantee O(lg _n_) performance, in\n[Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sat Feb 15 16:37:46 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//090.heaps/reading-notes-9.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-9a.html</h2>

<pre>Hash
{"title"=>"Introduction to heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9a",
 "morea_summary"=>"Basic ideas about heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=0zh4IiKaVN0",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-9a.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-9a.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-9b.html</h2>

<pre>Hash
{"title"=>"Building heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9b",
 "morea_summary"=>"Understanding how to build heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=oAfSx7aRkZM",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-9b.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-9b.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-9c.html</h2>

<pre>Hash
{"title"=>"Analyzing heap building",
 "published"=>true,
 "morea_id"=>"reading-screencast-9c",
 "morea_summary"=>"Correctness and run time analysis of heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gMwtzAPDupI",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-9c.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-9c.md"}
</pre>

<h2>/morea/090.heaps/reading-screencast-9d.html</h2>

<pre>Hash
{"title"=>"Applications of heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9d",
 "morea_summary"=>"Heapsort and priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8O5iBigvDIw",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/090.heaps/reading-screencast-9d.html",
 "content"=>"",
 "path"=>"morea//090.heaps/reading-screencast-9d.md"}
</pre>

<h2>/morea/100.quicksort/experience-quicksort-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort (again)",
 "published"=>true,
 "morea_id"=>"experience-quicksort-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort (at home).",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/experience-quicksort-2.html",
 "url"=>"/morea/100.quicksort/experience-quicksort-2.html",
 "content"=>
  "### Quicksort\n\n#### 12 points\n\nShow the operation of Partition (not randomized) on this 1-based array:\n\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5], p=1, q=9 \n    \n\nand the two sub-partitions that result as directed below. In other words, you\nwill trace the three calls to Partition that are highest in the recursion\ntree. (They are _not_ the first three calls: #1 is the first call and #2 is\nthe second call, but the call marked as #3 below takes place after all the\nrecursive calls breaking down #2.)\n\nIn order to make the desired response format clear and to make it easy for the\nTA to grade, I am providing a template for your response. You are to fill in\nwherever the underscore character \"_\" appears. Use a plain text editor with\n_fixed-width font_. Be sure to fill in all fields marked with underscore: use\nsearch to make sure you get them all. I start you off with the first few\nlines: continue in the same pattern.\n\n    \n    \n     \n    **(#1) Call to Partition (A, 1, 9) made in Line 2 of the initial call to Quicksort:**\n    \n      Initially: \n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=0, j=1, pivot = A[r] = A[9] = 5 \n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=1, exchanged A[1] with A[1]\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=2, no exchange \n    \n      ... you fill in the rest until the loop exits ... \n    \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=3, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=4, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=5, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=6, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=7, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=8, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does Partition(A, 1, 9) return? __\n    \n    Continuing execution of the top level call to Quicksort, identify the two\n    partitions that will be handled by the recursive calls to Quicksort at\n    this level: \n    (#2) On what subarray will Quicksort in line 3 be called? A[_, _]\n    (#3) On what subarray will Quicksort in line 4 be called? A[_, _]\n    \n    Now trace these two calls in a manner similar to above. \n    \n    **(#2) Call to Partition(A, _, _) handled in the first call to Quicksort line 3: **\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this second call to Partition return? __\n    \n    **(#3) Call to Partition(A, _, _) handled in the first call to Quicksort line 4:**\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this third call to Partition return? __\n    \n\nNot graded, but you might think about:\n\n  * What pattern do you see in the second call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What pattern do you see in the third call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What do these observations tell us about the runtime of Quicksort on data organized as in these partitions? \n\n* * *\n\nDan Suthers Last modified: Wed Mar 19 23:22:39 HST 2014\n\n",
 "path"=>"morea//100.quicksort/experience-quicksort-2.md"}
</pre>

<h2>/morea/100.quicksort/experience-quicksort.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort",
 "published"=>true,
 "morea_id"=>"experience-quicksort",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/experience-quicksort.html",
 "url"=>"/morea/100.quicksort/experience-quicksort.html",
 "content"=>
  "### 1\\. Finding _i_th largest element with Partition\n\n#### 1-2 points TBD\n\nHow would you _use Quicksort's `Partition` procedure_ to write an algorithm\nfor _finding the _i_th smallest element of an unsorted array_? _Hint: what\ndoes the returned value of Partition tell you about the rank ordering of the\npivot?_\n\n  * Describe the strategy in English\n  * Then if you have time after finishing the next question, write pseudocode for an algorithm.\n\n### 2\\. Calls to Partition in Worst and Best Case\n\n#### 3-4 points TBD\n\nWhen we measure runtime efficiency in terms of _number of comparisons_ to be\nmade, Quicksort is Θ(_n_2) in the worst case (when the pivot is always chosen\nto be the smallest or largest element), and Θ(_n_ lg _n_) in the best case\n(when the pivot is always the median key). But we might also try to _measure\nefficiency in terms of number of calls to `Partition`_, since all the work is\ndone in there.\n\n**(a)**   Asymptotically, _how many calls to `Partition` are made in the **worst case runtime**_ as defined above (when the pivot is always chosen to be the smallest or largest element)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\n**(b)**   Asymptotically, _how many calls to `Partition` are made in the **best case runtime**_ as defined above (when the pivot is always the median key)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\nArgue for your conclusions!! _(One approach is to write and solve recurrence\nrelations. Another approach is to notice that the Quicksort recursion trees\nare binary, and use quantitative facts about binary trees. For a big-O rather\nthan Θ reply, a simple counting argument based on the pseudocode is possible,\nbut not as rigorous.)_\n\n![](fig/pseudocode-quicksort.jpg) ![](fig/pseudocode-quicksort-partition.jpg)\n\n\n",
 "path"=>"morea//100.quicksort/experience-quicksort.md"}
</pre>

<h2>/morea/100.quicksort/module-quicksort.html</h2>

<pre>Hash
{"title"=>"Quicksort",
 "published"=>true,
 "morea_id"=>"quicksort",
 "morea_outcomes"=>["outcome-quicksort"],
 "morea_readings"=>
  ["reading-screencast-10a",
   "reading-screencast-10b",
   "reading-screencast-10c",
   "reading-cormen-7",
   "reading-cormen-8",
   "reading-notes-10"],
 "morea_experiences"=>["experience-quicksort", "experience-quicksort-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/100.quicksort/module-quicksort.png",
 "morea_sort_order"=>100,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/100.quicksort/module-quicksort.html",
 "content"=>
  "Randomizing, lower bounds on comparison sorts, counting sort, radix sort, bucket sort.\n",
 "path"=>"morea//100.quicksort/module-quicksort.md"}
</pre>

<h2>/modules/quicksort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-quicksort.md",
 "title"=>"Quicksort",
 "url"=>"/modules/quicksort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/quicksort/index.html"}
</pre>

<h2>/morea/100.quicksort/outcome-quicksort.html</h2>

<pre>Hash
{"title"=>"Understand quicksort",
 "published"=>true,
 "morea_id"=>"outcome-quicksort",
 "morea_type"=>"outcome",
 "morea_sort_order"=>100,
 "referencing_modules"=>[#Jekyll:Page @name="module-quicksort.md"],
 "url"=>"/morea/100.quicksort/outcome-quicksort.html",
 "content"=>
  "Understand the quicksort algorithm and how it differs from mergesort.\n",
 "path"=>"morea//100.quicksort/outcome-quicksort.md"}
</pre>

<h2>/morea/100.quicksort/reading-cormen-7.html</h2>

<pre>Hash
{"title"=>"CLRS 7 - Quicksort",
 "published"=>true,
 "morea_id"=>"reading-cormen-7",
 "morea_summary"=>
  "Description and performance of quicksort, a randomized version, and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "20 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-cormen-7.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-cormen-7.md"}
</pre>

<h2>/morea/100.quicksort/reading-cormen-8.html</h2>

<pre>Hash
{"title"=>"CLRS 8 - Sorting in linear time",
 "published"=>true,
 "morea_id"=>"reading-cormen-8",
 "morea_summary"=>
  "Lower bounds for sorting, counting sort, radix sort, bucket sort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-cormen-8.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-cormen-8.md"}
</pre>

<h2>/morea/100.quicksort/reading-notes-10.html</h2>

<pre>Hash
{"title"=>"Notes on Quicksort",
 "published"=>true,
 "morea_id"=>"reading-notes-10",
 "morea_summary"=>"Notes on quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>10,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/100.quicksort/reading-notes-10.html",
 "url"=>"/morea/100.quicksort/reading-notes-10.html",
 "content"=>
  "## Notes on quicksort\n\n  1. Quicksort \n  2. Analysis of Quicksort \n  3. Lower Bound for Comparison Sorts \n  4. O(n) Sorts (briefly)\n\n### Motivations\n\nQuicksort, like Mergesort, takes a divide and conquer approach, but on a\ndifferent basis.\n\nIf we have done two comparisons among three keys and find that _x_ < _p_ and\n_p_ < _y_, do we ever need to compare _x_ to _y_? Where do the three belong\nrelative to each other in the sorted array?\n\nQuicksort uses this idea to partition the set of keys to be sorted into those\nless than the pivot _p_ and those greater than the pivot. (It can be\ngeneralized to allow keys equal to the pivot.) It then recurses on the two\npartitions.\n\n![](fig/quicksort-recursion.jpg)\n\nCompare this to Mergesort.\n\n  * Both take a recursive divide-and-conquer approach.\n  * Mergesort does its work on the way back up the recursion tree (merging), while Quicksort does its work on the way down the recursion tree (partitioning).\n  * Mergesort always partitions in half; for Quicksort the size of the partitions depends on the pivot (this results in Θ(_n_2) worst case behavior, but expected case remains Θ(_n_ lg _n_).\n  * Mergesort requires axillary arrays to copy the data; while as we shall see Quicksort can operate entirely within the given array: it is an **in-place sort**.\n\nQuicksort performs well in practice, and is one of the most widely used sorts\ntoday.\n\n### The Quicksort Algorithm\n\nTo sort any subarray A[_p_ .. _r_],   _p_ < _r_:\n\n**_Divide:_**\n    Partition A[_p_ .. _r_] into two (possibly empty) subarrays \n\n  * A[_p_ .. _q-1_], where every element is ≤ A[_q_]\n  * A[_q + 1_ .. _r_], where A[_q_] ≤ every element\n**_Conquer:_**\n    Sort the two subarrays by recursive calls\n**_Combine:_**\n    No work is needed to combine: all subarrays (including the entire array) are sorted as soon as recursion ends.\n\nAn array is sorted with a call to `QUICKSORT(A, 1, A.length)`:\n\n![](fig/pseudocode-quicksort.jpg)\n\nThe work is done in the PARTITION procedure. A[_r_] will be the pivot. (Note\nthat the _end_ element of the array is taken as the pivot. Given random data,\nthe choice of the position of the pivot is arbitrary; working with an end\nelement simplifies the code):\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nPARTITION maintains four regions.\n\n![](fig/Fig-7-2-partition-regions.jpg)\n\nThree of these are described by the following loop invariants, and the fourth\n(A[_j_ .. _r_-1]) consists of elements that not yet been examined:\n\n> **Loop Invariant:**\n\n>\n\n>   1. All entries in A[_p_ .. _i_] are ≤ pivot.\n\n>   2. All entries in A[_i_+1 .. _j_-1] are > pivot.\n\n>   3. A[_r_] = pivot.\n\n### Example Trace\n\nIt is worth taking some time to trace through and explain each step of this\nexample of the PARTITION procedure, paying particular attention to the\nmovement of the dark lines representing partition boundaries.\n\n![](fig/pseudocode-quicksort-partition.jpg) \n![](fig/quicksort-trace-1.jpg)\n\nContinuing ...\n\n![](fig/quicksort-trace-2.jpg)\n\nHere is the [Hungarian Dance version of\nquicksort](http://www.youtube.com/watch?v=kDgvnbUIqT4), in case that helps to\nmake sense of it!\n\n### Correctness\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nHere use the loop invariant to show correctness:\n\n  1. All entries in A[_p_ .. _i_] are ≤ pivot.\n  2. All entries in A[_i_+1 .. _j_ −1] are > pivot.\n  3. A[_r_] = pivot. \n\n**_Initialization:_**\n    Before the loop starts, _x_ is assigned the pivot A[_r_] (satisfying condition 3), and the subarrays a[_p_ .. _i_] and A[_i_+1 .. _j_−1] are empty (trivially satisfying conditions 1 and 2). \n**_Maintenance:_**\n    While the loop is running, \n\n  * if A[_j_] ≤ pivot, then _i_ is incremented, A[_j_] and A[_i_] are swapped, and _j_ is incremented. Because of the swap, A[_i_] ≤ _x_ for condition 1. The item swapped into A[_j_-1] > _x_ by the loop invariant, for condition 2.\n  * If A[_j_] > pivot, then _j_ is incremented, sustaining condition 2 (the others are unchanged), as the element added was larger\n**_Termination:_**\n    The loop terminates when _j_=_r_, so all elements in A are partitioned into one of three cases: A[_p_ .. _i_] ≤ pivot, A[_i_+1 .. _r_-1] > pivot, and A[_r_] = pivot. The last two lines fix the placement of A[_r_] by moving it between the two subarrays.\n\n* * *\n\n##  Informal Analysis\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nThe formal analysis will be done on a randomized version of Quicksort. This\ninformal analysis helps to motivate that randomization.\n\nFirst, PARTITION is Θ(_n_): We can easily see that its only component that\ngrows with _n_ is the `for` loop that iterates proportional to the number of\nelements in the subarray).\n\nThe runtime depends on the partitioning of the subarrays:\n\n### Worst Case\n\nThe worst case occurs when the subarrays are completely unbalanced, i.e.,\nthere are 0 elements in one subarray and _n_-1 elements in the other subarray\n(the single pivot is not processed in recursive calls). This gives a familiar\nrecurrence (compare to that for insertion sort):\n\n![](fig/analysis-quicksort-worst-recurrence.jpg)\n\nOne example of data that leads to this behavior is when the data is already\nsorted: the pivot is always the maximum element, so we get partitions of size\n_n_−1 and 0 each time. Thus, _quicksort is O(_n_2) on sorted data_. Insertion\nsort actually does better on a sorted array! (O(_n_))\n\n### Best Case\n\nThe best case occurs when the subarrays are completely balanced (the pivot is\nthe median value): subarrays have about _n_/2 elements. The reucurrence is\nalso familiar (compare to that for merge sort):\n\n![](fig/analysis-quicksort-best-recurrence.jpg)\n\n### Effect of Unbalanced Partitioning\n\nIt turns out that expected behavior is closer to the best case than the worst\ncase. Two examples suggest why expected case won't be that bad.\n\n#### Example: 1-to-9 split\n\nSuppose each call splits the data into 1/10 and 9/10. This is highly\nunbalanced: won't it result in horrible performance?\n\n![](fig/Fig-7-4-quicksort-1-9-recursion-tree.jpg)\n\nWe have log10_n_ full levels and log10/9_n_ levels that are nonempty.\n\nAs long as it's constant, the base of the log does not affect asymptotic\nresults. Any split of constant proportionality will yield a recursion tree of\ndepth Θ(lg _n_). In particular (using ≈ to indicate truncation of low order\ndigits),\n\n> log10/9_n_ = (log2_n_) / (log210/9)     _by formula 3.15_  \n            ≈ (log2_n_) / 0.152   \n            = 1/0.152 (log2_n_)  \n            ≈ 6.5788 (log2_n_)  \n            = Θ(lg _n_), where _c_ = 6.5788. \n\nSo the recurrence and its solution is:\n\n![](fig/analysis-quicksort-9-1-recurrence.jpg)\n\nA general lesson that might be taken from this: sometimes, even very\nunbalanced divide and conquer can be useful.\n\n#### Example: extreme cases cancel out\n\nWith random data there will usually be a mix of good and bad splits throughout\nthe recursion tree.\n\nA mixture of worst case and best case splits is asymptotically the same as\nbest case:\n\n![](fig/Fig-7-5-quicksort-unbalanced-splits.jpg)\n\nBoth these trees have the same two leaves. The extra level on the left hand\nside only increases the height by a factor of 2, and this constant disappears\nin the Θ analysis.\n\nBoth result in O(_n_ lg _n_), though with a larger constant for the left.\n\n* * *\n\n##  Randomized Quicksort\n\n![](fig/no-badguy.jpg)\n\nWe expect good average case behavior if all input permutations are equally\nlikely, but what if it is not?\n\nTo get better performance on sorted or nearly sorted data -- and to foil our\nadversary! -- we can randomize the algorithm to get the same effect as if the\ninput data were random.\n\nInstead of explicitly permuting the input data (which is expensive),\nrandomization can be accomplished trivially by **random sampling** of one of\nthe array elements as the pivot.\n\nIf we swap the selected item with the last element, the existing PARTITION\nprocedure applies:\n\n![](fig/pseudocode-randomized-quicksort.jpg)  \n![](fig/pseudocode-randomized-partition.jpg)\n\nNow, even an already sorted array will give us average behavior.\n\n_Curses! Foiled again!_\n\n* * *\n\n##  Quicksort Analysis\n\nThe analysis assumes that all elements are unique, but with some work can be\ngeneralized to remove this assumption (Problem 7-2 in the text).\n\n### Worst Case\n\nThe previous analysis was pretty convincing, but was based on an assumption\nabout the worst case. This analysis proves that our selection of the worst\ncase was correct, and also shows something interesting: we can solve a\nrecurrence relation with a \"max\" term in it!\n\nPARTITION produces two subproblems, totaling size _n_-1. Suppose the partition\ntakes place at index _q_. The recurrence for the worst case always selects the\nmaximum cost among all possible ways of splitting the array (i.e., it always\npicks the worst possible _q_):\n\n![](fig/analysis-quicksort-worst-1.jpg)\n\nBased on the informal analysis, we guess T(_n_) ≤ _cn_2 for some _c_.\nSubstitute this guess into the recurrence:\n\n![](fig/analysis-quicksort-worst-2.jpg)\n\nThe maximum value of _q_2 \\+ (_n_ \\- _q_ \\- 1)2 occurs when _q_ is either 0 or\n_n_-1 (the second derivative is positive), and has value (_n_ \\- 1)2 in either\ncase:\n\n![](fig/analysis-quicksort-worst-3.jpg)\n\nSubstituting this back into the reucrrence:\n\n![](fig/analysis-quicksort-worst-4.jpg)\n\nWe can pick _c_ so that _c_(2_n_ \\- 1) dominates Θ(_n_). Therefore, the worst\ncase running time is O(_n_2).\n\nOne can also show that the recurrence is Ω(_n_2), so worst case is Θ(_n_2).\n\n### Average (Expected) Case\n\nWith a randomized algorithm, expected case analysis is much more informative\nthan worst-case analysis.\n_[Why?](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10/why-\nexpected.txt)_\n\nThis analysis nicely demonstrates the use of indicator variables and two\nuseful strategies.\n\n#### Setup\n\nThe dominant cost of the algorithm is partitioning. PARTITION removes the\npivot element from future consideration, so is called at most _n_ times.\n\nQUICKSORT recurses on the partitions. The amount of work in each call is a\nconstant plus the work done in the `for` loop. We can count the number of\nexecutions of the `for` loop by counting the number of comparisons performed\nin the loop.\n\nRather than counting the number of comparisons in each call to QUICKSORT, it\nis easier to derive a bound on the number of comparisons across the entire\nexecution.\n\nThis is an example of a strategy that is often useful: **if it is hard to\ncount one way** (e.g., \"locally\"), **then count another way** (e.g.,\n\"globally\").\n\nLet _X_ be the total number of comparisons in all calls to PARTITION. The\ntotal work done over the entire execution is O(_n_ \\+ _X_), since QUICKSORT\ndoes constant work setting up _n_ calls to PARTITION, and the work in\nPARTITION is proportional to _X_. But what is _X_?\n\n#### Counting comparisons\n\nFor ease of analysis,\n\n  * Call the elements of A _z_1, _z_2, ... _z__n_, with _z__i_ being the _i_th smallest element. \n  * Define the set Z_ij_ = {_z__i_, _z__i_ \\+ 1, ... _z__j_} to be the set of elements between _z__i_ and _z__j_ inclusive. \n\nWe want to count the number of comparisons. Each pair of elements is compared\nat most once, because elements are compared only to the pivot element and then\nthe pivot element is never in any later call to PARTITION.\n\nIndicator variables can be used to count the comparisons. (Recall that we are\ncounting across all calls, not just during one partition.)\n\n> Let _Xij_ = I{ _zi_ is compared to _zj_ }\n\nSince each pair is compared at most once, the total number of comparisons is:\n\n![](fig/analysis-quicksort-expected-1.jpg)\n\nTaking the expectation of both sides, using linearity of expectation, and\napplying Lemma 5.1 (which relates expected values to probabilities):\n\n![](fig/lemming.jpg) ![](fig/analysis-quicksort-expected-2.jpg)\n\n#### Probability of comparisons\n\nWhat's the probability of comparing _z_i to _z_j?\n\nHere we apply another useful strategy: **if it's hard to determine when\nsomething happens, think about when it does _ not_ happen**.\n\nElements (keys) in separate partitions will not be compared. If we have done\ntwo comparisons among three elements and find that _zi_ < _x_ <_zj_, we do not\nneed to compare _zi_ to _zj_ (no further information is gained), and QUICKSORT\nmakes sure we do not by putting _zi_ and _zj_ in different partitions.\n\nOn the other hand, if either _zi_ or _zj_ is chosen as the pivot before any\nother element in Z_ij_, then that element (as the pivot) will be compared to\n_all_ of the elements of Z_ij_ except itself.\n\n  * The probability that _zi_ is compared to _zj_ is the probability that either is the first element chosen.\n  * Since there are _j_ \\- _i_ \\+ 1 elements in Z_ij_, and pivots are chosen randomly and independently, the probability that any one of them is chosen first is 1/(_j_ \\- _i_ \\+ 1). \n\nTherefore (using the fact that these are mutually exclusive events):\n\n![](fig/analysis-quicksort-expected-3.jpg)\n\nWe can now substitute this probability into the analyis of E[_X_] above and\ncontinue it:\n\n![](fig/analysis-quicksort-expected-4.jpg)\n\nThis is solved by applying equation A.7 for harmonic series, which we can\nmatch by substituting _k_ = _j_ \\- _i_ and shifting the summation indices down\n_i_:\n\n![](fig/analysis-quicksort-expected-5.jpg)\n\nWe can get rid of that pesky \"+ 1\" in the denominator by dropping it and\nswitching to inequality (after all, this is an upper bound analysis), and now\nA7 (shown in box) applies:\n\n![](fig/A7-Harmonic-Series.jpg) ![](fig/analysis-quicksort-expected-6.jpg)\n\nAbove we used the fact that logs of different bases (e.g., ln _n_ and lg _n_)\ngrow the same asymptotically.\n\nTo recap, we started by noting that the total cost is O(_n_ \\+ _X_) where _X_\nis the number of comparisons, and we have just shown that _X_ = O(_n_ lg _n_).\n\nTherefore, the _average running time of QUICKSORT on uniformly distributed\npermutations (random data)_ and the _expected running time of randomized\nQUICKSORT_ are both O(_n_ \\+ _n_ lg _n_) = **O(_n_ lg _n_)**.\n\nThis is the same growth rate as merge sort and heap sort. _Empirical studies\nshow quicksort to be a very efficient sort in practice (better than the other\n_n_ lg _n_ sorts) whenever data is not already ordered._ (When it is nearly\nordered, such as only one item being out of order, insertion sort is a good\nchoice.)\n\n* * *\n\n##  Lower Bound for Comparison Sorts\n\nWe have been studying sorts in which the only operation that is used to gain\ninformation is pairwise comparisons between elements. So far, we have not\nfound a sort faster than O(_n_ lg _n_).\n\nIt turns out it is not possible to give a better guarantee than O(_n_ lg _n_)\nin a comparison sort.\n\nThe proof is an example of a different level of analysis: of all _possible_\nalgorithms of a given type for a problem, rather than particular algorithms\n... pretty powerful.\n\n### Decision Tree Model\n\nA decision tree abstracts the structure of a comparison sort. A given tree\nrepresents the comparisons made by a specific sorting algorithm on inputs of a\ngiven size. Everything else is abstracted, and we count only comparisons.\n\n#### Example Decision Tree\n\nFor example, here is a decision tree for insertion sort on 3 elements.\n\n![](fig/decision-tree-insertion-sort.jpg)\n\nEach internal node represents a branch in the algorithm based on the\ninformation it determines by comparing between elements indexed by their\noriginal positions. For example, at the nodes labeled \"2:3\" we are comparing\nthe item that was originally at position 2 with the item originally at\nposition 3, although they may now be in different positions.\n\nLeaves represent permutations that result. For example, \"⟨2,3,1⟩\" is the\npermutation where the first element in the input was the largest and the third\nelement was the second largest.\n\nThis is just an example of one tree for one sort algorithm on 3 elements. Any\ngiven comparison sort has one tree for each _n_. The tree models all possible\nexecution traces for that algorithm on that input size: a path from the root\nto a leaf is one computation.\n\n#### Reasoning over All Possible Decision Trees\n\nWe don't have to know the specific structure of the trees to do the following\nproof. We don't even have to specify the algorithm(s): the proof works for any\nalgorithm that sorts by comparing pairs of keys. We don't need to know what\nthese comparisons are. Here is why:\n\n  * The root of the tree represents the unpermuted input data.\n  * The leaves of the tree represent the possible permuted (sorted) results.\n  * The branch at each internal node of the tree represents the outcome of a comparision that changes the state of the computation. \n  * The paths from the root to the leaves represent possible courses that the computation can take: to get from the unsorted data at the root to the sorted result at a leaf, the algorithm must traverse a path from the root to the correct leaf by making a series of comparisons (and permuting the elements as needed) \n  * The length of this path is the runtime of the algorithm on the given data.\n  * Therefore, if we can derive a lower bound on the height of _any_ such tree, we have a lower bound on the running time _any_ comparison sort algorithm. \n\n### Proof of Lower Bound\n\nWe get our result by showing that the number of leaves for a tree of input\nsize _n_ implies that the tree must have minimum height O(_n_ lg _n_). This\nwill be a lower bound on the running time of _any_ comparison sort algorithm.\n\n  * There are at least _n_! leaves because every permutation appears at least once (the algorithm must correctly sort every possible permutation): _l_ ≥ _n_! \n  * Any binary tree of height _h_ has _l_ ≤ 2_h_ leaves ([Notes #8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html))\n  * Putting these facts together:   _n_! ≤ _l_ ≤ 2_h_   or   2_h_ ≥ _n_!\n  * Taking logs:   _h_ ≥ lg(_n_!) \n  * Using Sterling's approximation (formula 3.17):   _n_! > (_n_/_e_)_n_\n  * Substituting into the inequality: \n\n> _h_   ≥   lg(_n_/_e_)_n_  \n    =   _n_ lg(_n_/_e_)  \n    =   _n_ lg _n_ \\- _n_ lg _e_   \n    =   Ω (_n_ lg _n_). \n\nThus, the height of a decision tree that permutes _n_ elements to all possible\npermutations cannot be less than _n_ lg _n_.\n\nA path from the leaf to the root in the decision tree corresponds to a\nsequence of comparisons, so there will always be some input that requires at\nleast O(_n_ lg _n_) comparisions in _any_ comparision based sort.\n\nThere may be some specific paths from the root to a leaf that are shorter. For\nexample, when insertion sort is given sorted data it follows an O(_n_) path.\nBut to give an o(_n_ lg _n_) guarantee (i.e, strictly better than O(_n_ lg\n_n_)), one must show that _ all_ paths are shorter than O(_n_ lg _n_), or that\nthe tree height is o(_n_ lg _n_) and we have just shown that this is\nimpossible since it is Ω(_n_ lg _n_).\n\n* * *\n\n##  O(n) Sorts\n\nUnder some conditions it is possible to sort data without comparing two\nelements to each other. If we know something about the structure of the data\nwe can sometimes achieve O(n) sorting. Typically these algorithms work by\nusing information about the keys themselves to put them \"in their place\"\nwithout comparisons. We only introduce these algorithms very briefly so you\nare aware that they exist.\n\n### Counting Sort\n\nAssumes (requires) that keys to be sorted are integers in {0, 1, ... _k_}.\n\nFor each element in the input, determines how many elements are less than that\ninput.\n\nThen we can place the element directly in a position that leaves room for the\nelements below it.\n\n![](fig/pseudocode-counting-sort.jpg)\n\nAn example ...\n\n![](fig/Fig-8-2-counting-sort-trace.jpg)\n\nCounting sort is a **stable sort**, meaning that two elements that are equal\nunder their key will stay in the same order as they were in the original\nsequence. This is a useful property ...\n\nCounting sort requires Θ(_n_ \\+ _k_). Since _k_ is constant in practice, this\nis Θ(_n_).\n\n### Radix Sort\n\n![](fig/320px-Punch_card_sorter.JPG)\n\nUsing a stable sort like counting sort, we can sort from least to most\nsignificant digit:\n\n![](fig/Fig-8-3-radix-sort-trace.jpg)\n\nThis is how punched card sorters used to work. _ (When I was an undergraduate\nstudent my University still had punched cards, and we had to do an assignment\nusing them mainly so that we would appreciate not having to use them!)_\n\nThe code is trivial, but requires a stable sort and only works on _n_ _d_-\ndigit numbers in which each digit can take up to _k_ possible values:\n\n![](fig/pseudocode-radix-sort.jpg)\n\nIf the stable sort used is Θ(_n_ \\+ _k_) time (like counting sort) then RADIX-\nSORT is Θ(_d_(_n_ \\+ _k_)) time.\n\n### Bucket Sort\n\nThis one is reminiscent of hashing with chaining.\n\nIt maps the keys to the interval [0, 1), placing each of the _n_ input\nelements into one of _n_-1 buckets. If there are collisions, chaining (linked\nlists) are used.\n\nThen it sorts the chains before concatenating them.\n\nIt assumes that the input is from a random distribution, so that the chains\nare expected to be short (bounded by constant length).\n\n![](fig/pseudocode-bucket-sort.jpg)\n\n#### Example:\n\nThe numbers in the input array A are thrown into the buckets in B according to\ntheir magnitude. For example, 0.78 is put into bucket 7, which is for keys 0.7\n≤ _k_ < 0.8. Later on, 0.72 maps to the same bucket: like chaining in hash\ntables, we \"push\" it onto the beginning of the linked list.\n\n![](fig/Fig-8-4-bucket-sort-trace.jpg)\n\nAt the end, we sort the lists (B shows the lists after they are sorted;\notherwise we would have 0.23, 0.21, 0.26) and then copy the values from the\nlists back into an array.\n\nBut sorting linked lists is awkward, and I am not sure why CLRS's pseudocode\nand figure imply that one does this. In an alternate implementation, steps 7-9\ncan be done simultaneously: scan each linked list in order, inserting the\nvalues into the array and keeping track of the next free position. Insert the\nnext value at this position and then scan back to find where it belongs,\nswapping if needed as in insertion sort.\n\nSince the values are already partially sorted, an insertion procedure won't\nhave to scan back very far. For example, suppose 0.78 had been inserted after\n0.72. The insertion would only have to scan over one item to put 0.78 in its\nplace, as all values in lists 0..6 are smaller.\n\n* * *\n\n## Comparing the Sorts\n\n![](fig/comparing-sorts.jpg)\n\nYou can also compare some of the sorts with these animations (set to 50\nelements): <http://www.sorting-algorithms.com/>. Do the algorithms make more\nsense now?\n\n* * *\n\n## Next\n\nWe return to the study of trees, with balanced trees.\n\n* * *\n\nDan Suthers Last modified: Wed Feb 19 02:14:38 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition, and from Wikipedia commons.  \n\n",
 "path"=>"morea//100.quicksort/reading-notes-10.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-10a.html</h2>

<pre>Hash
{"title"=>"Introduction to quicksort",
 "published"=>true,
 "morea_id"=>"reading-screencast-10a",
 "morea_summary"=>"Basic ideas about quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=v1ghdc_hwMI",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-10a.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-10a.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-10b.html</h2>

<pre>Hash
{"title"=>"Quicksort: Randomization and analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-10b",
 "morea_summary"=>"Randomizing and analyzing quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=qS9oMz4_kTU",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-10b.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-10b.md"}
</pre>

<h2>/morea/100.quicksort/reading-screencast-10c.html</h2>

<pre>Hash
{"title"=>"Bounds on sorting",
 "published"=>true,
 "morea_id"=>"reading-screencast-10c",
 "morea_summary"=>"Lower bounds on comparison sorts, and O(n) sorts",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gZmEYyqHefk",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/100.quicksort/reading-screencast-10c.html",
 "content"=>"",
 "path"=>"morea//100.quicksort/reading-screencast-10c.md"}
</pre>

<h2>/morea/110.balanced-trees/experience-balanced-trees-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of red-black trees",
 "published"=>true,
 "morea_id"=>"experience-balanced-trees-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about balanced trees (at home).",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/110.balanced-trees/experience-balanced-trees-2.html",
 "url"=>"/morea/110.balanced-trees/experience-balanced-trees-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### RBT and (2,4) Deletion\n\n**2\\. (6 pts)** Delete the keys **8, 6, 5, 3, 2, 1** in that order from the red/black tree shown below with its (2,4) representation. Following the format we used in class (see above), for each key:\n   \n(a) Show the RBT after the BST-style deletion but before RB-Delete-Fixup  \n(b) Identify (in writing) whether there is a **double black (identifying the node)**, corresponding to underflow. \n(c) Identify (in writing) the situation (the color of the sibling and its\nchildren) and its remedy (adjustment, recolor, and/or or restructure?), using\nthe cases in the web notes.  \n(d) Show the RBT after RB-Delete-Fixup  \n(e) Show the (2,4) tree representation that results.\n\nThe solution to Thursday's sequence of insertions is shown below, to be sure\nyou start with a correct tree. You may use a drawing program or just do it on\npaper in dark ink (or soft dark pencil that reproduces well) and scan or\nphotograph your work.\n\n![](fig/Problem-Set-6-Class-2014-Final.jpg)\n\n### Cormen et al. RBT Code\n\nThe lecture notes were based on Goodrich & Tamassia's textbook, because they\nshow the correspondence of RBTs to 2-4 trees, which makes the former easier to\nunderstand as balanced trees.\n\nThe CLRS version differs somewhat. Because of CLRS's reputation, I would tend\nto trust their version for an actual implementation, even if their\npresentation is more difficult to understand. Below are a few questions to\nhelp you understand the CLRS version.\n\nThe cases for insertion are similar between G&T and CLRS, but the terminology\ndiffers (e.g., what the letters w, x, y, and z refer to). The cases for\ndeletion differ: G&T have 3 while CLRS have 4! Be careful because there are\nmirror images of every situation (e.g., is the double black node a left child\nor a right child?): G&T and CLRS may be describing the same situation with\nmirror image graphs.\n\nThe top level methods in CLRS for RB-INSERT (p. 315) and RB-DELETE (p. 324)\nessentially do legal binary search three (BST) insertion and deletion, and\nthen call \"FIXUP\" methods to fix the red-black properties. Thus they are very\nsimilar to the BST methods TREE-INSERT (p. 294) and TREE-DELETE (p. 298). The\nreal work specific to RBTs is in these fixup methods, so we will focus on them\nin these questions, but you should also study the top level methods to\nunderstand them as BST methods.\n\n#### In RB-INSERT-FIXUP (p. 316):\n\n**3\\. (1 pt)** Which lines of the code handle incorrect representation of the 2-4 node (G&T case 1 in the web notes)? \n\n**4\\. (1 pt)** Which lines of the code handle overflow of the 2-4 node (G&T case 2 in the web notes)? \n\n#### In RB-DELETE-FIXUP (p. 326):\n\n**5\\. (1 pt)** _During the while loop of RB-DELETE-FIXUP, which line(s) of code remove double-black from node x?_ Note: do NOT answer \"line 23\" as this is outside the while loop: I am asking how double black moves up the tree inside the while loop. You will need to read the text: the code doesn't make it obvious. But once you have understood this, the cases in figure 13.7 will be easier to understand for the next question. \n\n**6\\. (1 pt)** G&T deletion has three cases (see web notes) while CLRS deletion has four (see figure 13.7 and explanation in the text). Two of the G&T cases correspond to the CLRS cases. Which G&T deletion cases map directly to which CLRS deletion cases? (Give two pairs). Explain why. \n\nThe correspondence between the other cases is harder to understand in a simple\nway. If you see it, you are welcome to try to explain it.\n\n* * *\n\nDan Suthers Last modified: Sat Mar 1 03:35:53 HST 2014\n\n",
 "path"=>"morea//110.balanced-trees/experience-balanced-trees-2.md"}
</pre>

<h2>/morea/110.balanced-trees/experience-balanced-trees.html</h2>

<pre>Hash
{"title"=>"Gaining insight into Red-Black tree operations",
 "published"=>true,
 "morea_id"=>"experience-balanced-trees",
 "morea_type"=>"experience",
 "morea_summary"=>"Play with insertion and deletion",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/110.balanced-trees/experience-balanced-trees.html",
 "url"=>"/morea/110.balanced-trees/experience-balanced-trees.html",
 "content"=>
  "## In Class Thursday\n\nInsert the keys **6, 5, 4, 2, 3, 1** in that order into a Red-Black Tree\nrepresentation of a (2,4) balanced tree. For each key:  \n\n(a) Show the RBT after the insertion but before RB-Insert-Fixup  \n\n(b) Identify (in writing) the RBT property that is violated and whether that\nviolation corresponds to overflow in or incorrect representation of the\ncorresponding (2,4) tree (see next item to help you decide this).  \n\n(c) Identify (in writing) the situation (red uncle/sibling or black\nuncle/sibling?) and its remedy (recolor or restructure?).  \n\n(d) Show the RBT after RB-Insert-Fixup  \n\n(e) Show the (2,4) tree representation that results.  \n\nThe first three keys have been inserted to get you started.\n\n![](fig/Problem-Set-6-Class-2014-Start-a.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-b.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-c.jpg)  \n![](fig/Problem-Set-6-Class-2014-Start-d.jpg)  \n\n",
 "path"=>"morea//110.balanced-trees/experience-balanced-trees.md"}
</pre>

<h2>/morea/110.balanced-trees/module-balanced-trees.html</h2>

<pre>Hash
{"title"=>"Balanced Trees",
 "published"=>true,
 "morea_id"=>"balanced-trees",
 "morea_outcomes"=>["outcome-balanced-trees-algorithm"],
 "morea_readings"=>
  ["reading-screencast-11a",
   "reading-screencast-11b",
   "reading-screencast-11c",
   "reading-screencast-11d",
   "reading-cormen-13",
   "reading-sedgewick-15",
   "reading-notes-11"],
 "morea_experiences"=>
  ["experience-balanced-trees",
   "experience-balanced-trees-2",
   "experience-project-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/110.balanced-trees/module-balanced-trees.png",
 "morea_sort_order"=>110,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/110.balanced-trees/module-balanced-trees.html",
 "content"=>
  "(2,4) trees, red-black trees, insertion, deletion, rotations, comparison of dictionary implementations.\n",
 "path"=>"morea//110.balanced-trees/module-balanced-trees.md"}
</pre>

<h2>/modules/balanced-trees/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-balanced-trees.md",
 "title"=>"Balanced Trees",
 "url"=>"/modules/balanced-trees/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/balanced-trees/index.html"}
</pre>

<h2>/morea/110.balanced-trees/outcome-balanced-trees-algorithm.html</h2>

<pre>Hash
{"title"=>"Understand the balanced tree algorithm",
 "published"=>true,
 "morea_id"=>"outcome-balanced-trees-algorithm",
 "morea_type"=>"outcome",
 "morea_sort_order"=>110,
 "referencing_modules"=>[#Jekyll:Page @name="module-balanced-trees.md"],
 "url"=>"/morea/110.balanced-trees/outcome-balanced-trees-algorithm.html",
 "content"=>
  "Be able to step through insertion and deletion procedures for red-black trees. \n",
 "path"=>"morea//110.balanced-trees/outcome-balanced-trees-algorithm.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-cormen-13.html</h2>

<pre>Hash
{"title"=>"CLRS 13 - Red-Back Trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-13",
 "morea_summary"=>"Properties, rotations, insertion, and deletion",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "31 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-cormen-13.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-cormen-13.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-notes-11.html</h2>

<pre>Hash
{"title"=>"Notes on balanced trees",
 "published"=>true,
 "morea_id"=>"reading-notes-11",
 "morea_summary"=>"Balanced trees and operations on them",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/110.balanced-trees/reading-notes-11.html",
 "url"=>"/morea/110.balanced-trees/reading-notes-11.html",
 "content"=>
  "## Outline\n\n  * Balanced and Multi-Way Trees \n  * 2-3-4 or (2,4) Trees \n  * Red-Black Trees \n    * as a binary representation of (2,4) trees\n    * as binary search trees\n  * Insertion in Red-Black Trees\n  * Deletion in Red-Black Trees\n  * Comparison of Dictionary Implementations\n\n_(Much of this material is derived from Goodrich & Tamassia's slides widely\navailable on the Web.)_\n\n* * *\n\n##  Multi-Way Trees\n\nA **multi-way search tree** is an ordered tree such that\n\n  * Each internal node has at least two children and stores _d_-1 key-element items (_ki_, _oi_), where _d_ is the number of children\n  * For a node with children _v1_ _v2_ ... _vd_ storing keys _k1_ _k2_ ... _kd-1_\n    * keys in the subtree of _v1_ are less than _k1_\n    * keys in the subtree of _vi_ are between _ki-1_ and _ki_ (_i_ = 2, ..., _d_ \\- 1)\n    * keys in the subtree of _vd_ are greater than _kd-1_\n  * The leaves store no items and serve as placeholders\n\n![](fig/GT-multiway-search-tree.jpg)\n\n**Multi-way inorder traversal** can be defined by extension of BST inorder traversal to visit the keys in increasing order:\n\n> Visit item (_ki_, _oi_) of node _v_ between the recursive traversals of the\nsubtrees of _v_ rooted at children _vi_ and _vi+1_.\n\n![](fig/GT-multiway-inorder-traversal.jpg)\n\n**Searching** can similarly be extended to multi-way trees by searching within each node as well as down the tree:\n\n  * At each internal node with children _v1_ _v2_ ... _vd_ and keys _k1_ _k2_ ... _kd-1_: \n    * _k_ = _ki_ (_i_ = 1, ... , _d_ \\- 1): the search terminates with success\n    * _k_ < _k1_: we continue the search in child _v1_\n    * _ki-1_ < _k_ < _ki_ (_i_ = 2, ... , _d_ \\- 1): we continue the search in child _vi_\n    * _k_ > _kd-1_: we continue the search in child _vd_\n  * Reaching an external node terminates the search unsuccessfully\n\nFor example, searching for key 30:\n\n![](fig/GT-multiway-searching.jpg)\n\n* * *\n\n## (2,4), 2-4 or 2-3-4 Trees\n\nThese are multi-way trees restricted in two ways:\n\n  * **Node Size Property**: every internal node has at least two children (one key) and at most four children (three keys).\n  * **Depth Property**: all of the external nodes have the same depth. (The tree is balanced.)\n\nThe internal nodes are called 2-nodes, 3-nodes or 4-nodes, depending on the\nnumber of children they have.\n\n![](fig/GT-2-4-trees.jpg)\n\n### Height of (2,4) Trees and Searching\n\n**_Theorem:_ A (2,4) tree storing n items has height Θ(log n).**\n\nProof:\n\nLet _h_ be the height of a (2,4) tree with _n_ items. The tallest possible\ntree (worst case) for a fixed _n_ is when all internal nodes are 2-nodes\n(i.e., the tree is equivalent to a binary tree), so we restrict consideration\nto this case. Due to the depth property, the tree at depth _h_-1 is filled, so\nit is a complete binary tree.\n\n![](fig/GT-2-4-tree-height.jpg)\n\nThe figure illustrates the number of nodes in each level of a complete binary\ntree. Since there are at least 2_i_ items at depth _i_ = 0, ... , _h_-1 and no\nitems at depth _h_ (the leaves store no items):  \n    _n_ ≥ 1 + 2 + 4 + ... + 2_h_-1  \n(we use ≥ because there could be more items in internal 3-nodes or 4-nodes,\nleading to \"better cases\" where _n_ increases without a penalty in _h_).\n\n![](fig/formula-A-5.jpg)\n\nApplying formula A5 (shown) for geometric series with _n_ = _h_-1 and _x_ = 2,  \n1 + 2 + 4 + ... + 2_h_-1 = Σ_k_=0,_h_-12_k_ = (2(h-1) + 1 \\- 1)/(2 - 1) = 2_h_\n\\- 1, so  \n    _n_ ≥ = 2_h_ \\- 1   or   n + 1 ≥ 2_h_\n\nTaking the log of both sides:   lg (_n_ \\+ 1) ≥ _h_.  \nThus, _h_ = Θ(lg _n_).\n\n(See also similar facts concerning full binary trees in [Topic\n8](http://www2.hawaii.edu/~suthers/courses/ics311f12/Notes/Topic-08.html).)\n\nSince searching in a (2,4) tree with _n_ items requires time proportional to a\npath from root to leaves, searching is **O(lg _n_)** time.\n\n### (2,4) Tree Insertion\n\nWe will examine insertion and deletion briefly to understand the conceptual\ncases.\n\nInsert a new item keyed by _k_ into _(not below)_ the parent of the leaf\nreached by searching for _k_. (_In this respect, (2,4) trees differ from\nbinary search trees._)\n\nThis preserves depth but may cause **overflow** (a node may become a 5-node).\n\n_Example:_ Inserting 30, we find its position between 27 and 32. However\ninserting here causes overflow:\n\n![](fig/GT-2-4-tree-insertion.jpg)\n\nOverflow is handled with a **split operation**, as illustrated below with a\nsimpler tree:\n\n  * The 5-node containing keys _k1_, _k2_, _k3_, _k4_ is split into a 3-node with keys _k1_, _k2_ and a 2-node with key _k4_.\n  * Key _k3_ is inserted into the parent node (as would be the case with the tree above).\n  * Overflow may propagate to the parent node.\n  * A new root may be created if the root overflows.\n\n![](fig/GT-2-4-tree-overflow-split.jpg)\n\n_(Note: Sedgewick splits 4-nodes on the way down while searching for the\ninsertion position, guaranteeing that there will be no overflow. Both Goodrich\n& Tamassia and Cormen et al. take the other approach, propagating splits\nupwards only as needed. The asymptotic time complexity remains the same.)_\n\n#### Time Complexity of (2,4) Insertion\n\nA tree with _n_ items has Θ(lg _n_) height. The algorithm first searches for\nthe insertion location, which may require visiting _h_ = Θ(lg _n_) nodes (Θ,\nnot O, because we must go to the leaves in all cases). The insertion takes\nΘ(1) time. If there is overflow, splits (taking Θ(1) time each) may be\npropagated upwards to as many as O(lg _n_) nodes. Since the Θ(lg _n_)\noverrides the possibility of slower growing functions in O(lg _n_), insertion\nis **Θ(lg _n_)**.\n\n### (2,4) Tree Deletion\n\nIf the entry to be deleted is in a node that has internal nodes as children,\nwe replace the entry to be deleted with its inorder successor and delete the\nlatter entry. Example: to delete key 24, we replace it with 27 (inorder\nsuccessor):\n\n![](fig/GT-2-4-tree-deletion.jpg)\n\nThis reduces deletion of an entry to the case where the item is at the node\nwith leaf children.\n\nDeletion of an entry from a node _v_ may cause **underflow,** where node _v_\nbecomes a 1-node with one child and no keys. Underflow at node _v_ with parent\n_u_ is handled in two cases.\n\n**_Case 1_**: An adjacent sibling of _v_ is a 2-node. Perform a **fusion operation**, merging _v_ with the adjacent 2-node sibling _w_ and moving an entry from _u_ to the merged node _v'_.\n\n![](fig/GT-2-4-tree-underflow-fusion.jpg)\n\nAfter a fusion, the underflow may propagate to the parent u, for at most O(lg\n_n_) adjustments up the tree.\n\n**_Case 2_**: An adjacent sibling _w_ of _v_ is a 3-node or a 4-node. Perform a **transfer operation:** move a child of _w_ to _v_; an item from _u_ to _v_; and an item from _w_ to _u_. \n\n![](fig/GT-2-4-tree-underflow-transfer.jpg)\n\nA transfer eliminates underflow.\n\n#### Time Complexity of (2,4) Deletion\n\nThe algorithm first searches for the item to delete, which requires visiting\n_h_ = Θ(lg _n_) nodes on the way down the tree, either to find a bottom level\nkey to delete, or to find the successor of a key in an internal node to\ndelete. Underflow is handled with up to O(lg _n_) fusions and transfers, each\ntaking Θ(1) time. Thus deletion is **Θ(lg _n_)**.\n\n* * *\n\n##  Red-Black Trees\n\n### Red-Black Tree Properties\n\n![](fig/Simple-Red-Black-Tree.jpg)\n\nA red-black tree (RBT) is a binary search tree with the following additional\nproperties:\n\n  1. **Color property**: Every node is either red or black. _(We can indicate this either by coloring the node or by coloring its parent link.)_\n  2. **Root property**: The root is black\n  3. **External property**: Every leaf is black.\n  4. **Internal property**: If a node is red, then both of its children are black. _(Hence, no two reds in a row are allowed on a simple path from the root to a leaf.)_\n  5. **Depth property**: For each node, all the paths from the node to descendant leaves contain the same number of black nodes (the **black height** of the node).\n\nThese properties seem rather arbitrary until we consider the correspondence\nwith (2,4) trees shortly, but first let's see how the properties hold in an\nexample ...\n\n![](fig/Fig-13-1-RBT-Representation-a.jpg)\n\n### Red-Black Tree Representation\n\nA single extra bit is required on each node to mark it as \"red\" or \"black\".\n\nTo save space, we can represent the leaf nodes _and_ the parent with a single\nnode, T.nil:\n\n![](fig/Fig-13-1-RBT-Representation-b.jpg)\n\nThis also simplifies the code, as we can follow pointers without having to\ncheck for null pointers.\n\nWe usually don't draw T.nil:\n\n![](fig/Fig-13-1-RBT-Representation-c.jpg)\n\n### RBTs as a Binary Representation of (2,4) Trees\n\nIt would be rather complex to implement and manipulate 2-nodes, 3-nodes and\n4-nodes. One motivation for red-black trees is that they provide a binary tree\nrepresentation of (2,4) trees, enabling us to manipulate only one kind of\nnode. The mapping is as follows (__you should make sure you understand this\nwell before going on!__):\n\n![](fig/GT-From-2-4-to-RBT.jpg)\n\n**Red nodes (and the links from their parents) capture the _internal structure of a (2,3) node_;**\n\n**Black nodes (and the links from their parents) capture the _structure of the (2,3) tree_ itself.**\n\n### RBTs as Binary Search Trees\n\nAt the same time as they represent (2,4) trees, _**RBTs are also Binary Search\nTrees**_: they satisfy the Binary Search Tree property. For example, here is a\nRBT: we can search for keys or enumerate elements in order as usual:\n\n![](fig/GT-RBTs.jpg)\n\nIn order to maintain the Red-Black-Tree properties, it will be necessary to do\nstructural rotations. These rotations are designed to not disrupt the BST\nproperty. For example, this rotation does not disturb the BST ordering of keys\n9, 11, 12, 14, 17, 18, 19:\n\n![](fig/Fig-13-3-Rotation-BST.jpg)\n\n#### Height of Red-Black Trees and Searching\n\nTheorem: A red-black tree storing n items has height Θ(lg _n_).  \nProof:\n\n  * Let _h_ be the height of a red-black tree with _n_ items\n  * By property 4, there cannot be more red nodes (and links) on a simple path from the root to a leaf than there are black nodes (and links). \n  * Therefore the black height of the root of the tree is between _h_ and _h_/2. \n  * The black height of the root of the red-black tree corresponds to the height _h'_ of the (2,4) tree that the red-black tree represents (since red nodes/links in the RBT represent the internal structure of the nodes in the (2,4) tree). \n  * From the theorem concerning the height of (2,4) trees, _h'_ is Θ(lg _n_). \n  * Since _h_ is no more than twice _h'_, _h_ is also Θ(lg _n_).\n\n(See Cormen et al. for a proof not relying on (2,4) trees.)\n\nTherefore, searching in a red-black tree with _n_ items takes **O(lg _n_)**\ntime (O rather than Θ as we may find the key in an internal node).\n\nWe now consider insertion and deletion. Please see the textbook for the many\ndetails of implementation in pseudocode, etc.: here we will concentrate on\nseeing how the RBT operations correspond to (2,4) tree operations.\n\n###  Insertion in Red-Black Trees\n\nTo insert an element with key _k_, perform the insertion for binary search\ntrees (except that conceptually we insert _k_ in an internal node with null\nchildren, not at a leaf node), and color the newly inserted node _z_ red,\nunless it is the root.\n\nThis preserves the color, root, external, and depth properties. _(You should\ncheck this in the example below.)_\n\nIf the parent _v_ of _z_ is black, this also preserves the internal property\nand we are done.\n\nElse (_v_ is red), we have a **double red** (i.e., a violation of the internal\nproperty), which requires a reorganization of the tree. For example, insert 4:\n\n![](fig/GT-RBT-insertion.jpg)\n\nA double red with child _z_ and parent _v_ is dealt with in two cases. Let _w_\nbe the sibling of _v_ (and hence the uncle of _z_).\n\n**_Case 1:_** _w_ is black. The double red is an _**incorrect representation**_ of a 4-node. (We will fix this with restructuring). For example, the RBT on the left is an incorrect representation of the (2,4) tree on the right:\n\n![](fig/GT-RBT-double-red-case-1.jpg)\n\n**_Case 2:_** w is red. The double red corresponds to an _**overflow**_ in the (2,4) tree. (We will fix this with recoloring, which is the equivalent of a (2,4) split.) For example:\n\n![](fig/GT-RBT-double-red-case-2.jpg)\n\n#### Restructuring\n\n**Restructuring** remedies a child-parent double red when the parent red node has a black sibling. It restores the correct representation (internal property) of a 4-node, leaving other RBT and BST properties intact: \n\n![](fig/GT-RBT-restructuring.jpg)\n\nThere are four restructuring configurations depending on whether the double\nred nodes are left or right children. They all lead to the same end\nconfiguration of a black with two red children:\n\n![](fig/GT-RBT-restructuring-configurations.jpg)\n\nAfter a restructuring, the double red has been remedied without violating any\nof the other properties _(you should verify this)_: there is no need to\npropagate changes upwards.\n\nNotice that the height of the subtree tree has been reduced by one. **_This is\nthe operation that keeps the trees balanced to within a constant factor of\nlg(_n_) height_**, by ensuring that the height of the RBT is no more than\ntwice that of the (necessarily balanced) 2-4 tree it represents. _Do you see\nwhy?_\n\n#### Recoloring\n\n**Recoloring** remedies a child-parent double red when the parent red node has a red sibling. The parent _v_ and its sibling _w_ become black and the grandparent _u_ becomes red, unless it is the root.\n\nIt is equivalent to performing a split on a 5-node in a (2,4) tree. (When\nthere is a double red and yet another red in the parent's sibling, we are\ntrying to collect too many keys under the grandparent.) For example, the RBT\nrecoloring on the top corresponds to the (2,4) transformation on the bottom:\n\n![](fig/GT-RBT-recoloring.jpg)\n\nNotice that in this example the parent \"4\" is now red, meaning it belongs to\nits parent node in the (2,4) tree. The double red violation may propagate to\nthis parent in the RBT, which corresponds to the overflow propagating up the\n(2,4) tree, requiring further repair.\n\n#### Time Complexity of RBT Insertion\n\nWe already established that insertion in (2,4) trees is Θ(lg _n_) due to their\nheight. Since RBTs are only at most twice as high, we might expect this result\nto transfer, and it does, but it needs to be shown separately since the\nmanipulations of the RBT are different. So:\n\n  * The algorithm first searches for the insertion location, which will require visiting _h_ = Θ(lg _n_) nodes on the way down the tree (since we are searching for a leaf node and the tree is balanced).\n  * Adding the item takes O(1). \n  * Recolorings and restructurings are Θ(1) each, and we perform at most O(lg _n_) recolorings and _one_ restructuring propagating structural changes back up the tree.\n\nThus insertion is **Θ(lg _n_).**\n\nNote: A top-down version of this algorithm is also possible, restructuring on\nthe way down and requiring only one pass through the tree. See the Sedgewick\nreading distributed.\n\n###  Deletion in Red-Black Trees\n\nTo remove item with key _k_, we first perform the BST deletion (modified for\nour representational changes using T.nil).\n\nBecause deletion of a node higher in the tree involves replacing it with its\nsuccessor, which is then deleted, deletion always involves an internal and an\nexternal node.\n\nWe can preserve the RBT properties at the new internal location of the\nsuccessor by giving the successor the color of the node deleted, so we need\nonly be concerned with possible violations of RBT properties at the bottom of\nthe tree, where the successor was moved from, or where a node without a\nsuccessor was deleted.\n\nLet _v_ be the internal node removed, _w_ the external node removed, and _r_\nthe sibling of _w_:\n\n    \n    \n        x       \n         \\               x                    \n          v       ==>     \\\n         / \\               r \n        r   w\n    \n\nIf either _v_ or _r_ was red, we color _r_ black and we are done (the number\nof black nodes has not changed).\n\nElse (_v_ and _r_ were both black), we have removed a black node, violating\nthe depth property. We fix this by coloring _r_ **double black,** a fictional\ncolor. (Intuitively, the black of both _v_ and _r_ have been absorbed into\n_r_.) Now we have the correct \"amount\" of black on this path from root to\nleaf, but the double black violates the color property.\n\nFixing this will require a reorganization of the tree. Example: deletion of 8\ncauses a double black:\n\n![](fig/GT-RBT-deletion-double-black.jpg)\n\nA double black corresonds to _**underflow**_ in (2,4) trees (and here the\nimages I am borrowing from Goodrich & Tamassia go to greyscale!):\n\n![](fig/GT-double-black-as-underflow.jpg)\n\nGoodrich & Tamassia's algorithm for remedying a double black node _w_ with\nsibling _y_ considers _three cases_, discussed below. _(Note that these are\ndifferent from CLRS's four cases!)_\n\n**_Case 1:_** _y_ is black and has a red child: Perform a RBT **restructuring**, equivalent to a (2,4) **transfer**, and we are done.\n\nFor example, if we have the RBT on the left corresponding to underflow in the\n(2,4) tree on the right:\n\n![](fig/GT-RBT-DB-remedy-case-1-1.jpg)\n\n... we do the following restructuring:\n\n![](fig/GT-RBT-DB-remedy-case-1-2.jpg)\n\n**_Case 2:_** _y_ is black and its children are both black: Perform a RBT **recoloring**, equivalent to a (2,4) **fusion**, which may propagate up the double black violation.\n\nIf the double-black reaches the root we can just remove it, as it is now on\n_all_ of the paths from the root to the leaves, so does not affect property 5,\nthe depth property.\n\nFor example, if we have the RBT on the left corresponding to underflow in the\n(2,4) tree on the right:\n\n![](fig/GT-RBT-DB-remedy-case-2-1.jpg)\n\n... we do the following recoloring: the black node _y_ is colored red, and the\ndouble black node _r_ is colored ordinary black:\n\n![](fig/GT-RBT-DB-remedy-case-2-2.jpg)\n\nThe root of the above subtree takes on an extra black, which propagates only\nif it was previously black and is not the root. If it was red it merely turns\nblack; if it was the root the extra black no longer affects the balanced black\nheight of the tree.\n\n**_Case 3:_** _y_ is red: Perform a RBT **adjustment**, equivalent to choosing a different representation of a 3-node, after which either Case 1 or Case 2 applies.\n\n![](fig/GT-RBT-DB-remedy-case-3.jpg)\n\nThese are both representations of the following 2-4 tree, but the\ntransformation allows one of the other cases to apply, reducing duplication of\ncases.\n\n![](fig/GT-RBT-DB-remedy-case-3-2-4.jpg)\n\nThe CLRS chapter divides the situation up into four cases: try to see whether\nyou can map between the above cases and theirs!\n\n#### Time Complexity of RBT Deletion\n\nThe analysis is similar to the previous ones: Θ(lg _n_) search to find the\ndeletion point (the item to delete may be in an internal node, but we always\nfind its successor in any case, which is at the bottom of the tree), followed\nby deletion and restructuring O(1) operations that are propagated at most up\nO(lg _n_) levels. Deletion is **Θ(lg _n_)**.\n\n### RBT Animations\n\nYou may want to look at these:\n\n<http://secs.ceas.uc.edu/~franco/C321/html/RedBlack/redblack.html>\n\n    A java applet. You can go step by step and it tells you the rules violated and the fixes. Must click on \"next step\" until done with process. To delete, click on Delete and then on the node to be deleted.\n<http://www.csanimated.com/animation.php?t=Red-black_tree>\n\n     A flash animation: slides with voice-over. It goes kind of fast (little time to figure out what property is being fixed in each case), and does not let you control slide by slide. \n\n* * *\n\n## Related Data Structures\n\n**AVL Trees,** named for their authors, are the oldest balanced trees. They are binary trees with the requirement that the heights of the left and right subtree of any given node differ at most by 1\\. A small amount of extra storage is needed to record height differences. Their operations are O(lg _n_) like RBTs, but may require O(lg _n_) rotations to rebalance. \n\n**Splay Trees** are binary trees in which an adjustment moving a node towards the root called _splaying_ is done after every access (including search). There are no rules about properties to maintain and no labels. Amazingly, splaying alone is enough to guarantee O(lg _n_) behavior in an amortized sense: we will use these as an example when we cover chapter 17 Amortized analysis. They also make frequently accessed items more accessible. \n\n**B-Trees,** covered in Chapter 18 of Cormen et al. (but not in this course), are balanced multi-way trees that allow up to M keys per node for large M. They are used for trees in external (disk) storage, where speed is optimized by making the size of a node be the same as the size of a block read in by one disk read.\n\n* * *\n\n##  Comparison of Dictionary Implementations\n\nFirst, here is a summary of the correspondence between (2,4) and Red-Black\ntree operations:\n\n<table width=\"100%\" border=\"1\">\n  <tr>\n    <th colspan=\"3\" scope=\"col\"><div align=\"left\">Insertion: Remedy double red</div></th>\n  </tr>\n  <tr>\n    <th scope=\"row\"><div align=\"left\">(2,4) tree action</div></th>\n    <td><div align=\"left\"><strong>Red-Black Tree Action</strong></div></td>\n    <td><div align=\"left\"><strong>Result</strong></div></td>\n  </tr>\n  <tr>\n    <td>Change of 4-node representation</td>\n    <td>Restructuring</td>\n    <td>Double red removed</td>\n  </tr>\n  <tr>\n    <td>Split</td>\n    <td>Recoloring</td>\n    <td>Double red removed or propagated up</td>\n  </tr>\n  <tr>\n    <th colspan=\"3\" scope=\"row\">&nbsp;</th>\n  </tr>\n  <tr>\n    <th colspan=\"3\" scope=\"row\"><div align=\"left\">Deletion: Remedy double black</div></th>\n  </tr>\n  <tr>\n    <th scope=\"row\"><div align=\"left\">(2,4) tree action</div></th>\n    <td><strong>Red-Black Tree Action</strong></td>\n    <td><strong>Result</strong></td>\n  </tr>\n  <tr>\n    <td>Transfer</td>\n    <td>Restructuring</td>\n    <td>Double black removed</td>\n  </tr>\n  <tr>\n    <td>Fusion</td>\n    <td>Recoloring</td>\n    <td>Double black removed or propagated up</td>\n  </tr>\n  <tr>\n    <td>Change of 3-node representation</td>\n    <td>Adjustment</td>\n    <td>Restructuring or recoloring follows</td>\n  </tr>\n</table>\n\n\n### A comparison of run times.\n\n<table width=\"100%\" border=\"1\">\n  <tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Search</th>\n    <th scope=\"col\">Insert</th>\n    <th scope=\"col\">Delete</th>\n    <th scope=\"col\">Notes</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">Hash Table</th>\n    <td>O(1) expected</td>\n    <td>O(1) expected</td>\n    <td>O(1) expected</td>\n    <td><p>No ordered dictionary methods. Simple to implement.</p>\n    </td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Doubly Linked List</th>\n    <td>O(<i>n</i>)</td>\n    <td>O(1) if not sorted; O(<i>n</i>) if sorted </td>\n    <td>&Theta;(1) if node given, O(<i>n</i>) otherwise</td>\n    <td>Simple to implement.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Skip List</th>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>O(lg <i>n</i>) with high probability</td>\n    <td>Randomized insertion. Simple to implement.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Binary Tree</th>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>O(<i>n</i>) worst case, O(lg <i>n</i>) random </td>\n    <td>Moderately complex to implement deletion.</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">Red-Black Tree</th>\n    <td>O(lg <i>n</i>) worst case</td>\n    <td>&Theta;(lg <i>n</i>)</td>\n    <td>&Theta;(lg <i>n</i>) </td>\n    <td>Complex to implement.</td>\n  </tr>\n</table>\n\n\nFrom this we can see that hash tables are most efficient expected behavior\nwhen no ordered methods are needed, and red-black trees give us the best\nguarantee when ordering matters.\n\n* * *\n\nDan Suthers Last modified: Mon Mar 3 20:13:52 HST 2014  \nImages are from lecture slides provided by Michael Goodrich and Roberto\nTamassia, and from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//110.balanced-trees/reading-notes-11.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-11a.html</h2>

<pre>Hash
{"title"=>"Introduction to (2,4) Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11a",
 "morea_summary"=>"Basic ideas about balanced trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=N-Sot-yf3As",
 "morea_labels"=>["Screencast", "Suthers", "11 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-11a.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-11a.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-11b.html</h2>

<pre>Hash
{"title"=>"Insertion and deletion in (2,4) Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11b",
 "morea_summary"=>"Conceptual overview of balanced tree operations",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=W49P7wqdIuE",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-11b.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-11b.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-11c.html</h2>

<pre>Hash
{"title"=>"Red-Black Trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-11c",
 "morea_summary"=>"Red-Black trees as 2-4 and BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=a6LaiKa3ES0",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-11c.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-11c.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-screencast-11d.html</h2>

<pre>Hash
{"title"=>"Red-Black Tree Mutation",
 "published"=>true,
 "morea_id"=>"reading-screencast-11d",
 "morea_summary"=>"Red-Black tree insertion and deletion",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=HlwQ_n56MHQ",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-screencast-11d.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-screencast-11d.md"}
</pre>

<h2>/morea/110.balanced-trees/reading-sedgewick-15.html</h2>

<pre>Hash
{"title"=>"Sedgewick 15 - Balanced Trees",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-15",
 "morea_summary"=>"Top-down 2-3-4 trees, red-black trees, other algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/110.balanced-trees/reading-sedgewick-15.html",
 "content"=>"",
 "path"=>"morea//110.balanced-trees/reading-sedgewick-15.md"}
</pre>

<h2>/morea/120.dynamic-programming/experience-dynamic-programming-2.html</h2>

<pre>Hash
{"title"=>"Dynamic programming sample problem: matrix chain multiplication",
 "published"=>true,
 "morea_id"=>"experience-dynamic-programming-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply dynamic programming principles again",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/120.dynamic-programming/experience-dynamic-programming-2.html",
 "url"=>"/morea/120.dynamic-programming/experience-dynamic-programming-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### Dynamic Programming\n\n#### 2\\. (8 pts) Matrix Chain\n\nThe Matrix Chain multiplication problem is the classic dynamic programming\nproblem: every algorithms textbook I have seen uses it as an example. Thus you\nshould at least be familiar with it, and this gives us a chance to get a feel\nfor what a Dynamic Programming computation is like.\n\nRead the Matrix Chain section of the book first (it's not in my lectures) so\nyou understand the optimal substructure (page 373-374, summarized in formula\n15.7) and how the matrices **_m_** and **_s_** function in the code.\n\n![](fig/equation-matrix-decomposition.jpg)\n\n_Briefly:_ Given we have an optimal solution that includes the optimal choice\nof where to put the parentheses at the top level, the solutions to the\nsubproblems of how to recursively parenthesize within each half must also be\noptimal, because otherwise we could substitute in a subproblem solution with\nfewer multiplications: the top level number of multiplications would not\nchange, so there would be fewer multiplications overall, contradicting the\nassumption that the solution was optimal. But we don't know what the optimal\ntop level choice is until we have solved all the subproblems: this is the min\nin the bottom half of formula 15.7. So we will compute the cost of multiplying\neach pair of matrices together, then optimize multiplying 3 matrices together,\nthen optimize 4, on up to 5, building a table of the costs as we go.\n\n![](fig/code-matrix-chain-order.jpg)\n\nThe code is shown to the right. A table **_p_** of dimensions is given. The\nalgorithm iterates for chains of length **_l_**, starting with 2. Variables\n**_i_** and **_j_** control the left and right boundaries of the chain, and\n**_k_** is the current split being considered. The algorithm records the\nminimal number of _m_ultiplications needed for each chain in matrix **_m_**,\nand where we _s_plit the chain into two with the parentheses in matrix\n**_s_**.\n\nYou will solve it for this chain:\n\n> A1(5x15), A2(15x2), A3(2x10), A4(10x5), A5(5x100)\n\nThe chain is represented by this table **_p_**:\n\n![](fig/Problem-Set-07-Matrix-Chain-HW-p-table.jpg)\n\nI provide the solution and the full computations for **_l_=2** below, and give\nyou the template for the computations for **_l_=3**. You will:\n\n**(a)** Fill out the computations for **_l_=3**, put the results in the table, and continue for **_l_=4** and **_l_=5** to complete the table.\n\n**(b)** Write down the output of `Print-Optimal-Parens(_s_,1,5)` assuming the table you produced in (a) is `_s_`. \n\n![](fig/Problem-Set-07-Matrix-Chain-HW-Start.jpg)\n\n    \n    \n    n = 5 \n    \n    ---\n    l=2 // compute the optimal way to multiply each pair\n        // I demonstrate this for you \n    \n      i=1, j=2, k=1:\n        q = m[1,1] + m[2,2] + p0*p1*p2\n          = 0 + 0 + 150 \n          = 150 \n      i=2, j=3, k=2:\n        q = m[2,2] + m[3,3] + p1*p2*p3 \n          =  0 + 0 + 300\n          = 300 \n      i=3, j=4, k=3:\n        q = m[3,3] + m[4,4] + p2*p3*p4 \n          = 0 + 0 + 100\n          = 100\n      i=4, j=5, k=4:\n        q = m[4,4] + m[5,5] + p3*p4*p5 \n          = 0 + 0 + 5000\n          = 5000\n    \n    \n    ---\n    l=3 // compute the optimal way to multiply each triplet\n        // there will be more than one value of k: choose the minimum result\n        // this will indicate the top level parenthesization of the chain\n      i=1, j=3 \n        k=1: \n          q = m[1,1] + m[2,3] + p0*p1*p3 \n            = \n        k=2: \n          q = m[1,2] + m[3,3] + p0*p2*p3 \n            = \n      i=2, j=4 \n        k=2: \n          q = m[2,2] + m[3,4] + p1*p2*p4 \n            = \n        k=3: \n          q = m[2,3] + m[4,4] + p1*p3*p4 \n            = \n      i=3, j=5 \n        k=3: \n          q = m[3,3] + m[4,5] + p2*p3*p5 \n            = \n        k=4: \n          q = m[3,4] + m[5,5] + p2*p4*p5 \n            = \n    \n    ---\n    l=4 // compute the optimal way to multiply each set of 4 matrices \n    \n    What's the pattern? fill out as above, but now you go over 3 values of k. \n    \n    ---\n    l=5 // compute the optimal way to multiply all 5 matrices\n        // then you are ready to give the answer in part (b) \n    \n\n\n\n* * *\n\n### Meet Mr. Fibonacci\n\nHe has some numbers he is proud of (they [seem to show up in nature a\nlot](http://jwilson.coe.uga.edu/emat6680/parveen/fib_nature.htm)), but needs\nyour help in generating and storing them. His first and second numbers are 1,\nand then each successive number is generated by adding up the previous two\nnumbers. He has written a recursive procedure that generates these numbers:\n\n    \n    \n    Fibonacci (n)\n        if n < 2\n            return 1\n        else\n            return Fibonacci (n-1) + Fibonacci(n-2) \n    \n\nBut it is very slow!\n\n#### 3\\. (4 pts) Analysis of Recursive Fibonacci\n\nExplain to him the asymptotic complexity of his algorithm, as follows.\n(Formula A.5 will come in handy.)\n\n**(a)** Draw the recursion tree.\n\n**(b)** Identify these quantities: \n\n  * How many edges go down the left hand side, following the recursive calls for _n_−1? This is an upper bound on tree height.\n  * How many edges go down the right hand side, following the recursive calls for _n_−2? This is a lower bound on tree height.\n  * How many vertices are at level _i_ (where root is level 0)? \n\n**(c)** Then, assuming Θ(1) work at each vertex, how much work is in the tree, as determined by the upper and lower bounds on tree height and work per level that you just computed?\n\n#### 4\\. (4 pts) Dynamic Programming Solution\n\n**(a)** _Rewrite his algorithm to use dynamic programming,_ saving and re-using previous values rather than re-computing them: it's a simple iterative solution. (Did I mention that the solution is _simple_? If you are doing anything complicated you're over-thinking it.) \n\n**(b)** _What's the asymptotic complexity of your re-written algorithm?_ Justify your conclusion.\n\n#### 5\\. (4 pts) Huffman Coding of Fibonacci Numbers\n\nNow that we can efficiently generate his numbers, he has observed that certain\nconfigurations in the flowers in his garden occur with frequencies following\nhis number. He has given each configuration letters. Configuration \"A\" and \"B\"\noccur only once. Configuration \"C\" occurs twice, \"D\" 3 times, \"E\" 5 times, \"F\"\n8 times, \"G\" 13 times and \"H\" 21 times. Show him how he can encode his\nconfigurations with less space using Huffman coding.\n\n**(a)** _Draw the Huffman Tree for these first 8 letters using the observed frequencies._   \n_Note:_ Assume a heap implementation of a min-priority queue where _keys of\nthe same value come out in FIFO order_. For example, when letters A and B are\nmerged to form a node of weight 2, the node for C of weight 2 will be dequeued\nbefore that for A and B. This means that singletons will always be the left\nchild and subtrees the right child. Then you will get a prettier tree and the\npattern will be clear.\n\n**(b)** If we extended this to _n_ letters, _describe the pattern for what the code will look like for any _i_th Fibonacci number_, _i_ ≤ _n_.\n\n\n\n* * *\n\nDan Suthers Last modified: Wed Apr 16 14:38:28 HST 2014\n\n",
 "path"=>"morea//120.dynamic-programming/experience-dynamic-programming-2.md"}
</pre>

<h2>/morea/120.dynamic-programming/experience-dynamic-programming.html</h2>

<pre>Hash
{"title"=>"Dynamic programming sample problem: longest simple path",
 "published"=>true,
 "morea_id"=>"experience-dynamic-programming",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply dynamic programming principles to a sample problem",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/120.dynamic-programming/experience-dynamic-programming.html",
 "url"=>"/morea/120.dynamic-programming/experience-dynamic-programming.html",
 "content"=>
  "# Longest Simple Path in a Directed Acylic Graph\n\nGiven a directed weighted acyclic graph G=(V,E) with real valued edge weights\nrepresenting the \u0093length\u0094 of each edge and two vertices s (start) and t\n(target), develop a dynamic programming approach for finding a longest\nweighted simple path from s to t.\n\nDefinitions:\n\n  * directed: it has arrows on the edges and you only go in the direction of the arrows \n\n  * acyclic: one can never get back to where one started\n\n  * path: a sequence of vertices connected by edges, e.g., ⟨u, v, w⟩ when (u,v) and (v,w) are in E. \n\n  * simple path: a path that does not repeat vertices. \n\nNote: in a DAG without self loops (v, v) all paths are simple. We\u0092ll assume no\nself loops.\n\n  \n\nNotations:\n\n  * Write |V| for number of vertices and |E| for number of edges. \n\n  * (u, v) \\- the directed edge from vertex to vertex v. \n\n  * w(u, v) \\- the weight on the edge (u, v), here interpreted as length.\n\n  * G.V \\- a list of vertices in G. \n\n  * G.Adj[u] \\- a list of edges in G that begin on u (e.g., (u,v), (u,w), (u,x) ). \n\nAlso, let\u0092s assume that vertices are named by integers {1, 2,  |V|} so we can\nuse vertices as indices into arrays. And draw pictures! After all, these are\ngraphs!\n\n  \n\nFollow these steps:\n\n  \n\n1. Characterize the Structure of an Optimal Solution\n\n  \n\nSince we need to reason about subpaths, we\u0092ll start at u (which can be s or\nany other vertex). Let p be a longest path from u to t. If u =  t then p is\nsimply ⟨u⟩ and has zero weight. Consider when u ≠ t. Then p has at least two\nvertices and looks like:\n\n  \n\np = ⟨u, v  t⟩   (it is possible that v = t).\n\n  \n\nLet p\u0092 = ⟨v  t⟩ and prove that p\u0092 must be a longest simple path from v to t.\n\n  \n  \n\n2\\. Recursively define the value of an optimal solution:\n\n  \n\nLet dist[u] be the distance of a longest path from u to t.  Fill out the\ndefinition to reflect the above structure: (You will need mathematical\nnotation that is easier to write on paper. Do it on paper first and figure out\nGoogle equations only if you have time):\n\n  \n\ndist[u] =\n\n  \n\n3\\. Compute the value of an optimal solution (simple recursive version):\n\n  \n\nWrite a recursive procedure that computes the value of an optimal solution as\ndefined by the above recursive definition. Do not memoize yet; that\u0092s the next\nstep.\n\n  \n\nLongest-Path-Value-Recursive (G, u, t)\n\n  \n  \n\n4\\. Compute the value of an optimal solution (dynamic programming version):\n\n  \n\nNow memoize your procedure above by passing the array dist[1..|V|] that\nrecords longest path distances dist[u] from each vertex u to t so you don\u0092t\nhave to repeat computations. We will assume that the caller has initialized\nall entries of dist to -∞.\n\n  \n\nLongest-Path-Value-Memoized (G, u, t, dist)\n\n  \n  \n\n5\\. Analyze the runtime of your solution in #4 in terms of |V| and |E|.\n\n  \n\nInclude the runtime (a) to initialize dist and (b) of Longest-Path-Value-\nMemoized.\n\n  \n  \n\n6\\. Extra Credit: Recover a Solution\n\n  \n\nRewrite Longest-Path-Value-Memoized to Longest-Path-Memoized that takes an\nadditional parameter next[1..|V|], and records the next vertex in the path\nfrom any given vertex u in next[u]. Assume that all entries of next are\ninitialized to -1 by the caller.\n\n  \n\nLongest-Path-Memoized (G, u, t, dist, next)\n\n  \n\nOnce that is done, you can easily write a procedure that recovers (e.g.,\nprints) the path from s to t by tracing through next (but you don\u0092t need to do\nit here).\n\n  \n\n7\\. To Think About\n\n  \n\nHow would you analyze the runtime of procedure Longest-Path-Recursive in terms\nof |V| and |E|? How does it compare to that of Longest-Path-Memoized?\n\n\n\n",
 "path"=>"morea//120.dynamic-programming/experience-dynamic-programming.md"}
</pre>

<h2>/morea/120.dynamic-programming/module-dynamic-programming.html</h2>

<pre>Hash
{"title"=>"Dynamic Programming",
 "published"=>true,
 "morea_id"=>"dynamic-programming",
 "morea_outcomes"=>["outcome-dynamic-programming"],
 "morea_readings"=>
  ["reading-screencast-12a",
   "reading-screencast-12b",
   "reading-screencast-12c",
   "reading-screencast-12d",
   "reading-cormen-15",
   "reading-sedgewick-37",
   "reading-notes-12"],
 "morea_experiences"=>
  ["experience-dynamic-programming", "experience-dynamic-programming-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>
  "/morea/120.dynamic-programming/module-dynamic-programming.gif",
 "morea_sort_order"=>120,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/120.dynamic-programming/module-dynamic-programming.html",
 "content"=>
  "Cut rod problem, longest common subsequence, matrix-chain multiplication, knapsack problem, optimal substructure.\n",
 "path"=>"morea//120.dynamic-programming/module-dynamic-programming.md"}
</pre>

<h2>/modules/dynamic-programming/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-dynamic-programming.md",
 "title"=>"Dynamic Programming",
 "url"=>"/modules/dynamic-programming/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/dynamic-programming/index.html"}
</pre>

<h2>/morea/120.dynamic-programming/outcome-dynamic-programming.html</h2>

<pre>Hash
{"title"=>"Use dynamic programming for problem solving",
 "published"=>true,
 "morea_id"=>"outcome-dynamic-programming",
 "morea_type"=>"outcome",
 "morea_sort_order"=>120,
 "referencing_modules"=>[#Jekyll:Page @name="module-dynamic-programming.md"],
 "url"=>"/morea/120.dynamic-programming/outcome-dynamic-programming.html",
 "content"=>
  "Be able to implement solutions for simple optimization problems based upon dynamic programming techniques.",
 "path"=>"morea//120.dynamic-programming/outcome-dynamic-programming.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-cormen-15.html</h2>

<pre>Hash
{"title"=>"CLRS 15 - Dynamic Programming",
 "published"=>true,
 "morea_id"=>"reading-cormen-15",
 "morea_summary"=>
  "Rod cutting, matrix-chain multiplication, elements of DP, longest common subsequence, optimal BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "55 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-cormen-15.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-cormen-15.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-notes-12.html</h2>

<pre>Hash
{"title"=>"Notes on dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-notes-12",
 "morea_summary"=>"Derivations of dynamic programming",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/120.dynamic-programming/reading-notes-12.html",
 "url"=>"/morea/120.dynamic-programming/reading-notes-12.html",
 "content"=>
  "## Outline\n\n  1. Problem Solving Methods and Optimization Problems \n  2. Introducing DP with the Rod Cutting Example \n  3. Illustrating DP with the Longest Common Subsequence Example \n  4. Summary and Comments on Optimal Substructure\n\n## Readings \n\n  * Read all of CLRS Chapter 15. The focus is on the problem solving strategy: Read the examples primarily to understand the Dynamic Programming strategy rather than to memorize the specifics of each problem (although you will be asked to trace through some of the algorithms). \n\n  * I have also posted a chapter by Sedgewick in Laulima. In this case, I don't think that Sedgewick is any clearer than Cormen et al. The Rod-Cutting example in Cormen et al. illustrates the basics of DP quite well. Also, although usually it is easier to understand examples first, DP examples involve tedious combinations of subproblems, so you may be better off trying to understand the gist of the strategy first in this case.\n\n\n* * *\n\n##  Setting the Context\n\n### Problem Solving Methods\n\nIn this course we study many well defined algorithms, including (so far) those\nfor ADTs, sorting and searching, and others to come to operate on graphs.\nQuality open source implementations exist: you often don't need to implement\nthem.\n\nBut we also study problem solving methods that guide the design of algorithms\nfor your specific problem. Quality open source implementations may not exist\nfor your specific problem: you may need to:\n\n  * Understand and identify characteristics of your problem\n  * Match these characteristics to algorithmic design patterns.\n  * Use the chosen design patterns to design a custom algorithm.\n\nSuch problem solving methods include divide & conquer, dynamic programming,\nand greedy algorithms (among others to come).\n\n### Optimization Problems\n\nAn **optimization problem** requires finding a/the \"best\" of a set of\nalternatives (alternative approaches or solutions) under some quality metric\n(which we wish to maximize) or cost metric (which we wish to minimize).\n\nDynamic Programming is one of several methods we will examine. (Greedy\nalgorithms and linear programming can also apply to optimization problems.)\n\n### Basic Idea of Dynamic Programming\n\nDynamic programming solves optimization problems by combining solutions to\nsubproblems.\n\nThis sounds familiar: **divide and conquer** also combines solutions to\nsubproblems, but **_applies when the subproblems are disjoint_**. For example,\nhere is the recursion tree for merge sort on an array A[1..8]. Notice that the\nindices at each level do not overlap):\n\n![](fig/merge-sort-recursive-structure.jpg)\n\n**Dynamic programming _applies when the subproblems overlap_**. For example, here is the recursion tree for a \"rod cutting\" problem to be discussed in the next section (numbers indicate lengths of rods). Notice that not only do lengths repeat, but also that there are entire subtrees repeating. It would be redundant to redo the computations in these subtrees. \n\n![](fig/Fig-15-3-alt-recursion-tree.jpg)\n\n**Dynamic programming _solves each subproblem just once, and saves its answer in a table_**, to avoid the recomputation. It uses additional memory to save computation time: an example of a **time-memory tradeoff**.\n\nThere are many examples of computations that require exponential time without\ndynamic programming but become polynomial with dynamic programming.\n\n* * *\n\n##  Example: Rod Cutting\n\nThis example nicely introduces key points about dynamic programming.\n\nSuppose you get different prices for steel rods of different lengths. Your\nsupplier provides long rods; you want to know how to cut the rods into pieces\nin order to maximize revenue. Each cut is free. Rod lengths are always an\nintegral number of length units (let's say they are centimeters).\n\n> **Input:** A length _n_ and a table of prices _pi_ for _i_ = 1, 2, ..., _n_.  \n  \n**Output:** The maximum revenue obtainable for rods whose lengths sum to _n_, computed as the sum of the prices for the individual rods. \n\nWe can choose to cut or not cut at each of the _n_-1 units of measurement.\nTherefore one can cut a rod in 2_n_-1 ways.\n\nIf _pn_ is large enough, an optimal solution might require no cuts.\n\n### Example problem instance\n\n![](fig/cut-rod-8-prices.jpg)\n\nSuppose we have a rod of length 4. There are 2_n_-1 = 23 = 8 ways to cut it up\n(the numbers show the price we get for each length, from the chart above):\n\n![](fig/Fig-15-2-alt-rod-cutting.jpg)\n\nHaving enumerated all the solutions, we can see that for a rod of length 4 we\nget the most revenue by dividing it into two units of length 2 each: _p_2 \\+\n_p_2 = 5 + 5 = 10.\n\n###  Optimal Substructure of Rod Cutting\n\nAny optimal solution (other than the solution that makes no cuts) for a rod of\nlength > 2 results in at least one subproblem: a piece of length > 1 remaining\nafter the cut.\n\n_Claim:_ The optimal solution for the overall problem must include an optimal\nsolution for this subproblem.\n\n_Proof:_ The proof is a \"cut and paste\" proof by contradiction: if the overall\nsolution did not include an optimal solution for this problem, we could cut\nout the nonoptimal subproblem solution, paste in the optimal subproblem\nsolution (which must have greater value), and thereby get a better overall\nsolution, contradicting the assumption that the original cut was part of an\noptimal solution.\n\nTherefore, rod cutting exhibits **optimal substructure: _The optimal solution\nto the original problem incorporates optimal solutions to the subproblems,\nwhich may be solved independently._** This is a hallmark of problems amenable\nto dynamic programming. (Not all problems have this property.)\n\n###  Continuing the example\n\n![](fig/cut-rod-8-prices.jpg)\n\nHere is a table of _ri_, the maximum revenue for a rod of length _i_, for this\nproblem instance.\n\n![](fig/cut-rod-8-manual-solutions.jpg)\n\nTo solve a problem of size 7, find the best solution for subproblems of size\n7; 1 and 6; 2 and 5; or 3 and 4. Each of these subproblems also exhibits\noptimal substructue.\n\nOne of the optimal solutions makes a cut at 3cm, giving two subproblems of\nlengths 3cm and 4cm. We need to solve both optimally. The optimal solution for\na 3cm rod is no cuts. As we saw above, the optimal solution for a 4cm rod\ninvolves cutting into 2 pieces, each of length 2cm. These subproblem optimal\nsolutions are then used in the solution to the problem of a 7cm rod.\n\n###  Quantifying the value of an optimal solution\n\nThe next thing we want to do is write a general expression for the value of an\noptimal solution that captures its recursive structure.\n\nFor any rod length _n_, we can determine the optimal revenues _rn_ by taking\nthe maximum of:\n\n  * _pn_: the price we get by not making a cut, \n  * _r_1 \\+ _r__n_-1: the maximum revenue from a rod of 1cm and a rod of _n_-1cm, \n  * _r_2 \\+ _r__n_-2: the maximum revenue from a rod of 2cm and a rod of _n_-2cm, .... \n  * _r__n_-1 \\+ _r_1\n\nSo, _rn_ = max (_pn_, _r_1 \\+ _r__n_-1, _r_2 \\+ _r__n_-2, .... _r__n_-1 \\+\n_r_1).\n\nThere is redundancy in this equation: if we have solved for _ri_ and _r__n_-\n_i_, we don't also have to solve for _r__n_-_i_ and _ri_.\n\n#### A Simpler Decomposition\n\nRather than considering all ways to divide the rod in half, leaving two\nsubproblems, consider all ways to cut off the first piece of length _i_,\nleaving only one subproblem of length _n_ \\- _i_:\n\n![](fig/equation-cut-rod-decomposition.jpg)\n\nWe don't know in advance what the first piece of length _i_ should be, but we\ndo know that one of them must be the optimal choice, so we try all of them.\n\n### Recursive Top-Down Solution\n\nThe above equation leads immediately to a direct recursive implementation (_p_\nis the price vector; _n_ the problem size):\n\n![](fig/code-cut-rod.jpg)\n\nThis works but is inefficient. It calls itself repeatedly on subproblems it\nhas already solved (circled). Here is the recursion tree for _n_ = 4:\n\n![](fig/Fig-15-3-alt-recursion-tree-annotated.jpg)\n\nIn fact we can show that the growth is exponential. Let _T_(_n_) be the number\nof calls to Cut-Rod with the second parameter = _n_.\n\n![](fig/recurrence-rod-cutting.jpg)\n\nThis has solution 2_n_. (Use the inductive hypothesis that it holds for _j_ <\n_n_ and then use formula A5 of Cormen et al. for an exponential series.)\n\n### Dynamic Programming Solutions\n\nDynamic programming arranges to solve each sub-problem just once by saving the\nsolutions in a table. There are two approaches.\n\n#### Top-down with memoization\n\nModify the recursive algorithm to store and look up results in a table _r_.\n**Memoizing** is remembering what we have computed previously.\n\n![](fig/code-memoized-cut-rod.jpg) \n\n![](fig/code-memoized-cut-rod-aux.jpg)\n\nThe top-down approach has the advantages that it is easy to write given the\nrecursive structure of the problem, and only those subproblems that are\nactually needed will be computed. It has the disadvantage of the overhead of\nrecursion.\n\n#### Bottom-up\n\nOne can also sort the subproblems by \"size\" (where size is defined according\nto which problems use which other ones as subproblems), and solve the smaller\nones first.\n\n![](fig/code-bottom-up-cut-rod.jpg)\n\nThe bottom-up approach requires extra thought to ensure we arrange to solve\nthe subproblems before they are needed. (Here, the array reference _r_[_j_ \\-\n_i_] ensures that we only reference subproblems smaller than _j_, the one we\nare currently working on.)\n\nThe bottom-up approach can be more efficient due to the iterative\nimplementation (and with careful analysis, unnecessary subproblems can be\nexcluded).\n\n#### Asymptotic running time\n\nBoth the top-down and bottom-up versions run in Θ(_n_2) time.\n\n  * _Bottom-up:_ there are doubly nested loops, and the number of iterations for the inner loop forms an arithmetic series. \n  \n\n  * _Top-down:_ Each subproblem is solved just once. Subproblems are solved for sizes 0, 1, ... _n_. To solve a subproblem of size _n_, the `for` loop iterates _n_ times, so over all recursive calls the total number of iterations is an arithmetic series. (This uses aggregate analysis, covered in a later lecture.) \n\n### Constructing a Solution\n\nThe above programs return the value of an optimal solution. To construct the\nsolution itself, we need to record the choices that led to optimal solutions.\nUse a table _s_ to record the place where the optimal cut was made (compare to\nBottom-Up-Cut-Rod):\n\n![](fig/code-extended-bottom-up-cut-rod.jpg)\n\nFor our problem, the input data and the tables constructed are:\n\n![](fig/cut-rod-8-prices.jpg) \n\n![](fig/cut-rod-8-computed-solutions.jpg)\n\nWe then trace the choices made back through the table _s_ with this procedure:\n\n![](fig/code-print-cut-rod-solution.jpg)\n\nTrace the calls made by `Print-Cut-Rod-Solution(_p_, 8)`...\n\n* * *\n\n## Four Steps of Problem Solving with Dynamic Programming\n\nIn general, we follow these steps when solving a problem with dynamic\nprogramming:\n\n  1. **Characterize the structure of an optimal solution**: \n    * How are optimal solutions composed of optimal solutions to subproblems?\n    * Assume you have an optimal solution and show how it must decompose\n    * Sometimes it is useful to write a brute force solution, observe its redunancies, and characterize a more refined solution\n    * e.g., our observation that a cut produces one to two smaller rods that can be solved optimally\n  \n\n  2. **Recursively define the value of an optimal solution**: \n    * Write a recursive cost function that reflects the above structure\n    * e.g., the recurrence relation shown\n  \n\n  3. **Compute the value of an optimal solution**: \n    * Write code to compute the recursive values, memoizing or solving smaller problems first to avoid redundant computation\n    * e.g., `Bottom-Up-Cut-Rod`\n  \n\n  4. **Construct an optimal solution from the computed information**: \n    * Augment the code as needed to record the structure of the solution\n    * e.g., `Extended-Bottom-Up-Cut-Rod` and `Print-Cut-Rod-Solution`\n\n![](fig/equation-cut-rod-decomposition.jpg)\n\nThe steps are illustrated in the next example.\n\n* * *\n\n##  Example: Longest Common Subsequence\n\nA **subsequence** of sequence _S_ leaves out zero or more elements but\npreserves order.\n\n_Z_ is a ** common subsequence ** of _X_ and _Y_ if _Z_ is a subsequence of\nboth _X_ and _Y_.  \n_Z_ is a **longest common subsequence** if it is a subsequence of maximal\nlength.\n\n### The LCS Problem\n\nGiven two sequences _X_ = ⟨ _x_1, ..., _x__m_ ⟩ and _Y_ = ⟨ _y_1, ..., _y__n_\n⟩, find a subsequence common to both whose length is longest. Solutions to\nthis problem have applications to DNA analysis in bioinformatics. The analysis\nof optimal substructure is elegant.\n\n#### Examples\n\n![](fig/LCS-examples.jpg)\n\n### Brute Force Algorithm\n\nFor every subsequence of _X_ = ⟨ _x_1, ..., _x__m_ ⟩, check whether it is a\nsubsequence of _Y_ = ⟨ _y_1, ..., _y__n_ ⟩, and record it if it is longer than\nthe longest previously found.\n\n  * There are 2_m_ subsequences of _X_ to check. \n  * For each subsequence, scan _Y_ for the first letter. From there scan for the second letter, etc., up to the _n_ letters of _Y_. \n  * Therefore, Θ(_n_2_m_). \n\nThis involves a lot of redundant work.\n\n  * If a subsequence _Z_ of _X_ fails to match _Y_, then any subsequence having _Z_ as a prefix will also fail. \n  * If a subsequence _Z_ of _X_ matches _Y_, then there is no need to check prefixes of _Z_. \n\nMany problems to which dynamic programming applies have exponential brute\nforce solutions that can be improved on by exploiting redundancy in subproblem\nsolutions.\n\n### Step 1. Optimal Substructure of LCS\n\nThe first step is to characterize the structure of an optimal solution,\nhopefully to show it exhibits optiomal stubstructure.\n\nOften when solving a problem we start with what is known and then figure out\nhow to contruct a solution. The optimal substructure analysis takes the\nreverse strategy: _ _assume_ you have found an optional solution_ (Z below)\n_and figure out what you must have done to get it_!\n\nNotation:\n\n  * _Xi_ = prefix ⟨ _x_1, ..., _x__i_ ⟩\n  * _Yi_ = prefix ⟨ _y_1, ..., _y__i_ ⟩\n\n**_Theorem:_ ** Let _Z_ = ⟨ _z_1, ..., _z__k_ ⟩ be any LCS of _X_ = ⟨ _x_1, ..., _x__m_ ⟩ and _Y_ = ⟨ _y_1, ..., _y__n_ ⟩. Then \n\n  1. If _xm_ = _yn_, then _zk_ = _xm_ = _yn_, and _Z__k_-1 is an LCS of _X__m_-1 and _Y__n_-1.\n  2. If _xm_ ≠ _yn_, then _zk_ ≠ _xm_ ⇒ _Z_ is an LCS of _X__m_-1 and _Y_. \n  3. If _xm_ ≠ _yn_, then _zk_ ≠ _yn_ ⇒ _Z_ is an LCS of _X_ and _Y__n_-1. \n\n_Sketch of proofs:_\n\n(1) can be proven by contradiction: if the last characters of _X_ and _Y_ are\nnot included in _Z_, then a longer LCS can be constructed by adding this\ncharacter to _Z_, a contradiction.\n\n(2) and (3) have symmetric proofs: Suppose there exists a subsequence _W_ of\n_X__m_-1 and _Y_ (or of _X_ and _Y__n_-1) with length > _k_. Then _W_ is a\ncommon subsequence of _X_ and _Y_, contradicting _Z_ being an LCS.\n\nTherefore, **an LCS of two sequences contains as prefix an LCS of prefixes of\nthe sequences.** We can now use this fact construct a recursive formula for\nthe value of an LCS.\n\n### Step 2. Recursive Formulation of Value of LCS\n\nLet _c_[_i_, _j_] be the length of the LCS of prefixes _Xi_ and _Yj_. The\nabove recursive substructure leads to the definition of _c_:\n\n![](fig/LCS-recursive-formulation.jpg)\n\nWe want to find _c_[_m_, _n_].\n\n### Step 3. Compute Value of Optimal Solution to LCS\n\nA recursive algorithm based on this formulation would have lots of repeated\nsubproblems, for example, on strings of length 4 and 3:\n\n![](fig/LCS-4-3-recurrence-tree.jpg)\n\n![](fig/LCS-recursive-formulation.jpg) Dynamic programming avoids\nthe redundant computations by storing the results in a table. We use\n_c_[_i_,_j_] for the length of the LCS of prefixes _Xi_ and _Yj_ (hence it\nmust start at 0). (_b_ is part of the third step and is explained next\nsection.)\n\nTry to find the correspondence betweeen the code below and the recursive\ndefinition shown in the box above.\n\n![](fig/code-LCS-length.jpg)\n\nThis is a bottom-up solution: Indices _i_ and _j_ increase through the loops,\nand references to _c_ always involve either _i_-1 or _j_-1, so the needed\nsubproblems have already been computed.\n\nIt is clearly **Θ(_m__n_)**; _much better than Θ(_n_2_m_)_!\n\n### Step 4. Construct an Optimal Solution to LCS\n\nIn the process of computing the _value_ of the optimal solution we can also\nrecord the _choices_ that led to this solution. Step 4 is to add this latter\nrecord of choices and a way of recovering the optimal solution at the end.\n\nTable _b_[_i_, _j_] is updated above to remember whether each entry is\n\n  * a common substring of _X__i_-1 and _Y__j_-1 (diagonal arrow), in which case the common character _xi_ = _yj_ is included in the LCS;\n  * a common substring of _X__i_-1 and _Y_ (↑); or\n  * a common substring of _X_ and _Y__j_-1 (<-).\n\nWe reconstruct the path by calling Print-LCS(_b_, _X_, _n_, _m_) and following\nthe arrows, printing out characters of _X_ that correspond to the diagonal\narrows (a Θ(_n_ \\+ _m_) traversal from the lower right of the matrix to the\norigin):\n\n![](fig/code-print-LCS.jpg)\n\n### Example of LCS\n\nWhat do \"spanking\" and \"amputation\" have in common?\n\n![](fig/LCS-spanking-amputation.jpg)\n\n* * *\n\n##  Other Applications\n\nTwo other applications are covered in the Cormen et al. text, and many others\nin the Problems at the end of the chapter. I omit them to keep this lecture\nfrom being too long, and trust that the student will read them in the text.\n\n### Optimizing Matrix-Chain Multiplication\n\nMany scientific and business applications involve multiplication of chains of\nmatrices ⟨ A1, A2, A3, ... A_n_ ⟩. Since matrix multiplication is associative,\nthe matrices can be multiplied with their neighbors in this sequence in any\norder. The order chosen can have a huge difference in the number of\nmultiplications required. For example suppose you have A, a 2x100 matrix, B\n(100x100) and C (100x20). To compute A*B*C:\n\n> (A*B) requires 2*100*100 = 20000 multiplications, and results in a 2x100\nmatrix. Then you need to multiply by C: 2*100*20 = 4000 multiplications, for a\ntotal of 24,000 multiplications (and a 2x20 result).\n\n> (B*C) requires 100x100x20 = 200000 multiplications, and results in a 100x20\nmatrix. Then you need to multiply by A: 2*100*20 = 4000 multiplications, for a\ntotal of 204,000 multiplications (and the same 2x20 result).\n\nThe Matrix-Chain Multiplication problem is to determine the optimal order of\nmultiplications (_not_ to actually do the multiplications). For three matrices\nI was able to figure out the best sequence by hand, but some problems in\nscience, business and other areas involve many matrices, and the number of\ncombinations to be checked grows exponentially.\n\nPlanning matrix multiplication is perhaps the most \"canonical\" example of\ndynamic programming: it is used in most introductory presentations. I chose to\npresent LCS instead because matrix multiplication optimization will be built\ninto turnkey software, and current students will more likely be interested in\nbioinformatics applications\n\n###  Optimal Binary Search Tree\n\nWe saw in [Topic\n8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html) that\nan unfortunate order of insertions of keys into a binary search tree (BST) can\nresult in poor performance (e.g., linear in _n_). If we know all the keys in\nadvance and also the probability that they will be searched, we can optimize\nthe construction of the BST to minimize search time in the aggregate over a\nseries of queries. An example application is when we want to construct a\ndictionary from a set of terms that are known in advance along with their\nfrequency in the language. The reader need only try problem 15.5-2 from the\nCormen et al. text (manual simulation of the algorithm) to appreciate why we\nwant to leave this tedium to computers!\n\n* * *\n\n##  Further Observations Concerning Optimal Substructure\n\nTo use dynamic programming, we must show that any optimal solution involves\nmaking a choice that leaves one or more subproblems to solve, and the\nsolutions to the subproblems used within the optimal solution must themselves\nbe optimal.\n\n### The optimal choice is not known before solving the subproblems\n\nWe may not know what that first choice is. Consequently:\n\n  * To show that there is optimal substructure, we suppose that the choice has been made, and show that the subproblems that result must also be solved optimally. This argument is often made using a cut-and-paste proof by contradiction.\n  * Then when writing the code, we must ensure that enough potential choices and hence their supbproblems are considered that we find the optimal first choice. This usually shows up as iteration in which we find the maximum or minimum according to some objective function across all choices.\n\n### Optimal substructure varies across problem domains:\n\nHow many subproblems are used in an optimal solution may vary:\n\n  * Rod Cutting: 1 subproblem (of size _n_ \\- _i_)\n  * LCS: 1 subproblem (LCS of the prefix sequence(s).) \n  * Optimal BST: 2 subproblems (given _kr_ has been chosen as the root, _ki_ ..., _k__r_-1 and _k__r_+1 ..., _kj_) \n\nHow many choices in determining which subproblem(s) to use may vary:\n\n  * Rod cutting: _n_ choices (for each value of _i_)\n  * LCS: Either 1 choice (if _xi_ = _yj_, take LCS of _X__i_-1 and _Y__j_-1), or 2 choices (if _xi_ ≠ _yj_, check both LCS of _X__i_-1 and _Y_, and LCS of _X_ and _Y__j_-1)\n  * Optimal BST: _j_ \\- _i_ \\+ 1 choices for the root _kr_ in _ki_ ..., _kj_: see text.\n\nInformally, running time depends on (# of subproblems overall) x (# of\nchoices).\n\n  * Rod Cutting: Θ(_n_) subproblems overall, ≤ _n_ choices for each ⇒ O(_n_2) running time.\n  * LCS: Θ(_m__n_) subproblems overall; ≤ 2 choices for each ⇒ O(_m__n_) running time.\n  * Optimal BST: Θ(_n_2) subproblems overall; O(_n_) choices for each ⇒ O(_n_3) running time. \n\n(We'll have a better understanding of \"overall\" when we cover amortized\nanalysis.)\n\n### Not all optimization problems have optimal substructure\n\n![](fig/shortest-path-optimal-substructure.jpg)\n\nWhen we study graphs, we'll see that finding the **shortest path** between two\nvertices in a graph has optimal substructure: if _p_ = _p_1 \\+ _p_2 is a\nshortest path between _u_ and _v_ then _p_1 must be a shortest path between\n_u_ and _w_ (etc.). Proof by cut and paste.\n\nBut finding the **longest simple path** (the longest path not repeating any\nedges) between two vertices is not likely to have optimal substructure.\n\n![](fig/longest-path-nonoptimal-substructure.jpg)\n\nFor example, _q_ -> _s_ -> _t_ -> _r_ is longest simple path from _q_ to _r_,\nand _r_ -> _q_ -> _s_ -> _t_ is longest simple path from _r_ to _t_, but the\ncomposed path is not even legal: the criterion of simplicity is violated.\n\nDynamic programming requires _overlapping_ yet _independently solveable_\nsubproblems.\n\nLongest simple path is NP-complete, a topic we will cover at the end of the\nsemester, so is unlikely to have any efficient solution.\n\n### Dynamic programming uses optimal substructure bottom up\n\nAlthough we wrote the code both ways, in terms of the order in which solutions\nare found, dynamic programming _first_ finds optimal solutions to subproblems\nand _then_ choses which to use in an optimal solution to the problem. It\napplies when one cannot make the top level choice until subproblem solutions\nare known.\n\nIn [Topic\n13](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-13.html),\nwe'll see that ** greedy algorithms** work top down: _first_ make a choice\nthat looks best, _then_ solve the resulting subproblem. Greedy algorithms\napply when one can make the top level choice without knowing how subproblems\nwill be solved.\n\n* * *\n\n##  Summary\n\nDynamic Programming applies when the problem has these characteristics:\n\n**Recursive Decomposition**\n    The problem has recursive structure: it breaks down into smaller problems of the same type. _This characterisic is shared with divide and conquer, but dynamic programming is distinguished from divide and conquer by the next item._\n**Overlapping Subproblems**\n    The subproblems solved by a recursive solution overlap (the same subproblems are revisited more than once). _This means we can save time by preventing the redundant computations._\n**Optimal Substructure**\n    Any optimal solution involves making a choice that leaves one or more subproblems to solve, and the solutions to the subproblems used within the optimal solution must themselves be optimal. _This means that optimized recursive solutions can be used to construct optimized larger solutions._\n  \n\nDynamic programming can be approached top-down or bottom-up:\n\n**Top-Down with memoization:**\n    Write a recursive procedure to solve the problem, computing subproblems as needed. Each time a sub-problem is encountered, see whether you have stored it in a table, and if not, solve it and store the solution.\n  \n**Bottom-Up:**\n    Order the subproblems such that \"smaller\" problems are solved first, so their solutions are available in the table before \"larger\" problems need them. (This ordering need not be based on literal size.) \n\nBoth have the same asympotic running time. The top-down procedure has the\noverhead of recursion, but computes only the subproblems that are actually\nneeded. Bottom-up is used the most in practice.\n\nWe problem solve with dynamic programming in four steps:\n\n  1. **Characterize the structure of an optimal solution**: \n    * How are optimal solutions composed of optimal solutions to subproblems?\n  2. **Recursively define the value of an optimal solution**: \n    * Write a recursive cost function that reflects the above structure\n  3. **Compute the value of an optimal solution**: \n    * Write code to compute the recursive values, memoizing or solving smaller problems first to avoid redundant computation\n  4. **Construct an optimal solution from the computed information**: \n    * Augment the code as needed to record the structure of the solution\n\n* * *\n\n## Wrapup\n\nThere is an online presentation focusing on LCS at [ http://www.csanimated.com\n/animation.php?t=Dynamic_programming](http://www.csanimated.com/animation.php?\nt=Dynamic_programming).\n\nIn the next Topic 13 we look at a related optimization strategy: greedy\nalgorithms.\n\n* * *\n\nDan Suthers Last modified: Sun Mar 2 05:24:02 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//120.dynamic-programming/reading-notes-12.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-12a.html</h2>

<pre>Hash
{"title"=>"Introduction to dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-screencast-12a",
 "morea_summary"=>"Example of dynamic programming using the cut rod problem.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=RYPsOJmhwgE",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-12a.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-12a.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-12b.html</h2>

<pre>Hash
{"title"=>"Dynamic programming using steps and LCS",
 "published"=>true,
 "morea_id"=>"reading-screencast-12b",
 "morea_summary"=>"Example of dynamic programming using steps and LCS.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=0tQ3ah0Fddw",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-12b.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-12b.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-12c.html</h2>

<pre>Hash
{"title"=>"Dynamic programming using LCS (continued)",
 "published"=>true,
 "morea_id"=>"reading-screencast-12c",
 "morea_summary"=>"Example of dynamic programming using LCS.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=dFObo5BeJ0k",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-12c.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-12c.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-screencast-12d.html</h2>

<pre>Hash
{"title"=>"Dynamic programming applications, substructure, and summary",
 "published"=>true,
 "morea_id"=>"reading-screencast-12d",
 "morea_summary"=>"Summary of dynamic programming.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=QzgqDJIJtNY",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-screencast-12d.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-screencast-12d.md"}
</pre>

<h2>/morea/120.dynamic-programming/reading-sedgewick-37.html</h2>

<pre>Hash
{"title"=>"Sedgewick 37 - Dynamic Programming",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-37",
 "morea_summary"=>
  "Knapsack problem, matrix chain product, optimal BSTs, shortest paths, time and space requirements",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/120.dynamic-programming/reading-sedgewick-37.html",
 "content"=>"",
 "path"=>"morea//120.dynamic-programming/reading-sedgewick-37.md"}
</pre>

<h2>/morea/130.greedy-algorithms/experience-greedy-algorithms.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms sample problem: activity scheduling",
 "published"=>true,
 "morea_id"=>"experience-greedy-algorithms",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply the greedy strategy to activity scheduling",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/130.greedy-algorithms/experience-greedy-algorithms.html",
 "url"=>"/morea/130.greedy-algorithms/experience-greedy-algorithms.html",
 "content"=>
  "### Activity Scheduling\n\nThe activity scheduling problem is to _**find the largest possible set **_\n(maximum _number_, not duration) _**of activities that do not overlap with\neach other**_, given their start and finish times.\n\nCLRS show that this problem has the _**greedy choice property**_: a globally\noptimal solution can be assembled from locally optimal choices. They show it\nby example with this _greedy strategy_: Always select the remaining compatible\nactivity that ends first, and then solve the subproblem of scheduling the\nactivities that start after this activity. The local optimization is to select\nthe activity that finishes first, and the global optimization is scheduling\nthe maximum number of activities possible.\n\nThere are other greedy strategies, but some work and some don't.\n\n#### Alternative Greedy Strategies\n\nIn the following three problems you determine whether alternative locally\ngreedy strategies lead to globally optimal solutions. For each strategy below,\n\n  * either show that the strategy leads to an optimal solution by outlining the algorithm or approach,\n  \n\n  * or give a counterexample: write down a set of activities (defined by their start and finish times) that the strategy fails on, and show what goes wrong.\n\n**1.** _Greedy strategy_: Always select the remaining compatible activity that has the **least duration**. _Rationale:_ Leave the most time remaining for other activities. \n\n**2.** _Greedy strategy_: Always select the remaining compatible activity that has the **latest start time**. _Rationale:_ Leave the most time remaining at the beginning for other activities.\n\n**3.** _Greedy strategy_: Always select the remaining compatible activity that **overlaps with the fewest number of remaining activities**. _Rationale:_ Eliminate the fewest number of remaining activities from consideration.\n\n####  Value Optimization\n\n**4.** Suppose now that different activities earn different amounts of revenue. In addition to their start and finish times _s__i_ and _f__i_, each activity _a__i_ has _revenue_ _r__i_, and our objective is now to maximize the total revenue Σ_a__i_∈_A_ _r__i_. \n\n  * Does this problem exhibit the greedy choice property?\n  * If so, do any of the above strategies apply, or another greedy strategy you can think of?\n  * If not, why not, and what alternative problem solving strategy would work on this problem? \n\n\n",
 "path"=>"morea//130.greedy-algorithms/experience-greedy-algorithms.md"}
</pre>

<h2>/morea/130.greedy-algorithms/module-greedy-algorithms.html</h2>

<pre>Hash
{"title"=>"Greedy Algorithms",
 "published"=>true,
 "morea_id"=>"greedy-algorithms",
 "morea_outcomes"=>["outcome-greedy-algorithms"],
 "morea_readings"=>
  ["reading-screencast-13a",
   "reading-screencast-13b",
   "reading-screencast-13c",
   "reading-cormen-16",
   "reading-notes-13"],
 "morea_experiences"=>["experience-greedy-algorithms"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/130.greedy-algorithms/module-greedy-algorithms.png",
 "morea_sort_order"=>130,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/130.greedy-algorithms/module-greedy-algorithms.html",
 "content"=>
  "Dynamic programming, activity scheduling, the greedy strategy, Huffman codes.\n",
 "path"=>"morea//130.greedy-algorithms/module-greedy-algorithms.md"}
</pre>

<h2>/modules/greedy-algorithms/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-greedy-algorithms.md",
 "title"=>"Greedy Algorithms",
 "url"=>"/modules/greedy-algorithms/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/greedy-algorithms/index.html"}
</pre>

<h2>/morea/130.greedy-algorithms/outcome-greedy-algorithms.html</h2>

<pre>Hash
{"title"=>"Use greedy algorithms for problem solving",
 "published"=>true,
 "morea_id"=>"outcome-greedy-algorithms",
 "morea_type"=>"outcome",
 "morea_sort_order"=>120,
 "referencing_modules"=>[#Jekyll:Page @name="module-greedy-algorithms.md"],
 "url"=>"/morea/130.greedy-algorithms/outcome-greedy-algorithms.html",
 "content"=>
  "Be able to implement solutions for simple optimization problems based upon greedy programming techniques.",
 "path"=>"morea//130.greedy-algorithms/outcome-greedy-algorithms.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-cormen-16.html</h2>

<pre>Hash
{"title"=>"CLRS 16 - Greedy algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-16",
 "morea_summary"=>
  "Activity selection problems, elements of the greedy algorithm strategy, huffman codes (16.1 -16.3)",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-cormen-16.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-cormen-16.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-notes-13.html</h2>

<pre>Hash
{"title"=>"Notes on greedy algorithms",
 "published"=>true,
 "morea_id"=>"reading-notes-13",
 "morea_summary"=>
  "Greedy algorithms, the activity selection problem, greedy strategy, and Huffman codes.",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/130.greedy-algorithms/reading-notes-13.html",
 "url"=>"/morea/130.greedy-algorithms/reading-notes-13.html",
 "content"=>
  "## Outline\n\n  1. Prelude: Greedy Algorithms and Dynamic Programming\n  2. Example: Activity Selection Problem \n  3. Greedy Strategy \n  4. Huffman Codes \n\n## Readings and Screencasts\n\n  * Read the first three sections of CLRS Chapter 16, although you need not read the details of the proofs. We are not covering Matroids (the 4th section).\n\nThis presentation follows the CLRS reading fairly closely, selecting out the\nmost relevant parts and explaining a few things in more detail. (The\nassociated videos change the ordering somewhat: 13A provides a conceptual\nintroduction, leaving the activity selection example for 13B.)\n\n* * *\n\n##  Prelude: Greedy Algorithms and Dynamic Programming\n\nBoth Dynamic Programming and Greedy Algorithms are ways of solving\n_**optimization problems**_: a solution is sought that optimizes (minimizes or\nmaximizes) an _**objective function**_.\n\n**Dynamic Programming:**\n\n  * Finds solutions bottom-up (solves subproblems before solving their super-problem) \n  * Exploits overlapping subproblems for efficiency (by reusing solutions)\n  * Can handle subproblem interdependence \n\n**Greedy Algorithms** \"greedily\" take the choice with the most immediate gain. \n\n  * Find solutions top-down (commit to a choice, then solve sub-problems) \n  * Assume that if the objective function is optimized locally it will be optimized globally\n  * Cannot handle interdependent subproblems \n\nFor some problems, but not all, local optimization actually results in global\noptimization.\n\nWe'll use an example to simultaneously review dynamic programming and motivate\ngreedy algorithms, as the two approaches are related (but distinct).\n\n* * *\n\n##  Activity Selection Problem\n\nSuppose that _activities_ require exclusive use of a common resource, and you\nwant to schedule as many as possible.\n\nLet _S_ = {_a_1, ..., _a__n_} be a set of _n_ activities.\n\nEach activity _ai_ needs the resource during a time period starting at _si_\nand finishing before _fi_, i.e., during [_si_, _fi_).\n\n(_Why not_ [_si_, _fi_]?)\n\nThe optimization problem is to select the largest set of non-overlapping\n(mutually compatible) activities from _S_.\n\nWe assume that activities are sorted by finish time _f_1 ≤ _f_2 ≤ ... _f__n_-1\n≤ _f__n_ (this can be done in Θ(_n_ lg _n_)).\n\n### Example\n\nConsider these activities:\n\n![](fig/activities.jpg)\n\nHere is a graphic representation:\n\n![](fig/activity-timeline.jpg)\n\nSuppose we chose one of the activities that _start first_, and then look for\nthe next activity that starts after it is done. This could result in {_a_4,\n_a_7, _a_8}, but this solution is not optimal.\n\nAn optimal solution is {_a_1, _a_3 _a_6, _a_8}. (It maximizes the objective\nfunction of number of activities scheduled.)\n\nAnother one is {_a_2, _a_5, _a_7, _a_9}. (Optimal solutions are not\nnecessarily unique.)\n\nHow do we find (one of) these optimal solutions? Let's consider it as a\ndynamic programming problem ...\n\n### Optimal Substructure Analysis\n\nA dynamic programming analysis begins by identifying the choices to be made,\nand assuming that you can make an optimal choice (without yet specifying what\nthat choice is) that will be part of an optimal solution.\n\nIt then specifies the possible subproblems that result in the most general way\n(to ensure that possible components of optimal solutions are not excluded),\nand shows that an an optimal solution must recursively include optimal\nsolutions to the subproblems. (This is done by reasoning about the value of\nthe solutions according to the objective function.)\n\nWe'll approach Activity Selection similarly. I'll try to clarify the reasoning\nin the text ...\n\nFor generality, we define the problem in a way that applies both to the\noriginal problem and subproblems.\n\nSuppose that due to prior choices we are working on a time interval from _i_\nto _j_. This could be after some already-scheduled activity _ai_ and before\nsome already-scheduled event _aj_, or for the original problem we can define\n_i_ and _j_ to bound the full set of activities to be considered.\n\nThen the candidate activities to consider are those that start after _ai_ and\nend before _aj_:\n\n![](fig/compatible-activities.jpg)  \n![](fig/compatible-activities-timeline.jpg)\n\nNow let's define _Aij_ to be an optimal solution, i.e., a maximal set of\nmutualy compatible activities in _Sij_. What is the structure of this\nsolution?\n\nAt some point we will need to make a choice to include some activity _ak_ with\nstart time _sk_ and finishing by _fk_ in this solution. This choice will leave\ntwo sets of compatible candidates after _ak_ is taken out:\n\n  * _Sik_ : activities that start after _ai_ finishes, and finish before _ak_ starts \n  * _Skj_ : activities that start after _ak_ finishes, and finish before _aj_ starts \n\n(Note that _Sij_ may be a proper superset of _Sik_ ∪ {_ak_} ∪ _Skj_, as\nactivities incompatible with _ak_ are excluded.)\n\nUsing the same notation as above, define the optimal solutions to these\nsubproblems to be:\n\n  * _Aik_ = _Aij_∩ _Sik_: the optimal solution to _Sik_\n  * _Akj_ = _Aij_ ∩ _Skj_: the optimal solution to _Skj_\n\nSo the structure of an optimal solution _Aij_ is:\n\n> _Aij_ = _Aik_ ∪ {_ak_} ∪ _Akj_\n\nand the number of activities is:\n\n> |_Aij_| = |_Aik_| + 1 + |_Akj_|\n\nBy the \"cut and paste argument\", an optimal solution _Aij_ for _Sij_ must\ninclude the optimal solutions _Aik_ for _Sik_ and _Akj_ for _Skj_, because if\nsome suboptimal solution _A'ik_ were used for _Sik_ (or similarly _A'kj_ for\n_Skj_), where |_A'ik_| < |_Aik_|, we could substitute _Aik_ to increase the\nnumber of activities (a contradiction to optimality).\n\nTherefore the Activity Scheduling problem exhibits optimal substructure.\n\n### Recursive Solution\n\nSince the optimal solution _A__ij_ must include optimal solutions to the\nsubproblems for _S__ik_ and _S__kj_, we could solve by dynamic programming.\n\nLet _c_[_i_, _j_] = size of optimal solution for _S__ij_ (_c_[_i_, _j_] has\nthe same value as |_Aij_|, but apparently we are switching notation to\nindicate that this is for any optimal solution). Then\n\n> _c_[_i_, _j_] = _c_[_i_, _k_] + _c_[_k_, _j_] + 1   (the 1 is to count\n_ak_).\n\nWe don't know which activity _ak_ to choose for an optimal solution, so we\ncould try them all:\n\n![](fig/activity-scheduling-recurrence-16-2.jpg)\n\nThis suggests a recursive algorithm that can be memoized, or we could develop\nan equivalent bottom-up approach, filling in tables in either case.\n\nBut it turns out we can solve this without considering multiple subproblems.\n\n### Being Greedy\n\nWe are trying to optimize the number of activities. Let's be greedy!\n\n  * The more time that is left after running an activity, the more subsequent activities we can fit in. \n  * If we **choose the first activity to _finish,_** the most time will be left.\n  * Since activities are sorted by finish time, we will always start with _a_1. \n  * Then we can solve the single subproblem of activity scheduling in this remaining time.\n\nSince there is only a single subproblem, the _Sij_ notation, bounding the set\nat both ends, is more complex than we need. We'll simplify the notation to\nindicate the activities that start after _ak_ finishes:\n\n> _S_k = {_ai_ ∈ _S_ : _si_ ≥ _fk_}\n\nSo, after choosing _a_1 we just have _S_1 to solve (and so on after choices in\nrecursive subproblems).\n\nBy optimal substructure, _if_ _a_1 is part of an optimal solution, then an\noptimal solution to the original problem consists of _a_1 plus all activities\nin an optimal solution to _S_1.\n\nBut we need to prove that _a_1 is always part of some optimal solution (i.e.,\nto prove our original intuition).\n\n_**Theorem:**_ If _S_k is nonempty and _am_ has the earliest finish time in\n_S_k, then _am_ is included in some optimal solution.\n\n_Proof:_ Let _Ak_ be an optimal solution to _S_k, and let _aj_ ∈ _Ak_ have the\nearliest finish time in _Ak_. If _aj_ = _am_ we are done. Otherwise, let\n_A'__k_ = (_Ak_ \\- {_aj_}) ∪ {_am_} (substitute _am_ for _aj_).\n\n> _Claim:_ Activities in _A'k_ are disjoint.\n\n>\n\n> _Proof of Claim:_ Activities in _Ak_ are disjoint because it was a solution.  \nSince _aj_ is the first activity in _Ak_ to finish, and fm ≤ fj (_am_ is the\nearliest in _Sk_), _am_ cannot overlap with any other activities in _A'k_.  \nNo other changes were made to _Ak_, so _A'k_ must consist of disjoint\nactivities.\n\nSince |_A'k_| = |_Ak_| we can conclude that _A'k_ is also an optimal solution\nto _S_k, and it includes _am_.\n\nTherefore we don't need the full power of dynamic programming: we can just\nrepeatedly choose the activity that finishes first, remove any activities that\nare incompatible with it, and repeat on the remaining activities until no\nactivities remain.\n\n### Greedy Algorithm Solution\n\nLet the start and finish times be represented by arrays _s_ and _f_, where _f_\nis assumed to be sorted in monotonically increasing order.\n\nAdd a fictitious activity _a_0 with _f_0 = 0, so _S_0 = _S_ (i.e., the entire\ninput sequence).\n\nOur initial call will be RECURSIVE-ACTIVITY-SELECTOR(_s_, _f_, 0, _n_).\n\n![](fig/pseudocode-recursive-activity.jpg)\n\nThe algorithm is Θ(_n_) because each activity is examined exactly once across\nall calls: each recursive call starts at _m_, where the previous call left\noff. (Another example of aggregate analysis.)\n\nIf the activities need to be sorted, the overall problem can be solved in\nΘ(_n_ lg _n_)).\n\nThis algorithm is nearly tail recursive, and can easily be converted to an\niterative version:\n\n![](fig/pseudocode-greedy-activity.jpg)\n\nLet's trace the algorithm on this:\n\n![](fig/activity-timeline.jpg)\n\n* * *\n\n##  A Closer Look at the Greedy Strategy\n\nInstead of starting with the more elaborate dynamic programming analysis, we\ncould have gone directly to the greedy approach.\n\nTypical steps for designing a solution with the greedy strategy (and two\nproperties that are key to determining whether it might apply to a problem):\n\n  1. Consider how we can make a greedy choice (local optimization of the objective function), leaving one subproblem to solve.\n  2. **Greedy Choice Property:** Prove that the greedy choice is always part of some optimal solution.\n  3. **Optimal Substructure:** Demonstrate that an optimal solution to the problem contains within it optimal solutions to the subproblems.\n\nThen we can construct an algorithm that combines the greedy choice with an\noptimal solution to the remaining problem.\n\n###  Dynamic Programming compared to Greedy Strategy:\n\nBoth require optimal substructure, but ...\n\n**Dynamic Programming**\n\n  * Each choice depends on knowing the optimal solutions to subproblems.\n  * Bottom-up: Solve subproblems first\n\n**Greedy Strategy**\n\n  * Each choice depends only on local optimization \n  * Top-down: Make choice before solving subproblems \n\n### Example: Knapsack Problems\n\nThese two problems demonstrate that the two strategies do not solve the same\nproblems. Suppose a thief has a knapsack of fixed carrying capacity, and wants\nto optimize the value of what he takes.\n\n![](fig/Fig-16-2-0-1-a-knapsack-example.jpg)\n\n#### 0-1 knapsack problem:\n\nThere are _n_ items. Item _i_ is worth $_vi_ and weighs _wi_ pounds. The thief\nwants to take the most valuable subset of items with weight not exceeding _W_\npounds. It is called 0-1 because the thief must either not take or take each\nitem (they are discrete objects, like gold ingots).\n\nIn the example, item 1 is worth $6/pound, item 2 $5/pound and item 3 $4/pound.\n\nThe greedy strategy of optimizing value per unit of weight would take item 1\nfirst.\n\n#### Fractional knapsack problem:\n\nThe same as the 0-1 knapsack problem except that the thief _can take a\nfraction of each item_ (they are divisible substances, like gold powder).\n\nBoth have optimal substructure _(why?)._\n\nOnly the fractional knapsack problem has the greedy choice property:\n\n_Fractional:_ One can fill up as much of the most valuable substance by weight\nas one can hold, then as much of the next most valuable substance, etc., until\n_W_ is reached:\n\n![](fig/Fig-16-2-0-1-c-knapsack-example.jpg)\n\n_0-1:_ A greedy strategy could result in empty space, reducing the overall\ndollar density of the knapsack. After choosing item 1, the optimal solution\n(shown third) cannot be achieved:\n\n![](fig/Fig-16-2-0-1-b-knapsack-example-reordered.jpg)\n\n* * *\n\n##  Huffman Codes\n\nWe are going to see several greedy algorithms throughout the semester. The\nactivity scheduler was good for illustration, but is not important in\npractice. We should look at one important greedy algorithm today ...\n\nHuffman codes provide an efficient way to compress text, and are constructed\nusing a greedy algorithm. We only have time to review how this important\nalgorithm works; see the text for analysis.\n\n### Binary Codes\n\n**Fixed-length binary codes** (e.g., ASCII) represent each character with a fixed number of bits (a binary string of fixed length called a **codeword**).\n\n**Variable-length binary codes** can vary the number of bits allocated to each character. This opens the possibility of space efficiency by using fewer bits for frequent characters.\n\nExample: Suppose we want to encode documents with these characters:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n\nWith a 3 bit code it would take 300,000 bits to code a file of 100,000\ncharacters, but the variable-length code shown requires only 224,000 bits.\n\n**Prefix codes** (better named **prefix-free codes**) are codes in which no codeword is a prefix of another.\n\nFor any data, it is always possible to construct a prefix code that is optimal\n(though not all prefix codes are optimal, as we will see below).\n\nPrefix codes also have the advantage that each character in an input file can\nbe \"consumed\" unambiguously, as the prefix cannot be confused with another\ncode.\n\n### Binary Tree Representation of Prefix Codes\n\nWe can think of the 0 and 1 in a prefix code as directions for traversing a\nbinary tree: 0 for left and 1 for right. The leaves store the coded character.\nFor example, here is the fixed-length prefix code from the table above\nrepresented as a binary tree:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n![](fig/Fig-16-4-a-coding-tree.jpg)\n\nConsuming bits from an input file, we traverse the tree until the character is\nidentified, and then start over at the top of the tree for the next character.\n\n_Exercise: Decode 101100100011_\n\nBut the above tree uses three bits per character: it is not optimal. It can be\nshown that an optimal code is always represented by a full binary tree (every\nnon-leaf node has two children).\n\nFor example, an optimal prefix code (from the table reproduced again here) is\nrepresented by this tree:\n\n![](fig/Fig-16-4-b-coding-tree.jpg) \n![](fig/Fig-16-3-character-coding-problem.jpg)\n\n_Exercise: Decode 10111010111_\n\n### Huffman's Algorithm\n\nHuffman's greedy algorithm constructs optimal prefix codes called **Huffman\nCodes**.\n\nIt is given a set _C_ of _n_ characters, where each character has frequency\n_c.freq_ in the \"text\" to be encoded.\n\nThe optimality of a code is relative to a \"text\", which can be what we\nnormally think of as texts, or can be other data encoded as sequences of bits,\nsuch as images.\n\n  * We can construct a generic Huffman code for a universe of texts, such as all texts in English, by estimating the frequency of characters in this universe of texts.\n  * More commonly, we contruct Huffman codes optimized for particular documents. Then the document-specific code needs to be passed on along with the compressed document.\n\nThe algorithm creates a binary tree leaf node for each character, annotated\nwith its frequency, and the tree nodes are then put on a min-priority queue\n(this is only implied in line 2 below).\n\nThen the first two subtrees on the queue (those with minimum frequency) are\ndequeued with `Extract-Min`, merged into a single tree, annotated with the sum\nof their frequencies, and this single node is re-queued.\n\nThis process is repeated until only one tree node remains on the queue (the\nroot). Since a tree is being constructed and |_E_| = |_V_|−1 we can just run\nthe loop until _n_−1 and know that there will be one node left at this point.\n\nHere is the algorithm:\n\n![](fig/pseudocode-huffman.jpg)\n\n#### Informal Correctness\n\nThe \"greedy\" aspect is the choice to merge min-frequency nodes first, and\nassume that this local minimization will result in an optimal global solution.\n\nIntuitively, this approach will result in an optimal solution because the\nlowest frequency items will be \"pushed down\" deeper in the tree, and hence\nhave longer codes; while higher frequency items will end up nearer the root,\nand hence have the shortest codes.\n\nCormen et al. prove correctness with two Lemmas for the two properties:\n\n  * Greedy choice property: there exists an optimal prefix code where two characters having the lowest frequency in _C_ are encoded with equal length strings that differ only in the last bit, as they are leaf nodes. \n  * Optimal-substructure property: if the tree constructed by merging two nodes is optimal it must have been constructed from an optimal tree for the subproblem.\n\n#### Complexity\n\nThe initial BUILD-MIN-HEAP implied by line 2 requires O(_n_) time.  \nThe loop executes _n_ times, with O(lg _n_) required for each heap operation.  \nThus, HUFFMAN is O(_n_ lg _n_).\n\n### An Example of Huffman Coding\n\nThe characters are in a min priority queue by frequency:\n\n![](fig/Fig-16-5-a-huffman-trace.jpg)\n\nTake out the two lowest frequency items and make a subtree that is put back on\nthe queue as if it is a combined character:\n\n![](fig/Fig-16-5-b-huffman-trace.jpg) \n![](fig/pseudocode-huffman.jpg)\n\nCombine the next lowest frequency characters:\n\n![](fig/Fig-16-5-c-huffman-trace.jpg)\n\nContinuing, tree fragments themselves become subtrees:\n\n![](fig/Fig-16-5-d-huffman-trace.jpg)\n\nTwo subtrees are merged next:\n\n![](fig/Fig-16-5-e-huffman-trace.jpg)\n\nThe highest frequency character gets added to the tree last, so it will have a\ncode of length 1:\n\n![](fig/Fig-16-3-character-coding-problem.jpg)\n![](fig/Fig-16-5-f-huffman-trace.jpg)\n\nOne might wonder why the second most frequent character does not have a code\nof length 2. This would force the other characters to be deeper in the tree,\ngiving them excessively long codes.\n\n* * *\n\n## Wrapup\n\nWe will encounter several examples of greedy algorithms later in the course,\nincluding classic algorithms for finding minimum spanning trees (Topic\n[17](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-17.html))\nand shortest paths in graphs (Topics\n[18](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-18.html)\nand\n[19](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-19.html)).\n\n* * *\n\nDan Suthers Last modified: Sat Mar 1 17:53:22 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//130.greedy-algorithms/reading-notes-13.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-13a.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms and dynamic programming",
 "published"=>true,
 "morea_id"=>"reading-screencast-13a",
 "morea_summary"=>"Dynamic programming and greedy algorithsm.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=hWtmauUMYD8",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-13a.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-13a.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-13b.html</h2>

<pre>Hash
{"title"=>"Greedy algorithms and activity scheduling",
 "published"=>true,
 "morea_id"=>"reading-screencast-13b",
 "morea_summary"=>"Illustration of the greedy strategy.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=_-1nKy0bnwE",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-13b.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-13b.md"}
</pre>

<h2>/morea/130.greedy-algorithms/reading-screencast-13c.html</h2>

<pre>Hash
{"title"=>"Huffmann Codes",
 "published"=>true,
 "morea_id"=>"reading-screencast-13c",
 "morea_summary"=>"Design and implementation of Huffman Codes.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=UgJk87jazLQ",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/130.greedy-algorithms/reading-screencast-13c.html",
 "content"=>"",
 "path"=>"morea//130.greedy-algorithms/reading-screencast-13c.md"}
</pre>

<h2>/morea/140.graphs/experience-graph-bfs-dfs.html</h2>

<pre>Hash
{"title"=>"Breadth first search and depth first search",
 "published"=>true,
 "morea_id"=>"experience-graph-bfs-dfs",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with breadth first search and depth first search",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-graph-bfs-dfs.html",
 "url"=>"/morea/140.graphs/experience-graph-bfs-dfs.html",
 "content"=>
  "## Breadth first search and depth first search\n\n### Peer Credit Assignment\n\n**1.** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate across all members (_NOT_ 3 points per member!).\n  * If two members besides yourself were present, you have a total of 4 points to allocate across all members.\n  * If only one other member was present, you have a total of 6 points to allocate across all members.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### Breadth first search\n\n#### 2 points\n\n![](fig/pseudocode-BFS-Smaller.jpg)\n\nThe text (p. 597) analyzes Breadth-First-Search (BFS), using an _aggregate\nanalysis_ (what you are reading about for Tuesday's upcoming class) to show\nthat because the procedure scans the adjacency list of each vertex only when\nthe vertex is dequeued, and this happens only once, the total time spent\nscanning adjacency lists is O(E). Along with O(V) initialization this gives\nO(V+E) for the total running time.\n\n**2.** What would be the running time of BFS if we replace the adjacency list with an adjacency matrix and modify it at line 12 to handle this form of input? Justify your answer. \n\n(_Note:_ Change of representation is a common situation and I will also ask\nyou about the implications of change of representation on the next exam.)\n\n### Depth first search and Cycles\n\n#### 8 points; 2 each\n\n![](fig/pseudocode-DFS-DFS-Visit.jpg)\n\nDFS classifies edges as tree edges, back edges, forward edges, and cross edges\n(see p. 609).\n\n**3.** How can you modify DFS to detect back-edges? Say what code you would modify or insert, referencing CLRS line numbers.\n\n**4.** How can you modify DFS to determine whether a graph has a cycle? Again, say what code you would modify or insert, referencing CLRS line numbers. \n\n**5.** What is the run time of this modified cycle-detecting algorithm on _Directed_ graphs? Assume that it exits the DFS as soon as a cycle is found. Justify your answer.\n\n**6.** What is the run time of this modified cycle-detecting algorithm on _**Un**directed_ graphs? Assume that it exits the DFS as soon as a cycle is found. Justify your answer. \n\n* * *\n\nDan Suthers Last modified: Sat Mar 29 21:44:46 EDT 2014\n\n",
 "path"=>"morea//140.graphs/experience-graph-bfs-dfs.md"}
</pre>

<h2>/morea/140.graphs/experience-graph-scc.html</h2>

<pre>Hash
{"title"=>"Strongly connected components",
 "published"=>true,
 "morea_id"=>"experience-graph-scc",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Finding the strongly connected components through depth first search",
 "morea_sort_order"=>2,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-graph-scc.html",
 "url"=>"/morea/140.graphs/experience-graph-scc.html",
 "content"=>
  "## Finding SCC of a graph with DFS\n\n**1.** _Run DFS on this graph_. To make grading easier, _visit vertices in alphabetical order_ (both in the main loop of DFS and the adjacency list loop of DFS-Visit). For each vertex, show values _d_ (discovery), _f_ (finish), and π (parent). \n\n![](fig/SCC-Class-Problem-Graph.jpg)\n\nThe finish times from largest to smallest order the nodes in a manner\nconsistent with a topological sort of their SCCs.\n\n**2.** Now run DFS on the transpose graph, visiting vertices in order of finish time (highest to lowest) from the DFS of step 1 (as required by the SCC algorithm). Again, show values _d_ (discovery), _f_ (finish), and π (parent).\n\n![](fig/SCC-Class-Problem-Graph-Transpose.jpg)\n\n**3.** List the strongly connected components you found by first _listing the tree edges_ in the transpose graph that define the SCC, and then _listing the vertices_ in the SCC (the first is shown): \n    \n    \n    SCC 1:  Tree edges: { }; Vertices: {l} \n    SCC 2:  Tree edges: {...}; Vertices: {...} \n    SCC 3:  ... \n    \n\n\n",
 "path"=>"morea//140.graphs/experience-graph-scc.md"}
</pre>

<h2>/morea/140.graphs/experience-graph-transpose.html</h2>

<pre>Hash
{"title"=>"Computing Transpose",
 "published"=>true,
 "morea_id"=>"experience-graph-transpose",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply transpose under different representations",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/experience-graph-transpose.html",
 "url"=>"/morea/140.graphs/experience-graph-transpose.html",
 "content"=>
  "### Computing Transpose Under Different Representations\n\nThe **transpose** of a directed graph _G_ = (_V_, _E_) is the graph _G__T_ =\n(_V_, _E__T_), where _E__T_ = {(_v_, _u_) such that (_u_, _v_) ∈ _E_}. In\nother words, _G__T_ is the graph _G_ with all of its edges reversed.\n\nAlgorithms for computing transpose by copying to a new graph are trivial to\nwrite under matrix and adjacency list representations. However, we will be\nworking with very large graphs and want to avoid copies. Can we transpose a\ngraph \"in place\" without making a copy, and minimizing the amount of extra\nspace needed? What is the time and space cost to do so?\n\n(In the following, you are writing algorithms that work \"inside\" the ADT: you\nare allowed to access the matrix and list representations directly.)\n\n**1.** Describe an efficient algorithm for computing _G__T_ from _G_, _in place_ (don't make a copy), for the _adjacency-matrix_ representation of _G_. Do this by actually _modifying the matrix_. Analyze the space and time requirements of your algorithm. \n\n**2.** Now describe a way to compute _G__T_ from _G_ under the _adjacency-matrix_ representation _without modifying the matrix_, and by _using only one bit_ of extra storage! Analyze space and time requirements. (_Hint:_ You need only make it _appear_ to the client to be transposed.)\n\n**3.** Describe an efficient algorithm for computing _G__T_ from _G_ using the _adjacency-list_ representation of _G_, using _as little space as possible_. Can you do it \"in place\"? Analyze the space and time requirements of your algorithm. \n\n**4.** Now suppose you have an _edge-list_ representation of _G_. What algorithm would efficiently compute _G__T_ with the edge-list, and with what space and time requirements? \n\n\n",
 "path"=>"morea//140.graphs/experience-graph-transpose.md"}
</pre>

<h2>/morea/140.graphs/module-graphs.html</h2>

<pre>Hash
{"title"=>"Graphs",
 "published"=>true,
 "morea_id"=>"graphs",
 "morea_outcomes"=>["outcome-graphs"],
 "morea_readings"=>
  ["reading-screencast-14a",
   "reading-screencast-14b",
   "reading-screencast-14c",
   "reading-screencast-14d",
   "reading-screencast-14e",
   "reading-screencast-14f",
   "reading-cormen-22",
   "reading-sedgewick-32",
   "reading-sedgewick-wayne-4",
   "reading-goodrich-graphs",
   "reading-notes-14"],
 "morea_experiences"=>
  ["experience-graph-transpose",
   "experience-graph-scc",
   "experience-graph-bfs-dfs"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/140.graphs/module-graphs.png",
 "morea_sort_order"=>140,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/140.graphs/module-graphs.html",
 "content"=>
  "Definitions, methods, breadth first search, depth first search, shortest path, asymptotic complexity, topological sort, strongly connected components. \n",
 "path"=>"morea//140.graphs/module-graphs.md"}
</pre>

<h2>/modules/graphs/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-graphs.md",
 "title"=>"Graphs",
 "url"=>"/modules/graphs/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/graphs/index.html"}
</pre>

<h2>/morea/140.graphs/outcome-graphs.html</h2>

<pre>Hash
{"title"=>"Understand graphs",
 "published"=>true,
 "morea_id"=>"outcome-graphs",
 "morea_type"=>"outcome",
 "morea_sort_order"=>140,
 "referencing_modules"=>[#Jekyll:Page @name="module-graphs.md"],
 "url"=>"/morea/140.graphs/outcome-graphs.html",
 "content"=>
  "Be able to assess when to use different graph algorithms and why they might be preferred for a given problem context. ",
 "path"=>"morea//140.graphs/outcome-graphs.md"}
</pre>

<h2>/morea/140.graphs/reading-cormen-22.html</h2>

<pre>Hash
{"title"=>"CLRS 22 - Elementary graph algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-22",
 "morea_summary"=>
  "Representations of graphs, breadth-first search, depth-first search, topological sort, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "24 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-cormen-22.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-cormen-22.md"}
</pre>

<h2>/morea/140.graphs/reading-goodrich-graphs.html</h2>

<pre>Hash
{"title"=>"Goodrich and Tommassia 13 - Graphs",
 "published"=>true,
 "morea_id"=>"reading-goodrich-graphs",
 "morea_summary"=>
  "The graph ADT, data structures for graphs, graph traversals, directed graphs, weighted graphs, shortest paths, minimum spanning trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://ww0.java4.datastructures.net/",
 "morea_labels"=>["Textbook", "70 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-goodrich-graphs.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-goodrich-graphs.md"}
</pre>

<h2>/morea/140.graphs/reading-notes-14.html</h2>

<pre>Hash
{"title"=>"Notes on graphs",
 "published"=>true,
 "morea_id"=>"reading-notes-14",
 "morea_summary"=>
  "Graph definitions, examples, the ADT, representations, breadth first seach, depth first search, topological sort, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>10,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/140.graphs/reading-notes-14.html",
 "url"=>"/morea/140.graphs/reading-notes-14.html",
 "content"=>
  "## Outline\n\nTuesday\n\n  1. Graph Definitions & examples\n  2. Graph ADT\n  3. Representations (Implementations) of Graph ADT\n  4. Breadth-first Search \n\nThursday\n\n  1. Depth-first Search \n  2. Topological Sort \n  3. Strongly Connected Components \n  4. Related Concepts \n\nThe video lectures and notes below provide material not found in the textbook:\ndefining graphs, an ADT, and implementations. This material is important for\nProject 2.\n\n##  Graphs\n\n### Definitions\n\nA graph _G_ is a pair\n\n> _G_ = (_V_, _E_)\n\nwhere\n\n> _V_ = {_v_1, _v_2, ... _v_n}, a set of **vertices**\n\n> _E_ = {_e_1, _e_2, ... _e_m} ⊆ _V_ ⊗ _V_, a set of **edges**.\n\n#### Undirected Graphs\n\nIn an **undirected graph** the edge set E consists of _unordered pairs_ of\nvertices. That is, they are sets _e_ = {_u_, _v_}. Edges can be written with\nthis notation when clarity is desired, but we will often use parentheses (_u_,\n_v_).\n\nNo self loops are allowed in undirected graphs. That is, we cannot have (_v_,\n_v_), which would not make as much sense in the set notation {_v_, _v_}.\n\nWe say that _e_ = {_u_, _v_} is **incident on** _u_ and _v_, and that the\nlatter vertices are **adjacent**. The **degree** of a vertex is the number of\nedges incident on it.\n\nThe **handshaking lemma** is often useful in proofs:\n\n> Σ_v_∈_V_degree(_v_) = 2|_E_|\n\n(Each edge contributes two to the sum of degrees.)\n\n#### Directed Graphs\n\nIn a **directed graph** or **digraph** the edges are ordered pairs (_u_, _v_).\n\nWe say that _e_ = (_u_, _v_) is **incident from** or **leaves** _u_ and is\n**incident to** or **enters** _v_. The **in-degree** of a vertex is the number\nof edges incident to it, and the **out-degree** of a vertex is the number of\nedges incident from it.\n\n**Self loops** (_v_, _v_) are allowed in directed graphs.\n\n#### Paths\n\nA **path** of length _k_ is a sequence of vertices ⟨_v_0, _v_1, _v_2, ...\n_v_k⟩ where (_v__i_-1, _v__i_) ∈ _E_, for _i_ = 1, 2, ... _k_. (Some authors\ncall this a \"walk\".) The path is said to **contain** the vertices and edges\njust defined.\n\nA **simple path** is a path in which all vertices are distinct. (The \"walk\"\nauthors call this a \"path\").\n\nIf a path exists from _u_ to _v_ we say that _v_ is **reachable** from _u_.\n\nIn an undirected graph, a path ⟨_v_0, _v_1, _v_2, ... _v_k⟩ forms a **cycle**\nif _v_0 = _v_k and _k_ ≥ 3 (as no self-loops are allowed).\n\nIn a directed graph, a path forms a **cycle** if _v_0 = _v_k and the path\ncontains at least one edge. (This is clearer than saying that the path\ncontains at least two vertices, as self-loops are possible in directed\ngraphs.) The cycle is **simple** if _v_1, _v_2, ... _v_k are distinct (i.e.,\nall but the designated start and end _v_0 = _v_k are distinct). A directed\ngraph with no self-loops is also **simple**.\n\nA graph of either type with no cycles is **acyclic**. A directed acyclic graph\nis often called a **dag**.\n\n#### Connectivity\n\nA graph _G'_ = (_V'_, _E'_) is a **subgraph** of _G_ = (_V_, _E_) if _V'_ ⊆\n_V_ and _E'_ ⊆ _E_.\n\nAn undirected graph is **connected** if every vertex is reachable from all\nother vertices. In any connected undirected graph, |_E_| ≥ |_V_| - 1 (see also\n[discussion of tree properties](http://www2.hawaii.edu/~suthers/courses/ics311\ns14/Notes/Topic-08.html)). The **connected components** of _G_ are the maximal\nsubgraphs _G_1 ... _G__k_ where every vertex in a given subgraph is reachable\nfrom every other vertex in that subgraph, but not reachable from any vertex in\na different subgraph.\n\nA directed graph is **strongly connected** if every two vertices are reachable\nfrom each other. The **strongly connected components** are the subgraphs\ndefined as above. A directed graph is thus strongly connected if it has only\none strongly connected component. A directed graph is **weakly connected** if\nthe underlying undirected graph (converting all tuples (_u_, _v_) ∈ _E_ into\nsets {_u_, _v_} and removing self-loops) is connected.\n\n#### Variations\n\nA **bipartite** graph is one in which _V_ can be partitioned into two sets\n_V_1 and _V_2 such that every edge connects a vertex in _V_1 to one in _V_2.\nEquivalently, there are no odd-length cycles.\n\nA **complete** graph is an undirected graph in which every pair of vertices is\nadjacent.\n\nA **weighted** graph has numerical weights associated with the edges. (The\nallowable values depend on the application. Weights are often used to\nrepresent distance, cost or capacity in networks.)\n\n### Graph Size in Analysis\n\nAsymptotic analysis is often in terms of both |_V_| and |_E_|. Within\nasymptotic notation we leave out the \"|\" for simplicity, for example, writing\nO(_V_ \\+ _E_), O(_V_2 lg _E_), etc.\n\n### Many Applications ...\n\n![](fig/GT-Map-Graph-Example.jpg) ![](fig/GT-Circuit-Graph-Example.jpg) ![](fig/social-network.jpg)\n![](fig/240px-Internet_map_1024.jpg)\n\n* * *\n\n## Graph ADT\n\nThese are detailed slightly more in [Project\n2](http://www2.hawaii.edu/~suthers/courses/ics311s14/Projects/Project-2.html),\nand in the Goodrich & Tamassia excerpt uploaded to Laulima.\n\n#### Graph Accessors\n\n**numVertices()**  \n    Returns the number of vertices |_V_|\n\n**numEdges()**   \n    Returns the number of edges |_E_|\n\n**vertices()**  \n    Returns an iterator over the vertices _V_\n\n**edges()**  \n    Returns an iterator over the edges _E_\n\n#### Accessing Undirected Graphs\n\n**degree(_v_)**  \n    Returns the number of edges (directed and undirected) incident on _v_.\n\n**adjacentVertices(_v_)**  \n    Returns an iterator of the vertices adjacent to _v_.\n\n**incidentEdges(_v_)**  \n    Returns an iterator of the edges incident on _v_. \n\n**endVertices(_e_)**  \n    Returns an array of the two end vertices of _e_.\n\n**opposite(_v_,_e_)**  \n    Given _v_ is an endpoint of _e_.  \n    Returns the end vertex of _e_ different from _v_.   \n    Throws InvalidEdgeException when _v_ is not an endpoint of _e_.\n\n**areAdjacent(_v_1,_v_2)**  \n    Returns true iff _v_1 and _v_2 are adjacent by a single edge. \n\n####  Accessing Directed Graphs\n\n**directedEdges()**  \n    Returns an iterator over the directed edges of _G_. \n\n**undirectedEdges()**  \n    Returns an iterator over the undirected edges of _G_. \n\n**inDegree(_v_)**  \n    Returns the number of directed edges (arcs) incoming to _v_. \n\n**outDegree(_v_)**  \n    Returns the number of directed edges (arcs) outgoing from _v_.\n\n**inAdjacentVertices(_v_)**  \n    Returns an iterator over the vertices adjacent to _v_ by incoming edges. \n\n**outAdjacentVertices(_v_)**  \n    Returns an iterator over the vertices adjacent to _v_ by outgoing edges. \n\n**inIncidentEdges(_v_)**  \n    Returns an iterator over the incoming edges of _v_. \n\n**outIncidentEdges(_v_)**  \n    Returns an iterator over the outgoing edges of _v_. \n\n**destination(_e_)**  \n    Returns the destination vertex of _e_, if _e_ is directed.  \n    Throws InvalidEdgeException when _e_ is undirected.\n\n**origin(_e_)**  \n    Returns the origin vertex of _e_, if _e_ is directed.  \n    Throws InvalidEdgeException when _e_ is undirected. \n\n**isDirected(_e_)**  \n    Returns true if _e_ is directed, false otherwise\n\n#### Mutators (Undirected and Directed)\n\n**insertEdge(_u_,_v_)**  \n**insertEdge(_u_,_v_,_o_)**  \n    Inserts a new undirected edge between two existing vertices, optionally containing object _o_.  \n    Returns the new edge. \n\n**insertVertex()**  \n**insertVertex(_o_)**  \n    Inserts a new isolated vertex optionally containing an object _o_ (e.g., the label associated with the vertex).  \n    Returns the new vertex. \n\n**insertDirectedEdge(_u_,_v_)**  \n**insertDirectedEdge(_u_,_v_,_o_)**  \n    Inserts a new directed edge from an existing vertex to another.  \n    Returns the new edge. \n\n**removeVertex(_v_)**  \n    Deletes a vertex and all its incident edges.  \n    Returns object formerly stored at _v_.\n\n**removeEdge(_e_)**  \n    Removes an edge.  \n    Returns the object formerly stored at _e_.\n\n#### Annotators (for vertices and all types of edges)\n\nMethods for annotating vertices and edges with arbitrary data.\n\n**setAnnotation(Object _k_, _o_)**  \n    Annotates a vertex or edge with object _o_ indexed by key _k_.\n\n**getAnnotation(Object _k_)**  \n    Returns the object indexed by _k_ annotating a vertex or edge.\n\n**removeAnnotation(Object _k_)**  \n    Removes the annotation on a vertex or edge indexed by _k_ and returns it.\n\n#### Changing Directions\n\nThere are various methods for changing the direction of edges. I think the\nonly one we will need is:\n\n**reverseDirection(_e_)**  \n    Reverse the direction of an edge.  \n    Throws InvalidEdgeException if the edge is undirected\n\n* * *\n\n##  Graph Representations\n\nThere are two classic representations: the adjacency list and the adjacency\nmatrix.\n\nIn the **adjacency list**, vertices adjacent to vertex _v_ are listed\nexplicitly on linked list _G_.Adj[_v_] (assuming an array representation of\nlist headers).\n\nIn the **adjacency matrix**, vertices adjacent to vertex _v_ are indicated by\nnonzero entries in the row of the matrix indexed by v, in the columns for the\nadjacent vertices.\n\nAdjacency List and Matrix representations of an undirected graph:\n\n![](fig/Fig-22-1-representations-undirected.jpg)\n\nAdjacency List and Matrix representations of a directed graph:\n\n![](fig/Fig-22-2-representations-directed.jpg)\n\n_Consider this before reading on: What are the asymptotic complexities of\nthese methods in each representation? _\n\n  * List vertices/edges \n  * areAdjacent \n  * access (out)AdjacentVertices or (out)IncidentEdges (outdegree) \n  * access (in)AdjacentVertices or (in)IncidentEdges (indegree) \n\n_Are edges first class objects in the above representations? Where do you\nstore edge information in the undirected graph representations?_\n\n### Complexity Analysis\n\n#### Adjacency List\n\nSpace required: Θ(_V_ \\+ _E_).\n\nTime to list all vertices adjacent to u: Θ(degree(_u_)).\n\nTime to determine whether (_u_, _v_) ∈ _E_: O(degree(_u_)).\n\n#### Adjacency Matrix\n\nSpace required: Θ(_V_2).\n\nTime to list all vertices adjacent to _u_: Θ(_V_).\n\nTime to determine whether (_u_, _v_) ∈ _E_: Θ(1).\n\nSo the matrix takes more space and more time to list adjacent matrices, but is\nfaster to test adjacency of a pair of matrices.\n\n### \"Modern\" Adjacency Representation\n\nGoodrich & Tamassia (reading in Laulima) propose a representation that\ncombines an edge list, a vertex list, and an adjacency list for each vertex:\n\n![](fig/GT-adjacency-list-structure.jpg)\n\nThe sets _V_ and _E_ can be represented using a dictionary ADT (from\nImplementation Project #1). In many applications, it is especially important\nfor _V_ to enable fast access by key, and may be important to access in order.\nEach vertex object has an adjacency list _I_ (I for incident), and the edges\nreference both the vertices they connect and the entries in this adjacency\nlist. There's a lot of pointers to maintain, but this enables fast access in\nany direction you need, and for large sparse graphs the memory allocation is\nstill less than for a matrix representation.\n\nSee also Newman (2010) chapter 9, posted in Laulima, for discussion of graph\nrepresentations.\n\n* * *\n\n## BFS and DFS Overview\n\nBefore starting with Cormen et al.'s more complex presentation, let's discuss\nhow BFS and DFS can be implemented with nearly the same algorithm, but using a\nqueue for BFS and a stack for DFS. You should be comfortable with this\nrelationship between BFS/queues and DFS/stacks.\n\nSketch of both algorithms:\n\n  1. Pick a starting vertex and put it on the queue (BFS) or stack (DFS) \n  2. Repeat until the queue/stack is empty:\n    1. Dequeue (BFS) or pop (DFS) the next vertex _v_ from the appropriate data structure\n    2. If _v_ is unvisited, \n      * Mark _v_ as visited (and process it as needed for the specific application). \n      * Find the unvisted neighbors of _v_ and queue (BFS) or push (DFS) them on the appropriate data structure. \n\nTry starting with vertex q and run this using both a stack and a queue:\n\n![](fig/small-example-digraph.jpg)\n![](fig/small-example-digraph.jpg)\n\nBFS's FIFO queue explores nodes at each distance before going to the next\ndistance. DFS's LIFO stack explores the more distant neighbors of a node\nbefore continuing with nodes at the same distance (\"goes deep\").\n\nSearch in a directed graph that is weakly but not strongly connected may not\nreach all vertices.\n\n* * *\n\n## Breadth-first Search\n\nGiven a graph _G_ = (_V_,_E_) and a source vertex _s_ ∈ _V_, output _v.d_, the\nshortest distance (# edges) from _s_ to _v_, for all _v_ ∈ _V_. Also record\n_v.π_ = _u_ such that (_u_,_v_) is the last edge on a shortest path from _s_\nto _v_. (We can then trace the path back.)\n\n_Analogy_ Send a \"tsunami\" out from _s_ that first reaches all vertices 1 edge\nfrom _s_, then from them all vertices 2 edges from _s_, etc. Like a tsunami,\nequidistant destinations are reached at the \"same time\".\n\nUse a FIFO queue _Q_ to maintain the wavefront, such that _v_ ∈ _Q_ iff the\ntsunami has hit _v_ but has not come out of it yet.\n\n![](fig/bfs-nature-of-computation.jpg) ![](fig/pseudocode-BFS.jpg)\n\nAt any given time _Q_ has vertices with _d_ values _i, i, ... i, i+1, i+1, ...\ni+1_. That is, there are at most two distances on the queue, and values\nincrease monotonically.\n\n### Examples\n\n#### Book's Example: Undirected Graph\n\n![](fig/Fig-22-3-operation-BFS.jpg)\n\n#### A directed example:\n\nLet's do another (number the nodes by their depth, then click to compare your\nanswer):\n\n![](fig/BFS-directed-graph-example-1.jpg)\n\n### Time Analysis\n\n(This is an aggregate analysis.) Every vertex is enqueued at most once. We\nexamine edge (_u_, _v_) only when _u_ is dequeued, so every edge is examined\nat most once if directed and twice if undirected. Therefore, O(_V_ \\+ _E_).\n\n### Shortest Paths\n\n**Shortest distance** δ(_s_, _v_) from _s_ to _v_ is the minimum number of edges across all paths from _s_ to _v_, or ∞ if no such path exists.\n\nA **shortest path** from _s_ to _v_ is a path of length δ(_s_, _v_).\n\nIt can be shown that BFS is guaranteed to find the shortest paths to all\nvertices from a start vertex _s_: _v_._d_ = δ(_s_, _v_), ∀ _v_ at the\nconclusion of the algorithm. See book for tedious proof.\n\nInformally, we can see that all vertices at distance 1 from _s_ are enqueued\nfirst, then via them all nodes of distance 2 are reached and enqueued, etc.,\nso inductively it would be a contradiction if BFS reached a vertex _c_ by a\nlonger path than the shortest path because the last vertex _u_ on the shortest\npath to the given vertex _v_ would have been enqueued first and then dequeued\nto reach _v_.\n\n### Breadth-First Trees\n\nThe **predecessor subgraph** of G is\n\n> Gπ = (Vπ, Eπ) where  \nVπ= {_v_ ∈ V : _v_.π ≠ NIL} ∪ {_s_} and  \nEπ = {(_v_.π, _v_) : _v_ ∈ Vπ \\- {_s_}}\n\nA predecessor subgraph Gπ is a **breadth-first tree** if Vπ consists of\nexactly all vertices reachable from _s_ and for all _v_ in Vπ the subgraph Gπ\ncontains unique simple and _ shortest_ paths from _s_ to _v_.\n\nBFS constructs π such that Gπ is a breadth-first tree.\n\n* * *\n\n## Depth-first Search\n\nGiven _G_ = (_V_, _E_), directed or undirected, DFS explores the graph from\nevery vertex (no source is vertex given), constructing a _forest_ of trees and\nrecording two time stamps on each vertex:\n\n  * _v.d_ = discovery time\n  * _v.f_ = finishing time\n\nTime starts at 0 before the first vertex is visited, and is incremented by 1\nfor every discovery and finishing event (as explained below). These attributes\nwill be used in other algorithms later on.\n\nSince each vertex is discovered once and finished once, discovery and\nfinishing times are unique integers from 1 to 2|_V_|, and for all _v_, _v.d_ <\n_v.f_.\n\n_(Some presentations of DFS pose it as a way to visit nodes, enabling a given\nmethod to be applied to the nodes with no output specified. Others present it\nas a way to construct a tree. The CLRS presentation is more complex but\nsupports a variety of applications.)_\n\nDFS explores _every_ edge and starts over from different vertices if necessary\nto reach them (unlike BFS, which may fail to reach subgraphs not connected to\n_s_).\n\nAs it progresses, every vertex has a color:\n\n> WHITE = undiscovered\n\n>  \nGRAY = discovered, but not finished (still exploring vertices reachable from\nit)\n\n>     _v_._d_ records the moment at which _v_ is _discovered_ and colored\ngray.\n\n>  \nBLACK = finished (have found everything reachable from it)\n\n>     _v_._f_ records the moment at which _v_ is _finished_ and colored black.\n\n![](fig/dfs-nature-of-computation.jpg)\n\n### Pseudocode\n\n![](fig/pseudocode-DFS.jpg) ![](fig/pseudocode-DFS-Visit.jpg)\n\nWhile BFS uses a queue, DFS operates in a stack-like manner (using the\nimplicit recursion stack in the algorithm above).\n\n  * BFS's FIFO queue explores nodes at each distance before going to the next distance\n  * DFS's implicit LIFO stack explores the more distant neighbors of a node before continuing with nodes at the same distance (\"goes deep\").\n\nAnother major difference in the algorithms as presented here is that DFS will\nsearch from every vertex until all edges are explored, while BFS only searches\nfrom a designated start vertex.\n\n  * This is not an essential difference: both could be either restricted to a start vertex or run from all vertices; \n  * This reflects how the algorithms are used in practice (BFS for finding shortest paths; DFS for exposing structure of the graph, as will be explained shortly). \n\n### Example:\n\nOne could start DFS with any arbitrary vertex, and continue at any remaining\nvertex after the first tree is constructed. Regularities in the book's\nexamples (e.g., processing vertices in alphabetical order, or always starting\nat the top of the diagram) do not reflect a requirement of the algorithm.\n\n![](fig/Fig-22-4-operation-DFS-a.jpg)\n![](fig/Fig-22-4-operation-DFS-b.jpg)\n![](fig/Fig-22-4-operation-DFS-c.jpg)\n![](fig/Fig-22-4-operation-DFS-d.jpg)\n![](fig/Fig-22-4-operation-DFS-e.jpg)\n![](fig/Fig-22-4-operation-DFS-f.jpg)\n![](fig/Fig-22-4-operation-DFS-g.jpg)\n![](fig/Fig-22-4-operation-DFS-h.jpg)\n![](fig/Fig-22-4-operation-DFS-i.jpg)\n![](fig/Fig-22-4-operation-DFS-j.jpg)\n![](fig/Fig-22-4-operation-DFS-k.jpg)\n![](fig/Fig-22-4-operation-DFS-l.jpg)\n![](fig/Fig-22-4-operation-DFS-m.jpg)\n![](fig/Fig-22-4-operation-DFS-n.jpg)\n![](fig/Fig-22-4-operation-DFS-o.jpg)\n![](fig/Fig-22-4-operation-DFS-p.jpg)\n\nLet's do this example (start with the upper left node, label the nodes with\ntheir d and f, then click to compare your answer):\n\n![](fig/DFS-directed-graph-example-0.jpg)\n\n#### Time Analysis\n\n![](fig/pseudocode-DFS-DFS-Visit.jpg)\n\nThe analysis uses aggregate analysis, and is similar to the BFS analysis,\nexcept that DFS is guaranteed to visit every vertex and edge, so it is Θ not\nO:\n\n> Θ(_V_) to visit all vertices in lines 1 and 5 of `DFS`;  \n  \nΣ_v_∈_V_ |Adj(_v_)| = Θ(_E_) to process the adjacency lists in line 4 of `DFS-\nVisit`.  \n  \n(_Aggregate analysis:_ we are not attempting to count how many times the loop\nof line 4 executes each time it is encountered, as we don't know |Adj(_v_)|.\nInstead, we sum the number of passes through the loop in total: all edges will\nbe processed.)  \n  \nThe rest is constant time.\n\nTherefore, Θ(_V_ \\+ _E_).\n\n### Classification of Edges\n\nThis classification will be useful in forthcoming proofs and algorithms.\n\n  * **Tree Edge**: in the **depth-first forest** constructed by DFS: found by exploring (_u_,_v_). \n  * **Back Edge**: (_v_,_u_), where _v_ is a descendant of _u_.\n  * **Forward Edge**: (_u_,_v_), where _v_ is a descendant of _u_ but not a tree edge.\n  * **Cross Edge**: any other edge. They can go between vertices in the same depth-first tree or in different depth-first trees.\n\nHere's a graph with edges classified, and redrawn to better see the structural\nroles of the different kinds of edges:\n\n![](fig/Fig-22-5-DFS-properties-a.jpg)\n![](fig/Fig-22-5-DFS-properties-c.jpg)\n\n### DFS Properties\n\nThese theorems show important properties of DFS that will be used later to\nshow how DFS exposes properties of the graph.\n\n#### Parentheses Theorem\n\nAfter any DFS of a graph _G_, for any two vertices _u_ and _v_ in _G_, exactly\none of the following conditions holds:\n\n  * The intervals [_u_._d_, _u_._f_] and [_v_._d_, _v_._f_] are entirely disjoint, and neither _u_ nor _v_ is a descendant of the other in the DFS forest.\n  * The interval [_u_._d_, _u_._f_] is contained entirely within the interval [_v_._d_, _v_._f_], and _u_ is a descendant of _v_ in a DFS tree. \n  * The interval [_v_._d_, _v_._f_] is contained entirely within the interval [_u_._d_, _u_._f_], and _v_ is a descendant of _u_ in a DFS tree. \n\nEssentially states that the _d_ and _f_ visit times are well nested. See text\nfor proof. For the above graph:\n\n![](fig/Fig-22-5-DFS-properties-b.jpg)\n![](fig/Fig-22-5-DFS-properties-a.jpg)\n\n#### Corollary: Nesting of Descendant's Intervals\n\nVertex _v_ is a **_proper descendent_** of vertex _u_ in the DFS forest of a\ngraph iff _u_._d_ < _v_._d_ < _v_._f_ < _u_._f_. (Follows immediately from\nparentheses theorem.)\n\nAlso, (_u_, _v_) is a **_back edge_** iff _v_._d_ ≤ _u_._d_ < _u_._f_ ≤\n_v_._f_; and a **_cross edge_** iff _v_._d_ < _v_._f_ < _u_._d_ < _u_._f_.\n\n#### White Path Theorem\n\nVertex _v_ is a descendant of _u_ iff at time _u.d_ there is a path from _u_\nto _v_ consisting of only white vertices (except for _u_, which was _just_\ncolored gray).\n\n(Proof in textbook uses _v_._d_ and _v_._f_. Metaphorically and due to its\ndepth-first nature, if a search encounters an unexplored location, all the\nunexplored territory reachable from this location will be reached before\nanother search gets there.)\n\n#### DFS Theorem\n\nDFS of an undirected graph produces only tree and back edges: never forward or\ncross edges.\n\n(Proof in textbook uses _v_._d_ and _v_._f_. Informally, this is because the\nedges being bidirectional, we would have traversed the supposed forward or\ncross edge earlier as a tree or back edge.)\n\n* * *\n\n##  Topological Sort\n\nA **directed acyclic graph** (DAG) is a good model for processes and\nstructures that have partial orders: You may know that _a_ > _c_ and _b_ > _c_\nbut may not have information on how _a_ and _b_ compare to each other.\n\nOne can always make a **total order** out of a partial order. This is what\ntopological sort does. A **topological sort** of a DAG is a linear ordering of\nvertices such that if (_u_, _v_) ∈ _E_ then _u_ appears somewhere before _v_\nin the ordering.\n\n### Outline of Algorithm:\n\n`Topological-Sort(G)` is actually a modification of `DFS(G)` in which each\nvertex _v_ is inserted onto the front of a linked list as soon as finishing\ntime _v_._f_ is known.\n\n### Examples\n\nSome real world examples include\n\n  * Scheduling 100,000 independent tasks on a high performance computing system (research by Dr. Henri Casanova) \nProducing 5,000,000 documents that reference each other such that each\ndocument is produced before the ones that reference it.\n\nHere is the book's example ... a hypothetical professor (not me!) getting\ndressed _(what node did they start the search at? Could it have been done\ndifferently?)_:\n\n![](fig/Fig-22-7-topological-sort.jpg)\n\nWe can make it a bit more complex, with catcher's outfit (click to compare\nyour answer):\n\n![](fig/DAG-topological-sort-example-1.jpg)\n\n_The answer given starts with the batting glove and works left across the\nunvisted nodes. What if we had started the search with socks and worked right\nacross the top nodes? If you put your clothes on differently, how could you\nget the desired result? Hint: add an edge._\n\nAs noted previously, one could start with any vertex, and once the first tree\nwas constructed continue with any artibrary remaining vertex. It is not\nnecessary to start at the vertices at the top of the diagram. _Do you see\nwhy?_\n\n![](fig/pseudocode-topological-sort.jpg)\n\n### Time Analysis\n\nTime analysis is based on simple use of DFS: Θ(_V_ \\+ _E_).\n\n### Correctness\n\n**_Lemma_**: A directed graph _G_ is acyclic iff a DFS of _G_ yields no back edges. \n\nSee text for proof, but it's quite intuitive:\n\n> ⇒ A back edge by definition is returning to where one started, which means\nit completes a cycle.  \n⇐ When exploring a cycle the last edge explored will be a return to the vertex\nby which the cycle was entered, and hence classified a back edge.\n\n**_Theorem:_** If `G` is a DAG then `Topological-Sort(G)` correctly produces a topological sort of `G`. \n\nIt sufficies to show that\n\n> if (_u_, _v_) ∈ _E_ then _v.f_ < _u.f_\n\nbecause then the linked list ordering by _f_ will respect the graph topology).\n\nWhen we explore (_u_, _v_), what are the colors of _u_ and _v_?\n\n  * _u_ is gray, because it is being explored when (_u_, _v_) is found. \n  * Can _v_ be gray too? No, because then _v_ would be an ancestor of _u_, meaning (_u_, _v_) is a back edge, contradicting the DAG property by the lemma above.\n  * Is _v_ white? Then it becomes a descendant of _u_. By the parentheses theorem, _u.d_ < _v.d_ <**_v.f_ < _u.f_**. \n  * Is _v_ black? Then _v_ is finished. Since we are exploring (_u_, _v_) we have not finished _u_. Therefore **_v.f_ < _u.f_**.\n\n* * *\n\n##  Strongly Connected Components\n\nGiven a directed graph _G_ = (_V_, _E_), a **strongly connected component\n(SCC)** of _G_ is a maximal set of vertices _C_ ⊆ _V_ such that for all _u_,\n_v_ ∈ _C_, there is a path both from _u_ to _v_ and from _v_ to _u_.\n\n#### Example:\n\nWhat are the Strongly Connected Components? (Click to see.)\n\n![](fig/SCC-example-0.jpg)\n\n### Algorithm\n\nThe algorithm uses _GT_= (_V_, _ET_), the **transpose** of _G_ = (_V_, _E_).\n_GT_ is _G_ with all the edges reversed.\n\n    \n    \n    Strongly-Connected-Components (G)\n    1.  Call DFS(G) to compute finishing times _u_._f_ for each vertex _u_ ∈ _E_. \n    2.  Compute _GT_\n    3.  Call modified DFS(_GT_) that considers vertices\n        in order of decreasing _u_._f_ from line 1.\n    4.  Output the vertices of each tree in the depth-first forest\n        formed in line 3 as a separate strongly connected component. \n    \n\n### Example 1\n\n#### First Pass of DFS:\n\n![](fig/Fig-22-9-SCC-by-DFS-a.jpg)\n\n#### Second Pass of DFS:\n\n![](fig/Fig-22-9-SCC-by-DFS-b.jpg)\n\n### Why it Works\n\n#### Informal Explanation\n\n_(This is from my own attempt to understand the algorithm. It differs from the\nbook's formal proof.)_\n\n_G_ and _GT_ have the same SCC.     _Proof:_\n\n  * If _u_ and _v_ are in the same SCC in _G_, then there is a path _p_1 from _u_ to _v_ and a path _p_2 from _v_ to _u_.\n  * Reversing the edges, path _p_1 becomes a path from _v_ to _u_ and _p_2 becomes a path from _u_ to _v_.\n\nA DFS from any vertex _v_ in a SCC _C_ will reach _all_ vertices in _C_ (by\ndefinition of SCC).\n\n  * Then why can't we call DFS on unvisited vertices to find the SCCs in the first pass, line 1? \n  * Because this first unconstrained DFS could also get vertices _not_ in _C_, as there may be a path from _v_ in _C_ to _v'_ where there is no path from _v'_ to _v_ (so _v'_ is not in _C_)!\n\nSo how does the second search on _GT_ help avoid inadvertent inclusion of _v'_\nin _C_?\n\n  * _v'_ will have an earlier finishing time than some of the other vertices in _C_, because at least some of those vertices (in particular, _v_ from which _v'_ was reached) are still active (gray) when _v'_ is finished (Parentheses Theorem). \n  * In the second search, the component _C_ to which _v_ belongs is processed before _v'_ and its component, because _v_ has a later finishing time, so the entire component will be explored before other components (in particular, that containing _v'_). \n  * Since _GT_ has the same SCC as _G_, the component found from _v_ in the second search is the same component as in the previous search. \n  * But in this second search, _v'_ will _not_ be reached. Why? Because we are using reversed edges in _GT_. If _v'_ could be reached from _C_ in _GT_, then _v_ would be reachable from _v'_ in _G_, and so _v'_ would be a member of _C_, a contradiction.\n  * So, due to the topological sort, the trees constructed in the second search cannot contain vertices _v'_ that do not belong to the SCC of the other vertices in the tree. This along with the fact that any DFS from a vertex _v_ in a SCC _C_ will find _all_ vertices in _C_ means that the trees constructed by the second DFSs find exactly the vertices in the SCCs.\n\nIn the example above, notice how node _c_ corresponds to _v_ and _g_ to _v'_\nin the argument above. But we need to also say why nodes like _b_ will never\nbe reached from _c_ in the second search. It is because _b_ finished later in\nthe first search, so was processed earlier and already \"consumed\" by the\ncorrect SCC in the second search, before the search from _c_ could reach it.\nThe following fact is useful in understanding why this would be the case.\n\n#### Component Graph\n\n  * Define _GSCC_ to be a graph of the SCCs of G obtained by collapsing all the vertices in each SCC into one vertex for the component but retaining the edges between SCCs.\n  * Then this **component graph _GSCC_ is a Directed Acyclic Graph**. (If there were any cycles, vertices in each component would be reachable from all others, so they would be one component.) \n\nHere is _GSCC_ for the above example:\n\n![](fig/Fig-22-9-SCC-by-DFS-c.jpg)\n\nThe first pass of the SCC algorithm essentially does a topological sort of the\ngraph _GSCC_ (by doing a topological sort of constituent vertices). The second\npass visits the components of _GTSCC_ in topologically sorted order such that\n**_each component is searched before any component that can reach that\ncomponent_**.\n\nThus, for example, the component _abe_ is processed first in the second\nsearch, and since this second search is of _GT_ (reverse the arrows above) one\ncan't get to _cd_ from _abe_. When _cd_ is subsequently searched, one can get\nto _abe_ but it's vertices have _already been visited_ so can't be incorrectly\nincluded in _cd_.\n\n### Example 2\n\nStart at the node indicated by the arrow; conduct a DFS; then click to compare\nyour answer:\n\n![](fig/SCC-example-0-start.jpg)\n\n### Analysis\n\nWe have provided an informal justification of correctness: please see the CLRS\nbook for a formal proof of correctness for the SCC algorithm.\n\n![](fig/pseudocode-SCC.jpg)\n\nThe CLRS text says we can create _GT_ in Θ(_V_ \\+ _E_) time using adjacency\nlists.\n\n  * The easy approach is to simply copy the graph, but given the size of some graphs we work with, it would be much better to reverse the edges in place (and reverse them back when done). \n  * _ Problem for class: How can this be done? (A naive implementation could end up undoing some of its own work, as it confuses already-reversed edges with those to be reversed.)_\n\nThe SCC algorithm also has two calls to DFS, and these are Θ(V + E) each.\n\nAll other work is constant, so the overall time complexity is Θ(V + E).\n\n* * *\n\n##  Related Graph Concepts\n\n![](fig/Fig-22-10-articulations-bridges-biconnected.jpg)\n\nAn **articulation point** or **cut vertex** is a vertex that when removed\ncauses a (strongly) connected component to break into two components.\n\nA **bridge** is an edge that when removed causes a (strongly) connected\ncomponent to break into two components.\n\nA **biconnected component** is a maximal set of edges such that any two edges\nin the set lie on a common simple cycle. This means that there is no bridge\n(at least two edges must be removed to disconnect it). This concept is of\ninterest for network robustness.\n\n* * *\n\n##  Up Next\n\nWe take a brief diversion from graphs to introduce amortized analysis and\nefficient processing of union and find operations on sets, both of which will\nbe used in subsequent work on graphs. Then we return to graphs with concepts\nof minimum spanning trees, shortest paths, and flows in networks.\n\n* * *\n\nDan Suthers Last modified: Sat Mar 8 00:37:35 HST 2014  \nSome images are from the instructor's material for Cormen et al. Introduction\nto Algorithms, Third Edition.  \n\n",
 "path"=>"morea//140.graphs/reading-notes-14.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14a.html</h2>

<pre>Hash
{"title"=>"Graph definitions",
 "published"=>true,
 "morea_id"=>"reading-screencast-14a",
 "morea_summary"=>"Graph definitions",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=IdStgDmUlXM",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14a.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14a.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14b.html</h2>

<pre>Hash
{"title"=>"Graph ADT and representations",
 "published"=>true,
 "morea_id"=>"reading-screencast-14b",
 "morea_summary"=>"Methods in the graph ADT",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=pyDxf58rK_0",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14b.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14b.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14c.html</h2>

<pre>Hash
{"title"=>"Breadth first search",
 "published"=>true,
 "morea_id"=>"reading-screencast-14c",
 "morea_summary"=>
  "Breadth first search and applications to finding the shortest path",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=A8SKOFseOyU",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14c.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14c.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14d.html</h2>

<pre>Hash
{"title"=>"Depth first search",
 "published"=>true,
 "morea_id"=>"reading-screencast-14d",
 "morea_summary"=>"Depth first search and its asymptotic complexithy",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=emiWSGizJlc",
 "morea_labels"=>["Screencast", "Suthers", "10 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14d.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14d.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14e.html</h2>

<pre>Hash
{"title"=>"Depth first search: example and properties",
 "published"=>true,
 "morea_id"=>"reading-screencast-14e",
 "morea_summary"=>"Depth first search trace.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://www.youtube.com/watch?v=HMPaUoGfsPo",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14e.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14e.md"}
</pre>

<h2>/morea/140.graphs/reading-screencast-14f.html</h2>

<pre>Hash
{"title"=>"Topological sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-14f",
 "morea_summary"=>"Topological sort and strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://www.youtube.com/watch?v=TZDQHplPrNo",
 "morea_labels"=>["Screencast", "Suthers", "26 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-screencast-14f.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-screencast-14f.md"}
</pre>

<h2>/morea/140.graphs/reading-sedgewick-32.html</h2>

<pre>Hash
{"title"=>"Sedgewick 32 - Directed graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-32",
 "morea_summary"=>
  "Depth first search, transitive closure, topological sorting, strongly connected components",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "12 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-sedgewick-32.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-sedgewick-32.md"}
</pre>

<h2>/morea/140.graphs/reading-sedgewick-wayne-4.html</h2>

<pre>Hash
{"title"=>"Sedgewick and Wayne 4 - Graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-wayne-4",
 "morea_summary"=>
  "Undirected graphs, directed graphs, minimum spanning trees, shortest paths",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://algs4.cs.princeton.edu/40graphs/",
 "morea_labels"=>["Textbook", "12 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/140.graphs/reading-sedgewick-wayne-4.html",
 "content"=>"",
 "path"=>"morea//140.graphs/reading-sedgewick-wayne-4.md"}
</pre>

<h2>/morea/150.amortized-analysis/experience-amortized-analysis.html</h2>

<pre>Hash
{"title"=>"The accounting method",
 "published"=>true,
 "morea_id"=>"experience-amortized-analysis",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with amortized analysis",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/150.amortized-analysis/experience-amortized-analysis.html",
 "url"=>"/morea/150.amortized-analysis/experience-amortized-analysis.html",
 "content"=>
  "## Amortized Analysis: Accounting Method\n\n_(This is similar to the problem that was solved using aggregate analysis in\nmy lecture notes, but the numbers are slightly different: in the lecture, the\ntable grows at sizes 2i-1; here at 2i; and here you use the accounting method.\nDon't do it with aggregate analysis: that won't count! I provide guidance on\napproaching the accounting analysis. The problem is relevant to Java hash\ntables.)_\n\nSuppose we perform a sequence of _n_ operations on a data structure in which\nthe _i_th operation costs _i_ if _i_ is an exact power of 2, and 1 otherwise.\n_Use the **accounting method** to determine the amortized cost per operation._\n\n**(a)** Choose the (fixed) cost per operation ĉ that you will charge. \n\n**(b)** Make a table showing \n\n  * operation number (_i_ = 1, 2, 3, ... up to about 20 to see the pattern), \n  * actual cost of the _i_th operation\n  * credit available at the conclusion of the _i_th operation (credit_i_ = credit_i_−1 \\+ ĉ − actual cost). \n\n**(c)** Write an expression for credit at each power of two, 2_i_. Does this credit increase, decrease, or stay the same? \n\n**(d)** Does credit increase, decrease or stay the same _between_ each power of 2? \n\n**(e)** Say why this shows that the amortized cost ĉ that you chose in step (a) is an upper bound on the actual cost. \n\n**(f)** Conclude by giving the big-O upper bound on amortized per-operation cost that (a) and (e) imply. \n\n\n",
 "path"=>"morea//150.amortized-analysis/experience-amortized-analysis.md"}
</pre>

<h2>/morea/150.amortized-analysis/module-amortized-analysis.html</h2>

<pre>Hash
{"title"=>"Amortized analysis",
 "published"=>true,
 "morea_id"=>"amortized-analysis",
 "morea_outcomes"=>["outcome-amortized-analysis"],
 "morea_readings"=>
  ["reading-screencast-15a",
   "reading-screencast-15b",
   "reading-cormen-17",
   "reading-notes-15"],
 "morea_experiences"=>["experience-amortized-analysis"],
 "morea_type"=>"module",
 "morea_icon_url"=>
  "/morea/150.amortized-analysis/module-amortized-analysis.gif",
 "morea_sort_order"=>150,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/150.amortized-analysis/module-amortized-analysis.html",
 "content"=>
  "Aggregate method, accounting method, potential method, dynamic tables, multipop example.\n",
 "path"=>"morea//150.amortized-analysis/module-amortized-analysis.md"}
</pre>

<h2>/modules/amortized-analysis/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-amortized-analysis.md",
 "title"=>"Amortized analysis",
 "url"=>"/modules/amortized-analysis/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/amortized-analysis/index.html"}
</pre>

<h2>/morea/150.amortized-analysis/outcome-amortized-analysis.html</h2>

<pre>Hash
{"title"=>"Understand amortized analysis",
 "published"=>true,
 "morea_id"=>"outcome-amortized-analysis",
 "morea_type"=>"outcome",
 "morea_sort_order"=>150,
 "referencing_modules"=>[#Jekyll:Page @name="module-amortized-analysis.md"],
 "url"=>"/morea/150.amortized-analysis/outcome-amortized-analysis.html",
 "content"=>"Understand when and how to apply amortized analysis.",
 "path"=>"morea//150.amortized-analysis/outcome-amortized-analysis.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-cormen-17.html</h2>

<pre>Hash
{"title"=>"CLRS 17 - Amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-cormen-17",
 "morea_summary"=>
  "aggregate analysis, the accounting method, the potential method, dynamic tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "30 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-cormen-17.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-cormen-17.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-notes-15.html</h2>

<pre>Hash
{"title"=>"Notes on amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-notes-15",
 "morea_summary"=>
  "The general idea, multipop example, aggregate analysis, accounting method, potential method, dynamic tables.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/150.amortized-analysis/reading-notes-15.html",
 "url"=>"/morea/150.amortized-analysis/reading-notes-15.html",
 "content"=>
  "## Outline\n\n  1. Amortized Analysis: The General Idea \n  2. Multipop Example \n  3. Aggregate Analysis \n  4. Accounting Method\n  5. Potential Method\n  6. Dynamic Table Example (first look)\n  7. Other Examples\n\n##  Amortized Analysis: The General Idea\n\nWe have already used _aggregate_ analysis several times in this course. For\nexample, when analyzing the BFS and DFS procedures, instead of trying to\nfigure out how many times their inner loops\n\n> `for each _v_ ∈ G.Adj[_u_]`\n\nexecute (which depends on the degree of the vertex being processed), we\nrealized that no matter how the edges are distributed, there are at most |_E_|\nedges, so in aggregate across all calls the loops will execute |_E_| times.\n\nBut that analysis concerned itself only with the complexity of a single\noperation. In practice a given data structure will have associated with it\nseveral operations, and they may be applied many times with varying frequency.\n\nSometimes a given operation is designed to pay a larger cost than would\notherwise be necessary to enable other operations to be lower cost.\n\n_Example:_ Red-black tree insertion. We pay a greater cost for balancing so\nthat future searches will remain O(lg _n_).\n\n_Another example:_ Java Hashtable.\n\n  * These grow dynamically when a specified load factor is exceeded.\n  * Copying into a new table is expensive, but copying is infrequent and table growth makes access operations faster.\n\nIt is \"fairer\" to average the cost of the more expensive operations across the\nentire mix of operations, as all operations benefit from this cost.\n\nHere, \"average\" means average cost in the worst case (thankfully, no\nprobability is involved, which greatly simplifies the analysis).\n\nWe will look at three methods. The notes below use Stacks with Multipop to\nillustrate the methods. See the text for binary counter examples.\n\n(We have already seen examples of aggregate analysis throughout the semseter.\nWe will see examples of amortized analysis later in the semester.)\n\n* * *\n\n##  Multipop Example\n\nWe already have the stack operations:\n\n  * `Push(_S, x_)`: O(1) each call, so O(_n_) for any sequence of _n_ operations.\n  * `Pop(_S_)`: O(1) each call, so O(_n_) for any sequence of _n_ operations.\n\nSuppose we add a `Multipop` (this is a generalization of `ClearStack`, which\nempties the stack):\n\n![](fig/Fig-17-1-multipop.jpg) ![](fig/pseudocode-multipop.jpg)\n\nThe example shows a `Multipop(S,4)` followed by another where _k_ ≥ 2.\n\nRunning time of `Multipop`:\n\n  * Linear in number of `Pop` operations (one per loop iteration)\n  * Number of iterations of `while` loop is min(_s_, _k_), where _s_ = number of items on the stack\n  * Therefore, total cost = min(_s_, _k_). \n\nWhat is the worst case of a sequence of _n_ `Push`, `Pop` and `Multipop`\noperations?\n\nUsing our existing methods of analysis we can identify a _loose bound:_:\n\n  * The most expensive operation is `Multipop`, potentially O(_n_).\n  * Therefore, potentially O(_n2_) over _n_ operations.\n\n* * *\n\n##  Aggregate Analysis\n\nWe can tighten this loose bound by aggregating the analysis over all _n_\noperations:\n\n  * Each object can only be popped once per time that it is pushed.\n  * There are at most _n_ `Push`es, so at most n `Pop`s, including those in `Multipop`\n  * Therefore, total cost = O(_n_)\n  * Averaging over the _n_ operations we get O(1) per operation.\n\nThis analysis shows O(1) per operation on average in a sequence of _n_\noperations without using any probabilities of operations.\n\nSee text for example of aggregate analysis of binary counting. An example of\naggregate analysis of dynamic tables is at the end of these notes.\n\nSome of our previous analyses with indicator random variables have been a form\nof aggregate analysis, e.g., our analysis of the expected number of inversions\nin sorting, [Topic 5\nNotes](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-05.html).\n\nAggregate analysis treats all operations equally. The next two methods let us\ngive different operations different costs, so are more flexible.\n\n* * *\n\n##  Accounting Method\n\n#### Metaphor:\n\n  * View the computer as a coin operated appliance that requires one _cyber-dollar_ (CY$) per basic operation.\n  * The banks are wary of making loans these days, so when an operation is to be performed we must have enough cyber-dollars available to pay for it. \n  * We are permitted to charge some operations more than they actually cost so we can save enough to pay for the more expensive operations.\n\n**Amortized cost** = amount we charge each operation.\n\nThis differs from aggregate analysis:\n\n  * In aggregate analysis, all operations have the same cost.\n  * In the accounting method, different operations can have different costs.\n\nWhen an operation is overcharged (amortized cost > actual cost), the\ndifference is associated with _specific objects_ in the data structure as\n_credit_.\n\nWe use this credit to pay for operations whose actual cost > amortized cost.\n\nThe credit must never be negative. Otherwise the amortized cost may not be an\nupper bound on actual cost for some sequences.\n\nLet\n\n  * _ci_ = actual cost of _i_th operation. \n  * _ĉi_ = amortized cost of _i_th operation _(notice the 'hat')_. \n\nRequire ∑_i_=1,_n__ĉi_   ≥   ∑_i_=1,_n__ci_ for all sequences of _n_\noperations. That is, the difference between these sums always ≥ 0: we never\nowe anyone anything.\n\n### Stack Example\n\nWhenever we `Push` an object (at actual cost of 1 cyberdollar), we potentially\nhave to pay CY$1 in the future to `Pop` it, whether directly or in `Multipop`.\n\nTo make sure we have enough money to pay for the `Pops`, we charge `Push`\nCY$2.\n\n  * CY$1 pays for the push\n  * CY$1 is prepayment for the object being popped (metaphorically, this CY$1 is stored \"on\" the object).\n\nSince each object has CY$1 credit, the credit can never go negative.\n\n![](fig/stack-amortized-cost-table.jpg)\n\nThe total amortized cost _ĉ_ = ∑_i_=1,_n__ĉi_ for _any_ sequence of _n_\noperations is an upper bound on the total actual cost _c_ = ∑_i_=1,_n__ci_ for\nthat sequence.\n\nSince _ĉ_ = O(_n_), also _c_ = O(_n_).\n\nNote: we don't actually store cyberdollars in any data structures. This is\njust a metaphor to enable us to compute an amortized upper bound on costs.\n\n* * *\n\n##  Potential Method\n\nInstead of credit associated with objects in the data structure, this method\nuses the metaphor of _potential_ associated with the _entire data structure._\n\n(I like to think of this as potential _energy,_ but the text continues to use\nthe monetary metaphor.)\n\nThis is the most flexible of the amortized analysis methods, as it does not\nrequire maintaining an object-to-credit correspondence.\n\nLet\n\n  * D0 = initial data structure \n  * D_i_ = data structure after _i_th operation\n  * _ci_ = actual cost of _i_th operation. \n  * _ĉi_ = amortized cost of _i_th operation. \n\nPotential Function **Φ**: D_i_ -> ℜ, and we say that Φ(D_i_) is the\n**potential** associated with data structure D_i_.\n\nWe define the amortized cost _ĉi_ to be the actual cost _ci_ plus the change\nin potential due to the _i_th operation:\n\n> _ĉi_ = _ci_ \\+ Φ(D_i_) − Φ(D_i-1_)\n\n  * If at the _i_th operation, Φ(D_i_) − Φ(D_i-1_) is positive, then the amortized cost _c_'_i_ is an _overcharge_ and the potential of the data structure increases.\n  * On the other hand, if Φ(D_i_) &minus: Φ(D_i-1_) is negative then _c_'_i_ is an undercharge, and the decrease of the potential of the data structure pays for the difference (as long as it does not go negative). \n\nThe total amortized cost across _n_ operations is:\n\n> ∑_i_=1,_n__ĉi_   =   ∑_i_=1,_n_(_ci_ \\+ Φ(D_i_) - Φ(D_i-1_))   =\n(∑_i_=1,_n__ci_) + (Φ(D_n_) - Φ(D0))\n\n(The last step is taken because the middle expression involves a telescoping\nsum: every term other than D_n_ and D0 is added once and subtracted once.)\n\nIf we require that Φ(D_i_) ≥ Φ(D0) for all _i_ then the amortized cost will\nalways be an upper bound on the actual cost no matter which _i_th step we are\non.\n\nThis is usually accomplished by defining Φ(D0) = 0 and then showing that\nΦ(D_i_) ≥ 0 for all _i_. (Note that this is a constraint on Φ, not on _ĉ_. _ĉ_\ncan go negative as long as Φ(D_i_) never does.)\n\n### Stack Example\n\nDefine Φ(D_i_) = number of objects in the stack.\n\nThen Φ(D0) = 0 and Φ(D_i_) ≥ 0 for all _i_, since there are never less than 0\nobjects on the stack.\n\nCharge as follows (recalling that _ĉi_ = _ci_ \\+ Φ(D_i_) - Φ(D_i-1_)):\n\n![](fig/stack-potential-table.jpg)\n\nSince we charge 2 for each `Push` and there are O(n) Pushes in the worst case,\nthe amortized cost of a sequence of _n_ operations is O(_n_).\n\nDoes it seem strange that we charge `Pop` and `Multipop` 0 when we know they\ncost something?\n\n  * Remember that this is just a way of counting the total cost over a sequence of operations more precisely.\n  * It is not a claim about the actual cost of a specific procedural call.\n  * Like with the accounting method, we are guaranteeing that we have just enough credit on hand to pay for the operations when they happen.\n  * The methods give a tight bound on amortized cost, but with much easier counting than if we had to reason about probability distributions, etc.\n\n* * *\n\n## Application: Dynamic Tables\n\nThere is often a tradeoff between time and space, for example, with hash\ntables. Bigger tables give faster access but take more space.\n\nDynamic tables, like the Java Hashtable, grow dynamically as needed to keep\nthe load factor reasonable.\n\nReallocation of a table and copying of all elements from the old to the new\ntable is expensive!\n\nBut this cost is amortized over all the table access costs in a manner\nanalogous to the stack example: We arrange matters such that table-filling\noperations build up sufficient credit before we pay the large cost of copying\nthe table; so the latter cost is averaged over many operations.\n\n### A Familiar Definition\n\n**Load factor α** = _num_/_size_, where _num_ = # items stored and _size_ = the allocated size of the table.\n\nFor the boundary condition of _size_ = _num_ = 0, we will define α = 1.\n\nWe never allow α > 1 (no chaining).\n\n### Insertion Algorithm\n\nWe'll assume the following about our tables. (See Exercises 17.4-1 and 17.4-3\nconcerning different assumptions.):\n\nWhen the table becomes full, we double its size and reinsert all existing\nitems. This guarantees that α ≥ 1/2, so we are not wasting a lot of space.\n\n    \n    \n    Table-Insert (T,x)\n    1   if T.size == 0\n    2       allocate T.table with 1 slot \n    3       T.size = 1\n    4   if T.num == T.size\n    5       allocate newTable with 2*T.size slots\n    6       insert all items in T.table into newTable\n    7       free T.table\n    8       T.table = newTable \n    9       T.size = 2*T.size \n    10  insert x into T.table \n    11  T.num = T.num + 1\n    \n\nEach _elementary insertion_ has unit actual cost. Initially _T.num_ = _T.size_= 0.\n\n### Aggregate Analysis of Dynamic Table Expansion\n\nCharge 1 per elementary insertion. Count only elementary insertions, since all\nother costs are constant per call.\n\n**_ci_** = actual cost of _i_th operation.\n\n![](fig/pseudocode-table-insert.jpg)\n\n  * If the table is not full, _ci_ = 1 (for lines 1, 4, 10, 11). \n  * If full, there are _i_ \\- 1 items in the table at the start of the _i_th operation. Must copy all of them (line 6), and then insert the _i_th item. Therefore _ci_ = _i_ \\- 1 + 1 = _i_. \n\nA sloppy analysis: In a sequence of _n_ operations where any operation can be\nO(_n_), the sequence of _n_ operations is O(_n_2).\n\nThis is \"correct\", but inprecise: we rarely expand the table! A more precise\naccount of _ci_:\n\n![](fig/c_i-definition.jpg)\n\nThen we can sum the total cost of all _ci_ for a sequence of _n_ operations:\n\n![](fig/analysis-table-expansion.jpg)\n![](fig/formula-A-5.jpg)\n\n_Explain:_ Why the _n_? What is the summation counting? Why does the summation\nstart at _j_ = 0? Why does it end at _j_ = lg _n_?\n\nTherefore, the amortized cost per operation = 3: we are only paying a small\nconstant price for the expanding table.\n\nThe text also gives accounting and potential analyses of table expansion.\n\nThis analysis assumed that the table never shrinks. See section 17.4 (and your\nhomework) for an analysis using the potential method that covers shrinking\ntables.\n\n* * *\n\n## Other Examples\n\nHere are some other algorithms for which amortized analysis is useful:\n\n### Red-Black Trees\n\nAn amortized analysis of Red-Black Tree restructuring (Problem 17-4 of CLRS)\nimproves upon our analysis earlier in the semester:\n\n  * Any sequence of _m_ `RB-Insert` and `RB-Delete` operations performs O(_m_) structural modifications (rotations), \n  * This each operation does **O(1) structural modifications on average**, regardless of the size of the tree!\n  * An operation still may need to do O(lg _n_) recolorings, but these are very simple operations.\n\n### Self-Organizing Lists\n\n  * Self-organizing lists use a **_move-to-front heuristic_**: Immediately after searching for an element, it is moved to the front of the list.\n  * This makes frequently accessed items more readily available near the front of the list.\n  * An amortized analysis (Problem 17-5) shows that the heuristic is no more than 4 times worse than optimal.\n\n### Splay Trees\n\n  * Splay trees are ordinary binary search trees (no colors, no height labels, etc.)\n  * After every access (every insertion, deletion, or search), the element operated on (or its parent in the case of deletion) is moved towards the top of the tree.\n  * This movement uses three **_splaying_** operations called \"zig\", \"zig-zig\" and \"zig-zag\".\n  * Although in the worst case a splay tree can degenerate into an O(_n_) linked list, amortized analysis shows that the expected case is O(lg _n_)\n  * Randomization can be used to make the worst case very unlikely.\n  * If a single element is accessed at least _m_/4 times where _m_ is the number of operations, then the amortized running time of each of these accesses is O(1).\n  * Thus, splay-trees self-organize to provide fast access to frequently accessed items.\n  * This makes them good for locality of reference in memory, but multithreaded access must be implemented carefully.\n\n### To Be Continued\n\nAmortized analysis will be used in analyses of\n\n  * Graph search (Topic 14, Ch. 22) \n  * Disjoint set operations (Topic 16, Ch. 21) \n  * Dijkstra's Algorithm for Shortest Paths (Topic 18, Ch. 24) \n\n* * *\n\nDan Suthers Last modified: Sun Mar 16 02:03:09 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//150.amortized-analysis/reading-notes-15.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-screencast-15a.html</h2>

<pre>Hash
{"title"=>"Introduction to amortized analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-15a",
 "morea_summary"=>"Aggregate, accounting, and potential methods",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=rlIka0W814U",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-screencast-15a.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-screencast-15a.md"}
</pre>

<h2>/morea/150.amortized-analysis/reading-screencast-15b.html</h2>

<pre>Hash
{"title"=>"Amortized analysis example",
 "published"=>true,
 "morea_id"=>"reading-screencast-15b",
 "morea_summary"=>"aggregate analysis of dynamic tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=iy-WhloN6vA",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/150.amortized-analysis/reading-screencast-15b.html",
 "content"=>"",
 "path"=>"morea//150.amortized-analysis/reading-screencast-15b.md"}
</pre>

<h2>/morea/160.disjoint-sets/experience-disjoint-sets.html</h2>

<pre>Hash
{"title"=>"Disjoint sets",
 "published"=>true,
 "morea_id"=>"experience-disjoint-sets",
 "morea_type"=>"experience",
 "morea_summary"=>"Working with disjoint sets",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/160.disjoint-sets/experience-disjoint-sets.html",
 "url"=>"/morea/160.disjoint-sets/experience-disjoint-sets.html",
 "content"=>
  "## Disjoint sets\n\n![](fig/pseudocode-disjoint-set-forest.jpg)\n\n\n### 1. State of the Union\n\n**Show the result of Union(e,j) on this forest.**\n\n![](fig/disjoint-set-forest-1.jpg)\n\n\n### 2. Iterative Find-Set\n\nFind-Set as written in the text is not tail recursive, so a compiler won't be\nable to automatically generate efficient iterative executable code. **Write an\niterative version of Find-Set.**\n\n\n### 3. Connected-Components \n  \n![](fig/pseudocode-connected-components.jpg) \n\nDuring the execution\nof Connected-Components on an undirected graph _G_=(_V_,_E_) with _k_\nconnected components,\n\n**(a) How many times is Find-Set called directly by Connected-Components? **  \n  \n**(b) How many times is Union called directly by Connected-Components? **\n\nExpress your answer in terms of _V_, _E_ and _k_ (but NOT asymptotic\nnotation). (Hint: solve for _k_=1 first and then think about what happens when\n_k_=2.)\n\n\n",
 "path"=>"morea//160.disjoint-sets/experience-disjoint-sets.md"}
</pre>

<h2>/morea/160.disjoint-sets/module-disjoint-sets.html</h2>

<pre>Hash
{"title"=>"Disjoint sets",
 "published"=>true,
 "morea_id"=>"disjoint-sets",
 "morea_outcomes"=>["outcome-disjoint-sets"],
 "morea_readings"=>
  ["reading-screencast-16a", "reading-cormen-21", "reading-notes-16"],
 "morea_experiences"=>["experience-disjoint-sets"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/160.disjoint-sets/module-disjoint-sets.jpg",
 "morea_sort_order"=>160,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/160.disjoint-sets/module-disjoint-sets.html",
 "content"=>
  "Union-find, linked list representation, forest representation, finding connected components.\n",
 "path"=>"morea//160.disjoint-sets/module-disjoint-sets.md"}
</pre>

<h2>/modules/disjoint-sets/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-disjoint-sets.md",
 "title"=>"Disjoint sets",
 "url"=>"/modules/disjoint-sets/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/disjoint-sets/index.html"}
</pre>

<h2>/morea/160.disjoint-sets/outcome-disjoint-sets.html</h2>

<pre>Hash
{"title"=>"Understand disjoint sets",
 "published"=>true,
 "morea_id"=>"outcome-disjoint-sets",
 "morea_type"=>"outcome",
 "morea_sort_order"=>160,
 "referencing_modules"=>[#Jekyll:Page @name="module-disjoint-sets.md"],
 "url"=>"/morea/160.disjoint-sets/outcome-disjoint-sets.html",
 "content"=>"Understand the principles and applications of disjoint sets. ",
 "path"=>"morea//160.disjoint-sets/outcome-disjoint-sets.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-cormen-21.html</h2>

<pre>Hash
{"title"=>"CLRS 21 - Data structures for disjoint sets",
 "published"=>true,
 "morea_id"=>"reading-cormen-21",
 "morea_summary"=>
  "disjoint set operations, linked-list representation of disjoint sets, disjoint-set forests, analysis of union by rank with path compression",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "25 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/160.disjoint-sets/reading-cormen-21.html",
 "content"=>"",
 "path"=>"morea//160.disjoint-sets/reading-cormen-21.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-notes-16.html</h2>

<pre>Hash
{"title"=>"Notes on disjoint sets",
 "published"=>true,
 "morea_id"=>"reading-notes-16",
 "morea_summary"=>
  "Disjoint sets, finding connected components, linked list representations, and forest representations",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/160.disjoint-sets/reading-notes-16.html",
 "url"=>"/morea/160.disjoint-sets/reading-notes-16.html",
 "content"=>
  "## Outline\n\n  1. Disjoint Dynamic Sets \n  2. Finding Connected Components with Disjoint Sets \n  3. Linked List Representations of Disjoint Sets \n  4. Forest Representations of Disjoint Sets\n\n\n##  Dynamic Disjoint Sets (Union Find)\n\nTwo sets _A_ and _B_ are **disjoint** if they have no element in common.\n\nSometimes we need to group n distinct elements into a collection \u008A of disjoint\nsets \u008A = {_S_1, ..., _Sk_} that may change over time.\n\n  * \u008A is a set of sets: { { _x_, ... }, ..., { _y_, ... } } \n  * Each set _Si_ ∈ \u008A is identified by a **representative**, which is some member of the set (e.g., _x_ and _y_).\n  * It does not matter which member is the representative, as long as the representative remains the same while the set is not modified.\n\nDisjoint set data structures are also known as **Union-Find** data structures,\nafter the two operations in addition to creation. (Applications often involve\na mixture of searching for set membership and merging sets.)\n\n### Operations\n\n> **Make-Set(_x_)**: make a new set _Si_ = {_x_} _(_x_ will be its\nrepresentative)_ and add _Si_ to \u008A.\n\n> **Union(_x_, _y_)**: if _x_ ∈ _Sx_ and _y_ ∈ _Sy_, then \u008A <- \u008A − _Sx_ − _Sy_\n∪ {_Sx_ ∪ _Sy_} _(that is, combine the two sets _Sx_ and _Sy_)_.\n\n>\n\n>   * The representative of _Sx_ ∪ _Sy_ is any member of that new set\n(implementations often use the representative of one of _Sx_ or _Sy_.)\n\n>   * Destroys _Sx_ and _Sy_, since the sets must be disjoint (they cannot co-\nexist with _Sx_ ∪ _Sy_).\n\n> **Find-Set(_x_)**: return the representative of the set containing _x_.\n\n### Analysis\n\nWe analyze in terms of:\n\n  * _n_ = number of Make-Set operations, i.e., the number of sets initially involved\n  * _m_ = total number of operations\n\nSome facts we can rely on:\n\n  * _m_ ≥ _n_\n  * Can have at most _n_−1 Union operations, since after _n_−1 Unions, only 1 set remains.\n  * It can be helpful for analysis to assume that the first _n_ operations are Make-Set operations (put all the elements we will be working with in singleton sets to start with).\n\n* * *\n\n##  Applications of Disjoint Sets\n\nUnion-Find on disjoint sets is used to find structure in other data\nstructures, such as a graph. We initially assume that all the elements are\ndistinct by putting them in singleton sets, and then we merge sets as we\ndiscover the structure by which the elements are related.\n\n### Finding Connected Components\n\nRecall from [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)\nthat for a graph _G_ = (_V_, _E_), vertices _u_ and _v_ are in the same\n**connected component** if and only if there is a path between them.\n\nHere are the algorithms for computing connected components and then for\ntesting whether two items are in the same component:\n\n![](fig/pseudocode-connected-components.jpg) ![](fig/pseudocode-same-components.jpg)\n\n_Would that work with a directed graph?_\n\n#### Example\n\n![](fig/Fig-21-1-finding-connected-components.jpg)\n![](fig/ti-chats.jpg)\n\nAlthough it is easy to see the connected components above, the utility of the\nalgorithm becomes more obvious when we deal with large graphs (such as\npictured)!\n\n#### Alternatives\n\nIn a _static_ undirected graph, it is faster to run Depth-First Search\n(exercise 22.3-12), or for static directed graphs the strongly connected\ncomponents algorithm of [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)\n(section 22.5), which consists of two DFS. But in some applications edges may\nbe added to the graph. In this case, union-find with disjoint sets is faster\nthan re-running the DFS.\n\n### Minimum Spanning Trees\n\nNext week we cover algorithms to find _minimum spanning trees_ of graphs.\nKruskal's algorithm will use Union-Find operations.\n\n* * *\n\n##  Linked List Representations of Disjoint Sets\n\nOne might think that lists are the simplest approach, but there is a better\napproach that is not any more complex: this section is mainly for comparision\npurposes.\n\n### Representation\n\nEach set is represented using an unordered singly linked list. The list object\nhas attributes:\n\n  * **head**: pointing to the first element in the list, the set's representative.\n  * **tail**: pointing to the last element in the list.\n\n![](fig/Fig-21-2-linked-list-S1.jpg)\n\nEach object in the list has attributes for:\n\n  * **next**\n  * The **set member** (e.g., the vertex in the graph being analyzed)\n  * A pointer to the list object that represents the **set**\n\n### Operations\n\nFirst try:\n\n  * Make-Set(_x_): create a singleton list containing _x_\n  * Find-Set(_x_): follow the pointer back to the list object, and then follow the `head` pointer to the representative\n  * Union(_x_, _y_): append _y_'s lists onto the end of _x_'s list. \n    * Use _x_'s tail pointer to find the end.\n    * Need to update the pointer back to the set object for every node on _y_'s list.\n\nFor example, let's take the union of _S_1 and _S_2, replacing _S_1:\n\n![](fig/Fig-21-2-linked-list-representation.jpg)\n\nThis can be slow for large data sets. For example, suppose we start with _n_\nsingletons and always happend to append the larger list onto the smaller one\nin a sequence of merges:\n\n![](fig/Fig-21-3-worst-case-alt.jpg)\n\nIf there are _n_ Make-Sets and _n_ Unions, the amortized time per operation is\nO(_n_)!\n\nA **weighted-union heuristic** speeds things up: always append the smaller\nlist to the larger list (so we update fewer set object pointers). Althought a\nsingle union can still take Ω(_n_) time (e.g., when both sets have _n_/2\nmembers), a sequence of _m_ operations on _n_ elements takes O(_m_ \\+ _n_ lg\n_n_) time.\n\n**_Sketch of proof:_** Each Make-Set and Find-Set still takes O(1). Consider how many times each object's set representative pointer must be updated during a sequence of _n_ Union operations. It must be in the smaller set each time, and after each Union the size of this smaller set is at least double the size. So: \n\n![](fig/representative-update-bound.jpg)\n\nEach representative set for a given element is updated ≤ lg _n_ times, and\nthere are _n_ elements plus _m_ operations. However, we can do better!\n\n* * *\n\n##  Forest Representations of Disjoint Sets\n\nThe following is a classic representation of Union-Find, due to Tarjan (1975).\nThe set of sets is represented by a forest of trees. The code is as simple as\nthe analysis of runtime is complex.\n\n### Representation\n\n![](fig/disjoint-set-forest.jpg)\n\n  * Each tree represents a set.\n  * The root of the tree is the set representative.\n  * Each node points only to its parent (no child pointers needed).\n  * The root points to itself as parent. \n\n### Operations\n\n  * Make-Set(_x_): create a single node tree with _x_ at the root\n  * Find-Set(_x_): follow parent pointers back to the root\n  * Union(_x_, _y_): make one root a child of the other. (This in itself could degenerate to a linear list-like tree, but we will fix this below.)\n\n![](fig/disjoint-set-union-alt.jpg)\n\n#### Heuristics\n\nIn order to avoid degeneration to linear trees, and achieve amazing amortized\nperformance, these two heuristics are applied:\n\n**Union by Rank**: make the root of the \"smaller\" tree a child of the root of the \"larger\" tree. But rather than size we use **rank**, an upper bound on the height of each node (stored in the node).\n\n  * Rank of singleton sets is 0.\n  * When taking the Union of two trees of equal rank, choose one arbitrarily to be the parent and increment its rank by one. _(Why is it incremented?)_\n  * When taking the Union of two trees of unequal rank, the tree with lower rank becomes the child, and ranks are unchanged. _(Why does this make sense?)_\n\n**Path Compression**: When running Find-Set(_x_), make all nodes on the path from _x_ to the root direct children of the root. For example, Find-Set(a):\n\n![](fig/Fig-21-5-path-compression-alt.jpg)\n\n### Algorithms\n\nThe algorithms are very simple! (But their analysis is complex!) We assume\nthat nodes _x_ and _y_ are tree nodes with the client's element data already\ninitialized.\n\n![](fig/pseudocode-disjoint-set-forest.jpg)\n\nLink implements the union by rank heuristic.\n\nFind-Set implements the path compression heuristic. It makes a recursive pass\nup the tree to find the path to the root, and as recursion unwinds it updates\neach node on the path to point directly to the root. (This means it is not\ntail recursive, but as the analysis shows, the paths are very unlikely to be\nlong.)\n\n### Time Complexity\n\nThe analysis can be found in section 21.4. It is very involved, and I only\nexpect you to know what is discussed below. It is based on a very fast growing\nfunction:\n\n![](fig/A_k-j.jpg)\n\n_Ak_(_j_) is a variation of **Ackerman's Function**, which is what you will\nfind in most classic texts on the subject. The function grows so fast that\n_A_4(1) = 16512 is _much_ larger than the number of atoms in the observable\nuniverse (1080)!\n\nThe result uses **α(_n_),** a single parameter inverse of _Ak_(_j_) defined as\nthe lowest _k_ for which _Ak_(1) is at least _n_:\n\n![](fig/growth-inverse-ackermann.jpg)\n\n> α(_n_) = min{_k_ : _Ak_(1) ≥ _n_}\n\nα(_n_) grows _very_ slowly, as shown in the table. We are highly unlikely to\never encounter α(_n_) > 4 (we would need input size much greater than the\nnumber of atoms in the universe). Although its growth is strictly larger than\na constant, for all practical purposes we can treat α(_n_) as a constant.\n\nThe analysis of section 21.4 shows that the running time is **O(_m_ α(_n_))**\nfor a sequence of _m_ `Make-Set`, `Find-Set` and `Union` operations. Thus for\nall practical purposes, the cost of a sequence of _m_ such operations is\nO(_m_), or O(1) amortized cost per operation!\n\n* * *\n\n## Wrapup\n\nWe now return to Graphs. We'll see Union-Find used when we cover minimum\nspanning trees.\n\n* * *\n\nDan Suthers Last modified: Thu Apr 17 15:35:22 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//160.disjoint-sets/reading-notes-16.md"}
</pre>

<h2>/morea/160.disjoint-sets/reading-screencast-16a.html</h2>

<pre>Hash
{"title"=>"Sets and union-find",
 "published"=>true,
 "morea_id"=>"reading-screencast-16a",
 "morea_summary"=>"Introduction to disjoint sets",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=RUBU6eHAAaU",
 "morea_labels"=>["Screencast", "Suthers", "27 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/160.disjoint-sets/reading-screencast-16a.html",
 "content"=>"",
 "path"=>"morea//160.disjoint-sets/reading-screencast-16a.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree-2.html</h2>

<pre>Hash
{"title"=>"More minimum spanning trees",
 "published"=>true,
 "morea_id"=>"experience-minimum-spanning-tree-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Minimum spanning trees and Kruskal's algorithm",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree-2.html",
 "url"=>
  "/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree-2.html",
 "content"=>
  "## Minimum Spanning Graph\n\n#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n#### 2\\. (4 pts) Positive and Negative Weights: Minimum Spanning Graph?\n\nGiven a graph _G_ = (_V_, _E_), must any subset of edges _T_ ⊆ _E_ that\nconnects all vertices in V and have minimal total weight be a tree, or can it\nbe some other subgraph? Answer this question separately for two cases:\n\n> **a.** All of the edges have positive weight.  \n  \n**b.** Some edges may have negative weights. \n\nIf you think \"yes it must be a tree\" then argue why this is the case (hint:\nsuppose it's not a tree: find a contradiction to connectedness or minimality).\nIf you think \"no, it need not be a tree\" then give an example where a\nconnected graph that is not a tree has lower weight.\n\n#### 3\\. (1 pt) Kruskal Alternative Spanning Tree\n\nThe graph of Figure 23.4 has several edges of equal weight. Sometimes both are\nused; sometimes one can't be used because it forms a cycle; and sometimes the\nchoice is arbitrary: either edge could have been used. In this third case, the\nchoice of which one was used depends on the order in which edges are sorted in\nline 4 (nondecreasing order by weight).\n\n![](fig/Fig-23-4-Kruskal-Example-n.jpg)\n\nThe actual ordering used in the book's example is:\n\n> ` (g, h), (c, i), (f, g), (a, b), (c, f), (g, i), (c, d),  \n(h, i), (a, h), (b, c), (d, e), (e, f), (b, h), (d, f) `\n\nGive an ordering that would result in a different minimum spanning tree than\nthe one shown in figure 23.4 by rewriting the above list swapping just one\npair of edges.\n\n#### 4\\. (5 pts) Building Low Cost Bridges\n\nSuppose we are in Canada's Thousand Islands National Park, and in one\nparticular lake there are eight small islands that park officials want to\nconnect with floating bridges so that people can experience going between\nislands without a canoe. The cost of constructing a bridge is proportional to\nits length, and the table below shows the distances between pairs of islands\nin meters (Canada is trying to free itself of the archaic British system of\nmeasurement based on the sizes of a dead king's body parts.)\n\nWhich bridges should they build to connect the eight islands at minimal cost?\nSay in a sentence or two how you found the solution, and give the solution as\na list of pairs of islands, for example, (A, B) ....\n\n<table width=\"50%\" border=\"1\">\n  <tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">A</th>\n    <th scope=\"col\">B</th>\n    <th scope=\"col\">C</th>\n    <th scope=\"col\">D</th>\n    <th scope=\"col\">E</th>\n    <th scope=\"col\">F</th>\n    <th scope=\"col\">G</th>\n    <th scope=\"col\">H</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">A</th>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">240</div></td>\n    <td><div align=\"center\">210</div></td>\n    <td><div align=\"center\">340</div></td>\n    <td><div align=\"center\">280</div></td>\n    <td><div align=\"center\">200</div></td>\n    <td><div align=\"center\">345</div></td>\n    <td><div align=\"center\">120</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">B</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">265</div></td>\n    <td><div align=\"center\">175</div></td>\n    <td><div align=\"center\">215</div></td>\n    <td><div align=\"center\">180</div></td>\n    <td><div align=\"center\">185</div></td>\n    <td><div align=\"center\">155</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">C</th>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">260</div></td>\n    <td><div align=\"center\">115</div></td>\n    <td><div align=\"center\">350</div></td>\n    <td><div align=\"center\">435</div></td>\n    <td><div align=\"center\">195</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">D</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">160</div></td>\n    <td><div align=\"center\">330</div></td>\n    <td><div align=\"center\">295</div></td>\n    <td><div align=\"center\">230</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">E</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">360</div></td>\n    <td><div align=\"center\">400</div></td>\n    <td><div align=\"center\">170</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">F</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">175</div></td>\n    <td><div align=\"center\">205</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">G</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">305</div></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">H</th>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">&minus;</div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n    <td><div align=\"center\">\n      <div align=\"center\">&minus;</div>\n    </div></td>\n  </tr>\n</table>",
 "path"=>
  "morea//170.minimum-spanning-tree/experience-minimum-spanning-tree-2.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree.html</h2>

<pre>Hash
{"title"=>"Minimum spanning trees",
 "published"=>true,
 "morea_id"=>"experience-minimum-spanning-tree",
 "morea_type"=>"experience",
 "morea_summary"=>"Basics of minimum spanning trees",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree.html",
 "url"=>
  "/morea/170.minimum-spanning-tree/experience-minimum-spanning-tree.html",
 "content"=>
  "## Basics of minimum spanning trees\n\nBelow is pseudocode for three different algorithms. Each one takes a connected\ngraph _G_ = (_V_,_E_) and a weight function _w_ as input and returns a set of\nedges _T_.\n\n#### Is it an MST algorithm?\n\nFor each algorithm, either prove that the set of edges _T_ produced by the\nalgorithm is always a minimum spanning tree, or find a counter-example where\n_T_ is not a minimum spanning tree\n\n**1\\. `Maybe-MST-A`**\n    \n    \n    Maybe-MST-A (G, w)\n    1  sort the edges into nonincreasing order of edge weights w\n    2  T = E\n    3  for each edge e, taken in sorted order\n    4      if T − {e} is a connected graph\n    5          T = T − {e}\n    6  return T\n    \n\n**2\\. `Maybe-MST-B`**\n    \n    \n    Maybe-MST-B (G, w)\n    1  T = {} // empty set\n    2  for each edge e ∈ E, taken in arbitrary order\n    3      if T ∪ {e} has no cycles\n    4          T = T ∪ {e}\n    5  return T\n    \n\n**3\\. `Maybe-MST-C`**\n    \n    \n    Maybe-MST-C (G, w)\n    1  T = {}\n    2  for each edge e ∈ E, taken in arbitrary order\n    3      T = T ∪ {e}\n    4      if T has a cycle _c_\n    5          let e' be a maximum weight edge on c\n    6          T = T − {e'}\n    7  return T\n    \n\n#### Efficient Implementation\n\n**4.** Once you are done, pick on of the above algorithms that you think computes a MST. Then describe an efficient implementation of that algorithm. Each algorithm above uses utility methods such as sorting, testing connectivity, detecting cycles, and finding the maximum weight edge on a cycle. Your response should focus on efficient implementations of these utility methods. \n\n\n",
 "path"=>
  "morea//170.minimum-spanning-tree/experience-minimum-spanning-tree.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/module-minimum-spanning-tree.html</h2>

<pre>Hash
{"title"=>"Minimum spanning tree",
 "published"=>true,
 "morea_id"=>"minimum-spanning-tree",
 "morea_outcomes"=>["outcome-minimum-spanning-tree"],
 "morea_readings"=>
  ["reading-screencast-17a",
   "reading-screencast-17b",
   "reading-screencast-17c",
   "reading-cormen-23",
   "reading-sedgewick-31",
   "reading-notes-17"],
 "morea_experiences"=>
  ["experience-minimum-spanning-tree", "experience-minimum-spanning-tree-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>
  "/morea/170.minimum-spanning-tree/module-minimum-spanning-tree.png",
 "morea_sort_order"=>170,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/170.minimum-spanning-tree/module-minimum-spanning-tree.html",
 "content"=>
  "Generic algorithm, safe edge algorithm, Kruskal's algorithm, Prim's algorithm, shortest path, dense paths.",
 "path"=>"morea//170.minimum-spanning-tree/module-minimum-spanning-tree.md"}
</pre>

<h2>/modules/minimum-spanning-tree/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-minimum-spanning-tree.md",
 "title"=>"Minimum spanning tree",
 "url"=>"/modules/minimum-spanning-tree/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n  <p>{{ page.morea_page.content | markdownify }}</p>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/minimum-spanning-tree/index.html"}
</pre>

<h2>/morea/170.minimum-spanning-tree/outcome-minimum-spanning-tree.html</h2>

<pre>Hash
{"title"=>"Understand minimum spanning tree",
 "published"=>true,
 "morea_id"=>"outcome-minimum-spanning-tree",
 "morea_type"=>"outcome",
 "morea_sort_order"=>170,
 "referencing_modules"=>[#Jekyll:Page @name="module-minimum-spanning-tree.md"],
 "url"=>"/morea/170.minimum-spanning-tree/outcome-minimum-spanning-tree.html",
 "content"=>"Understand when, why, and how to use minimum spanning trees. ",
 "path"=>"morea//170.minimum-spanning-tree/outcome-minimum-spanning-tree.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-cormen-23.html</h2>

<pre>Hash
{"title"=>"CLRS 23 - Minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-23",
 "morea_summary"=>
  "Growing a minimum spanning tree, the algorithms of Kruskal and Prim.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "19 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-cormen-23.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-cormen-23.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-notes-17.html</h2>

<pre>Hash
{"title"=>"Notes on minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-notes-17",
 "morea_summary"=>
  "Minimum spanning trees, the generic algorithm, Kruskal's and Prim's algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/170.minimum-spanning-tree/reading-notes-17.html",
 "url"=>"/morea/170.minimum-spanning-tree/reading-notes-17.html",
 "content"=>
  "## Outline\n\n  1. Minimum Spanning Trees \n  2. Generic Algorithm and Safe Edge Theorem \n  3. Kruskal's Algorithm \n  4. Prim's Algorithm \n\n##  Minimum Spanning Trees\n\n### Spanning Trees\n\nA **spanning tree** _T_ for a connected graph _G_ is a tree that includes all\nthe vertices of _G_: it _spans_ the graph.\n\nWithout calling them such, we have already encountered two kinds of spanning\ntrees in the introduction to graphs ([Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html)):\nthose generated by breadth-first search and depth-first search. We saw that _\nbreadth-first trees _ are one way of finding shortest paths in a graph, and _\ndepth-first forests _ (a collection of spanning trees, one for each connected\ncomponent) are good for uncovering the structure of a graph such as\ntopological sort and connectivity. These were defined on unweighted graphs.\n\n### Minimum Spanning Trees\n\nMany application areas (e.g., in communications, electronics, and\ntransportation) require finding the lowest cost way to connect a set of\nobjects or locations. For example, the cost may be measured in terms of\ncurrency or distance. We can model such situations with _**weighted graphs**_,\nintroduced in [Topic\n14](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-14.html) as\ngraphs where a real-valued number is associated with each edge. Then we want\nto find a spanning tree of minimum cost.\n\nMore formally, we can pose this as a problem on a graph representation _G_ =\n(_V_, _E_):\n\n  * The objects or locations are vertices _V_ and the available connections are edges _E_.\n  * A weight function _w_(_u_,_v_) gives the weight on each edge (_u_,_v_) ∈ _E_.\n  * We seek _T_ ⊆ _E_ such that \n    * _T_ connects all the vertices _V_ of _G_. \n    * The sum of weights _w_(_T_) = Σ(_u_,_v_)∈_T_ _w_(_u_,_v_) is minimized. \n\nA few facts can be noted:\n\n  * _G_ must be connected (consist of a single connected component) in order for _T_ to be possible. \n  * However, if _G_ is not connected we can generalize the problem to one of finding _T_1 ... _Tc_ for each of _c_ connected components of _G_.\n  * A subgraph of _G_ that connects its vertices _V_ at minimal cost will always be a tree. _Why?_\n\nTherefore we call this the **minimum spanning tree (MST)** problem (and the\ngeneralized version the minimum spanning forest problem).\n\nHere is an example of a minimum spanning tree (the shaded edges represent\n_T_):\n\n![](fig/example-MST-1.jpg)\n\n_Are minimum spanning trees unique?_\n\nLook at edges (_e_,_f_) and (_c_,_e_).\n\n* * *\n\n##  Generic Algorithm and Safe Edge Theorem\n\nWe specify a generic greedy algorithm for solving the MST problem. The\nalgorithm will be \"greedy\" in terms of always choosing a lowest cost edge.\nThis algorithm is instantiated into two versions, Kruskal's and Prim's\nalgorithms, which differ in how they define from what set of edges the lowest\ncost edge is chosen.\n\nLet's start by noting some properties that MSTs of _G_ = (_V_, _E_) must have\n\n  * A MST for _G_ has |_V_| − 1 edges. (See properties of trees, [Topic 8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html).)\n  * Any tree (and hence any MST) has no cycles. It has only one path between any two vertices.\n  * There might be more than one MST for _G_.\n\n### Building a Solution\n\n  * We will build a set of edges _A_. \n  * Initially _A_ has no edges.\n  * As we add edges to _A_, we maintain a loop invariant: _A_ is a subset of _some_ MST for _G_.\n\nDefine an edge (_u_,_v_) to be **safe** for _A_ iff _A_ ∪ {(_u_,_v_)} is also\na subset of some MST for _G_.\n\n(BTW, \"iff\" is not a spelling error: it is shorthand for \"if and only if\"\ncommonly used in proofs.\n\nIf we only add safe edges to _A_, once |_V_| − 1 edges have been added we have\na MST for _G_. This motivates the ...\n\n### Generic MST Algorithm\n\n![](fig/pseudocode-generic-MST.jpg)\n\n_Loop Invariant:_ _A_ is a subset of some MST for _G_\n\n  * _Initialization:_ The initially empty set trivially satisfies the loop invariant.\n  * _Maintenance:_ Since we add only safe edges, _A_ remains a subset of some MST.\n  * _Termination:_ We stop when _A_ is a spanning tree (|_A_| = |_V_| − 1), and it is a subset of itself.\n\nOK, great, but how do we find safe edges?\n\n### Finding Safe Edges\n\nEach time we add an edge we are connecting two sets of vertices that were not\npreviously connected by _A_. (Otherwise we would be forming a cycle.) A greedy\nalgorithm might try to keep the cost down by choosing the lowest cost edge\nthat connects previously unconnected vertices. (Perhaps we should call it a\n\"stingy\" algorithm!)\n\nBut is this greedy strategy \"safe\"? How do we know that after adding this edge\nwe still have a subset of an MST?\n\nFirst some definitions:\n\n  * A **cut** (_S_, _V_ − _S_) is a partition of vertices into disjoint sets _S_ and _V_ − _S_. \n  * Edge (_u_,_v_) ∈ _E_ **crosses** cut (_S_, _V_ − _S_) if one endpoint is in _S_ and the other is in _V_ − _S_. \n  * A cut **respects** _A_ iff no edge in _A_ crosses the cut.\n  * An edge is a **light edge** crossing a cut iff its weight is minimum over all edges crossing the cut. (There may be more than one light edge for a given cut.) \n\nThe following illustrates a cut and will be used in the proof below. There are\ntwo sets of vertices _S_ and _V_ − _S_. Four edges cross the cut (_S_, _V_ \\-\n_S_). Whether or not this respects _A_ depends on what is in _A_.\n\n![](fig/illustration-safe-edge-theorem.jpg)\n\n_Suppose A is the shaded edges. Does this cut respect A?_\n\n#### Safe Edge Theorem\n\nLet _G_ = (_V_, _E_) be a graph, _A_ be a subset of some MST for _G_, (_S_,\n_V_ − _S_) be a cut that respects _A_, and (_u_,_v_) be a light edge crossing\n(_S_, _V_ − _S_). Then (_u_,_v_) is safe for _A_.\n\n(_A light edge that crosses a cut that respects _A_ is safe for _A_._)\n\n_**Proof:**_ Let _T_ be a MST that includes _A_. Consider two cases:\n\nCase 1: _T_ contains (_u_,_v_). Then the theorem is proven, since _A_ ∪\n{(_u_,_v_)} ⊆ _T_ is a subset of some MST for _G_.\n\nCase 2: _T_ does not contain (_u_,_v_). We will show that we can construct a\ntree _T'_ that is a MST for _G_ and that contains _A_ ∪ {(_u_,_v_)}.\n\n![](fig/illustration-safe-edge-theorem.jpg)\n\nSince _T_ is a tree it contains a unique path _p_ between _u_ and _v_. Path\n_p_ must cross the cut (_S_, _V_ − _S_) at least once (otherwise _T_ would be\ndisconnected). Let (_x_,_y_) be an edge of _p_ that crosses the cut.\n\n(Except for the dashed edge (_u_,_v_), all the edges shown in the figure are\nin _T_. _A_ is not shown in the figure, but it cannot contain any edges that\ncross the cut, since the cut respects _A_. Shaded edges are the path _p_.)\n\nSince the cut respects _A_, edge (_x_,_y_) is not in _A_.\n\nTo form _T'_ from _T_: Remove (_x_,_y_). This breaks _T_ into two components.\nAdd (_u_,_v_). This reconnects the tree. So _T'_ = T - {(_x_,_y_)} ∪ (_u_,_v_)\nis a spanning tree.\n\nTo show that _T'_ is a minimal spanning tree: _w_(_T'_) = _w_(_T_) -\n_w_(_x_,_y_) + _w_(_u_,_v_) ≤ _w_(_T_) since (_u_,_v_) is light.\n\nWe still need to show that (_u_,_v_) is safe for _A_. Since _A_ ⊆ _T_ and\n(_x_,_y_) ∉ _A_ then A ⊆ _T'_. Therefore _A_ ∪ {(_u_,_v_)} ⊆ _T'_, a MST. ♦\n\n#### Further Observations\n\n_A_ is a forest containing connected components. Initially each component is a\nsingle vertex. Any safe edge merges two of these components into one. Each\ncomponent so constructed is a tree. Since an MST has exactly |_V_| − 1 edges,\nthe loop iterates |_V_| − 1 times before we are down to one component.\n\n#### Corollary\n\nIf _C_ = (_VC_, _EC_) is a connected component in the forest _GA_ = (_V_, _A_)\nand (_u_,_v_) is a light edge connecting _C_ to some other component _C'_ in\n_GA_ \\-- that is, (_u_,_v_) is a light edge crossing the cut (_VC_, _V_ \\-\n_VC_) -- then (_u_,_v_) is safe for _A_.\n\n_Proof:_ Set _S_ = _VC_ in the theorem. ♦\n\nThis idea (of thinking in terms of components rather than vertices) leads to\nKruskal's algorithm ...\n\n* * *\n\n##  Kruskal's Algorithm\n\nKruskal's algorithm starts with each vertex being its own component, and\nrepeatedly merges two components into one by choosing the light edge that\nconnects them. It does this greedily (or stingily?) by scanning the edges in\nincreasing order by weight. A disjoint-set data structure is used to determine\nwhether an edge connects vertices in two different components.\n\nThis algorithm has similarities with the connected components algorithm we\npreviously saw in [Topic\n16](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-16.html):\n\n![](fig/pseudocode-connected-components.jpg)\n\nHere is Kruskal's version:\n\n![](fig/pseudocode-Kruskal-MST.jpg)\n\n### Example\n\nLet's start with this example. The first edge has been chosen.\n\n![](fig/Fig-23-4-Kruskal-Example-a.jpg)\n\nAdd 4 more edges (notice we could add edges of weight 2 in either order, and\nsimilarly for 4) ...\n\n![](fig/Fig-23-4-Kruskal-Example-e.jpg)\n\nThe next edge considered is not added because it would connect already\nconnected vertices:\n\n[ ![](fig/Fig-23-4-Kruskal-Example-f.jpg)](http://www2.hawaii.edu/~\nsuthers/courses/ics311s14/Notes/Topic-17/Topic-17-K.html)\n\nKeep going until the MST is constructed, and click to see the final tree.\n\n### Analysis\n\n![](fig/pseudocode-Kruskal-MST.jpg)\n\nThe costs are:\n\n  * Initialize _A_: O(1)\n  * First `for` loop: |_V_| `MAKE-SET` operations\n  * Sort _E_: O(_E_ lg _E_) \n  * Second `for` loop: O(_E_) `FIND-SETs` and `UNIONs`\n\nIf we use the tree implementation of the disjoint-set data structure with\nunion by rank and path compression ([Topic\n16](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-16.html)),\nthe amortized cost per `MAKE-SET`, `UNION` and `FIND-SET` operation (across\n|_E_| operations) is O(α(_V_)), where α is a _very_ slowly growing function,\nthe inverse of Ackermann's function. (Lemma 21.11 states that MAKE-SET in\nisolation is O(1), but here we must treat it as O(α(_V_)) since we are making\na statement about the amortized cost per operation in a _sequence_ of _m_\noperations: see section 24.1. Also, using O(α(_V_)) simplifies the expression\nbelow.)\n\nDroping the lower order O(1) and substituting α(_V_) for the disjoint-set\noperations, the above list of costs sums to O((_V_ \\+ _E_)⋅α(_V_))+ O(_E_ lg\n_E_).\n\nSince G is connected, |_E_| ≥ |_V_| − 1, so we can replace _V_ with _E_ to\nsimplify the first term for the disjoint-set operations, O((_V_ \\+\n_E_)⋅α(_V_)), to O((_E_ \\+ _E_)⋅α(_V_)) or O(_E_⋅α(_V_)).\n\nFurthermore, α(_V_) = O(lg _V_) = O(lg _E_), so O(_E_⋅α(_V_)) is O(_E_ lg\n_E_), and hence the entire expression we started with, O((_V_ \\+ _E_)⋅α(_V_))+\nO(_E_ lg _E_), simplifies to O(_E_ lg _E_).\n\nFinally, since |_E_| ≤ |_V_|2, lg |_E_| = O(2 lg _V_) = O(lg _V_), so we can\nwrite the result as **O(_E_ lg _V_)** to obtain the growth rate in terms of\nboth |_E_| and |_V_|.\n\n(It is usually a good idea to include both _V_ and _E_ when giving growth\nrates for graph algorithms, unless one of them can be strictly limited to the\nother. Shortly we will see that O(_E_ lg _V_) enables comparison to Prim's\nalgorithm.)\n\n* * *\n\n##  Prim's Algorithm\n\nThis algorithm is also a greedy (stingy) algorithm, but it builds one tree,\nchoosing the lightest edge incident on the growing tree, so the set _A_ is\nalways a tree. The tree is initialized to be a single vertex, designated _r_\nfor root.\n\nAt each step it finds a light edge crossing the cut (_VA_, _V_ \\- _VA_), where\n_VA_= vertices that are incident on _A_, and adds this edge to _A_.\n(Initially, _A_ = {} and _VA_ = {_r_}.)\n\n![](fig/illustration-Prims-algorithm.jpg)\n\n(Edges of _A_ are shaded in the illustration.)\n\n### General Idea\n\nTo find the light edge quickly we use a priority queue _Q_ ([Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html)):\n\n  * Each queued object is a vertex in _V_ − _VA_ (the vertices that have not yet been connected to _A_). \n  * The key is the minimum weight of any edge (_u_,_v_) where _u_ ∈ _VA_. (We update this weight for _v_ whenever a new edge is found that reaches _v_ at a lower cost than before.) \n  * Thus the vertex returned by `EXTRACT-MIN` is for _v_ such that ∃ _u_ ∈ _VA_ and (_u_,_v_) is a light edge crossing (_VA_, _V_ \\- _VA_). \n  * If _v_ is not adjacent to any vertices in _VA_, the key of _v_ is ∞\n\nThe edges of _A_ will form a rooted tree with root _r_, given as input (_r_\ncan be any vertex).\n\n  * Each vertex keeps track of its parent by the attribute _v_.π = parent of _v_, or NIL if _v_ = _r_ or has no parent yet.\n  * As the algorithm progresses, _A_ = {(_v_, _v_.π) : _v_ ∈ _V_ \\- {_r_} − _Q_}. \n  * At termination, _Q_ is empty, so _A_ is a MST.\n\n### Pseudocode\n\nThis code _differs from the book's version_ in having explicit calls to the\nheap methods:\n\n![](fig/pseudocode-Prim-MST-improved.jpg)\n\nNotice that it is possible for the last `if` to execute multiple times for a\ngiven _v_. In other words, we may find an edge reaching vertex _v_, but before\nwe choose to use it (because other edges have lower key values), we find\nanother edge reaching _v_ for lower cost (key value). _Watch for this\nsituation in the example below._\n\n### Example\n\nLet's try it with this graph. The first three steps are shown. Every time a\nvertex is dequed, it is colored black and the cost of all adjacent vertices\nare updated as needed. For example, when **a** is dequeued, the cost of **b**\nis updated from infinite to 4, and the cost of **h** is updated from infinite\nto 8. Then when **b** is dequeued, its neighbors are updated and so on.\n\n![](fig/Fig-23-5-Prim-Example-a-d.jpg)\n\n_Did you see where a vertex's key was lowered from one non-infinite value to\nanother? Which one? _\n\n_Now finish it and click on the image to see final solution._\n\n![](fig/pseudocode-Prim-MST-improved.jpg)\n\n###  Analysis\n\nPerformance depends on the priority queue implementation. With a binary heap\nimplementation ([Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html)),\nthe costs are:\n\n  * Initialize _Q_ and iterate over |_V_| vertices in first `for` loop to insert in queue, each insert being O(lg _V_): O(_V_ lg _V_) total.\n  * Decrease key of r: O(lg _V_)\n  * The `while` loop has |_V_| `EXTRACT-MIN` calls -> O(_V_ lg _V_)\n  * By amortized analysis, the inner `for each` loop processes Θ(|_E_|) edges, O(_E_) of which result in O(lg _V_) `DECREASE-KEY` calls -> O(_E_ lg _V_)\n\nThe sum of the dominating terms is O(_V_ lg _V_) + O(_E_ lg _V_).\n\nIf G is connected, |_E_| ≥ |_V_| − 1, so we can replace O(_V_ lg _V_) with\nO(_E_ lg _V_), and the total is **O(_E_ lg _V_)**.\n\nThis is asympotitically the same as Kruskal's algorithm. A faster\nimplementation of O(_E_ \\+ _V_ lg _V_) is possible with Fibonacci Heaps, as\nexplained in the text.\n\n* * *\n\nDan Suthers Last modified: Thu Apr 3 12:36:42 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//170.minimum-spanning-tree/reading-notes-17.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-17a.html</h2>

<pre>Hash
{"title"=>"Introduction to minimum spanning trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-17a",
 "morea_summary"=>"Including the generic algorithm and the safe edge theorem",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=Mx-dvvSE4Qc",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-17a.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-17a.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-17b.html</h2>

<pre>Hash
{"title"=>"Kruskal's algorithm",
 "published"=>true,
 "morea_id"=>"reading-screencast-17b",
 "morea_summary"=>
  "Kruskal's minimum spanning tree algorithm, with example and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=1BnpXYm7LKY",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-17b.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-17b.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-screencast-17c.html</h2>

<pre>Hash
{"title"=>"Prim's algorithm",
 "published"=>true,
 "morea_id"=>"reading-screencast-17c",
 "morea_summary"=>
  "Prim's minimum spanning tree algorithm, with example and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=qegY0R78QMQ",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-screencast-17c.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-screencast-17c.md"}
</pre>

<h2>/morea/170.minimum-spanning-tree/reading-sedgewick-31.html</h2>

<pre>Hash
{"title"=>"Sedgewick 31 - Weighted graphs",
 "published"=>true,
 "morea_id"=>"reading-sedgewick-31",
 "morea_summary"=>
  "Minimum spanning trees, shortest path, dense paths, geometric problems",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://sciencelib.net/1496/algorithms-r-sedgewick-1983-ww.html",
 "morea_labels"=>["Textbook", "14 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/170.minimum-spanning-tree/reading-sedgewick-31.html",
 "content"=>"",
 "path"=>"morea//170.minimum-spanning-tree/reading-sedgewick-31.md"}
</pre>

<h2>/morea//footer.html</h2>

<pre>Hash
{"title"=>"Footer",
 "morea_id"=>"footer",
 "morea_type"=>"footer",
 "referencing_modules"=>[],
 "url"=>"/morea//footer.html",
 "content"=>
  "Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>\nsuthers@hawaii.edu\n\n",
 "path"=>"morea//footer.md"}
</pre>

<h2>/morea//home.html</h2>

<pre>Hash
{"title"=>"Home",
 "morea_id"=>"home",
 "morea_type"=>"home",
 "referencing_modules"=>[],
 "url"=>"/morea//home.html",
 "content"=>
  "## Welcome to ICS 311, Spring 2014\n\n**If you are an ICS 311, Spring 2014 student, do not use this site.**  Instead, refer to the [real site](http://www2.hawaii.edu/~suthers/courses/ics311s14/index.html).\n\nThis is a subset of the course material for ICS 311 Algorithms, Spring 2014, created to illustrate the use of the \nMorea Framework.\n\nGenerally the course is run within\n[Laulima](https://laulima.hawaii.edu/portal/site/MAN.82792.201430). Notes\nand other material are posted on this site.  However, course\nparticipants will need to log into Laulima regularly to watch\nscreencasts, take quizzes, submit assignments, and to use other\nfacilities such as discussions and the mail tool as needed. \n\nClasses are in [Webster 101](http://www2.hawaii.edu/~suthers/courses/ics311s14/map.jpg).",
 "path"=>"morea//home.md"}
</pre>

</div>



<div class="dark-blue-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a style="color: white" href="http://morea-framework.github.io/">Morea Framework</a><br>
       Last update on: <span>2014-04-18 16:12:25 -1000</span></p>
  </div>
</footer>
</div>
</body>
</html>

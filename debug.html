<!DOCTYPE html>
<html>
<head>
  <title> Debug | ICS 311 Spring 2014 </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/3.1.0/cerulean/bootstrap.min.css">

  <!--  Load site-specific customizations after bootstrap. -->
  <link rel="stylesheet" href="/ics311s14/css/style.css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:normal,italic,bold">
  <link rel="shortcut icon" href="/ics311s14/favicon.ico" type="image/x-icon" />

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.6.2/html5shiv.js"></script>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/respond.js/1.2.0/respond.js"></script>
  <![endif]-->

  <!-- Load Bootstrap JavaScript components -->
  <script src="http://code.jquery.com/jquery.min.js"></script>
  <script src="http://netdna.bootstrapcdn.com/bootstrap/3.1.0/js/bootstrap.min.js"></script>
</head>
<body>
<!-- Responsive navbar -->
<div class="navbar navbar-default navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
        <!--  Display three horizontal lines when navbar collapsed. -->
        <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="#"> ICS 311 Spring 2014 </a>
    </div>
    <div class="collapse navbar-collapse">
      <ul class="nav navbar-nav">
        <li><a href="/ics311s14/index.html">Home</a></li>
        <li><a href="/ics311s14/modules/">Modules</a></li>
        <li><a href="/ics311s14/outcomes/">Outcomes</a></li>
        <li><a href="/ics311s14/readings/">Readings</a></li>
        <li><a href="/ics311s14/experiences/">Experiences</a></li>
        <li><a href="/ics311s14/assessments/">Assessments</a></li>
        <li><a href="/ics311s14/schedule/">Schedule</a></li>
        
      </ul>
    </div>
  </div>
</div>


<div class="container">
  <h1>Debugging</h1>

<h2>Site</h2>

<pre>Hash
{"source"=>"./master/src",
 "destination"=>"./gh-pages",
 "plugins"=>"_plugins",
 "layouts"=>"_layouts",
 "data_source"=>"_data",
 "keep_files"=>[".git", ".svn"],
 "gems"=>[],
 "timezone"=>nil,
 "encoding"=>nil,
 "safe"=>false,
 "detach"=>false,
 "show_drafts"=>nil,
 "limit_posts"=>0,
 "lsi"=>false,
 "future"=>true,
 "pygments"=>true,
 "relative_permalinks"=>true,
 "markdown"=>"redcarpet",
 "permalink"=>"date",
 "baseurl"=>"/ics311s14",
 "include"=>[".htaccess"],
 "exclude"=>["morea"],
 "paginate_path"=>"/page:num",
 "markdown_ext"=>"markdown,mkd,mkdn,md",
 "textile_ext"=>"textile",
 "port"=>"4000",
 "host"=>"0.0.0.0",
 "excerpt_separator"=>"\n\n",
 "maruku"=>
  {"fenced_code_blocks"=>true,
   "use_tex"=>false,
   "use_divs"=>false,
   "png_engine"=>"blahtex",
   "png_dir"=>"images/latex",
   "png_url"=>"/images/latex"},
 "rdiscount"=>{"extensions"=>[]},
 "redcarpet"=>{"extensions"=>[]},
 "kramdown"=>
  {"auto_ids"=>true,
   "footnote_nr"=>1,
   "entity_output"=>"as_char",
   "toc_levels"=>"1..6",
   "smart_quotes"=>"lsquo,rsquo,ldquo,rdquo",
   "use_coderay"=>false,
   "coderay"=>
    {"coderay_wrap"=>"div",
     "coderay_line_numbers"=>"inline",
     "coderay_line_number_start"=>1,
     "coderay_tab_width"=>4,
     "coderay_bold_every"=>10,
     "coderay_css"=>"style"}},
 "redcloth"=>{"hard_breaks"=>true},
 "name"=>"ICS 311 Spring 2014",
 "morea_debug"=>false,
 "morea_module_pages"=>
  [#Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="module-quicksort.md"],
 "morea_outcome_pages"=>
  [#Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="outcome-quicksort.md"],
 "morea_reading_pages"=>
  [#Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="reading-notes-10.md"],
 "morea_experience_pages"=>
  [#Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-3.md"],
 "morea_assessment_pages"=>[],
 "morea_home_page"=>#Jekyll:Page @name="home.md",
 "morea_footer_page"=>#Jekyll:Page @name="footer.md",
 "morea_page_table"=>
  {"intro"=>#Jekyll:Page @name="module-introduction.md",
   "outcome-311"=>#Jekyll:Page @name="outcome-311.md",
   "outcome-algorithm"=>#Jekyll:Page @name="outcome-algorithm.md",
   "reading-algorithms"=>#Jekyll:Page @name="reading-algorithms.md",
   "reading-assessment"=>#Jekyll:Page @name="reading-assessment.md",
   "reading-assignments"=>#Jekyll:Page @name="reading-assignments.md",
   "reading-cormen-1"=>#Jekyll:Page @name="reading-cormen-1.md",
   "reading-course-info"=>#Jekyll:Page @name="reading-course-info.md",
   "reading-format"=>#Jekyll:Page @name="reading-format.md",
   "reading-policies"=>#Jekyll:Page @name="reading-policies.md",
   "reading-topic-overview"=>#Jekyll:Page @name="reading-topic-overview.md",
   "experience-1"=>#Jekyll:Page @name="experience-1.md",
   "experience-2"=>#Jekyll:Page @name="experience-2.md",
   "experience-3"=>#Jekyll:Page @name="experience-3.md",
   "examples-insertion-merge-sort"=>#Jekyll:Page @name="module-examples.md",
   "outcome-analysis-style"=>#Jekyll:Page @name="outcome-analysis-style.md",
   "reading-cormen-2"=>#Jekyll:Page @name="reading-cormen-2.md",
   "reading-notes-2"=>#Jekyll:Page @name="reading-notes-2.md",
   "reading-screencast-2a"=>#Jekyll:Page @name="reading-screencast-2A.md",
   "reading-screencast-2b"=>#Jekyll:Page @name="reading-screencast-2B.md",
   "reading-screencast-2c"=>#Jekyll:Page @name="reading-screencast-2C.md",
   "reading-screencast-2d"=>#Jekyll:Page @name="reading-screencast-2D.md",
   "reading-screencast-2e"=>#Jekyll:Page @name="reading-screencast-2E.md",
   "reading-screencast-mit-1"=>
    #Jekyll:Page @name="reading-screencast-mit-1.md",
   "experience-asymptotic-concepts"=>
    #Jekyll:Page @name="experience-asymptotic-concepts.md",
   "growth"=>#Jekyll:Page @name="module-growth.md",
   "outcome-growth"=>#Jekyll:Page @name="outcome-growth.md",
   "reading-cormen-3"=>#Jekyll:Page @name="reading-cormen-3.md",
   "reading-notes-3"=>#Jekyll:Page @name="reading-notes-3.md",
   "reading-screencast-3a"=>#Jekyll:Page @name="reading-screencast-3a.md",
   "reading-screencast-3b"=>#Jekyll:Page @name="reading-screencast-3b.md",
   "reading-screencast-3c"=>#Jekyll:Page @name="reading-screencast-3c.md",
   "reading-screencast-3d"=>#Jekyll:Page @name="reading-screencast-3d.md",
   "reading-screencast-mit-2"=>
    #Jekyll:Page @name="reading-screencast-mit-2.md",
   "experience-asymptotic-homework"=>
    #Jekyll:Page @name="experience-asymptotic-homework.md",
   "experience-asymptotic-basic-data-structures"=>
    #Jekyll:Page @name="experience-basic-data-structures.md",
   "adt"=>#Jekyll:Page @name="module-adt.md",
   "outcome-adt"=>#Jekyll:Page @name="outcome-adt.md",
   "reading-cormen-10"=>#Jekyll:Page @name="reading-cormen-10.md",
   "reading-notes-4"=>#Jekyll:Page @name="reading-notes-4.md",
   "reading-screencast-4a"=>#Jekyll:Page @name="reading-screencast-4a.md",
   "reading-screencast-4b"=>#Jekyll:Page @name="reading-screencast-4b.md",
   "experience-indicator-random-variables"=>
    #Jekyll:Page @name="experience-indicator-random-variables.md",
   "probabilistic"=>#Jekyll:Page @name="module-probabilistic.md",
   "outcome-probabilistic"=>#Jekyll:Page @name="outcome-probabilistic.md",
   "reading-cormen-5"=>#Jekyll:Page @name="reading-cormen-5.md",
   "reading-goodrich"=>#Jekyll:Page @name="reading-goodrich.md",
   "reading-notes-5"=>#Jekyll:Page @name="reading-notes-5.md",
   "reading-screencast-5a"=>#Jekyll:Page @name="reading-screencast-5a.md",
   "reading-screencast-5b"=>#Jekyll:Page @name="reading-screencast-5b.md",
   "reading-screencast-5c"=>#Jekyll:Page @name="reading-screencast-5c.md",
   "reading-screencast-5d"=>#Jekyll:Page @name="reading-screencast-5d.md",
   "reading-screencast-mit-skip-lists"=>
    #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   "experience-data-structures-homework"=>
    #Jekyll:Page @name="experience-data-structures-homework.md",
   "experience-deletion"=>#Jekyll:Page @name="experience-deletion.md",
   "hash-tables"=>#Jekyll:Page @name="module-hash-tables.md",
   "outcome-hash-tables"=>#Jekyll:Page @name="outcome-hash-tables.md",
   "reading-cormen-11"=>#Jekyll:Page @name="reading-cormen-11.md",
   "reading-notes-6"=>#Jekyll:Page @name="reading-notes-6.md",
   "reading-screencast-6a"=>#Jekyll:Page @name="reading-screencast-6a.md",
   "reading-screencast-6b"=>#Jekyll:Page @name="reading-screencast-6b.md",
   "reading-screencast-6c"=>#Jekyll:Page @name="reading-screencast-6c.md",
   "reading-screencast-6d"=>#Jekyll:Page @name="reading-screencast-6d.md",
   "reading-screencast-mit-hash-tables-1"=>
    #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   "reading-screencast-mit-hash-tables-2"=>
    #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   "experience-master-method"=>
    #Jekyll:Page @name="experience-master-method.md",
   "experience-substitution"=>#Jekyll:Page @name="experience-substitution.md",
   "divide-conquer"=>#Jekyll:Page @name="module-divide-conquer.md",
   "outcome-divide-conquer-apply"=>
    #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   "outcome-divide-conquer-recognize"=>
    #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   "reading-cormen-4"=>#Jekyll:Page @name="reading-cormen-4.md",
   "reading-notes-7"=>#Jekyll:Page @name="reading-notes-7.md",
   "reading-screencast-7a"=>#Jekyll:Page @name="reading-screencast-7a.md",
   "reading-screencast-7b"=>#Jekyll:Page @name="reading-screencast-7b.md",
   "reading-screencast-7c"=>#Jekyll:Page @name="reading-screencast-7c.md",
   "reading-screencast-7d"=>#Jekyll:Page @name="reading-screencast-7d.md",
   "reading-screencast-mit-divide-conquer"=>
    #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   "experience-binary-search-trees-2"=>
    #Jekyll:Page @name="experience-binary-search-trees-2.md",
   "experience-binary-search-trees"=>
    #Jekyll:Page @name="experience-binary-search-trees.md",
   "binary-search-trees"=>#Jekyll:Page @name="module-binary-search-trees.md",
   "outcome-binary-search-trees"=>
    #Jekyll:Page @name="outcome-binary-search-trees.md",
   "reading-cormen-12"=>#Jekyll:Page @name="reading-cormen-12.md",
   "reading-notes-8"=>#Jekyll:Page @name="reading-notes-8.md",
   "reading-screencast-8a"=>#Jekyll:Page @name="reading-screencast-8a.md",
   "reading-screencast-8b"=>#Jekyll:Page @name="reading-screencast-8b.md",
   "reading-screencast-8c"=>#Jekyll:Page @name="reading-screencast-8c.md",
   "reading-screencast-8d"=>#Jekyll:Page @name="reading-screencast-8d.md",
   "experience-heaps-2"=>#Jekyll:Page @name="experience-heaps-2.md",
   "experience-heaps"=>#Jekyll:Page @name="experience-heaps.md",
   "heaps"=>#Jekyll:Page @name="module-heaps.md",
   "outcome-heaps"=>#Jekyll:Page @name="outcome-heaps.md",
   "reading-cormen-6"=>#Jekyll:Page @name="reading-cormen-6.md",
   "reading-notes-9"=>#Jekyll:Page @name="reading-notes-9.md",
   "reading-screencast-9a"=>#Jekyll:Page @name="reading-screencast-9a.md",
   "reading-screencast-9b"=>#Jekyll:Page @name="reading-screencast-9b.md",
   "reading-screencast-9c"=>#Jekyll:Page @name="reading-screencast-9c.md",
   "reading-screencast-9d"=>#Jekyll:Page @name="reading-screencast-9d.md",
   "experience-quicksort-2"=>#Jekyll:Page @name="experience-quicksort-2.md",
   "experience-quicksort"=>#Jekyll:Page @name="experience-quicksort.md",
   "quicksort"=>#Jekyll:Page @name="module-quicksort.md",
   "outcome-quicksort"=>#Jekyll:Page @name="outcome-quicksort.md",
   "reading-cormen-7"=>#Jekyll:Page @name="reading-cormen-7.md",
   "reading-cormen-8"=>#Jekyll:Page @name="reading-cormen-8.md",
   "reading-notes-10"=>#Jekyll:Page @name="reading-notes-10.md",
   "reading-screencast-10a"=>#Jekyll:Page @name="reading-screencast-10a.md",
   "reading-screencast-10b"=>#Jekyll:Page @name="reading-screencast-10b.md",
   "reading-screencast-10c"=>#Jekyll:Page @name="reading-screencast-10c.md",
   "footer"=>#Jekyll:Page @name="footer.md",
   "home"=>#Jekyll:Page @name="home.md"},
 "morea_fatal_errors"=>false,
 "time"=>2014-04-15 17:25:58 -1000,
 "posts"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
 "pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="module-quicksort.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-quicksort.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-notes-10.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "html_pages"=>
  [#Jekyll:Page @name="index.md",
   #Jekyll:Page @name="debug.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="module-introduction.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-311.md",
   #Jekyll:Page @name="outcome-algorithm.md",
   #Jekyll:Page @name="reading-algorithms.md",
   #Jekyll:Page @name="reading-assessment.md",
   #Jekyll:Page @name="reading-assignments.md",
   #Jekyll:Page @name="reading-cormen-1.md",
   #Jekyll:Page @name="reading-course-info.md",
   #Jekyll:Page @name="reading-format.md",
   #Jekyll:Page @name="reading-policies.md",
   #Jekyll:Page @name="reading-topic-overview.md",
   #Jekyll:Page @name="experience-1.md",
   #Jekyll:Page @name="experience-2.md",
   #Jekyll:Page @name="experience-3.md",
   #Jekyll:Page @name="module-examples.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-analysis-style.md",
   #Jekyll:Page @name="reading-cormen-2.md",
   #Jekyll:Page @name="reading-notes-2.md",
   #Jekyll:Page @name="reading-screencast-2A.md",
   #Jekyll:Page @name="reading-screencast-2B.md",
   #Jekyll:Page @name="reading-screencast-2C.md",
   #Jekyll:Page @name="reading-screencast-2D.md",
   #Jekyll:Page @name="reading-screencast-2E.md",
   #Jekyll:Page @name="reading-screencast-mit-1.md",
   #Jekyll:Page @name="experience-asymptotic-concepts.md",
   #Jekyll:Page @name="module-growth.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-growth.md",
   #Jekyll:Page @name="reading-cormen-3.md",
   #Jekyll:Page @name="reading-notes-3.md",
   #Jekyll:Page @name="reading-screencast-3a.md",
   #Jekyll:Page @name="reading-screencast-3b.md",
   #Jekyll:Page @name="reading-screencast-3c.md",
   #Jekyll:Page @name="reading-screencast-3d.md",
   #Jekyll:Page @name="reading-screencast-mit-2.md",
   #Jekyll:Page @name="experience-asymptotic-homework.md",
   #Jekyll:Page @name="experience-basic-data-structures.md",
   #Jekyll:Page @name="module-adt.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-adt.md",
   #Jekyll:Page @name="reading-cormen-10.md",
   #Jekyll:Page @name="reading-notes-4.md",
   #Jekyll:Page @name="reading-screencast-4a.md",
   #Jekyll:Page @name="reading-screencast-4b.md",
   #Jekyll:Page @name="experience-indicator-random-variables.md",
   #Jekyll:Page @name="module-probabilistic.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-probabilistic.md",
   #Jekyll:Page @name="reading-cormen-5.md",
   #Jekyll:Page @name="reading-goodrich.md",
   #Jekyll:Page @name="reading-notes-5.md",
   #Jekyll:Page @name="reading-screencast-5a.md",
   #Jekyll:Page @name="reading-screencast-5b.md",
   #Jekyll:Page @name="reading-screencast-5c.md",
   #Jekyll:Page @name="reading-screencast-5d.md",
   #Jekyll:Page @name="reading-screencast-mit-skip-lists.md",
   #Jekyll:Page @name="experience-data-structures-homework.md",
   #Jekyll:Page @name="experience-deletion.md",
   #Jekyll:Page @name="module-hash-tables.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-hash-tables.md",
   #Jekyll:Page @name="reading-cormen-11.md",
   #Jekyll:Page @name="reading-notes-6.md",
   #Jekyll:Page @name="reading-screencast-6a.md",
   #Jekyll:Page @name="reading-screencast-6b.md",
   #Jekyll:Page @name="reading-screencast-6c.md",
   #Jekyll:Page @name="reading-screencast-6d.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-1.md",
   #Jekyll:Page @name="reading-screencast-mit-hash-tables-2.md",
   #Jekyll:Page @name="experience-master-method.md",
   #Jekyll:Page @name="experience-substitution.md",
   #Jekyll:Page @name="module-divide-conquer.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-divide-conquer-apply.md",
   #Jekyll:Page @name="outcome-divide-conquer-recognize.md",
   #Jekyll:Page @name="reading-cormen-4.md",
   #Jekyll:Page @name="reading-notes-7.md",
   #Jekyll:Page @name="reading-screencast-7a.md",
   #Jekyll:Page @name="reading-screencast-7b.md",
   #Jekyll:Page @name="reading-screencast-7c.md",
   #Jekyll:Page @name="reading-screencast-7d.md",
   #Jekyll:Page @name="reading-screencast-mit-divide-conquer.md",
   #Jekyll:Page @name="experience-binary-search-trees-2.md",
   #Jekyll:Page @name="experience-binary-search-trees.md",
   #Jekyll:Page @name="module-binary-search-trees.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-binary-search-trees.md",
   #Jekyll:Page @name="reading-cormen-12.md",
   #Jekyll:Page @name="reading-notes-8.md",
   #Jekyll:Page @name="reading-screencast-8a.md",
   #Jekyll:Page @name="reading-screencast-8b.md",
   #Jekyll:Page @name="reading-screencast-8c.md",
   #Jekyll:Page @name="reading-screencast-8d.md",
   #Jekyll:Page @name="experience-heaps-2.md",
   #Jekyll:Page @name="experience-heaps.md",
   #Jekyll:Page @name="module-heaps.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-heaps.md",
   #Jekyll:Page @name="reading-cormen-6.md",
   #Jekyll:Page @name="reading-notes-9.md",
   #Jekyll:Page @name="reading-screencast-9a.md",
   #Jekyll:Page @name="reading-screencast-9b.md",
   #Jekyll:Page @name="reading-screencast-9c.md",
   #Jekyll:Page @name="reading-screencast-9d.md",
   #Jekyll:Page @name="experience-quicksort-2.md",
   #Jekyll:Page @name="experience-quicksort.md",
   #Jekyll:Page @name="module-quicksort.md",
   #Jekyll:Page @name="index.html",
   #Jekyll:Page @name="outcome-quicksort.md",
   #Jekyll:Page @name="reading-cormen-7.md",
   #Jekyll:Page @name="reading-cormen-8.md",
   #Jekyll:Page @name="reading-notes-10.md",
   #Jekyll:Page @name="reading-screencast-10a.md",
   #Jekyll:Page @name="reading-screencast-10b.md",
   #Jekyll:Page @name="reading-screencast-10c.md",
   #Jekyll:Page @name="footer.md",
   #Jekyll:Page @name="home.md"],
 "categories"=>
  {"jekyll"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"],
   "update"=>[#Jekyll:Post @id="/jekyll/update/2014/01/14/welcome-to-jekyll"]},
 "tags"=>{},
 "data"=>{}}
</pre>

<h2>Pages</h2>

<h2>/assessments/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Assessments",
 "url"=>"/assessments/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Assessments <small>in module order</small></h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/intro/index.html\">Introduction</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/examples-insertion-merge-sort/index.html\">Analysis examples</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/growth/index.html\">Growth of functions</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/adt/index.html\">Abstract data types</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/probabilistic/index.html\">Probabilistic Analysis</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/hash-tables/index.html\">Hash Tables</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/divide-conquer/index.html\">Divide and conquer</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/binary-search-trees/index.html\">Binary Search Trees</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/heaps/index.html\">Heapsort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"/ics311s14/modules/quicksort/index.html\">Quicksort</a></h2>\n\n    \n    <p>No assessments for this module.</p>\n    \n\n    \n  </div>\n</div>\n",
 "path"=>"assessments/index.md"}
</pre>

<h2>/debug.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Debug",
 "topdiv"=>"container",
 "url"=>"/debug.html",
 "content"=>
  "Debugging\n=========\n\nSite\n----\n\n{{ site | debug }}\n\nPages\n-----\n\n{% for page in site.pages %}\n{{ page.url }}\n--------------\n\n{{ page | debug }}\n{% endfor %}\n\n\n",
 "path"=>"debug.md"}
</pre>

<h2>/experiences/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Experiences",
 "url"=>"/experiences/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Experiential Learning <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_experiences %}\n      {% assign experience = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n             {{ experience.morea_summary | markdownify }}\n             <p>\n             {% for label in experience.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n       {% if forloop.index == 4 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 8 %}\n         </div><div class=\"row\">\n       {% endif %}\n      {% if forloop.index == 12 %}\n         </div><div class=\"row\">\n       {% endif %}\n       {% if forloop.index == 16 %}\n         </div><div class=\"row\">\n       {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"experiences/index.md"}
</pre>

<h2>/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Home",
 "topdiv"=>"container",
 "url"=>"/index.html",
 "content"=>
  "{% if site.morea_home_page %}\n  {{ site.morea_home_page.content | markdownify }}\n{% else %}\n  No home page content supplied.\n{% endif %}\n\n",
 "path"=>"index.md"}
</pre>

<h2>/modules/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Modules",
 "url"=>"/modules/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Modules <small>Topics covered in this class.</small></h1>\n  <div class=\"row\">\n     {% for module in site.morea_module_pages %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <img src=\"{{ site.baseurl }}{{ module.morea_icon_url }}\" width=\"100\" class=\"img-circle img-responsive\">\n            <div class=\"caption\">\n              <h3 style=\"text-align: center; margin-top: 0\">{{ forloop.index }}. {{ module.title }}</h3>\n              {{ module.content | markdownify }}\n              <p>\n              {% for label in module.morea_labels %}\n                <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n              </p>\n              {% if module.morea_coming_soon %}\n                <p class=\"text-center\"><a href=\"#\" class=\"btn btn-default\" role=\"button\">Coming soon...</a></p>\n              {% else %}\n                <p class=\"text-center\"><a href=\"{{ module.morea_id }}\" class=\"btn btn-primary\" role=\"button\">Learn more...</a></p>\n              {% endif %}\n            </div>\n          </div>\n        </div>\n       {% cycle '', '', '', '</div><div class=\"row\">' %}\n     {% endfor %}\n  </div>\n</div>\n\n\n",
 "path"=>"modules/index.md"}
</pre>

<h2>/outcomes/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Learning Outcomes",
 "url"=>"/outcomes/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Learning Outcomes</h1>\n</div>\n\n{% if site.morea_outcome_pages.size == 0 %}\n<p>No outcomes for this course.</p>\n{% endif %}\n\n\n{% for outcome in site.morea_outcome_pages %}\n\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Outcome:</small> {{ outcome.title }}</h2>\n    {{ outcome.content | markdownify }}\n    <p>\n    <em>Referencing modules:</em>\n    {% for module in outcome.referencing_modules %}\n      <a href=\"../modules/{{ module.morea_id }}\">{{ module.title }}</a>\n    {% endfor %}\n  </div>\n</div>\n\n{% endfor %}\n\n\n",
 "path"=>"outcomes/index.md"}
</pre>

<h2>/readings/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Readings",
 "url"=>"/readings/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1>Readings and other resources <small>in module order</small></h1>\n</div>\n\n{% for module in site.morea_module_pages %}\n{% if module.morea_coming_soon != true %}\n<div class=\"{% cycle 'light-gray-background', 'white-background' %}\">\n  <div class=\"container\">\n    <h2><small>Module:</small> <a href=\"{{ site.baseurl }}{{ module.module_page.url }}\">{{ module.title }}</a></h2>\n\n    {% if module.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n    {% for page_id in module.morea_readings %}\n      {% assign reading = site.morea_page_table[page_id] %}\n       <div class=\"col-sm-3\">\n         <div class=\"thumbnail\">\n           <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n             {{ reading.morea_summary | markdownify }}\n             <p>\n             {% for label in reading.morea_labels %}\n               <span class=\"badge\">{{ label }}</span>\n             {% endfor %}\n             </p>\n         </div>\n       </div>\n        {% if forloop.index == 4 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 8 %}\n          </div><div class=\"row\">\n        {% endif %}\n       {% if forloop.index == 12 %}\n          </div><div class=\"row\">\n        {% endif %}\n        {% if forloop.index == 16 %}\n          </div><div class=\"row\">\n        {% endif %}\n    {% endfor %}\n    </div>\n  </div>\n</div>\n{% endif %}\n{% endfor %}\n",
 "path"=>"readings/index.md"}
</pre>

<h2>/schedule/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "title"=>"Schedule",
 "url"=>"/schedule/index.html",
 "content"=>
  "<!-- Load FullCalendar for schedule page. -->\n<!-- Documentation available at: http://arshaw.com/fullcalendar/docs/google_calendar/ -->\n<link rel=\"stylesheet\" href=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.css\">\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/fullcalendar.min.js\"></script>\n<script src=\"http://cdnjs.cloudflare.com/ajax/libs/fullcalendar/1.6.4/gcal.js\"></script>\n<div class=\"container\">\n  <h1>Schedule</h1>\n  <div id='calendar'></div>\n</div>\n<script>\n  $(document).ready(function () {\n\n    // page is now ready, initialize the calendar. See documentation page above.\n    // In brief: make the calendar public, and paste in the XML feed.\n    $('#calendar').fullCalendar({\n      events: 'http://www.google.com/calendar/feeds/hawaii.edu_h008qigs793hp1llpbindcun50%40group.calendar.google.com/public/basic'\n    })\n  });\n</script>\n",
 "path"=>"schedule/index.html"}
</pre>

<h2>/morea/01.introduction/module-introduction.html</h2>

<pre>Hash
{"title"=>"Introduction",
 "published"=>true,
 "morea_id"=>"intro",
 "morea_outcomes"=>["outcome-algorithm", "outcome-311"],
 "morea_readings"=>
  ["reading-course-info",
   "reading-assessment",
   "reading-assignments",
   "reading-format",
   "reading-policies",
   "reading-topic-overview",
   "reading-algorithms",
   "reading-cormen-1"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/01.introduction/module-introduction.jpg",
 "morea_sort_order"=>1,
 "referencing_modules"=>[],
 "morea_experiences"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/01.introduction/module-introduction.html",
 "content"=>
  "An overview of ICS 311 policies and procedures and the material to be covered in this course.\n\n\n\n",
 "path"=>"morea//01.introduction/module-introduction.md"}
</pre>

<h2>/modules/intro/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-introduction.md",
 "title"=>"Introduction",
 "url"=>"/modules/intro/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/intro/index.html"}
</pre>

<h2>/morea/01.introduction/outcome-311.html</h2>

<pre>Hash
{"title"=>"Understand the procedures for ICS 311.",
 "published"=>true,
 "morea_id"=>"outcome-311",
 "morea_type"=>"outcome",
 "morea_sort_order"=>2,
 "referencing_modules"=>[#Jekyll:Page @name="module-introduction.md"],
 "url"=>"/morea/01.introduction/outcome-311.html",
 "content"=>
  "Understand the policies, course format, assignments, and assessment mechanisms for ICS 311.",
 "path"=>"morea//01.introduction/outcome-311.md"}
</pre>

<h2>/morea/01.introduction/outcome-algorithm.html</h2>

<pre>Hash
{"title"=>"Understand what algorithm analysis is.",
 "published"=>true,
 "morea_id"=>"outcome-algorithm",
 "morea_type"=>"outcome",
 "morea_sort_order"=>1,
 "referencing_modules"=>[#Jekyll:Page @name="module-introduction.md"],
 "url"=>"/morea/01.introduction/outcome-algorithm.html",
 "content"=>"Understand the formal and informal definitions of \"algorithm.\"",
 "path"=>"morea//01.introduction/outcome-algorithm.md"}
</pre>

<h2>/morea/01.introduction/reading-algorithms.html</h2>

<pre>Hash
{"title"=>"Chapter 1 Notes",
 "published"=>true,
 "morea_id"=>"reading-algorithms",
 "morea_summary"=>
  "Overview of algorithms and why we study them in this course.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-algorithms.html",
 "url"=>"/morea/01.introduction/reading-algorithms.html",
 "content"=>
  "## Algorithms and Programs\n\nBy now you have a working idea of what a \"program\" is because you have written\nmany. Programs are particular instructions that work on specific machines.\n\nIn this course we work with an abstraction of programs: algorithms.\n\n### Algorithms\n\nInformally, an algorithm is a well-defined computational procedure that takes\nsome value(s) as input and produces some value(s) as output.\n\nSomewhat more formally, an algorithm is a finite sequence of instructions\nchosen from a finite, fixed set of instructions, where the sequence of\ninstructions satisfies the following criteria:\n\n  * **Input:** It has zero or more input parameters. \n  * **Output:** It produces at least one output parameter. \n  * **Definiteness:** Each instruction must be clear and unambiguous. \n  * **Finiteness:** For every input, it executes only a finite number of instructions (it eventually halts). \n  * **Effectiveness:** Every instruction must be sufficiently basic so that a machine can execute the instruction.\n\n_Discussion:_ What is the difference between an algorithm and a program?\n\n_Discussion:_ What kinds of algorithms do you think are needed when you use\nyour smartphone (or any mobile phone for that matter?)\n\n* * *\n\n## Algorithm Design & Analysis\n\nHere is how algorithm design is situated within the phases of problem solving\nin software development:\n\nPhase 1: **Formulation of the Problem** (Requirements Specification)\n\nTo understand a real problem, **model it mathematically**, and specify\ninput and output of the problem clearly.\n\n  \nPhase 2: **Design and Analysis of an Algorithm** for the Problem (our focus)\n\n* Step 1: **Specification** of an Algorithm - **what is it?**\n* Step 2: **Verification** of the Algorithm - **is it correct?**\n* Step 3: **Analysis** of the Algorithms - **what is its time and space complexity?**\n\nPhase 3: **Implementation of the Algorithm**\n\nDesign data structures and realize the algorithm as executable code for\na targeted platform (lower level abstraction).\n\nPhase 4: **Performance Evaluation of the Implementation** (Testing)\n\nThe predicted performance of the algorithm can be evaluated/verified by\nempirical testing.\n\nAlgorithms are often specified in **pseudocode**, a mixture of programming\nlanguage control structures, mathematical notation, and natural language\nwritten to be as unambiguous as possible. Examples will be given in the next\nlecture. In this course, we will use mostly the notation used in the book.\n\n* * *\n\n###  Why not just test programs?\n\nWhy not just run experimental studies on programs? We can implement the\nalgorithms of interest, run them on a modern computer on various input sizes,\nand compare the results. Why bother with all this math in the book?\n\nI find the math painful too, but there are three major limitations to\nexperimental studies:\n\n  * To run experiments, you have to implement and run the algorithm. Implementation takes time, and some of these runs may take a long time.\n  * Experiments can only be done on a limited set of test inputs. Are you sure your results generalize to all possible inputs? \n  * It is difficult to compare the efficency of tests run on one hardware and software environment to what will happen on others. Are you sure that your results generalize across platforms? \n\nFormal analysis of algorithms:\n\n  * Can be performed on a high-level description of the algorithm without implementation.\n  * Takes into account all possible inputs. \n  * Allows comparisons of algorithms independently of hardware and software.\n\nSo, there is a good reason ICS 311 is THE central course of the ICS\ncurriculum! Stick with us.\n\n* * *\n\n\n## Computational Complexity\n\n### Input Size\n\nThe computational complexity of an algorithm generally depends on the amount\nof information given as input to the algorithm.\n\nThis amount can be formally defined as the **number of bits** needed to\nrepresent the input information with a reasonable, non-redundant coding\nscheme.\n\nTo simplify things, we often analyze algorithms in terms of larger constant-\nsized **data units** (e.g., signed integer, floating point number, string of\nbounded length, or data record).\n\nThese units are a _constant factor_ larger than a single bit, and are operated\non as a unit, so the result of the analysis is the same.\n\n### Measures of Complexity\n\nThe choice of algorithms and data structures has a critical impact on the\nfollowing, both of which are used as measures of computational complexity:\n\n  * Run **time** to solve a problem of a given input size\n  * Storage **space** for data, including auxiliary structures\n\n###  Example (preview of next lecture)\n\nFor example, suppose you have an input size of n elements, such as n strings\nto be sorted in lexicographic order. Suppose further that you have two\nalgorithms at your disposal (these algorithms will be examined in detail in\nthe next lecture):\n\n**Insertion sort**:\n    \n\n  1. start with an empty list \n  2. take each item to be sorted and insert it in its proper location \n  \n**Merge sort**:\n    \n\n  1. if the list has only one item, return it \n  2. otherwise, split the list in half, sort each half with this procedure, and then merge the results \n\nWe will see that given _n_ items to be sorted (it does not matter what they\nare as long as they are bounded by a constant size and can be compared by an <\noperator),\n\n  * _Insertion sort_ takes time proportional to _c_1_n_2 steps, where _c_1 is a constant depending on the implementation, and requires space proportional to _n_.\n  \n\n  * _Merge sort_ takes _c_2_n_ lg(_n_) steps, where _c_2 is another constant depending on the merge sort implementation, and requires space proportional to 2_n_. \n\n_Exercise:_ This would be a good place for you to pause and do excercise\n1.2-2, page 14:\n\nSuppose we are comparing implementations of insertion sort and merge sort on\nthe same machine, where c1=8 and c2=16. For which values of n does insertion\nsort beat merge sort?\n\nConstants matter for small input sizes, but since constants don't grow we\nignore them when concerned with the time complexity of large inputs: it is the\ngrowth in terms of _n_ that matters.\n\nIn the example above, ignoring the constants and factoring out the common n in\neach term shows that the difference in growth rate is _n_ versus lg(_n_). For\none million items to sort, this would be a time factor of one million for\ninsertion sort, but about 20 for merge sort.\n\n### Models of Computation\n\nRather than bother with determining the constant factors for any given\nimplementation or computer, algorithms for a problem are analyzed by using an\nabstract machine called a **model of computation**.\n\nMany models of computation have been proposed, but they are essentially\nequivalent to each other (Church-Turing Thesis) as long as computation\nexecuted on them are _deterministic_ and _sequential_. Commonly used models\nare _Turing Machines_ and _Random Access Machines_ (see Section 2.2 of the\ntextbook).\n\n### Run Times for Different Complexities\n\nIn general, suppose that you have a computer of speed 107 steps per second.\nThe running time of algorithms of the given complexity (rows) as a function of\n_n_ would be:\n\n    \n    \n      --------------------------------------------------------------------\n      size n    10       20       30       50      100       1000    10000\n      --------------------------------------------------------------------\n      n         0.001ms  0.002ms  0.003ms  0.005ms  0.01ms   0.1ms     1ms\n      n lg n    0.003ms  0.008ms  0.015ms  0.03ms   0.07ms   1ms      13ms\n      n^2       0.01ms   0.04ms   0.09ms   0.25ms   1ms      100ms    10s\n      n^3       0.1ms    0.8ms    2.7ms    12.5ms   100ms    100s     28h\n    \n      ...................................................................\n    \n      2^n       0.1ms    0.1s     100s     3yr      3x10^13c  inf     inf\n      --------------------------------------------------------------------\n    \n\n_Discussion:_ What is the difference between analysis of an algorithm and\nanalysis of an implementation (a program)?\n\n_Discussion:_ What is the relationship between the efficiency of an algorithm\nand the difficulty of the problem to be solved by that algorithm? (We return\nto this in the last two topics of the semester, but see also below.)\n\nConsider the example above: the problem of sorting a list of items. We saw two\nalgorithms for solving the problem, one more efficient than the other. Is it\npossible to make a statement about the time efficiency of _any possible_\nalgorithm for the problem of sorting? (We address this question in Topic #10.)\n\n### Easy vs Hard Problems\n\nTheoretical computer science has made substantial progress on understanding\nthe intrinsic difficulty of _problems_ (across all possible algorithms),\nalthough there are still significant open questions (one in particular).\n\nFirst of all, there are problems that we cannot solve, i.e., problems for\nwhich there does not exist any algorithm. Those problems are called\n**unsolvable** (or **undecidable** or **incomputable**), and include the\n_Halting Problem_ (refer to Section 3.1 pp. 176-177 of the textbook for ICS141\n& 241).\n\nWithin the problems that can be solved, there is a hierarchy of **complexity\nclasses** according to how difficult they are. Difficulty is based on proofs\nof the minimum complexity of _any_ algorithm that solves the problem, and on\nproofs of equivalences between problems (translating one into another). Here\nis a graphic:\n\n![](Complexity-Hierarchy.jpg)\n\n(Although algorithms can be ranked by this hierarchy, the above figure refers\nto problem classes, not algorithms.)\n\nSometimes small differences in a problem specification can make a big\ndifference in complexity.\n\nFor example, suppose you use a graph of vertices representing cities and\nweighted edges between the vertices representing the distance via the best\nroad traveling directly between the cities.\n\n  * The **Single Pair Shortest Paths** problem: what is the shortest path between a single pair of vertices (from one start vertex to one destination vertex) in the weighted graph? \n  * The **Shortest Paths** problem: what is the shortest path from one vertex to all of the other vertices in the weighted graph? \n  * The **All Pairs Shortest Paths** problem: what is the shortest path between every pair of vertices in the weighted graph? \n  * The **Traveling Salesman** problem: what is the shortest path that starts at given vertex in a weighted graph and visits all (or a specified set of) other vertices once before returning to the start vertex?\n\n_Discussion:_ How do these problems differ from each other? Which are easier\nand which are harder? Which are tractable (e.g., can be computed in polynomial\ntime) and which are potentially intractable (e.g, require exponential time)?\n\nComplexity theory will be the topic of our second to last lecture.\n\n* * *\n\n##  Abstract Data Types\n\nAlgorithms and data structures go together. We often study algorithms in the\ncontext of Abstract Data Types (ADTs). But let's start with Data Structures.\n\n### Data Structures\n\nYou are already familiar with Data Structures. They are defined by:\n\n  * **Operations:** Specifications of external appearance of a data structure. \n  * **Storage Structures:** Organizations of data implemented in lower level data structures. (We are almost always building abstractions on layers of abstractions above the actual physical implementation.) \n  * **Algorithms:** Description of how to manipulate information in the storage structures to obtain the results defined for the operations \n\nThe definition of a data structure requires that you specify implementation\ndetails such as storage structures and algorithms. It would be better to hide\nthese details until we are ready to deal with them.\n\nAlso we may want to write a specification in terms of desired behavior\n(Operations) and then compare alternative storage structures and algorithms as\npossible solutions meeting those specifications. Data structures don't work\nbecause they already assume a given solution. Abstract Data Types or ADTs let\nus do this by abstracting the representations and algorithms.\n\n### Definition of Abstract Data Types (ADTs)\n\nAn ADT is a class of instances (i.e., data objects) with a set of the\noperations that can be applied to the data objects.\n\nAn ADT tells us _what to do_ instead of how to do it. This provides the\nspecifications againsts which we can design different algorithms: the _how_\npart.\n\nAn ADT is specified by\n\n  1. **the type(s) of data objects involved **\n  2. **a set of operations that can be applied to those objects**, and \n  3. **a set of properties (called axioms) that all the objects and operations must satisfy**.\n\n### Example: Stack ADT\n\nObjects:\n\n    Stack, and Elements (of arbitrary type) \n  \nOperations (categorized into three types):\n\n\nConstructor\n\n    `new()` creates the empty stack and returns it.\n\nAccessors\n\n     `empty(_s_)` returns whether stack _s_ is empty.  \n`top(_s_)` returns the element of stack _s_ that has been inserted into s\nlast.\n\nMutators (or Modifiers)\n\n     `push(_s_,_e_)` inserts an element _e_ into _s_.  \n`pop(_s_)` deletes the top element from stack _s_.\n\n  \nProperties:\n\n  * `top(push(_s_,_e_)) returns value _e_`\n  * `pop(push(_s_,_e_)) leaves _s_` in the same state \n  * `empty(new()) = true `\n  * `empty(push(_s_,_i_)) = false `\n  * `pop(new()) is an error `\n  * `top(new()) is an error `\n\n### Specification and Implementation\n\nADTs can be specified in different languages:\n\n  * formal languages (axiomatic, algebraic, functional, denotational semantics, etc.)\n  * natural language\n\nImplementation of an ADT requires\n\n  * defining the storage for the data structures\n  * implementing the algorithms for the operations\n\n### Advantages of ADTs\n\nModularity (Encapsulation)\n\nAbstract operations mean a program using an ADT are isolated from (need not know about or be affected by) the implementation of the ADT.\n\n  * Implementation of ADT can be changed without modifying programs using ADT.\n  * Makes a program smaller, simpler, and have less side effects   \n  * Helps to construct correct programs \n  \nHierarchical Specification\n\n     Supports Top-Down Design and Stepwise Refinement\n  \nImplementation\n\n    ADTs map well to Object-Oriented Programming Languages\n\n_Discussion:_ What is the difference between an ADT and a data structure?\n\n* * *\n\n_Some of the material in this page was adopted with permission (and\nsignificant editing) from Kazuo Sugihara's spring 2011 Lecture Notes #02._\n\n",
 "path"=>"morea//01.introduction/reading-algorithms.md"}
</pre>

<h2>/morea/01.introduction/reading-assessment.html</h2>

<pre>Hash
{"title"=>"Assessment",
 "published"=>true,
 "morea_id"=>"reading-assessment",
 "morea_summary"=>"Grading in ICS 311",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-assessment.html",
 "url"=>"/morea/01.introduction/reading-assessment.html",
 "content"=>
  "## Overview\n\nThe approach to assignments, exams, and relative weighting is intended to\nassess multiple aspects of your developing expertise in algorithm design,\nanalysis and implementation; while providing a little flexibility to recognize\nindividual strengths. In summary, the components and their default weights\n(percentages and points, where 10 points = 1% of your grade) include:\n\n  * **Quizzes** (120 points = 12%, 24 quizzes of 5 points each),\n  * **Problem Sets** (305 points = 30.5%, 8 sets of 30 points, 3 sets of 15 points, 1 set of 20 points; of which 105 is _in-class_ work and 200 _homework_)\n  * **Projects** (260 points = 26%, three at 60, 100, and 100 points, respectively)\n  * **Midterm Exams** (240 points = 24%, three at 80 points each)\n  * **Final Exam** (100 points = 10% of grade). \n\nThis sums to 1025 points: you can miss 25 points of work and still earn 1000\npoints.\n\nExtra credit (Variable), may be earned by peer assessment of group\nparticipation in class, by additional work that will be specified in some\nhomework problems and implementation assignments, and by having others use\nyour software in the third assigment. (There will be no separate extra credit\nproblems or projects, as this creates more grading work than we have TA time.)\n\n## Points, Percents and Letter Grades\n\nA \"point\" will be worth 0.1% of your grade. For example, an item worth 100\npoints is 10% of your grade, and a perfect score is 1000 points. At the end of\nthe semester, I add up all your points and divide by 10 to get your percentage\nof points earned, capping it at 100%.\n\nTo determine letter grades, I use a 4-percent spread per grade increment,\ni.e., 100-97=A+, 96-93=A, 92-89=A-, 88-85=B+, 84-81=B, 80-77=B-, 76-73=C+,\n72-69=C, 68-65=C-, 64-61=D+, 60-57=D, 56-53=D-, 52-0=F. I will set it up this\nway in Laulima. If upon inspection of the distribution of grades I feel that\ntoo many students who understand the material are not getting the grades they\ndeserve, I may then make adjustments in favor of students ... but don't rely\non this!\n\n## Components\n\n**Quizzes (12%, 120 points):**\n\nBefore the classes that have lecture material, a brief quiz will be due online. These quizzes will test basic understanding of the chapter on which the day's topic is based, such as whether you can simulate the operation of the data structure or algorithms or get the main point of the analyses of their relative merits. Most quizzes will not involve mathematical analysis or proofs: problems requiring deeper thought will be left for the classwork and homework problems. Quizzes will be given in Laulima, and will be automatically graded. Solutions will be given in class immediately after the quizzes are due, so _quizzes cannot be made up_.\n  \nI am expecting 24 quizzes worth 5 points each. There are 25 topics. We don't\ninclude Topic 1 or the two special topics, which leaves 22 topics with\nquizzes, but Topic 2 and Topic 14 have two quizzes.\n\n  \n**Problems (30.5%, 305 points):**\n\nThese will relatively frequent assignments intended to help assess how well you understand the algorithms and analytic concepts being presented. They will require more thought than the problems given on the quizzes. They will come in two parts: an in-class portion done, turned in, and graded in groups, and an after-class portion turned in individually the following Monday. I attempt to choose in-class problems that help expose conceptual issues in the material and prepare you to work on the take-home portion on your own. Thus, by working in a group in class you help each other understand the problem that you will then solve and turn in individually. See also \"Peer Evaluation of Participation\" under Extra Credit below for how everyone can earn extra credit through participating in the group sessions.\n  \nPoints are allocated as follows. Most topics are allocated 15 points, of which\n5 are allocated to the group work turned in at the end of class, and 10 to the\nindividual work turned in later. In 8 of the weeks, the problem set will\nconsist of two topics, for 30 points each week (10 being groupwork turned in\nover the two days and 20 turned in by the individual). In 3 of the weeks,\nthere is one topic, for 15 points. When we cover Topic 14 on Graphs, there\nwill be two in-class problems and one homework for 20 points. The TA will\ngrade problem sets. We aspire to a one week or less turn-around. If you have\nquestions about solutions after they are due or graded, post them in Laulima\nand I will discuss solutions in class after the exercises are due.\n\n  \n**Projects (Analysis, Implementation, Testing) (26%, 260 points):**\n\nThere will be three projects. These typically involve writing Java implementations of abstract data types and associated algorithms, and testing these on sample data. You will also provide instructions on how to compile and run the program, document your design and implementation (including complexith analysis), and present and discuss test results. The assignments will progressively give you more responsibility. For the first project, you will be told what to implement, and it will be weighted 6% (60 points). For the second assignment you will need to make some implementation choices. The second assignment will be weighted 10% (100 points). The third assignment will require some research and decision making on your part to solve the problem. It will be weighted 10% (100 points). The TA will grade the first two projects, and both the TA and the instructor will grade the third.\n  \n**Midterms (24%, 240 points):**\n\nThere will be three midterm exams taking one class period each. They will include problems similar to those on the quizzes (for the easiest problems), and class and homework problems (for the harder ones), covering both understanding of the algorithms and how to analyze them. They will cover the most recent set of lecture topics, but cumulative \"review\" questions may also be included. Exams are open-book, open-notes on paper, but no electronic devices allowed. Each midterm is 80 points; there are three for 240 points or 24%. The instructor will grade all midterm exams.\n  \n**Final (10%, 100 points):**\n\nThe final exam will take place at the time scheduled by the university and will be longer than a midterm exam. It will cover the final set of lecture topics, but also include review of the entire semester. Since the final is longer and is cumulative as well as covering recent material, it is weighted more (100 points). Grading may be shared between TA and instructor.\n\n## Extra Credit (Variable Points)\n\nYou may earn extra credit several ways. _The extra credit points will be\nrecorded in a separate field in Laulima and allocated where they are needed at\nthe end of the semester. Thus they do not appear in the grade estimate\ncalculated by Laulima during the semester._\n\n**Peer Evaluation of Participation**\n\nEach week in which there is a problem set, each individual in the group may assign points distributed across the other individuals in the group to assess how effectively they collaborated in the group. You should allocate the points according to how well the others worked as team members, including their role in team functioning (e.g., keeping the group focused and organized, or playing another important role), and how much they helped others understand the material (e.g., by explaining what they understood), as well as their contributions to the actual problem solution.\n  \nTo ensure that students in smaller groups are not penalized, we use this\nscheme, which distributes 12 points across each group:  \n\n  * If _three members_ besides yourself were present at some time, you have a total of _3 points_ to allocate across all members (NOT 3 points per member!).\n  * If only _two members_ besides yourself were present, you have a total of _4 points_ to allocate across all members.\n  * If only _one other member_ was present, you have a total of _6 points_ to allocate to that person. \n  * You need not allocate all the points available to you.\n  * _You cannot allocate these points to yourself!_ Points allocated to yourself will not be recorded.\n  * You will allocate these points when you turn in your assignment. To encourage you to do this, _you will be given one extra credit point for each assignment in which you assess your peers_.\n  \nSome example scenarios: if everyone else contributed equally, you might give 1\npoint to each person. If one person in the group was taking the majority of\nthe initiative in a helpful way and the other two were not so engaged, you\nmight assign the helpful person all three points If there was one person who\ndid slightly more, one who helped some, and one slacker, you might allocate 2,\n1, and 0 points. Or if you had to do everything yourself you can allocate 0\npoints to everyone.  \n  \nObviously, a person who is helpful in the groups can earn extra credit this\nway. The maximum possible (very unlikely) is that they get all the points from\neveryone in every group they participate in: 9*12=108, or about a 10% grade\nincrease. If everyone gave their three points equally to all group members,\nthe result would be 3*12=36 points, or about a 3.5% grade increase. So, just\nby participating in class you get a little boost to your grade.\n\n  \n**Classwork, Homework and Project Add-ons**\n\nExtra credit problems will be included in some class sessions, homework problem sets, and projects. For example, an extra problem may be provided in class for those groups who finish early; a more difficult problem may be included in a homework set; or a student may program more challenging graph manipulations for extra credit in the programming projects. Points will be specified when they are given.\n\n**Software Reuse**\n\nIn the third assignment, students will have the option of using other student's implementations of ADTs from prior assignments. If your software is chosen by another student, you will be awarded 5 points for each \"customer\" _in proportion to their grade and use of your software_. For example, if a student uses your Graph ADT from Project 2 in their Project 3 and gets 80%, you will get 4 points. Partial credit is given if only parts of your code are used. The \"customer\" student must acknowledge your code in their project documentation (Readme and Reference manual).\n\n## Flexibility\n\nWe each have our own strengths. If a student performs _significantly_ better\non one area than others, I may elect to put greater weight on the area that\ngives the student a better grade. I am more willing to do this with strong\nexam performance, as exams are not easy and are proctored so I know it's the\nstudent's work. However, excellent programming may be considered as well.\n\n",
 "path"=>"morea//01.introduction/reading-assessment.md"}
</pre>

<h2>/morea/01.introduction/reading-assignments.html</h2>

<pre>Hash
{"title"=>"Assignments",
 "published"=>true,
 "morea_id"=>"reading-assignments",
 "morea_summary"=>"Requirements for programming assignments.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-assignments.html",
 "url"=>"/morea/01.introduction/reading-assignments.html",
 "content"=>
  "Updated March 8, 2014 to discuss the use of GPL licenses.\n\nThis page discusses general procedures for implementation assignments and\nextra credit projects. See the individual assignment pages for details.\n\n## Implementation Assignments\n\n### Overview\n\nThere will be three implementation assignments. These will involve writing\nJava implementations of abstract data types and associated algorithms, and\ntesting the implementations on sample data. You will also provide instructions\non how to compile and run the program, document your design and implementation\n(including complexith analysis), and present and discuss test results. The\nassignments will progressively give you more responsibility. For the first\nproject, you will be told what to implement, and it will be weighted 6% (60\npoints). For the second assignment you will need to make some implementation\nchoices. The second assignment will be weighted 10% (100 points). The third\nassignment will require some research and decision making on your part to\nsolve the problem. It will be weighted 10% (100 points).\n\n### Software Requirements\n\nThe following requirements have been adopted from Dr. Sugihara:\n\n#### Programming Language\n\nAll software must be written in Java. Other programming languages may not be\nused except where specified by the assignment.\n\n**The software must be compilable on the default version of Java available on uhunix.hawaii.edu** at the time of the submission deadline. The reason for this is to ensure that there is a common reference environment against which we can resolve disputes. We can't grade projects on claims that \"it compiled on my machine\".\n\nUhunix is running [Solaris](http://www.oracle.com/technetwork/server-\nstorage/solaris/overview/index.html). At this writing, the Java version is:\n\n    \n    \n    java version \"1.7.0_51\"\n    Java(TM) SE Runtime Environment (build 1.7.0_51-b13)\n    Java HotSpot(TM) Server VM (build 24.51-b03, mixed mode)\n    \n\nProjects submitted with higher versions of Java (if they become available) are\nat your risk. Note that the instructor is presently running research projects\nin Java 1.6 in Snow Leopard on which Java 1.7 is not available. Some features\nof Java 1.7 do not compile in 1.6, so if you could stick to 1.6 features he\nwon't have to go to UH Unix to run your code.\n\n#### Program Type and Interface\n\nThe software should run as a Java application.\n\n  * Command line (console) applications are acceptable.\n  * GUI (graphical user interface versions) are also permissible.\n\n#### Source Code Requirements\n\nThe student shall **submit .java source files** (not class or jar files),\norganized in folders as required for your package structure, along with\ninstructions for compiling the program and other documentation discussed in\nthe next section.\n\n  * The Teaching Assistant will verify that the programs compile on the current version of Java on uhunix, as specified above.\n  * Submissions will only be evaluated for credit if they successfully compile.\n  * This procedure is intended to (1) enable us to verify that the software is based on the student's own source code and (2) provide an objective basis for evaluating whether code works..\n\nSource files should include the BSD License Header based on [ this\ntemplate,](../Resources/bsd_license_header.txt) with \"<year>\", \"<copyright\nholder>\" and \"<organization>\" replaced appropriately.\n\nOther open source licenses (e.g., [Apache](http://www.apache.org/licenses/) or\n[GNU](http://www.gnu.org/licenses/)) may be used with prior permission from\nthe instructor if the student has a specific reason for doing so and\nunderstands the consequences. See the discussion under Other Licenses two\nsections below.\n\nSource should include appropriate in-line comments documenting the software\ndesign. Comments should not document the obvious (e.g., \"this line adds 1 to\nthe index variable\"), but rather document functional intent and constraints\nsuch as loop invariants, explain something that would otherwise be obscure,\nmark places that need improvement, etc. Descriptive use of variable and method\nnames also constitutes internal documentation. See next section for external\ndocumentation requirements.\n\n#### Including Open Source Software\n\nEach assignment will specify where you are allowed to reuse source code of\nopen source software developed by other authors, and where you must write your\nown code for the assignment. Where allowed by the assignment, reuse of open\nsource code is allowed if the following conditions are also met:\n\n  * The software license of the reused open source code is compatible with the [ BSD license](http://www.opensource.org/licenses/bsd-license.php) (see discussion under Other Licenses below).\n  * The license header of reused source code is also inserted into the source files containing the reused source code.\n  * The `Readme.txt ` of your product clearly gives credit to the authors of the reused source code (i.e., including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\nWhen source code of a module is reused, add the name(s) of its original\nauthor(s) to an `@author` tag at the beginning of the reused module. If you\nmodified the source code for more than bug fixes, add your name as an author\nof a derivative from the original source code:\n\n    /**\n     *\n     * @author     Original Author     James Brown\n     * @author     Derivative Author   Robert Smith\n     *\n     */\n\n#### Other Licenses\n\nI am often asked whether one can use code under another license. You may use\nother open source licenses as long as (1) they give you the right to use the\ncode under conditions acceptable to you and (2) you document this as needed.\nAn example is the GPL license. You may use a [GPL compatible\nlicense](https://www.gnu.org/licenses/license-list.html), but you use this\nlicense, all the code you write _must_ also be under GPL. See\n<http://en.wikipedia.org/wiki/Free_software_license> for an overview, and read\nsome blogs about this hotly debated issue.\n\n### Documentation Requirements\n\nSoftware will be submitted with appropriate documentation, including the\nfollowing. (Think of your audience for this documentation as any potential\nusers, including your classmates as well as the TA and instructor.)\n\n**Readme.txt (plaintext file)**\n\nCritical information that a user should know about your product first, including:\n\n  * step-wise instructions for the user to reconstruct an application from your source code (including the version of JDK used),\n  * credits (acknowledgments to authors of open source reused at least in part for this product) including information such as the name of an author, the name of a reused product and a list of source file names of the reused product,\n  * revision history (a log of changes on the product), \n  * bug report (description of known bugs), and \n  * a listing of other documentation available (below).\n\n**Operation Manual (plaintext or PDF)**\n\nConcise, yet sufficient step-wise explanation about how to start and interact with the program, written for an end user who is concerned with using the program in an application domain (not with the code).\n\n**Reference Manuals (plaintext or PDF, and Javadoc HTML)**\n\nRequirements and design specifications; organization of modules; algorithms and data structures used; functionality of each class or method; etc. A reference manual is written for experienced users and/or programmers who perform various maintenance activities for correction, enhancement, adaptation, etc. Javadoc pages may also be included, and should be included if you intend to have others use your code.\n\n**Testing Document (plaintext or PDF)**\n\nTest plan describing objective(s) of testing, method(s) used for testing, assumption(s) of testing, and hardware/software environment in testing; test case specification describing classification of test cases; test data and I/O of test runs; and whatever else is useful to convince other people about the correctness and good features of your program. **For ICS 311 assignments the testing document will include your conclusions related to the purpose of the assignment.**\n\n### Submission Requirements\n\n  1. Place the files and folders required (as discussed above under Software and Documentation Requirements) in a folder titled using the scheme Last-First-A#, for example, Suthers-Dan-A1 for assignment 1, Suthers-Dan-A2 for assignment 2, etc. Extra credit projects should be submitted with extentions -E1, etc.\n  2. Zip (.zip) or gzip (.gz) this folder using commands by those names on uhunix, or appropriate equivalents on your platform.\n  3. It is suggested that you test unzipping, compiling and running the software per the instructions you gave before submitting the assignment.\n  4. Upload the zip file to the Laulima area for the given assignment.\n  5. You should receive email confirmation of your submission at the address registered in Laulima.\n  6. Unlimited resubmissions are allowed up to the assignment deadline. Extra credit for early submission, if any, will be based on the date of the last submission, not the first!\n\n### Evaluation Criteria for Implementation Assignments\n\n### Warning: these will be revised for spring 2014\n\nThese are our default grading criteria. Some adjustments may be made when we\nsee where the greatest effort is required.\n\nProgram: 60%\n\n    \n\n  * 50% if all operations and all ADTs are implemented.\n  * 5% for following instructions for input and output (although many more points will be deducted if the interface is so bad we can't verify that the operations and ADTs are implemented). \n  * 5% for adequate error handling. \nAnalysis and Documentation 40%\n\n    \n\n  * 30% for adequate analysis of the results\n  * 10% for Readme, Operation, Reference manuals\n\n## Use of Software by Other Students\n\nIf others elect to use your software in a subsequent assignment (e.g., using\nyour graph ADT implementation in the third assignment), we will give extra\ncredit. See discussion in [Assessement](Assessment.html) page. Use should be\ncredited in the Readme and Reference Manual.\n\n",
 "path"=>"morea//01.introduction/reading-assignments.md"}
</pre>

<h2>/morea/01.introduction/reading-cormen-1.html</h2>

<pre>Hash
{"title"=>"CLRS 1 - Role of algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-1",
 "morea_summary"=>"The role of algorithms in computing",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/01.introduction/reading-cormen-1.html",
 "content"=>"",
 "path"=>"morea//01.introduction/reading-cormen-1.md"}
</pre>

<h2>/morea/01.introduction/reading-course-info.html</h2>

<pre>Hash
{"title"=>"Course information",
 "published"=>true,
 "morea_id"=>"reading-course-info",
 "morea_summary"=>
  "Student learning outcomes, textbook, instructor information.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-course-info.html",
 "url"=>"/morea/01.introduction/reading-course-info.html",
 "content"=>
  "## Catalog Description\n\nICS 311 Algorithms (3 credits) Design and correctness of algorithms, including\ndivide-and-conquer, greedy and dynamic programming methods. Complexity\nanalyses using recurrence relations, probabilistic methods, and NP-\ncompleteness. Applications to order statistics, disjoint sets, B-trees and\nbalanced trees, graphs, network flows, and string matching. Pre: 211 and 241,\nor consent.\n\n## SLOs (Student Learning Outcomes)\n\n  * Students are aware of fundamental algorithms of computer science, and their associated data structures and problem solving techniques. \n  * Students can compose a problem formulation of a real-world problem mathematically.\n  * Students can decide whether given pseudocode is correct for a given problem formulation; construct a counterexample if the given pseudocode is incorrect; and outline a proof for its correctness otherwise.\n  * Students can design a correct algorithm for a given problem and describe the algorithm as pseudocode in a given pseudocode syntax.\n  * Students can analyze the worst-case and best-case space and time complexities of a given algorithm.\n  * Students can create a software program for accurately implementing an algorithm specified in pseudocode. Students can implement software objects meeting Abstract Data Type specifications.\n  * Students can produce a software product including documentation for given requirements & design specifications.\n\n_Comment:_ On the fall 2013 final exam one student wrote about a problem:\n\n> \"This question is too hard! We shouldn't have to know implementations we have not used before!\"\n\nI wrote back to thank the student for concisely expressing (the negation of)\nexactly what this course _ is_ intended to teach! You may not \"know\" an\nimplementation you have not encountered before, but this course should prepare\nyou with the tools to analyze and make informed decisions about new\nimplementations.\n\nDo not approach this course solely as a memorization task, where you can only\ndo algorithms you are trained to do, like a circus animal. We do want you to\nlearn a \"catalog\" of algorithms, but you should be understanding their\nanalyses as examples that enable you to analyze unexpected algorithms in the\nfuture. This is essential for being successful in a fast changing field where\n_you_ are expected to figure out whether a new idea will work, as _ you _ are\nthe computer scientist hired to do this.\n\n## Textbook\n\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein,\n[Introduction to Algorithms, Third\nEdition,](http://mitpress.mit.edu/algorithms/) The MIT Press, 2009.\n\nStudents are advised to purchase the textbook, as this book will serve as a\nlifelong reference (it is the second most cited publication in computer\nscience). Also, the exams will be open book but with no electronics permitted,\nso the PDF version of the book won't help you there.\n\nStudents are also advised to keep their ICS 241 (Discrete Mathematics for\nComputer Science) textbooks for reference.\n\n## Instructor\n\n[Daniel D. Suthers](http://www2.hawaii.edu/~suthers/)  \nProfessor of ICS\n\n  * Office: POST 309B\n  * Office Telephone: (808) 956-3890\n  * Email: [suthers@hawaii.edu](mailto:suthers@hawaii.edu) (Put \"ICS 311\" in the subject line. Use Laulima for questions that may also apply to other students.)\n  * Office Hours: Mondays 3:00-4:00, or by appointment. (It's best to let me know in advance if you plan to come to office hour. Please don't try to talk to me while I am setting up for class. Brief consultations at the end of class are OK unless I indicate that I need to leave.\n\n## Teaching Assistant\n\nRobert Ward  \nM.S./Ph.D. Student in ICS\n\n  * Office: POST 303 cubicle\n  * Email: [rward@hawaii.edu](mailto:rward@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Office Hours: TuTh 1-2:30 or by appointment.\n\n## Assistant Teaching Assistant\n\nAnthony Sarria  \nICS Undergrad and ICS 311 Survivor\n\n  * Lab: POST 318A\n  * Email: [aksarria@hawaii.edu](mailto:aksarria@hawaii.edu) (Put \"ICS 311\" in the subject line.)\n  * Lab Hours: Mondays 3:15-6:15.\n\n## Communications\n\n**Questions about Course Content**:   In general, questions about course content such as concepts, clarifications of assignments, etc. should be posted to the Laulima discussion forum of the week. This is because (1) other students can see our responses there, and thus also benefit; and (2) other students may notice the question and answer before the instructor or TA notices it (though we will check daily). If you email us a question, we will post the reply in Laulima unless personal information is involved.\n\n**Personal Topics**: For topics that are not of interest to other students or are personal, you may email us, or stop by office hours. (Of course you may also use office hours for course content questions.) If using email, put \"ICS 311\" in the subject line.\n\n**Communication with other students (e.g., group members)**: You can send email to other students in the course using the Laulima \"Mailtool\". You don't need to know their real email address to do this.\n\n##  Online Media\n\nWe use the **course website** [www2.hawaii.edu/~suthers/courses/ics311s14/](ww\nw2.hawaii.edu/~suthers/courses/ics311s14/) for posting schedules and notes.\n\nWe use **[Laulima**](https://laulima.hawaii.edu/portal) for all other online\nrequired course functions such as podcasts, quizzes, discussions and\nsubmitting assignments. Please see [this document on everything Laulima users\nshould\nknow](http://www.hawaii.edu/talent/laulima_tab/tabs/laulima_essentials.html).\n\nWe will use **Google Docs** for in-class problem solving, as it supports\nsimultaneous editing.\n\nScreencasts (videos) of lectures are available on **YouTube** and **iTunesU**\nas well as Laulima (your choice). They are linked from the individual Notes\npages (Topic-XX.html).\n\n",
 "path"=>"morea//01.introduction/reading-course-info.md"}
</pre>

<h2>/morea/01.introduction/reading-format.html</h2>

<pre>Hash
{"title"=>"Format",
 "published"=>true,
 "morea_id"=>"reading-format",
 "morea_summary"=>"Exam cycles, weekly routine, studying, and group work.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-format.html",
 "url"=>"/morea/01.introduction/reading-format.html",
 "content"=>
  "By \"routines\" I mean our periodic patterns of activity: what happens in a\ntypical class, a typical week, and a typical exam and assignment cycle. This\npage tells you what I will do and what you will be expected to do on a\nrecurring basis.\n\n## Exam Cycles\n\nThere are three midterm exams covering the core material. The first two are\ngiven one full week after the last problem set on the exam topics was due, so\nyou have time to get feedback on homework problems. Due to the university\ncutoff for midterm exams, the third is held the week after the relevant topics\nare covered. The first exam is at least half review of 211 and 241: results\nare returned before the withdrawal date so you can assess whether you are\nready for ICS 311.\n\n## Weekly Routine\n\nOn most weeks we cover one book chapter/topic in each of the two classes (two\nchapters per week). The exceptions are the first two weeks when we are getting\nour bearings and covering material that must be understood to comprehend the\nrest of the semester; the weeks we have midterms; and the chapter that\nintroduces graphs (we take a full week for it).\n\nThe pace is intense: set aside time almost every day for ICS 311. The basic\npattern is as follows:\n\n**Saturdays:**\n\n  * Instructor posts any updates to screencasts and notes for the material of the following week by Saturday night, and possibly earlier.\n\n**Sundays:**\n\n  * Projects will be due midnight* on a Sunday night.\n\n**Mondays:**\n\n  * Problem sets from the previous week are due midnight* the Monday after they are given.\n  * Read or review material to understand Tuesday's topic (your choice of my podcasts, my web notes, the CLRS textbook chapter, other readings offered, and/or the MIT lecture videos).\n  * Post any questions you have about the material in the appropriate Laulima discussion forum (instructor will review these after midnight when preparing for Tuesday's class). \n\n**Tuesdays:**\n\n  * The online quiz for Tuesday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n\n**Wednesdays:**\n\n  * Read or review material to understand Thursday's topic.\n  * Post any questions you have about the material in the appropriate Laulima discussion forum.\n\n**Thursdays:**\n\n  * The online quiz for Thursday's chapter is due 30 minutes before class (by 2:30pm).\n  * See \"Class Routine\" below.\n  * Problem sets for the weekend are usually posted Thursday evening.\n\n**Fridays through the weekend:**\n\n  * Use to do problem sets and/or projects in preparation for Sunday deadlines, and to watch screencasts and read book chapters. \n\n  * \"Midnight\": The deadline in Laulima will be set to 23:55 (11:55 pm) on the due date, to avoid any ambiguity of which day \"00:00\" refers to. Upload your solutions before midnight. There will be a 5 minute grace period just in case of network delays. The three projects can be submitted late for 1% off per hour up to 50% off, but the homework assignments cannot be late.\n\nThe Assistant Teaching Assistant will have lab hours on Mondays. This will be\nyour last chance for clarifications before assignments are due. Instructor\noffice hour will be Wednesdays (with some availability Monday by appointment).\nPlan in advance for programming projects due Sundays.\n\n## Class Routine\n\nThe focus of our 75 minute class will be student problem solving in groups,\nwith opportunites to get help. For at least the first part of the semester,\nthe groups will be formed anew _randomly_ each week. Each day you will solve a\nseries of conceptual problems and turn them in as a group for a group grade.\nThese problems prepare you to take on more substantial problems that you turn\nin individually on Sunday. At that time you will also allocate points to group\nmembers. (See [ Assessment](Assessment.html) for explanations of grading.)\n\nHere are typical schedules for 75 minute classes: Adjustments to the class\nroutine will likely be made to meet current needs.\n\n**Tuesdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Icebreaker for new groups (groups rotate weekly)\n  * 5: Solution to previous weekend's homework (if applicable) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n\n**Thursdays:**\n\n  * 5: Welcome and Administrative Comments\n  * 5: Discussion of Tuesday's problem (if needed) \n  * 5: Solution to today's quiz. \n  * 5: Introduction to today's problem (including questions submitted in advance) \n  * 45 (or more): Problem solving session, due before end of class.\n  * 5: Brief presentation of class problem solutions \n  * 5: Discussion of homework problems \n\nPlan to **bring your laptop or tablet to classes** held in Webster 101 (but\nnot to exams in BusAd A101). You'll need a VGA connector if you want to be\nable to project your laptop to your working group. Apple iPads can connect via\nApple TV. I'm not aware of another method. Of course, groups can function with\nonly one or two members having a projectable laptop, but it's better for you\nif you can be an active participant. At least one person in each group should\nhave the textbook handy in class as well.\n\n##  Other Comments\n\n### Inverted Classroom\n\nThis class is \"inverted\" in the sense that lectures are recorded and made\navailable outside of class, and classroom time is used for what can only be\ndone in person: collaboration and helping each other.\n\nLectures have their advantages, but they have problems too. For most students\nlistening to lectures is too passive an activity. The temptation to daydream\nor check Facebook may be too great, and it takes effort to keep your mind on\nthe material. Actual problem solving is more effective for learning. Also,\nlectures are a form of \"distance learning\": though we are all in the same room\nwe might as well be at a distance, as there is little interaction. When I ask\nworking professionals what skills they want our students to have, being able\nto collaborate in teams is ALWAYS mentioned on the first breath.\n\nFor these reasons, the inverted classroom puts lectures online so that\nstudents who benefit from them can have them, and even review them repeatedly;\nand uses the classroom time in ways that engage students more actively and\ntakes advantage of the unique opportunity provided by being in the same room.\n\n### Studying Before Class and Quizzes\n\nThe online quizzes before class are of course intended to motivate students to\nreview the material before class. There is another motivation: If you don't\nprepare in advance, you risk looking foolish in front of your peers, who may\nbe annoyed at you for being unprepared to help, and you'll miss a learning\nopportunity. You don't want to get a reputation for being the person who is\nnot prepared. It's a small world: someday your peers may be able to influence\nhiring decisions.\n\n### Groups\n\nMuch has been published by researchers and practitioners on how to organize\ngroups for collaborative learning. My approach is based on this research and\nmy experience with this course.\n\nDuring the first month, students will be assigned randomly to groups, rotating\nto new groups each week to help you get to know each other. (A survey of\nstudents fall 2013 indicated that many liked this format as it was a rare\nopportunity to get to know other ICS students.) Also it helps prevent reliance\non dysfunctional relationships (e.g., freeloading and \"the sucker effect\"): a\nstudent can't plan on being with someone who will do the work for him or her,\nand after a while people figure out who to avoid when forming groups.\n\nAfter the Midterm 1 exam, you may form voluntary groups. Students who form\nvoluntary groups will stay in them until the next midterm, at which time they\nmay elect to continue or disband. Other students will continue to be assigned\nrandomly.\n\nRegardless of whether you are in random or voluntary groups, this is an\nimportant opportunity to develop group collaboration skills and also to\ndevelop a good reputation with your peers. It may affect whether you are\nselected to be part of a good group in ICS 311, and I have seen some students\nafter graduation get hired while others fail to get a job because of the\nreputations they had with their peers.\n\n",
 "path"=>"morea//01.introduction/reading-format.md"}
</pre>

<h2>/morea/01.introduction/reading-policies.html</h2>

<pre>Hash
{"title"=>"Policies",
 "published"=>true,
 "morea_id"=>"reading-policies",
 "morea_summary"=>
  "Cheating, reuse of open source, abuse of facilities, makeups, and deadlines",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-policies.html",
 "url"=>"/morea/01.introduction/reading-policies.html",
 "content"=>
  "_These policies were adopted (with changes) from Kazuo Sugihara._\n\nStudents in this course are subject to all the policies of ICS Department and\nthe University of Hawaii at Manoa, including but not limited to the following:\n\n## Cheating\n\nAny form of cheating (such as plagiarism and unauthorized collaboration on an\nassignment) results in the grade of F and will be reported to ICS Department\nfor further actions. In general, I encourage students to help each other but\nyou should each produce your own final version of the homework assignments to\nturn in, except where use of another student's code is explicitly allowed.\nCheck with me if you are not sure.\n\nIn a technical report, a student may quote a few sentences written by another\nauthor with an explicit citation to its original source. Writing sentences or\nideas of the other author without giving credit to the original author is\nplagiarism. Copying content of online documents is also regarded as\nplagiarism, irrespective of how much the original content is modified, if the\nstudent fails to write an accurate citation to the original source. Avoid so-\ncalled \"patchwork plagiarism.\" Informative suggestions are given in [Avoiding\nPlagiarism](http://emedia.leeward.hawaii.edu/resources/plagiarism/05avoid.htm)\nby Professors Marilyn Bauer and Jacie Moriyama of Leeward Community College.\n\n## Reuse of Open Source\n\nIn a software product for an implementation assignment, a student is allowed\nto reuse open source if ALL of the following are met.\n\n  1. The software does not cover functionality that the assignment specifies that the student will write.\n  2. The software license of the open source reused gives permission for this reuse.\n  3. The license header of reused source code is copied into a source file containing the reused source code.\n  4. Documentation (e.g., Readme, User manual) of the software product clearly gives credits to authors of the reused source code (i.e., acknowledgments to the authors including information such as the name of an author, the name of a reused product and a list of file names of the reused source code).\n\n## Abuse of Facilities\n\nAny form of abuse of computing resources of ICS Department or the University\nof Hawaii will not be tolerated. It results in termination of your account on\ntheir servers any time the abuse is detected, will lead to the grade F, and\nwill be reported to ICS Department for further actions.\n\nPlease keep in your mind that access to and usage of our computing resources\nare your privilege, but not your right.\n\nInappropriate content that may be of an offensive nature to other students\nshould not be displayed on laptops or group workstations during class.\n\n## Makeup\n\nIf a student missed an exam due to illness or injury, a makeup exam will be\ngiven to a student only when the student has a doctor's note dated that day\nand contacts the instructor by email within 3 days after the exam date or the\ndate that the doctor's note suggests the student to recover enough to contact\nthe instructor. However, an ordinary doctor's appointment (scheduled by a\nstudent in advance) is not an acceptable reason for makeup unless it is\ninevitable to conflict with an exam beyond control of the student. The makeup\nexam must be completed before exam solutions are reviewed in class.\n\nFor an exceptional case other than illness or injury, a student must submit an\nofficial document to the instructor providing sufficiently convincing evidence\nof the fact that the cause for missing an exam was beyond control of the\nstudent (e.g., in case of a traffic accident on a way to a class, a police\nrecord of the accident should be furnished).\n\n## Deadlines\n\n**If Laulima or uhunix.hawaii.edu becomes unexpectedly unavailable** in the last several hours before a quiz, homework or project deadline, **_email your solutions to the instructor or TA_** once you are sure that you will not be able to upload it but before the deadline. Please note that some quiz questions use random ordering for multiple choice: specify the content of your response, not just the letter.\n\nThe deadline of every assignment is firm. No late submission will be accepted\n(unless explicitly stated otherwise, possibly with penalties). No extension\nwill be given except the following cases.\n\n  * If Laulima is unexpectedly down for an extended period before the deadline preventing most students from uploading, the deadline will be extended for the downtime. Please notify the instructor of any unexpected downtime when you notice it so that a decision can be made and announced to all students in a timely manner. On the other hand, no extension will be given for scheduled downtime.\n  * A student has a doctor's note dated on the day of a deadline of the assignment and contacts the instructor by email within 3 days after the deadline or the date that the doctor's note suggests that the student has recovered enough to email.\n  * For any other exceptional circumstance other than illness or injury, a student is required to submit to the instructor an official document that is a sufficiently convincing evidence of the fact that the cause for missing a deadline was beyond control of the student.\n\n## Data Backup\n\nEach student is responsible for taking a periodical backup of her/his work and\ncomputer resources needed to meet course requirements. Especially, files for\nassignments should frequently be saved on an external storage device. An\nunexpected failure of the computer is not an acceptable excuse for a late\nsubmission. CS major students should exercise automatic, periodical backup\nprocedures. If you do not practice periodical cloning and backup, start to do\nit from now.\n\nA simple solution is to put all your coursework in a cloud environment such as\nDropbox, but you still need to ensure that you have backup of a working\ncomputer to access this data.\n\nMy own procedure (in OS X) uses two external drives. External drives make it\neasy to move to another computer if yours becomes inoperable. One external\ndrive is about the same size as my internal drive, and I use SuperDuper to\nmake a bootable clone about once a week. This does not save intermediate or\nprevious versions. The other drive is twice the size of the internal drive,\nand I use Time Machine to automatically record previous versions of documents.\nTime Machine does not produce a bootable disk. If my computer failed, I could\nboot off the SuperDuper clone disk and use the Time Machine disk to restore to\nmy most recent versions.\n\nThis is a good procedure for one computer, but the frequency of Time Machine\nruns (once per hour) may be insufficient in time-critical situations. You\nmight manually initiate back up on a more frequent basis. I use four computers\nand a server, so I also use Interarchy to synchronize (via disk mirroring) all\nof my document folders with my server and across the four computers. I run\nsynchronization to my server after each major piece of work.\n\n",
 "path"=>"morea//01.introduction/reading-policies.md"}
</pre>

<h2>/morea/01.introduction/reading-topic-overview.html</h2>

<pre>Hash
{"title"=>"Topics",
 "published"=>true,
 "morea_id"=>"reading-topic-overview",
 "morea_summary"=>
  "Conceptual overview of how topics are grouped and sequenced.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/01.introduction/reading-topic-overview.html",
 "url"=>"/morea/01.introduction/reading-topic-overview.html",
 "content"=>
  "## _Part I: Introduction to Analysis_\n\nThis section introduces algorithms as abstractions of programs, and motivates\nwhy we need to do analysis of algorithms rather than just run empirical tests\nof programs. It then introduces some mathematical and conceptual tools for\ndoing analysis. Two useful sorting algorithms are used for illustration; we'll\nreturn to sorting later.\n\n  * **#1** \\- Chapter 1: Introduction to Course Format and to Algorithms \n  * **#2** \\- Chapter 2: Examples of Analysis with Insertion and Merge Sort\n  * **#3** \\- Chapter 3: Growth of Functions and Asymptotic Concepts \n\n## _Part II: Data Structures for Dictionaries & Sets_\n\nIn this section we introduce problem solving and analysis methods (chapters\n3-5), some of which have been covered in ICS 241, and also review review basic\ndata structures and algorithms (chapters 10-12) that were introduced in ICS\n211. The chapters from the text are interleaved and paired up in a manner that\nuses the basic dictionary and set data structures and algorithms to illustrate\nthe problem solving and analysis methods. Hopefully this is mostly review of\nthe two prerequisite courses with some added depth.\n\n  * **#4** \\- Chapter 10: Stacks, Queues, Lists and Trees (Review)\n  * **#5** \\- Chapter 5 and Goodrich & Tamassia section: Probabilistic Analysis and Randomized Algorithms; Skip list example\n  * **#6** \\- Chapter 11: Hash Tables\n  * **#7** \\- Chapter 4: Divide & Conquer and Associated Analysis Methods\n  * **#8** \\- Chapter 12: Binary Search Trees\n\n## _Part III: Sorting and Balanced Trees_\n\nWe continue into more advanced applications of trees (chapters 6 and 13), also\nproviding the basis for another efficient sorting algorithm. We compare\nadditional sorting algorithms to those from Chapter 2. Sorting is one of the\nmost fundamental and common applications of computers, so efficiency is very\nimportant. We consider the broader question of how fast _any_ sort algorithm\ncan be. This is an example of a powerful method of computer science: reasoning\nabout sets of possible algorithms rather than specific algorithms.\n\n  * **#9** \\- Chapter 6: Heapsort and Priority Queues\n  * **#10** \\- Chapters 7 & 8.1: Quicksort, Theoretical Limits, and O(n) sorts \n  * **#11** \\- Chapter 13 & Sedgewick Chapter 15: Balanced Trees \n\n## _Part IV: Problem Solving and Analysis Methods_\n\nThis part introduces two further problem solving methods, _dynamic\nprogramming_ and _greedy algorithms_, with example applications. We then cover\nanother important analytic method, _amortization _, with examples in the\nanalysis of the union-find representation of sets. (Chronologically, #14\ngraphs will be introduced in this sequence to help you get started on a\nprogramming assignment, but conceptually they belong in the next section.)\n\n  * **#12** \\- Chapter 15: Dynamic Programming\n  * **#13** \\- Chapter 16: Greedy Algorithms & Huffman Codes\n  * **#15** \\- Chapter 17 - Amortization\n  * **#16** \\- Chapter 21 - Sets and Union-Find\n\n## _Part V: Graphs_\n\nGraphs are a very flexible data structure for which many applications exist.\nEquipped with various problem solving and analytic tools, we examine the most\nimportant algorithms on graphs -- including some of the most classic work in\ncomputer science -- and their applications.\n\n  * **#14** \\- Chapter 22: Graph Representations and Basic Algorithms _(covered earlier)_\n  * **#17** \\- Chapter 23: Minimum Spanning Trees \n  * **#18** \\- Chapter 24: Single-Source Shortest Paths \n  * **#19** \\- Chapter 25: All Pairs Shortest Paths \n  * **#20** \\- Chapter 26: Maximum Flow\n\n## _Part VI: Selected Topics_\n\nThere are a number of other important or common application areas that have\ntheir own specialized algorithms of interest: multithreading (parallel\nalgorithms), matrix operations, linear programming, operations on polynomials,\nnumber-theory, string matching, and computational geometry. These are covered\nin the text, but we can't fit them all in. Linear Programming will be examined\nbecause it fills out an area not covered well above (numeric algorithms), and\nalso has interesting connections to previous material (e.g., you can solve\nflow problems with a linear program). Two other topics will be covered more\nlightly in class (no homework).\n\n  * **#21** \\- Sedgewick Chapters 5 and 38; Chapter 29: Linear Programming \n  * **#22** \\- Chapter 27: Multithreading \n  * **#23** \\- Chapter 32: String Matching \n\n## _Part VII: Complexity Theory and NP-Completeness_\n\nFinally, abstracting further from algorithms to problems, we deal with the\nimportant question of whether an efficient algorithm is known (or even\npossible) for a given problem, and what to do if none are known. We encounter\nthe most important open problem in theoretical computer science (indeed in all\nof mathematics), which is of _practical_ interest because awareness of\n\"intractable\" problems and approximation algorithms could save you\nconsiderable trouble if you encounter one of these very common problems on the\njob! The seminal book on this topic is the most cited reference in computer\nscience.\n\n  * **#24** \\- Chapter 34 - Complexity Theory & NP-Completeness\n  * **#25** \\- Chapter 35 - Approximation Algorithms \n\n",
 "path"=>"morea//01.introduction/reading-topic-overview.md"}
</pre>

<h2>/morea/02.examples/experience-1.html</h2>

<pre>Hash
{"title"=>"Linear Search",
 "published"=>true,
 "morea_id"=>"experience-1",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze the linear search algorithm",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/02.examples/experience-1.html",
 "url"=>"/morea/02.examples/experience-1.html",
 "content"=>
  "## In Class: Linear Search\n\n#### 5 points\n\n1. Consider the **searching problem**:\n\nInput:\n\n     A sequence of _n_ numbers _A_ = ⟨ _a_1, _a_2, .... _a__n_ ⟩ (in _no particular order_), and a value _v_.\n\nOutput:\n\n     An index _i_ such that _v_ = _A_[_i_] or the special value NIL if _v_ does not appear in _A_.\n\n**a. (1 pt):** Write pseudocode for `LinearSearch(A,v)`, an algorithm that scans through sequence _A_ looking for _v_, and provides the desired output (_i_ or NIL). Use Java style 0-based indexing, A[_i_] to access elements, and A.length to get _n_. \n\n**b. (2 pts):** Let **_p_** be the number of array positions checked. (_p_ will be replaced with a function of _n_. For example, when the element is not present, _p_ = _n_.) Let _c_1 be the cost of executing line 1, _c_2 the cost of executing line 2, etc. As was done in the lecture notes: \n\n  * Write the expression for the cost to execute each line in terms of _p_ and the _c__i_. \n  * Sum these to get the total cost _T_(_n_). \n  * Then collect the constants and rename them \"a\" and \"b\".\n\n**c. (1 pt):** What is _p_ for the _worst case_: precisely how many elements of the input sequence need to be checked? \n\n  * Rewrite your last expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ(____).) \n\n**d. (1pt):** What is _p_ for the _average case_: precisely how many elements of the input sequence need to be checked on average, assuming that the element being searched for is present and equally likely to be any element in the array? \n\n  * Rewrite your expression for this _p_. \n  * In what Θ (Theta) complexity class is this? (Write as Θ (____).) \n\nJustify all your answers!\n\n",
 "path"=>"morea//02.examples/experience-1.md"}
</pre>

<h2>/morea/02.examples/experience-2.html</h2>

<pre>Hash
{"title"=>"Binary Search",
 "published"=>true,
 "morea_id"=>"experience-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze binary search",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/02.examples/experience-2.html",
 "url"=>"/morea/02.examples/experience-2.html",
 "content"=>
  "## In Class: Mixing and Matching with `BinarySearch`\n\n####  5 points\n\n**1\\. (1 pt):** The while loop of `InsertionSort` uses a Θ(_n_) linear search to scan backwards through the sorted subarray A[1 .. _j_-1]. Since this subarray is sorted, can we use `BinarySearch` for this search to improve the overall worst case running time of InsertionSort to Θ(_n_ lg _n_)? If yes, argue why the hybrid algorithm would be Θ(_n_ lg _n_). If no, explain why it won't work. \n\n**2\\. (4 pts):** We know from Tuesday's class work that `LinearSearch` is Θ(_n_), so one might think it is always better to use the Θ(lg _n_) `BinarySearch`. However, in order to apply `BinarySearch` we have to sort the data, which (we will soon learn) requires Θ(_n_ lg _n_) time for the best known algorithms (e.g., `MergeSort`). This question explores when it is worth paying this extra cost of sorting the data in order to get fast search.\n\nSuppose you will be **searching a list of _n_ items _m_ times**. When is _m_\nbig enough relative to _n_ to make it worth sorting and using binary search\nrather than just using linear search?\n\nWe have two alternatives. Simply using `LinearSearch` _m_ times on _n_ items\nhas an expected (average) cost of Θ(_mn_). The second alternative is (a)\nbelow, and (b)-(d) explore the tradeoffs.\n\n**a.** What is the expected cost to apply `MergeSort` once to sort _n_ items, and then apply `BinarySearch` _m_ times to _n_ items? \n\n**b.** Suppose _m_ = 1: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**c.** Suppose _n_ = _m_: which strategy is better, and why? Use the above expressions to justify your answer mathematically. \n\n**d.** What is the cutoff point in terms of _m_ expressed as a function of _n_ between when it is faster to just apply the linear search and when it is faster to apply MergeSort? (Set up an equation and solve for m.) \n\n(Comment: a more accurate analysis would take into account the different\nconstants associated with each algorithm: `MergeSort` has a higher constant.\nBut it can be hard to know the numerical value of the constant to be used in\nthe analysis. So, if you are really concerned about performance, once the\nanalysis gives you the ballpark tradeoff, empirical studies can be used to\ndecide the best cutoff for a given implementation.)\n\n\n",
 "path"=>"morea//02.examples/experience-2.md"}
</pre>

<h2>/morea/02.examples/experience-3.html</h2>

<pre>Hash
{"title"=>"More on linear and binary search",
 "published"=>true,
 "morea_id"=>"experience-3",
 "morea_type"=>"experience",
 "morea_summary"=>"Analyze linear and binary search (homework)",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/02.examples/experience-3.html",
 "url"=>"/morea/02.examples/experience-3.html",
 "content"=>
  "## #1. Peer Credit Assignment\n\n### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## #2. Correctness of `LinearSearch`\n\n### 5 points\n\n**(a)** Show the pseudocode for `LinearSearch` that you will be analyzing. It should be code that you understand and believe is correct, so you may revise your group's solution if you wish. Give each line a number for reference in your analysis.\n\n**(b)** Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties (page 19).\n\n_(This will be a little tricky because the loop can exit for two reasons.\nRather than complicate your invariant trying to cover the two cases, you might\nwrite a simpler one that gets part of the work done, and reason about the two\nexit conditions when you show how the invariant _helps_ to prove\ncorrectness.)_\n\n## #3. Runtime of `BinarySearch`\n\n### 5 points\n\nThis problem steps you through a recursion tree analysis of BinarySearch (the\nalgorithm for searching a sorted array that was reviewed in class) to show\nthat it is Θ(lg _n_) in the worst case (that is, O(lg _n_) in general). Θ and\n\"big-O\" are concepts introduced in Topic 3: if they are not familiar to you,\njust think of this as meaning the longest possible execution on input of size\n_n_ will take time proportional to lg _n_.\n\n**(a)** Write the recurrence relation for `BinarySearch`, using the formula T(_n_) = _a_T(_n_/_b_) + D(_n_) + C(_n_). (We'll assume T(1) = some constant _c_, and you can use _c_ to represent other constants as well, since we can choose _c_ to be large enough to work as an upper bound everywhere it is used.)\n\n**(b)** Draw the recursion tree for `BinarySearch`, in the style shown in podcast 2E and in Figure 2.5. (Don't just copy the example for `MergeSort`: it will be incorrect. Make use of the recurrence relation you just wrote!)\n\n**(c)** Using a format similar to the counting argument in Figure 2.5 of the text or of podcast 2E, use the tree to show that `BinarySearch` is Θ(lg _n_) in the worst case. Specifically,\n\n  1. show what the row totals are,\n  2. write an expression for the tree height (justifying it), and\n  3. use this information to determine the total computation represented by the tree.\n\nSince this problem involves mathematical expressions and diagrams, you may\nwant to do your work on paper and digitize it (please compress images) and\nsubmit as jpg or pdf. Alternatively, use a drawing program.\n\n\n\n",
 "path"=>"morea//02.examples/experience-3.md"}
</pre>

<h2>/morea/02.examples/module-examples.html</h2>

<pre>Hash
{"title"=>"Analysis examples",
 "published"=>true,
 "morea_id"=>"examples-insertion-merge-sort",
 "morea_outcomes"=>["outcome-analysis-style"],
 "morea_readings"=>
  ["reading-screencast-2a",
   "reading-screencast-2b",
   "reading-screencast-2c",
   "reading-screencast-2d",
   "reading-screencast-2e",
   "reading-cormen-2",
   "reading-screencast-mit-1",
   "reading-notes-2"],
 "morea_experiences"=>["experience-1", "experience-2", "experience-3"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/02.examples/module-examples.gif",
 "morea_sort_order"=>2,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/02.examples/module-examples.html",
 "content"=>"Start analyzing algorithms with insertion and merge sort.\n",
 "path"=>"morea//02.examples/module-examples.md"}
</pre>

<h2>/modules/examples-insertion-merge-sort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-examples.md",
 "title"=>"Analysis examples",
 "url"=>"/modules/examples-insertion-merge-sort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/examples-insertion-merge-sort/index.html"}
</pre>

<h2>/morea/02.examples/outcome-analysis-style.html</h2>

<pre>Hash
{"title"=>"Understand analysis of algorithm styles.",
 "published"=>true,
 "morea_id"=>"outcome-analysis-style",
 "morea_type"=>"outcome",
 "morea_sort_order"=>3,
 "referencing_modules"=>[#Jekyll:Page @name="module-examples.md"],
 "url"=>"/morea/02.examples/outcome-analysis-style.html",
 "content"=>
  "By viewing examples, become familiar with the style of analysis used in ICS 311.",
 "path"=>"morea//02.examples/outcome-analysis-style.md"}
</pre>

<h2>/morea/02.examples/reading-cormen-2.html</h2>

<pre>Hash
{"title"=>"CLRS 2 - Getting started",
 "published"=>true,
 "morea_id"=>"reading-cormen-2",
 "morea_summary"=>"Getting started with analysis of algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "26 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-cormen-2.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-cormen-2.md"}
</pre>

<h2>/morea/02.examples/reading-notes-2.html</h2>

<pre>Hash
{"title"=>"Chapter 2 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-2",
 "morea_summary"=>"Modeling a problem, loop invariants, analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/02.examples/reading-notes-2.html",
 "url"=>"/morea/02.examples/reading-notes-2.html",
 "content"=>
  "## Outline\n\n  1. The Sorting Problem\n  2. Insertion Sort: An Incremental Strategy\n  3. Loop Invariants and Correctness of Insertion Sort\n  4. RAM Model; What do we count?\n  5. Analysis of Insertion Sort: Best and Worst Cases\n  6. Worst Case Rate of Growth and Θ (Theta)\n  7. Merge Sort: A Divide & Conquer Strategy\n  8. Brief Comment on Merge Sort Correctness\n  9. Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n## Modeling a Problem: The Sorting Problem\n\n### Problem Formulation\n\nClear and unambiguous definition of what to be solved in terms of:\n\n  * Input of the problem\n  * Output of the problem\n  * Assumptions in the problem\n\nDescriptions in a problem formulation must be declarative (not procedural).\nAll assumptions concerning input and output must be explicit. The problem\nformulation provides the requirements for an algorithm.\n\n### Problem Formulation for Sorting\n\nInput:\n\n    A sequence σ of n real numbers xi (1 ≤ i ≤ n)\nAssumptions:\n\n  1. n is a positive integer.\n  2. The real numbers xi (1 ≤ i ≤ n) are not necessarily distinct.\nOutput:\n\n    A permutation π = x'1 x'2  x'n of the given sequence σ such that x'j ≤ x'j+1 for every j (1 ≤ j < n)\n\nThe numbers are referred to as **keys**.\n\nAdditional information known as **satellite data** may be associated with each\nkey.\n\nSorting is hugely important in most applications of computers. We will cover\nseveral ways to solve this problem in this course.\n\n* * *\n\n## Insertion Sort: An Incremental Strategy\n\n![](fig/sorting-cards.jpg)\n\nInsertion sort takes an **incremental strategy** of problem solving: pick off\none element of the problem at a time and deal with it. Our first example of\nthe text's pseudocode:\n\n![](fig/code-insertion-sort.jpg)\n\nHere's a step by step example:\n\n![](fig/fig-2-2-insertion-sort-example.jpg)\n\n_Is the strategy clear? For fun, see the visualization at\n<http://youtu.be/ROalU379l3U>_\n\n* * *\n\n## Loop Invariants and Correctness of Insertion Sort\n\n### Loop Invariants\n\nA loop invariant is a formal property that is (claimed to be) true at the\nstart of each iteration. We can use loop invariants to prove the correctness\nof iteration in programs, by showing three things about the loop invariant:\n\n**Initialization:**\n    It is true prior to the first iteration.\n**Maintenance:**\n    If it is true prior to a given iteration, then it remains true before the next iteration.\n**Termination:**\n    When the loop terminates, the invariant (and the conditions of termination) gives us a useful property that helps to show that the algorithm is correct.\n\nNotice the similarity to mathematical induction, but here we have a\ntermination condition.\n\n### Correctness of Insertion Sort\n\n![](fig/code-insertion-sort.jpg)\n\n**Loop Invariant:**\n    At the start of each iteration of the outer `for` loop at line 1, the subarray A[1 .. _j_-1] consists of the elements originally in A[1 .. _j_-1] but in sorted order. \n**Initialization:**\n    We start with _j_=2. The subarray A[1 .. _j_-1] is the single element A[1], which is the element originally in A[1] and is trivially sorted.\n**Maintenance:**\n    A precise analysis would state and prove another loop invariant for the `while` loop. For simplicity, we'll note informally that at each iteration the elements A[_j_-1], A[_j_-2], A[_j_-3], etc. are shifted to the right (so they remain in the sequence in proper order) until the proper place for _key_ (the former occupant of A[_j_]) is found. Thus at the next iteration, the subarray A[1 .. _j_] has the same elements but in sorted order.\n**Termination:**\n    The outer `for` loop ends when _j_=_n_+1. Therefore _j_-1=_n_. Plugging _n_ into the loop invariant, the subarray A[1 .. _n_] (which is the entire array) consists of the elements originally in A[1 .. _n_] but in sorted order.\n\n_Convinced? Questions? Could you do it with another problem?_\n\n* * *\n\n## RAM Model: What do we count?\n\nIf we are going to tally up time (and space) requirements, we need to know\nwhat counts as a unit of time (and space). Since computers differ from each\nother in details, it is helpful to have a common abstract model.\n\n### Random Access Machine (RAM) Model\n\nThe RAM model is based on the design of typical von Neumann architecture\ncomputers that are most widely in use. For example:\n\n  * Instructions are executed one after the other (no concurrent operations).\n  * Instructions operate on a small number (one or two) of data \"words\" at a time.\n  * Data words are of a limited, constant size (cannot get arbitrarily large computation done in one operation by putting the data in an arbitrarily large word).\n\n### Categories of Primitive Operations\n\nWe identify the primitive operations that count as \"one step\" of computation.\nThey may differ in actual time taken, but all can be bounded by the same\nconstant, so we can simplify things greatly by counting them as equal.\n\n#### Data Manipulation\n\n  * Arithmetic operation: +, -, *, /, remainder, floor, ceiling, left/right shift\n  * Comparison: <, =, >, ≤, ≥\n  * Logical operation: ∧, ∨, ¬\n\n> _These assume bounded size data objects being manipulated, such as integers\nthat can be represented in a constant number of bits (e.g, a 64-bit word),\nbounded precision floating numbers, or boolean strings that are bounded in\nsize. Arbitrarily large integers, arbitrarily large floating point precision,\nand arbitrarily long strings can lead to nonconstant growth in computation\ntime._\n\n#### Flow Control\n\n  * Branch: case, if, etc.\n  * Loop; while, for   __   <-   ___   to   ___ \n\n> _Here we are stating that the time to execute the machinery of the\nconditional loop controllers are constant time. However, if the language\nallows one to call arbitrary methods as part of the boolean expressions\ninvolved, the overall execution may not be constant time._\n\n#### Miscellaneous\n\n  * Assignment: <-\n  * Subscription: [ ]\n  * Reference\n  * Setting up a procedure or function call (see below)\n  * Setting up an I/O operation (see below) \n\n> _The time to set up a procedure call is constant, but the time to execute\nthe procedure may not be. Count that separately. Similarly, the time to set up\nan I/O operation is constant, but the time to actually read or write the data\nmay be a function of the size of the data. Treat I/O as constant only if you\nknow that the data size is bounded by a constant, e.g., reading one line from\na file with fixed data formats._\n\n###  Input Size\n\nTime taken is a function of input size. How do we measure input size?\n\n  * It is often most convenient to use the number of items in the input, such as the number of numbers being sorted. \n  * For some algorithms we need to measure the size of data, such as the number of bits in two integers being multiplied. \n  * For other algorithms we need more than one number, such as the number of vertices _and_ edges in a graph.\n\n* * *\n\n## Analysis of Insertion Sort: Best and Worst Cases\n\nWe now undertake an exhaustive quantitative analysis of insertion sort. We do\nthis analysis in greater detail than would normally be done, to illustrate why\nthis level of detail is not necessary!!!\n\nFor each line, what does it cost, and how many times is it executed?\n\nWe don't know the actual cost (e.g., in milliseconds) as this varies across\nsoftware and hardware implementations. A useful strategy when you do not know\na quantity is to just give it a name ...\n\n![](fig/analysis-insertion-sort.jpg)\n\nThe _ci_ are the unknown but constant costs for each step. The _tj_ are the\nnumbers of times that line 5 is executed for a given _j_. These quantities\ndepend on the data, so again we just give them names.\n\nLet T(_n_) be the running time of insertion sort. We can compute T(_n_) by\nmultiplying each cost by the number of times it is incurred (on each line) and\nsumming across all of the lines of code:\n\n![](fig/equation-insertion-total-time.jpg)\n\n### Best Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is already sorted, we always find that A[_i_] ≤ _key_ the first\ntime the `while` loop is run; so all _tj_ are 1 and _tj-1_ are 0. Substituting\nthese values into the above:\n\n![](fig/equation-insertion-best.jpg)\n\nAs shown in the second line, this is the same as _a__n_ \\+ _b_ for suitable\nconstants _a_ and _b_. Thus the running time is a **linear function of n.**\n\n### Worst Case\n\n![](fig/analysis-insertion-sort-while-loop.jpg)\n\nWhen the array is in reverse sorted order, we always find that A[_i_] > _key_\nin the while loop, and will need to compare _key_ to all of the (growing) list\nof elements to the left of _j_. There are _j_-1 elements to compare to, and\none additional test for loop exit. Thus, _tj=j_.\n\n![](fig/equation-insertion-worst-1.jpg) ![](fig\n/equation-insertion-worst-2.jpg)\n\nPlugging those values into our equation:\n\n![](fig/equation-insertion-total-time.jpg)\n\nWe get the worst case running time, which we simplify to gather constants:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _an2 \\+ bn + c_ for some _a, b, c_: _T(n)_ is a\n**quadratic function of n**.\n\nSo we can draw these conclusions purely from mathematical analysis, with _ no\nimplementation or testing needed_: Insertion sort is very quick (linear) on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. But the worst case is slow for reverse sorted data.\n\n* * *\n\n## Worst Case Rate of Growth and Θ (Theta)\n\nFrom the above example we introduce two key ideas and a notation that will be\nelaborated on later.\n\n###  Worst Case Analysis\n\nAbove, both best and worst case scenarios were analyzed. We usually\nconcentrate on the worst-case running times for algorithms, because:\n\n  * This gives us a guaranteed upper bound.\n  * For some algorithms, the worst case occurs often (such as failing to find an item in a search). \n  * The average is often almost as bad as the worst case.\n\n_How long does it take on average to successfully find an item in an unsorted\nlist of n items?  \nHow long does it take in the worst case, when the item is not in the list?  \nWhat is the difference between the two?_\n\n###  Rate of Growth\n\nIn the above example, we kept track of unknown but named constant values for\nthe time required to execute each line once. In the end, we argued that these\nconstants don't matter!\n\n  * Their specific values don't matter because they all add up to summary constants in the equations (e.g., _a_ and _b_).\n  * Even their presence does not matter, because it is the growth of the function of _n_ that dominates the time taken to run the algorithm.\n\nThis is good news, because it means that all of that excruciating detail is\nnot needed!\n\nFurthermore, only the fastest growing term matters. In _an2 \\+ bn + c_, the\ngrowth of _n2_ dominates all the other terms (including _bn_) in its growth.\n\n###  Theta: Θ\n\nWe will use Θ notation to concentrate on the fastest growing term and ignore\nconstants.\n\nIf we conclude that an algorithm requires _an2 \\+ bn + c_ steps to run, we\nwill dispense with the constants and lower order terms and say that its growth\nrate (the growth of how long it takes as _n_ grows) is Θ(_n_2).\n\nIf we see _bn + c_ we will write Θ(_n_).\n\nA simple constant _c_ will be Θ(1), since it grows the same as the constant 1.\n\nWhen we combine Θ terms, we similarly attend only to the dominant term. For\nexample, suppose an analysis shows that the first part of an algorithm\nrequires Θ(_n_2) timeand the second part requires Θ(_n_) time. Since the\nformer term dominates, we need not write Θ(_n_2 \\+ _n_): the overall algorithm\nis Θ(_n_2).\n\nFormal definitions next week!\n\n\n\n* * *\n\n## Merge Sort: A Divide & Conquer Strategy\n\nAnother strategy is to **Divide and Conquer**:\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\nMerge Sort takes this strategy:\n\n**Divide:**\n    Given A[_p .. r_], split the given array into two subarrays A[_p .. q_] and A[_q+1 .. r_] where _q_ is the halfway point of A[_p .. r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure `Merge` that iterates over the subarrays from the smallest element up to copy the next smallest element into a result array.   \n(This is like taking two decks of sorted cards and picking the next smallest\none off to place face-down in a new pile to make one sorted deck.)\n\nThe strategy can be written simply and elegantly in recursive code ...\n\n![](fig/code-merge-sort.jpg)\n\nHere are examples when the input is a power of two, and another example when\nit is not a power of two:\n\n![](fig/example-merge-sort-1.jpg) ![](fig/example-merge-sort-2.jpg)\n\nNow let's look in detail at the merge procedure, implemented using ∞ as\n**sentinels** _(what do lines 1-2 do? lines 3-9 ? lines 10-17?)_:\n\n![](fig/code-merge-procedure.jpg)\n\nHere's an example of how the final pass of `MERGE(9, 12, 16)` happens in an\narray, starting at line 12. Entries with slashes have had their values copied\nto either L or R and have not had a value copied back in yet. Entries in L and\nR with slashes have been copied back into A.\n\n![](fig/example-merge-sort-3.jpg)\n\nWe can also dance this one: <http://youtu.be/XaqR3G_NVoo>\n\n* * *\n\n## Merge Sort Correctness\n\nA loop invariant is used in the book to establish correctness of the Merge\nprocedure. Since the loop is rather straightforward, we will leave it to the\nabove example. Once correctness of Merge is established, induction can be used\nto show that Merge-Sort is correct for any N.\n\n* * *\n\n## Analysis of Merge Sort: Recurrence Relations and Recursion Tree\n\n![](fig/code-merge-procedure-small.jpg)\n\nMerge Sort provides us with our first example of using recurrence relations\nand recursion trees for analysis. We will go into more detail on these methods\nwhen we cover Chapter 4.\n\n### Analysis of Merge\n\nAnalysis of the Merge procedure is straightforward. The first two `for` loops\n(lines 4 and 6) take Θ(_n1+n2_) = Θ(_n_) time, where _n_1+_n_2 = _n_. The last\n`for` loop (line 12) makes _n_ iterations, each taking constant time, for\nΘ(_n_) time. Thus total time is Θ(_n_).\n\n### Analyzing Divide-and-Conquer Algorithms\n\n**Recurrence equations** are used to describe the run time of Divide & Conquer algorithms. Let _T(n)_ be the running time on a problem of size _n_. \n\n  * If _n_ is below some constant (or often, _n=1_), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each _1/b_ size of the original. Often, as in Merge Sort, _a = b = 2_.\n  * We pay cost **_D(n)_** to divide the problems and **_C(n)_** to combine the solutions. \n  * We also pay cost **_aT(n/b)_** solving subproblems. \n\nThen the total time to solve a problem of size _n_ by dividing into _a_\nproblems of size _n_/_b_can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\n### Recurrence Analysis of Merge Sort\n\n![](fig/code-merge-sort.jpg)\n\nMerge-Sort is called with _p=1_ and _r=n_. For simplicity, assume that _n_ is\na power of 2. (We can always raise a given _n_ to the next power of 2, which\ngives us an upper bound on a tighter Θ analysis.) When _n≥2_, the time\nrequired is:\n\n  * **Divide** (line 2): Θ(1) is required to compute _q_ as the average of _p_ and _r_.\n  * **Conquer** (lines 3 and 4): 2_T_(_n_/2) is required to recursively solve two subproblems, each of size _n/2_.\n  * **Combine** (line 5): Merging an n-element subarray takes Θ(_n_) (this term absorbs the Θ(1) term for Divide). \n![](fig/recurrence-mergesort-theta.jpg)\n\nIn Chapter 4 we'll learn some methods for solving this, such as the Master\nTheorem, by which we can show that it has the solution T(_n_) = Θ(_n_\nlg(_n_)). Thus, Merge Sort is faster than Insertion Sort in proportion to the\ndifference in growth of lg(_n_) versus _n_.\n\n### Recursion Tree Analysis\n\nRecursion trees provide an intuitive understanding of the above result. In\ngeneral, recursion trees can be used to plan out a formal analysis, or even\nconstitute a formal analysis if applied carefully.\n\nLet's choose a constant _c_ that is the largest of all the constant costs in\nthe algorithm (the base case and the divide steps). Then the recurrence can be\nwritten:\n\n![](fig/recurrence-mergesort-c.jpg)\n\nIt costs _cn_ to divide the original problem in half and then to merge the\nresults. We then have to pay cost _T_(_n_/2) twice to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-1.jpg)\n\nFor each of the two subproblems, _n_/2 is playing the role of _n_ in the\nrecurrence. So, it costs _cn_/2 to divide and then merge the _n_/2 elements,\nand _T_(_n_/4) to solve the subproblems:\n\n![](fig/recurrence-tree-mergesort-2.jpg)\n\nIf we continue in this manner we eventually bottom out at problems of size 1:\n\n![](fig/recurrence-tree-mergesort-3.jpg)\n\nNotice that if we sum across the rows each level has cost _cn_. So, all we\nhave to do is multiply this by the number of levels. Cool, huh?\n\n_But how many levels are there?_ A little thought (or a more formal inductive\nproof you'll find in the book) shows that there are about (allowing for the\nfact that n may not be a power of 2) lg(_n_)+1 levels of the tree. This is\nbecause you can only divide a power of two in half as many times as that power\nbefore you reach 1, and _n_ = 2lg(_n_). The 1 counts the root note before we\nstart dividing: there is always at least one level.\n\n_Questions? Does it make sense, or is it totally mysterious?_\n\n### One more Animation\n\nRecapitulating our conclusions, we have seen that Insertion sort is quick on\nalready sorted data, so it works well when incrementally adding items to an\nexisting list. Due to its simplicity it is a good choice when the sequence to\nsort will always be small. But for large inputs Merge Sort will be faster than\nInsertion Sort, as _n_2 grows much faster than _n_lg(_n_). Each sort algorithm\nhas different strengths and weaknesses, and performance depends on the data.\nSome of these points are made in the following visualizations (also watch for\npatterns that help you understand the strategies):\n\n> <http://www.sorting-algorithms.com/> (set to 50 elements)\n\n* * *\n\n## Next\n\nNext week we cover Chapter 3: Growth of Functions and Asymptotic Concepts.\nProblems will be posted for my students in Laulima.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:09:15 HST 2014\n\nImages are from the instructor's manual for Cormen et al.\n\n",
 "path"=>"morea//02.examples/reading-notes-2.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-2A.html</h2>

<pre>Hash
{"title"=>"Insertion Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2a",
 "morea_summary"=>"Insertion sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-2A.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-2A.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-2B.html</h2>

<pre>Hash
{"title"=>"Loop Invariant",
 "published"=>true,
 "morea_id"=>"reading-screencast-2b",
 "morea_summary"=>"Loop invariant example",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://youtu.be/t1ranlQmofQ",
 "morea_labels"=>["Screencast", "Suthers", "6 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-2B.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-2B.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-2C.html</h2>

<pre>Hash
{"title"=>"Insertion sort analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2c",
 "morea_summary"=>"Insertion sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://youtu.be/UtEMLcKHcGc",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-2C.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-2C.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-2D.html</h2>

<pre>Hash
{"title"=>"Merge Sort",
 "published"=>true,
 "morea_id"=>"reading-screencast-2d",
 "morea_summary"=>"Merge sort example",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://youtu.be/9BI0Lw1kzkE",
 "morea_labels"=>["Screencast", "Suthers", "11 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-2D.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-2D.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-2E.html</h2>

<pre>Hash
{"title"=>"Merge Sort Analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-2e",
 "morea_summary"=>"Merge sort analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://youtu.be/1JbqqmK7e5s",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-2E.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-2E.md"}
</pre>

<h2>/morea/02.examples/reading-screencast-mit-1.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 1",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-1",
 "morea_summary"=>
  "Analysis of algorithms-insertion sort, asymptotic analysis, merge sort, recurrences",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec01/",
 "morea_labels"=>["Screencast", "Leiserson", "80 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/02.examples/reading-screencast-mit-1.html",
 "content"=>"",
 "path"=>"morea//02.examples/reading-screencast-mit-1.md"}
</pre>

<h2>/morea/03.growth/experience-asymptotic-concepts.html</h2>

<pre>Hash
{"title"=>"Asymptotic concepts",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-concepts",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of functions with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/03.growth/experience-asymptotic-concepts.html",
 "url"=>"/morea/03.growth/experience-asymptotic-concepts.html",
 "content"=>
  "## Asymptotic Concepts\n\n#### 5 points\n\n**1\\. (1 pt)** We can extend asymptotic notation to the case of two parameters n and m that can go to infinity independently at different rates. For example, we denote by O(g(n,m)) the set of functions:\n\n> O(_g_(_n_,_m_)) = {_f_(_n_,_m_) : there exists positive constants _c_, _n_0\nand _m_0 such that 0 ≤ _f_(_n_,_m_) ≤ _c__g_(_n_,_m_) for all _n_ ≥ _ _n0 or\n_m_ ≥ _m_0}\n\nGive a corresponding definition for Θ(_g_(_n_,_m_)).\n\n**2\\. (4 pts)** Indicate, for each pair of expressions (_f_(_n_), _g_(_n_)) in the table below, whether _f_(_n_) = ___(_g_(_n_)), where the ___ may be O, o, Ω, ω or Θ. Assume that k ≥ 1, _c_ > 1, and d > 0 are constants and we are analyzing growth rates in terms of the variable _n_. To respond, write \"Yes\" or \"No\" in each box. Grading will be based on these entries first, but if you give your justifications below we can give better feedback and possibly partial credit in case of wrong answers. \n\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\"><i>f</i>(<i>n</i>)</th>\n    <th scope=\"col\"><i>g</i>(<i>n</i>)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">a.</th>\n    <td>n<sup>lg <i>c</i></sup></td>\n    <td>c<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">b.</th>\n    <td>lg<sup><i>k</i></sup><i>n</i></td>\n    <td>n<sup><i>d</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">c.</th>\n    <td>2<sup><i>n</i></sup></td>\n    <td>2<sup><i>n</i>/2</sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">d.</th>\n    <td>lg(<i>n</i>!)</td>\n    <td>lg(<i>n</i><sup><i>n</i></sup>)</td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n",
 "path"=>"morea//03.growth/experience-asymptotic-concepts.md"}
</pre>

<h2>/morea/03.growth/module-growth.html</h2>

<pre>Hash
{"title"=>"Growth of functions",
 "published"=>true,
 "morea_id"=>"growth",
 "morea_outcomes"=>["outcome-growth"],
 "morea_readings"=>
  ["reading-screencast-3a",
   "reading-screencast-3b",
   "reading-screencast-3c",
   "reading-screencast-3d",
   "reading-cormen-3",
   "reading-notes-3",
   "reading-screencast-mit-2"],
 "morea_experiences"=>["experience-asymptotic-concepts"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/03.growth/module-growth.png",
 "morea_sort_order"=>3,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/03.growth/module-growth.html",
 "content"=>
  "How do we characterize functions with respect to their limiting, or asymptotic, behavior. \n",
 "path"=>"morea//03.growth/module-growth.md"}
</pre>

<h2>/modules/growth/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-growth.md",
 "title"=>"Growth of functions",
 "url"=>"/modules/growth/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/growth/index.html"}
</pre>

<h2>/morea/03.growth/outcome-growth.html</h2>

<pre>Hash
{"title"=>"Understand analysis of asymptotic growth.",
 "published"=>true,
 "morea_id"=>"outcome-growth",
 "morea_type"=>"outcome",
 "morea_sort_order"=>4,
 "referencing_modules"=>[#Jekyll:Page @name="module-growth.md"],
 "url"=>"/morea/03.growth/outcome-growth.html",
 "content"=>
  "Understand how to characterize the behavior of functions in terms of their limiting, or asymptotic behavior. ",
 "path"=>"morea//03.growth/outcome-growth.md"}
</pre>

<h2>/morea/03.growth/reading-cormen-3.html</h2>

<pre>Hash
{"title"=>"CLRS 3 - Growth of functions",
 "published"=>true,
 "morea_id"=>"reading-cormen-3",
 "morea_summary"=>
  "Asymptotic notation, standard notation, and common functions.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-cormen-3.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-cormen-3.md"}
</pre>

<h2>/morea/03.growth/reading-notes-3.html</h2>

<pre>Hash
{"title"=>"Chapter 3 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-3",
 "morea_summary"=>"Introduction to asymptotic analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/03.growth/reading-notes-3.html",
 "url"=>"/morea/03.growth/reading-notes-3.html",
 "content"=>
  "## Outline\n\n  1. Intro to Asymptotic Analysis\n  2. Big-O\n  3. Ω (Omega)\n  4. Θ (Theta)\n  5. Asymptotic Notation in Equations\n  6. Asymptotic Inequality\n  7. Properties of Asymptotic Sets\n  8. Common Functions\n\n* * *\n\n## Intro to Asymptotic Analysis\n\nThe notations discussed today are ways to describe behaviors of _functions,_\nparticularly _in the limit_, or _asymptotic_ behavior.\n\nThe functions need not necessarily be about algorithms, and indeed asymptotic\nanalysis is used for many other applications.\n\nAsymptotic analysis of algorithms requires:\n\n  1. Identifying ** what aspect of an algorithm we care about**, such as:    \n\n    * runtime;\n    * use of space;\n    * possibly other attributes such as communication bandwidth;\n  \n\n  2. Identifying **a function that characterizes that aspect; ** and \n  \n\n  3. Identifying **the asymptotic class of functions that this function belongs to**, where classes are defined in terms of bounds on growth rate. \n\nThe different asymptotic bounds we use are analogous to equality and\ninequality relations:\n\n  * O   ≈   ≤\n  * Ω   ≈   ≥\n  * Θ   ≈   =\n  * o   ≈   <\n  * ω   ≈   >\n\nIn practice, most of our analyses will be concerned with run time. Analyses\nmay examine:\n\n  * Worst case\n  * Best case\n  * Average case (according to some probability distribution across all possible inputs)\n\n* * *\n\n## Big-O (asymptotic ≤)\n\nOur first question about an algorithm's run time is often \"how bad can it\nget?\" We want a guarantee that a given algorithm will complete within a\nreasonable amount of time for typical n expected. This requires an\n**asymptotic upper bound**: the \"worst case\".\n\nBig-O is commonly used for worst case analyses, because it gives an upper\nbound on growth rate. Its definition in terms of set notation is:\n\n> O(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/graph-big-O.jpg)\n\nThis definition means that as _n_ increases, afer a point _f_(_n_) grows no\nfaster than _g_(_n_) (as illustrated in the figure): _g_(_n_) is an\n_asymptotic upper bound_ for _f_(_n_).\n\nSince O(_g_(_n_)) is a set, it would be natural to write _f_(_n_) ∈\nO(_g_(_n_)) for any given _f_(_n_) and _g_(_n_) meeting the definition above,\nfor example, _f_ ∈ O(_n_2).\n\nBut the algorithms literature has adopted the convention of using = instead of\n∈, for example, writing _f_(_n_) = O(_g_(_n_)). This \"abuse of notation\" makes\nsome manipulations possible that would be more tedious if done strictly in\nterms of set notation. (We do _not_ write O(_g_(_n_))=_f_(_n_); will return to\nthis point).\n\nUsing the = notation, we often see definitions of big-O in in terms of truth\nconditions as follows:\n\n> _f_(_n_) = O(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that 0 ≤\n_f_(_n_) ≤ _c__g_(_n_) ∀ _n_ ≥ _n_0.\n\nWe assume that all functions involved are asymptotically non-negative. Other\nauthors don't make this assumption, so may use |_f_(_n_)| etc. to cover\nnegative values. This assumption is reflected in the condition 0 ≤ _f_(_n_).\n\n### Examples\n\nShow that 2_n_2 is O(_n_2).\n\nTo do this we need to show that there exists some _c_ and _n_0 such that\n(letting 2_n_2 play the role of _f_(_n_) and _n_2 play the role of _g_(_n_) in\nthe definition):\n\n> 0 ≤ 2_n_2 ≤ _c__n_2 for all _n_ ≥ _n_0.\n\nIt works with _c_ = 2, since this makes the _f_ and _g_ terms equivalent for\nall _n_ ≥ _n_0 = 0. (We'll do a harder example under Θ.)\n\n#### What's in and what's out\n\nThese are all O(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_\n  * _n_1.99999\n  * _n_\n\n  * _n_3\n  * _n_2.00001\n  * _n_2 lg _n_\n\n#### Insertion Sort Example\n\nRecall that we did a tedious analysis of the worst case of insertion sort,\nending with this formula:\n\n![](fig/equation-insertion-worst-3.jpg)\n\n_T(n)_ can be expressed as _pn2 \\+ _q__n_ \\- r_ for suitable _p, q, r_ (_p_ =\n(_c_5/2 + _c_6/2 + _c_7/2), etc.).\n\nThe textbook (page 46) sketches a proof that __f_(_n_) = _a__n_2 \\+ _b__n_ \\+\n_c__ is Θ(_n_2), and we'll see shortly that Θ(_n_2) -> O(_n_2). This is\ngeneralized to all polynomials in Problem 3-1. So, any polynomial with highest\norder term _a__n__d_ (i.e., a polynomial in _n_ of degree _d_) will be\nO(_n__d_).\n\nThis suggests that the worst case for insertion sort _T_(_n_) ∈ O(_n_2). An\nupper bound on the worst case is also an upper bound on all other cases, so we\nhave already covered those cases.\n\nNotice that the definition of big-O would also work for __g_(_n_) = n3_,\n__g_(_n_) = 2n_, etc., so we can also say that _T_(_n_) (the worst case for\ninsertion sort) is O(_n_3), O(2_n_), etc. However, these loose bounds are not\nvery useful! We'll deal with this when we get to Θ (Theta).\n\n* * *\n\n## Ω (Omega, asymptotic ≥)\n\nWe might also want to know what the best we can expect is. In the last lecture\nwe derived this formula for insertion sort:\n\n![](fig/equation-insertion-best.jpg)\n\nWe could prove that this best-case version of T(n) is big-O of something, but\nthat would only tell us that the best case is no worse than that something.\nWhat if we want to know what is \"as good as it gets\": a lower bound below\nwhich the algorithm will never be any faster?\n\nWe must both pick an appropriate function to measure the property of interest,\nand pick an appropriate asymptotic class or comparison to match it to. We've\ndone the former with _T_(_n_), but what should it be compared to?\n\nIt makes more sense to determine the **asymptotic lower bound** of growth for\na function describing the best case run-time. In other words, what's the\nfastest we can ever expect, in the best case?\n\n![](fig/graph-Omega.jpg)\n\n**Ω (Omega)** provides what we are looking for. Its set and truth condition definitions are simple revisions of those for big-O:\n\n> Ω(_g_(_n_)) = {_f_(_n_) : ∃ positive constants _c_ and _n_0 such that 0 ≤\n_cg_(_n_) ≤ _f_(_n_) ∀ _n_ ≥ _n_0}.  \n_[The _f_(_n_) and _cg_(_n_) have swapped places.]_\n\n> _f_(_n_) = Ω(_g_(_n_)) iff ∃ positive constants _c_ and _n_0 such that\n_f_(_n_) ≥ _cg_(_n_) ∀ _n_ ≥ _n_0.  \n_[≤ has been replaced with ≥.]_\n\nThe semantics of Ω is: as _n_ increases after a point, _f_(_n_) grows no\nslower than _g_(_n_) (see illustration).\n\n### Examples\n\nSqrt(_n_) is Ω(lg _n_) with _c_=1 and _n_0=16.  \n_(At n=16 the two functions are equal; try at n=64 to see the growth, or graph\nit.)_\n\n####  What's In and What's Out\n\nThese are all Ω(_n_2):\n\nThese are not:\n\n  * _n_2\n  * _n_2 \\+ 1000n   _(It's also O(_n_2)!)_\n_\n\n  * 1000_n_2 \\+ 1000_n_\n  * 1000_n_2 \\- 1000_n_\n  * _n_3\n  * _n_2.00001\n__ _\n\n  * _n_1.99999\n  * _n_\n  * lg _n_\n\n#### Insertion Sort Example\n\nWe can show that insertion will take at least Ω(_n_) time in the best case\n(i.e., it won't get any better than this) using the above formula and\ndefinition.\n\n![](fig/equation-insertion-best.jpg)\n\n_T_(_n_) can be expressed as _pn - q_ for suitable _p, q_ (e.g., _q_ = _c_2 \\+\n_c_4 \\+ _c_5 \\+ _c_8, etc.). (In this case, _p_ and _q_ are positive.) This\nsuggests that _T_(_n_) ∈ Ω(_n_), that is, ∃ _c, n0_ s.t. _pn - q ≥ cn,_ ∀_n ≥\nn0_. This follows from the generalized proof for polynomials.\n\n* * *\n\n## Θ (Theta, asymptotic =)\n\nWe noted that there are _ loose _ bounds, such as _f_(_n_) = _n_2 is O(_n_3),\netc., but this is an overly pessimistic assessment. It is more useful to have\nan **asymptotically tight bound** on the growth of a function. In terms of\nalgorithms, we would like to be able to say (when it's true) that a given\ncharacteristic such as run time grows _no better and no worse_ than a given\nfunction. That is, we want to simultaneoulsy bound from above and below.\nCombining the definitions for O and Ω:\n\n![](fig/graph-Theta.jpg)\n\n> Θ(_g_(_n_)) = {_f_(_n_) : ∃ positive constants **_c_1, _c_2**, and _n_0 such\nthat 0 ≤ **_c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)**, ∀ _n_ ≥ _n_0}.\n\nAs illustrated, _g_(_n_) is an asymptotically tight bound for _f_(_n_): after\na point, _f_(_n_) grows no faster and no slower than _g_(_n_).\n\nThe book suggests the proof of this theorem as an easy exercise (just combine\nthe two definitions):\n\n> _f_(_n_) = Θ(_g_(_n_)) iff _f_(_n_) = Ω(_g_(_n_)) ∧ _f_(_n_) = O(_g_(_n_)).\n\nYou may have noticed that some of the functions in the list of examples for\nbig-O are also in the list for Ω. This indicates that the set Θ is not empty.\n\n### Examples\n\n> _Reminder:_ _f_(_n_) = Θ(_g_(_n_)) iff ∃ positive constants _c_1, _c_2, and\n_n_0 such that 0 ≤ _c_1_g_(_n_) ≤ _f_(_n_) ≤ _c_2_g_(_n_)∀ _n_ ≥ _n_0.\n\n_n_2 \\- 2_n_ is Θ(_n_2),   with _c_1 = 1/2; _c_2 = 1, and _n_0 = 4,   since:\n\n> _n_2/2   ≤   _n_2 \\- 2_n_   ≤   _n_2   for _n_ ≥ _n_0 = 4.\n\n#### Find an asymptotically tight bound (Θ) for\n\n  * 4_n_3\n  * 4_n_3 \\+ 2_n_. \n\nPlease try these before you [find solutions\nhere](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-03\n/Example-Analysis.html).\n\n#### What's in and what's out\n\nThese are all Θ(n2):\n\nThese are not\n\n  * _n_2\n  * _n_2 \\+ 1000_n_\n  * 1000_n_2 \\+ 1000_n_ \\+ 32,700\n  * 1000_n_2 \\- 1000_n_ \\- 1,048,315\n\n  * _n_3\n  * _n_2.00001\n  * _n_1.99999\n  * _n_ lg _n_\n\n* * *\n\n## Asymptotic Inequality\n\nThe O and Ω bounds may or may not be asymptotically tight. The next two\nnotations are for upper bounds that are strictly _not_ asymptotically tight.\nThere is an _analogy_ to inequality relationships:\n\n  * \"≤\" is to \"<\" as big-O (may or may not be tight) is to little-o (strictly not equal) \n  * \"≥\" is to \">\" as Ω (may or may not be tight) is to little-ω (strictly not equal). \n\n### o-notation (\"little o\", asymptotic <)\n\n> o(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _f_(_n_) **<** _cg_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/o-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _insignificant_ relative to _g_(_n_) as _n_\napproaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically smaller** than _g_(_n_) if _f_(_n_) =\no(_g_(_n_))\n\n  * _n_1.99999 ∈ o(_n_2)\n  * _n_2/lg _n_ ∈ o(_n_2)\n  * _n_2 ∉ o(_n_2) (similarly, 2 is not less than 2)\n  * _n_2/1000 ∉ o(_n_2) \n\n### ω-notation (\"little omega\", asymptotic >)\n\n> ω(_g_(_n_)) = {_f_(_n_) : ∀ constants _c_ > 0, ∃ constant _n_0 > 0 such that\n0 ≤ _cg_(_n_) **<** _f_(_n_) ∀ _n_ ≥ _n_0}.\n\n![](fig/omega-limit-definition.jpg)\n\nAlternatively, _f_(_n_) becomes _ arbitrarily large _ relative to _g_(_n_) as\n_n_ approaches infinity (see box):\n\nWe say that _f_(_n_) is **asymptotically larger** than _g_(_n_) if _f_(_n_) =\nω(_g_(_n_))\n\n  * _n_2.00001 ∈ ω(_n_2)\n  * _n_2lg _n_ ∈ ω(_n_2)\n  * _n_2 ∉ ω(_n_2)\n\nThe two are related:   **_f_(_n_) ∈ ω(_g_(_n_))   iff   _g_(_n_) ∈\no(_f_(_n_)).**\n\n* * *\n\n## Asymptotic Notation in Equations\n\nWe already noted that while asymptotic categories such as Θ(_n_2) are sets, we\nusually use \"=\" instead of \"∈\" and write (for example) _f_(_n_) = Θ(_n_2) to\nindicate that _f_ is in this set.\n\nPutting asymptotic notation in equations lets us do shorthand manipulations\nduring analysis.\n\n### Asymptotic Notation on Right Hand Side: ∃\n\nO(_g_(_x_)) on the right hand side stands for _some_ anonymous function in the\nset O(_g_(_x_)).\n\n> 2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ Θ(_n_)     means:  \n2_n_2 \\+ 3_n_ \\+ 1 = 2_n_2 \\+ _f_(_n_)       for _some_ __f_(_n_) ∈ Θ(_n_)_\n(in particular, _f_(_n_) = 3_n_ \\+ 1).\n\n### Asymptotic Notation on Left Hand Side: ∀\n\nThe notation is only used on the left hand side when it is also on the right\nhand side.\n\nSemantics: No matter how the anonymous functions are chosen on the left hand\nside, there is a way to choose the functions on the right hand side to make\nthe equation valid.\n\n> 2_n_2 \\+ Θ(_n_) = Θ(_n_2)     means   for _all_ _f_(_n_) ∈ Θ(_n_), there _\nexists_ _g_(_n_) ∈ Θ(_n_2) such that  \n2_n_2 \\+ _f_(_n_) = _g_(_n_).\n\n### Combining Terms\n\nWe can do basic algebra such as:\n\n> 2_n_2 \\+ 3_n_ \\+ 1   =   2_n_2 \\+ Θ(_n_)   =   Θ(_n_2)\n\n* * *\n\n## Properties\n\nIf we keep in mind the analogy to inequality, many of these make sense, but\nsee the end for a caution concerning this analogy.\n\n### Relational Properties\n\n**Transitivity**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_)) and _g_(_n_) = Θ(h(n))   ⇒   _f_(_n_) = Θ(h(n)).\n  * _f_(_n_) = O(_g_(_n_)) and _g_(_n_) = O(h(n))   ⇒   _f_(_n_) = O(h(n)).\n  * _f_(_n_) = Ω(_g_(_n_)) and _g_(_n_) = Ω(h(n))   ⇒   _f_(_n_) = Ω(h(n)).\n  * _f_(_n_) = o(_g_(_n_)) and _g_(_n_) = o(h(n))   ⇒   _f_(_n_) = o(h(n)).\n  * _f_(_n_) = ω(_g_(_n_)) and _g_(_n_) = ω(h(n))   ⇒   _f_(_n_) = ω(h(n)).\n**Reflexivity**:\n    \n\n  * _f_(_n_) = Θ(_f_(_n_))\n  * _f_(_n_) = O(_f_(_n_))\n  * _f_(_n_) = Ω(_f_(_n_))\n  * _What about o and ω?_\n**Symmetry**:\n    \n\n  * _f_(_n_) = Θ(_g_(_n_))   iff   _g_(_n_) = Θ(_f_(_n_)) \n  * _Should any others be here? Why or why not?_\n**Transpose Symmetry**:\n    \n\n  * _f_(_n_) = O(_g_(_n_))   iff   _g_(_n_) = Ω(_f_(_n_)) \n  * _f_(_n_) = o(_g_(_n_))   iff   _g_(_n_) = ω(_f_(_n_)) \n\n### Incomparability\n\nHere is where the analogy to numeric (in)equality breaks down: There is no\ntrichotomy. Unlike with constant numbers, we can't assume that one of <, =, >\nhold. Some functions may be incomparable.\n\nExample: _n_1 + _sin n_ is incomparable to _n_ since _sin n_ oscillates\nbetween -1 and 1, so 1 + _sin n_ oscillates between 0 and 2. _(Try graphing\nit.)_\n\n* * *\n\n## Common Functions and Useful Facts\n\nVarious classes of functions and their associated notations and identities are\nreviewed in the end of the chapter: please review the chapter and refer to ICS\n241 if needed. Here we highlight some useful facts:\n\n### Monotonicity\n\n  * _f_(_n_) is **monotonically increasing**   if   _m_ ≤ _n_   ⇒   _f_(_m_) ≤ _f_(_n_).\n  * _f_(_n_) is **monotonically decreasing**   if   _m_ ≥ _n_   ⇒   _f_(_m_) ≥ _f_(_n_).\n  * _f_(_n_) is **strictly increasing**   if   _m_ < _n_   ⇒   _f_(_m_) < _f_(_n_).\n  * _f_(_n_) is **strictly decreasing**   if   _m_ > _n_   ⇒   _f_(_m_) > _f_(_n_).\n\n### Polynomials\n\n  * _p_(_n_) = Θ(_n__d_), for asymptoptically positive polynomials in _n_ of degree _d_\n\n### Exponentials\n\n  * _n__b_ = o(_a__n_) for all real constants _a_ and _b_ such that _a_ > 1: **_Any exponential function with a base greater than 1 grows faster than any polynomial function._**\n  \n\n  * Useful identities: \n    * _a_-1 = 1/_a_\n    * (_a__m_)_n_ = _a__mn_\n    * _a__m__a__n_ = _a__m_ \\+ _n_\n\n### Logarithms\n\n  * (lg _n_)_b_ = lg_b__n_ = o(_n__a_), for a > 0: **_any positive polynomial function grows faster than any polylogarithmic function._**\n  \n\n  * Useful identities: \n    * _a_ = _b_log_b__a_   _(Definition of logs.)_\n    * log_a__n_ = log_b__n_/log_b__a_     \n    _(Base change. If _n_ is variable and _a_ and _b_ are constant, the denominator is constant: this is why asymptotic analysis can ignore the base.)_\n    * log_c_(_ab_) = log_c__a_ \\+ log_c__b_   _(Ask your slide rule!)_\n    * log_b__a_n__ = _n_ log_b__a_\n    * log_b_(1/_a_) = −log_b__a_\n    * log_b__a_ = 1 / log_a__b_\n    * _a_log_b__c_ = _c_log_b__a_   _(Useful for getting the variable where you want it.)_\n\n### Factorials\n\n  * _n_! = ω(2_n_):   _**factorials grow faster than exponentials** (but it could be worse):_\n  * _n_! = o(_n__n_)\n  * lg(_n_!) = Θ(_n_ lg _n_)\n  * See also the more complex **Stirling's approximation** from which these are derived.\n\n### Iterated Functions\n\n  * Definition: _f_(_i_)(_n_) is _f_ applied _i_ times to the initial value _n_. \n  * Iterated Logarithm: lg*_n_ = min{_i_ ≥ 0: lg(_i_)_n_ ≤ 1}   _(The iteration at which lg(_i_)_n_ is less than 1: a very slowly growing function.)_\n\n### Fibonacci Numbers\n\n  * Definition: _F_0 = 0; _F_1 = 1; and for _i_ > 1 _F__i_ = _F__i_-1 \\+ _F__i_-2. \n  * **_Fibonacci numbers grow exponentially._**\n\n* * *\n\nDan Suthers Last modified: Sat Jan 25 03:51:57 HST 2014  \nImages are from the instructor's manual for Cormen et al.  \n\n",
 "path"=>"morea//03.growth/reading-notes-3.md"}
</pre>

<h2>/morea/03.growth/reading-screencast-3a.html</h2>

<pre>Hash
{"title"=>"Asymptotic notations",
 "published"=>true,
 "morea_id"=>"reading-screencast-3a",
 "morea_summary"=>"Notations for this analysis",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=y86z2OrIYQQ",
 "morea_labels"=>["Screencast", "Suthers", "12 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-screencast-3a.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-screencast-3a.md"}
</pre>

<h2>/morea/03.growth/reading-screencast-3b.html</h2>

<pre>Hash
{"title"=>"Omega and Theta",
 "published"=>true,
 "morea_id"=>"reading-screencast-3b",
 "morea_summary"=>"The omega and theta notations",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=euEquYjVVcQ",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-screencast-3b.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-screencast-3b.md"}
</pre>

<h2>/morea/03.growth/reading-screencast-3c.html</h2>

<pre>Hash
{"title"=>"little-o and omega",
 "published"=>true,
 "morea_id"=>"reading-screencast-3c",
 "morea_summary"=>"The little guys, properties, and use in equations",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=uaqLI449XQw",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-screencast-3c.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-screencast-3c.md"}
</pre>

<h2>/morea/03.growth/reading-screencast-3d.html</h2>

<pre>Hash
{"title"=>"Common Functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-3d",
 "morea_summary"=>"Common functions and useful identities",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=f2czg61AtQw",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-screencast-3d.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-screencast-3d.md"}
</pre>

<h2>/morea/03.growth/reading-screencast-mit-2.html</h2>

<pre>Hash
{"title"=>"Introduction to algorithms, Lecture 2",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-2",
 "morea_summary"=>
  "Asymptotic notation, recurrences, substitution, master method (start at 16 min)",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec02/",
 "morea_labels"=>["Screencast", "Demaine", "71 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/03.growth/reading-screencast-mit-2.html",
 "content"=>"",
 "path"=>"morea//03.growth/reading-screencast-mit-2.md"}
</pre>

<h2>/morea/04.adt/experience-asymptotic-homework.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis: Homework",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Practice asymptotic analysis.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/04.adt/experience-asymptotic-homework.html",
 "url"=>"/morea/04.adt/experience-asymptotic-homework.html",
 "content"=>
  "## Homework Problems\n\n_There are 6 problems for a total of 20 points. Most of them are easier than\nthey look at first._\n\n_Please consider:_ The homework is really \"worth\" a lot more than 20 points\nbecause exams have similar problems. If you have not practiced with the\nhomeworks, you'll do worse on the exams. So, if you think these problems are\nnot worth the time for the points, think again.\n\n### #1. Peer Credit Assignment\n\n#### 1 Point Extra Credit for replying\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### #2. Proving asymptotic complexity\n\n#### 4 points\n\nUsing the truth-condition definition of big-O, prove that 3_n_2 \\+ 9 =\nO(_n_2).\n\n> The definition is the one that starts with \"_f_(_n_) = O(_g_(_n_)) iff ...\".\nYou will have to choose suitable _c_ and _n_0, plug them into the definition\nin \"...\", and argue that the condition is met.  \nThe 4 points are: (1) identification of _c_ and _n_0 that work; (1) writing\nout the definition, and (2) demonstrating that it is satsified as _n_ grows\nbeyond _n_0 (not just for _n_ = _n_0).\n\n* * *\n\n### #3. Relative growth rates of functions\n\n#### 3 points\n\nContinuing in the style of Tuesday's class exercise, fill in the table for\nthese pairs of functions with \"Yes\" or \"No\" in each empty box. Then, for each\nrow, justify your choice, preferably by showing mathematical relationships\n(e.g., transforming one expression into another, or into expressions that are\nmore easily compared).\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Asymptotic Relations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">f(n)</th>\n    <th scope=\"col\">g(n)</th>\n    <th scope=\"col\">O?</th>\n    <th scope=\"col\">o?</th>\n    <th scope=\"col\">&#937;?</th>\n    <th scope=\"col\">&#969;?</th>\n    <th scope=\"col\">&#920;?</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">e.</th>\n    <td>4<i>n</i><sup>2</sup></td>\n    <td>4<sup>lg <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">f.</th>\n    <td>2<sup>lg <i>n</i></sup></td>\n    <td>lg<sup>2</sup><i>n</i></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">g.</th>\n    <td>&#8730;<i>n</i></td>\n    <td>n<sup>sin <i>n</i></sup></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n### #4. Complexity of Dynamic Set Operations in List Implementations\n\n#### 2 points\n\nFor each of the four types of lists in the following table, what is the\nasymptotic worst-case running time for `predecessor` and `maximum`?\n\nAssume that _k_ is the key, _d_ the data associated with the key, and _p_ a\nposition in the data structure. This version of the ADT is similar to the\nbook's, but abstracts list elements _x_ to positions (returned by search).\n`predecessor` and `maximum` are with respect to ordering of keys in the set\nunder \"<\", NOT ordering of the data structure. Sorted lists are sorted in\nascending order. This continues your in-class work, which you may want to\nreview for correctness first.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  (continued)\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th colspan=\"5\" scope=\"row\"><i>... others done in class here ... </i></th>\n  </tr>\n  <tr>\n    <th scope=\"row\">predecessor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">maximum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n\n\n\n### #5. Tree Traversals\n\n#### 4 points\n\n_In class you wrote a recursive procedure for traversal of a binary tree in\nO(n) time, printing out the keys of the nodes. Here you write two other tree\ntraversal procedures. The first is a variation of what you wrote in class; the\nsecond is on a different kind of tree that you read about pages 248-249 and in\nmy lecture notes and screencast._\n\n**(a)** Write an O(_n_)-time **_non-recursive_** procedure that, given an _n_-node binary tree, prints out the key of each node of the tree in whatever order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your procedure takes a `TreeNode` as its argument (the root of the tree). **_Use a stack as an auxiliary data structure._**\n\n>     printBinaryTreeNodes(TreeNode root) {\n\n\n**(b)** Write an O(_n_)-time procedure that, given an _n_-node rooted tree with **_arbitrary_** number of children using the **_left-child, right-sibling representation_**, prints out the key of each node of the tree in whatever order you wish. Assume that for economy of code we are re-using our `TreeNode` class. The instance variable `left` points to the left child as before, but now `right` points to the right sibling instead of the right child (which is no longer unique). Your procedure takes a `TreeNode` as its argument (the root of the tree). You may choose to use either the recursive or non-recursive approach.\n\n>     printLCRSTreeNodes(TreeNode root) {\n\n\n### #6. A Hybrid Merge/Insertion Sort Algorithm\n\n#### 7 points\n\nAlthough MergeSort runs in Θ(_n_ lg _n_) worst-case time and InsertionSort\nruns in Θ(_n_2) worst-case time, the constant factors in insertion sort\n(including that fact that it can sort in-place) can make it faster in practice\nfor small problem sizes on many machines. Thus, it makes sense to\n**_coarsen_** the leaves of the MergeSort recursion tree by using\nInsertionSort within MergeSort when subproblems become sufficiently small.\n\nConsider a modification to MergeSort in which _n_/_k_ sublists of length _k_\nare sorted using InsertionSort and are then merged using the standard merging\nmechanism, where _k_ is a value to be determined in this problem. In the first\ntwo parts of the problem, we get expressions for the contributions of\nInsertionSort and MergeSort to the total runtime as a function of the input\nsize _n_ and the cutoff point between the algorithms _k_.\n\n**(a - 1pt)** Show that InsertionSort can sort the _n_/_k_ sublists, each of length _k_, in Θ(_nk_) worst-case time. To do this:\n\n  1. write the cost for sorting _k_ items with InsertionSort,\n  2. multiply by how many times you have to do it, and \n  3. show that the expression you get simplifies to Θ(_nk_).\n\n**(b - 3pts)** Show that MergeSort can merge the _n_/_k_ sublists of size _k_ in Θ(_n_ lg (_n_/_k_)) worst-case time. To do this: \n\n  1. draw the recursion tree for the merge (a modification of figure 2.5), \n  2. determine how many elements are merged at each level, \n  3. determine the height of the recursion tree from the _n_/_k_ lists that InsertionSort had already taken care of up to the single list that results at the end, and \n  4. show how you get the final expression Θ(_n_ lg (_n_/_k_)) from these two values. \n\n_**Putting it together:**_ The asymptotic runtime of the hybrid algorithm is\nthe sum of the two expressions above: the cost to sort the _n_/_k_ sublists of\nsize _k_, and the cost to divide and merge them. You have just shown this to\nbe\n\n> Θ(_n__k_ \\+ _n_ lg (_n_/_k_))\n\nIn the second two parts of the question we explore what _k_ can be.\n\n**(c - 2pts)** The bigger we make _k_ the bigger lists InsertionSort has to sort. At some point, its Θ(_n_2) growth will overcome the advantage it has over MergeSort in lower constant overhead. How big can _k_ get before InsertionSort starts slowing things down? Derive a theoretical answer by proving the largest value of _k_ for which the hybrid sort has the same Θ runtime as a standard Θ(_n_ lg _n_) MergeSort. This will be an upper bound on _k_. To do this: \n\n  1. Looking at the expression for the hybrid algorithm runtime Θ(_n__k_ \\+ _n_ lg (_n_/_k_)), identify the upper bound on _k_ expressed as a function of _n_, above which Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) would grow faster than Θ(_n_ lg _n_). _Give the _f_ for _k_ = Θ(_f_(_n_)) and argue for why it is correct._\n  2. Show that this value for _k_ works by substituting it into Θ(_n__k_ \\+ _n_ lg (_n_/_k_)) and showing that the resulting expression simplifies to Θ(_n_ lg _n_). \n\n**(d - 1pt)** Now suppose we have implementations of InsertionSort and MergeSort. How should we choose the optimal value of _k_ to use for these given implementations in practice? \n\n",
 "path"=>"morea//04.adt/experience-asymptotic-homework.md"}
</pre>

<h2>/morea/04.adt/experience-basic-data-structures.html</h2>

<pre>Hash
{"title"=>"Asymptotic analysis of basic data structures",
 "published"=>true,
 "morea_id"=>"experience-asymptotic-basic-data-structures",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Practice analysis of basic data structures with respect to their limiting behavior",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/04.adt/experience-basic-data-structures.html",
 "url"=>"/morea/04.adt/experience-basic-data-structures.html",
 "content"=>
  "## Basic Data Structures\n\n####  5 points\n\n**1\\. (4 pts)** For each of the four types of lists in the following table, what is the _asymptotic worst-case running time_ for each dynamic set operation listed, given a list of length _n_? \n\n  * Assume that __k_ is the key_, _d_ the data associated with the key, and __p_ a position_ in the data structure.\n  * As explained in class, positions abstract and provide encapsulated access to elements of a data structure. We don't have to search for the item if we have a position p for it.\n  * Sorted lists are sorted in _ascending order_.\n  * Predecessor, successor, minimum, and maximum are _with respect to ordering of keys in the set under \"<\"_, not necessarily ordering of the list data structure.\n  * The cells filled in are from the quiz.\n\n<table width=\"100%\" border=\"1\">\n  <caption>\n    Worst Case Linked List Operations\n  </caption>\n  <tbody><tr>\n    <th scope=\"col\">&nbsp;</th>\n    <th scope=\"col\">Unsorted, Singly Linked (no tail pointer)</th>\n    <th scope=\"col\">Sorted, Singly Linked (no tail ponter)</th>\n    <th scope=\"col\">Unsorted, Doubly Linked with Sentinel and Tail pointer</th>\n    <th scope=\"col\">Sorted, Doubly Linked with Sentinel and Tail pointer</th>\n  </tr>\n  <tr>\n    <th scope=\"row\">insert(k, d)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(1)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">search(k)</th>\n    <td>&nbsp;</td>\n    <td>&nbsp;</td>\n    <td>&#920;(n)</td>\n    <td>&#920;(n)</td>\n  </tr>\n  <tr>\n    <th scope=\"row\">delete(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">successor(p)</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n  <tr>\n    <th scope=\"row\">minimum()</th>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n    <td><strong>&nbsp;</strong></td>\n  </tr>\n</tbody></table>\n  \n\n**2\\. (1 pt)** Write a Θ(_n_)-time recursive procedure that, _given an _n_-node binary tree, prints out the key of each node of the tree_ in any order you wish. Assume that trees consist of vertices of class `TreeNode` with instance variables `parent`, `left`, `right`, and `key`. Your recursive procedure takes a `TreeNode` as its argument (the root of the tree or subtree being considered by the recursive call).\n    \n    \n    **\n    printTreeNodes(TreeNode root)\n        if (root != null) {\n    \n    \n    **\n\n\n",
 "path"=>"morea//04.adt/experience-basic-data-structures.md"}
</pre>

<h2>/morea/04.adt/module-adt.html</h2>

<pre>Hash
{"title"=>"Abstract data types",
 "published"=>true,
 "morea_id"=>"adt",
 "morea_outcomes"=>["outcome-adt"],
 "morea_readings"=>
  ["reading-screencast-4a",
   "reading-screencast-4b",
   "reading-cormen-10",
   "reading-notes-4"],
 "morea_experiences"=>
  ["experience-asymptotic-basic-data-structures",
   "experience-asymptotic-homework"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/04.adt/module-adt.png",
 "morea_sort_order"=>4,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/04.adt/module-adt.html",
 "content"=>"Stacks, queues, lists, and trees.\n",
 "path"=>"morea//04.adt/module-adt.md"}
</pre>

<h2>/modules/adt/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-adt.md",
 "title"=>"Abstract data types",
 "url"=>"/modules/adt/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/adt/index.html"}
</pre>

<h2>/morea/04.adt/outcome-adt.html</h2>

<pre>Hash
{"title"=>
  "Understand definition, implementation, and behavior of abstract data types.",
 "published"=>true,
 "morea_id"=>"outcome-adt",
 "morea_type"=>"outcome",
 "morea_sort_order"=>5,
 "referencing_modules"=>[#Jekyll:Page @name="module-adt.md"],
 "url"=>"/morea/04.adt/outcome-adt.html",
 "content"=>
  "Gain further practice in algorithm analysis through examination of stacks, queues, lists, and trees.",
 "path"=>"morea//04.adt/outcome-adt.md"}
</pre>

<h2>/morea/04.adt/reading-cormen-10.html</h2>

<pre>Hash
{"title"=>"CLRS 10 - Elementary Data Structures",
 "published"=>true,
 "morea_id"=>"reading-cormen-10",
 "morea_summary"=>
  "Stacks, queues, linked lists, pointers and objects, rooted trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "21 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/04.adt/reading-cormen-10.html",
 "content"=>"",
 "path"=>"morea//04.adt/reading-cormen-10.md"}
</pre>

<h2>/morea/04.adt/reading-notes-4.html</h2>

<pre>Hash
{"title"=>"Chapter 4 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-4",
 "morea_summary"=>"Introduction to abstract data types",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/04.adt/reading-notes-4.html",
 "url"=>"/morea/04.adt/reading-notes-4.html",
 "content"=>
  "## Outline\n\n  1. Stacks \n  2. Queues \n  3. Lists \n  4. First peek at Trees \n  5. Dynamic Set ADT\n\nHere we review some basic Abstract Data Types that organize information in\nuseful ways. This should be review, so will be covered briefly, although some\nnuances of implementation are discussed and we will also do asymptotic\nanalyses of the main operations of implementations.\n\n## Stacks\n\nStacks follow the **Last In, First Out (LIFO)** principle. They are useful\nwhen a problem has goal-subgoal structure, and we need to keep track of higher\nlevel goals or processes when we set them aside to pursue subgoals or sub-\nprocesses (e.g., the run-time stack of a computer operating system, or keeping\ntrack of neighbor vertices yet to be visited when searching a graph).\n\n### Stack ADT\n\nWe start by specifying the desired behavior of stacks before looking at\nimplementations. Here's the Stack ADT written as a simple Java interface:\n\n    \n    \n      public interface **Stack** {\n      // ADT that stores and retrieves Objects in a LIFO manner\n    \n         public **Stack**( ); \n         // Create an instance of ADT Stack and initialize it to the empty stack.\n    \n         public void **push**(Object o); \n         // Insert object o at the top of the stack.\n    \n         public Object **pop**( ); \n         // Remove and return the top (most recently pushed) object on the stack.\n         // Error occurs if the stack is empty. \n     \n         public int **size**( ); \n         // Return the number of objects in the stack.\n    \n         public boolean **isEmpty**( ); \n         // Return a boolean indicating whether the stack is empty.\n    \n         public Object **top**( ); \n         // Return the top (most recently pushed) object on the stack, without \n         // removing it. Error occurs if the stack is empty.\n      }\n    \n\n**Properties**, given `s` a stack instance:\n\n  1. { `push(_s_,_e_); _s_.top()` } returns value `_e_`\n  2. { `push(_s_,_e_); _s_.pop()` } returns value `_e_` and leaves `_s_` in the same state \n  3. { `_s_ = new(); _s_.isEmpty() } returns true `\n  4. { `push(_s_,_i_); _s_.isEmpty() } returns false `\n  5. if `s.isEmpty()` then `s.top()` is an error, and does not change `s`\n  6. if `s.isEmpty()` then `s.pop()` is an error, and does not change `s`\n  7. if `s.isEmpty()` then `s.size() == 0`\n  8. if `s.size() == _n_` then after ` s.push(o), s.size() == _n_+1`\n  9. if `¬s.isEmpty()` and `s.size() == _n_` then after `s.pop(o), s.size() == _n_-1.`\n\n_What is the relationship of stacks to method execution in the Java Virtual\nMachine?_\n\n_What is the relationship of stacks to recursion?_\n\n### Array Implementation\n\nAssume instance variables (fields) of object array `S` and `int top`. The\nthree essential operations follow. (I am modifying the book's pseudocode\nslightly.)\n\n![](fig/Fig-10-1-a-Stack.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if top == 0 \n      2       return TRUE\n      3     else\n      4       return FALSE\n    \n      void **push**(Object o)\n      1     top = top + 1\n      2     S[top] = o                // what might happen here?\n    \n      Object **pop**( )\n      1     if isEmpty()\n      2       error \"stack underflow\" // or throw new StackException (...) \n      3     else\n      4       top = top - 1\n      5       return S[top+1]         // we comment on this later \n    \n\n_What is the asymptotic complexity of these operations?_\n\nThe potential error in `push` is an implementation concern outside of the\nscope of the _logical_ definition of the stack ADT. How might it be handled?\n\n#### Example\n\nLet's start with this stack:\n\n![](fig/Fig-10-1-a-Stack.jpg)  \n  \n\nPush 17, and then 3:\n\nPop once:\n\n![](fig/Fig-10-1-b-Stack.jpg)\n\n![](fig/Fig-10-1-c-Stack.jpg)\n\n_What is the status of S[top+1] after pop returns? Why might that be a\nproblem?_\n\n####  An Improvement\n\n    \n    \n      Object **pop**( )  // version that dereferences objects for garbage collection\n      1     if isEmpty()\n      2       error \"stack underflow\" \n      3     else\n      4       o = S[top]\n      5       S[top] = null  // don't keep references to objects not really there \n      6       top = top - 1\n      7       return o \n    \n\n* * *\n\n## Queues\n\nQueues operate in a **First In, First Out (FIFO)**, like what the British call\na \"queue\" at the post office or bank. They are also very useful for managing\nprioritization of tasks in computing.\n\n### Queue ADT\n\nAgain, expressed as a simple Java interface:\n\n    \n    \n      public interface **Queue**{\n      // ADT that stores and retrieves Objects in a FIFO manner\n    \n        public **Queue**( ); \n        // Create an instance of ADT Queue and initialize it to the empty queue.\n    \n        public void **enqueue**(Object o); \n        // Insert object o at the rear of the queue.\n    \n        public Object **dequeue**( );\n        // Remove and return the frontmost (least recently queued) object from the queue. \n        // queue. Error occurs if the queue is empty.\n    \n        public int **size**( ); \n        // Return the number of objects in the queue.\n    \n        public boolean **isEmpty**( ); \n        // Return a boolean indicating whether the queue is empty.\n    \n        public Object **front**( ); \n        // Return the front (least recently queued) object in the queue, without \n        // removing it. Error occurs if the queue is empty.\n      }\n    \n\n**Properties** (given `q` a queue instance): are very similar to those for Stack, except for operations where ordering matters (FIFO rather than LIFO). Replace the first two properties for Stack with:\n\n  1. if `q.enqueue(o1) ` occurs before `q.enqueue(o2)` then successive `q.dequeue()` returns `o1` before `o2`\n  2. `q.front() ` returns the least recently enqueued element that has not been dequeued.\n\nThen rewrite the other properties with substitution `{enqueue/push,\ndequeue/pop, front/top}`.\n\n###  Array Implementation\n\nAssume three instance variables (fields): object array `Q`; `int head`\nindexing the next element to dequeue; and `int tail` indexing the next place a\nnew element may be placed.\n\n![](fig/Fig-10-2-a-Queue.jpg)\n\n    \n    \n      boolean **isEmpty** ( ) \n      1     if head == tail\n      2       return TRUE\n      3     else\n      4       return FALSE\n      \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     if tail == length  \n      3       tail = 1           // wrap around\n      4     else\n      5       tail = tail + 1\n      \n      Object **dequeue**( )     \n      1     o = Q[head]\n      2     if head == length\n      3       head = 1\n      4     else \n      5       head = head + 1\n      6     return o\n    \n\nThe queue is full when `head == tail + 1`; an error results if enqueue is\ncalled (again, this is an implementation concern outside the logical\ndefinition of the ADT).\n\n#### Example\n\nBeginning with this Queue:\n\n![](fig/Fig-10-2-a-Queue.jpg)  \n  \n\nEnqueue 17, 3 and 5 (notice wrap-around):\n\nDequeue once:\n\n![](fig/Fig-10-2-b-Queue.jpg)\n\n![](fig/Fig-10-2-c-Queue.jpg)\n\nThe same issue concerning object dereferencing applies.\n\n#### Variation using modular arithmetic\n\nThis version handles dereferencing but does not check for overflow or\nunderflow. It assumes that the array index starts with 0, but can be changed\nfor 1-based indexing.\n\n    \n    \n      void **enqueue**(Object o) \n      1     Q[tail] = o\n      2     tail = (tail + 1) mod length // mod is % in Java \n      \n      Object **dequeue**( )\n      1     o = Q[head]\n      2     Q[head] = null               // allow garbage collection!\n      3     head = (head + 1) mod length \n      4     return o\n    \n\n_What is the asymptotic complexity of these operations?_\n\n### Deques\n\nOne can combine the stack and queue concepts into a double-ended queue (deque)\nthat allows insertion and deletion at both ends. O(1) procedures are possible\nfor all insertion and deletion algorithms.\n\n\n\n* * *\n\n## Lists\n\nLists store objects in linear order. We will assume that list elements have a\n`key` and may have other satellite data.\n\nIn an **unsorted** list, we assume no particular order to the elements (the\norder is arbitrary). In a **sorted** list or set, the elements are ordered by\nkey.\n\nA suitable ADT for lists will be given later, in the form of `DynamicSet`.\n\n### Linked Lists\n\n**Linked lists** use list element objects to hold the data (here in the form of a `key`), and record the linear order using `next` pointers. **Doubly linked lists** also have `prev` pointers.\n\n  * `L.head` points to the first element in the list.\n  * If `x.next == nil` then x is the last element of the list.\n  * If `x.prev == nil` then x is the first element of the list.\n\n_What are the advantages of adding `prev` pointers?_\n\nOur examples will assume List instance variables for `head` and `tail`, and\nListElement instance variables `key`, `next`, and `prev`. (Note: public\ninterfaces for ADTs would probably not expose listElement: see discussion\nunder Dynamic Sets later.)\n\n### Searching\n\nThe procedure for seaching is the same for singly and doubly linked lists:\n\n    \n    \n      ListElement **listSearch**(Key k)\n      1     e = head\n      2     while e ≠ null and e.key ≠ k\n      3       e = e.next \n      4     return e\n    \n\n![](fig/Fig-10-3-a-DLL.jpg)\n\n_What is returned if `k` is not in the list?_\n\n_What is the worst case complexity of this algorithm?_\n\n### Inserting and Deleting\n\nSince you are familiar with singularly linked lists from your previous\nstudies, we'll go direct to doubly linked lists, but recall that with singly\nlinked lists you had to be careful to keep track of the tail end of the list\nthat you had \"snipped off\" during an insertion or deletion. The same applies\nhere, but we also have to manage prev pointers.\n\n    \n    \n      void **listInsert**(ListElement e) // inserts at beginning of list\n      1     e.next = head\n      2     if head ≠ null\n      3       head.prev = e \n      4     head = e\n      5     e.prev = null\n    \n\nInserting 25:  \n![](fig/Fig-10-3-b-DLL.jpg)\n\n    \n    \n      void **listDelete**(ListElement e) // removes from list, wherever it is \n      1     if e.prev ≠ null\n      2       e.prev.next = e.next\n      3     else \n      4       head = e.next \n      5     if e.next ≠ null\n      6       e.next.prev = e.prev\n    \n\nDeleting the element keyed by 4:  \n![](fig/Fig-10-3-c-DLL.jpg)\n\n_What is the worst case complexity of these algorithms?_\n\n_What about garbage collection in listDelete? Same problem as for pop and\ndequeue?_\n\n### Circular DLLs with Sentinels\n\nCLRS discuss adding an extra **sentinel** element that marks the beginning of\nthe list and making the linked list circular so that we don't have to check\nfor null (falling off the end of the list). It also enables us to get to the\nend of the list quickly\n\nSentinels remove the need for a conditional test, but this only speeds up\noperations a small constant, at the cost of an extra listElement object per\nevery list. Their use is more compelling if you often need to go to the end of\nthe list.\n\nFor example, here is the above list as a circular doubly linked list. (`L.nil`\nreferences the sentinel.)\n\n![](fig/Fig-10-4-b-DLL-Sentinel.jpg)\n\n    \n    \n     \n      void **listInsert**(ListElement e) // Sentinel version \n      1     e.next = nil.next           \n      2     nil.next.prev = e \n      3     nil.next = e \n      5     e.prev = nil\n    \n\nInsert 25: ![](fig/Fig-10-4-c-DLL-Sentinel.jpg)\n\n_Let's insert something into the empty list ..._  \n![](fig/Fig-10-4-a-DLL-Sentinel.jpg)\n\n(Left for you to try.)\n\nYou might check your understanding by doing exercises 10.2-1, 10.2-2 and\n10.2-3.\n\n* * *\n\n## Array Representations of Lists\n\nWe generally do not need to be concerned with the topic of this section in\nmodern programming languages, but if you ever have to program in FORTRAN, the\nsection shows how to store objects such as listElement in arrays:\n\n![](fig/Fig-10-5-DLL-Array.jpg)\n\n... and how to manage your own **free list** of available listElements\n(languages like Java and LISP do this automatically, but (cue old fart voice)\n\"when I was your age ...\"). Here is an array with both a DLL and a free list\nembedded in it:\n\n![](fig/Fig-10-7-a-Allocate-Free.jpg)\n\nAfter allocating one free cell to add 7 to the front of the list:\n\nAfter deleting list item 2 at array position 5:\n\n![](fig/Fig-10-7-b-Allocate-Free.jpg)\n\n![](fig/Fig-10-7-c-Allocate-Free.jpg)\n\nOf course, someone has to implement the memory management, and there is a\nlarge literature on methods of **garbage collection**.\n\n* * *\n\n## Binary Trees (A First Look)\n\nTrees in general and binary trees in particular are _hugely_ important data\nstructures in computer science. There are many ways to represent them. A\nlinked represention provides great flexibility and is widely used. In a few\nweeks we'll also see how trees can be embedded in arrays.\n\nAssume that class `BinaryTree` has instance variable `root`, and it consists\nof vertices of class `TreeNode` with instance variables `parent`, `left` and\n`right`, as well as possibly other data.\n\n![](fig/Fig-10-9-Binary-Tree.jpg)\n\nIn a few weeks we will study methods for search, insertion and deletion in\nspecial types of tree, **heaps** and **binary search trees**.\n\n_Do you have any thoughts on what insertion and deletion might involve, in\ngeneral?_\n\n_Exercises:_  \n10.4-2: write an O(n) recursive procedure to visit (e.g., print out) the nodes\nof the tree.  \n10.4-3: write an O(n) non-recursive procedure to visit the nodes of the tree.\nUse a stack.\n\n* * *\n\n## N-ary Trees\n\nWe can represent n-ary trees by providing each node with a fixed number _n_\nchild fields (child1, child2, child3 ... childn). An equivalent approach is\nused for **b-trees,** which are used for efficient disk access.\n\nBut a fixed _n_ is only viable if we can bound the number of children, and can\nbe wasteful of memory if many nodes do not have _n_ children.\n\nAn alternative representation allows each TreeNode to have an arbitrary number\nof children while still using O(n) space.\n\n### Left-Child Right-Sibling Representation\n\nThis implementation has instance variable `root`, but consists of vertices\nthat are instances of a class we'll call LCRSTreeNode with instance variables\n`parent`, `left-child` and `right-sibling`, as well as possibly other data.\n(Alternatively, we can just use TreeNode, but understand `left` to refer to\nthe left-child and `right` to refer to the right sibling.)\n\n![](fig/Fig-10-10-LC-RS-Tree.jpg)\n\nA good practice problem is to write a procedure for visiting (printing out)\nall the nodes of these kinds of trees.\n\n\n\n* * *\n\n## Dynamic Set ADT\n\nAbove we have been reviewing basic data structures for keeping track of\nobjects under specific organizational schemes (e.g., FIFO, LIFO, sequential,\nand hierarchical).\n\nAnother organizational scheme is the **set** or **ordered set**. We often need\nto keep track of a set of objects, query it for membership, and possibly\nmodify the set dynamically. Other operations are also possible if the elements\nof the set are ordered.\n\nThese capabilities can be implemented in different ways. The Dynamic Set ADT\ncaptures the requirements that implementations must meet. Many of the ADTs\n(and their implementations as data structures and algorithms) we will study\ncan be seen as specializations of the Dynamic Set ADT.\n\n### Text's Dynamic Set ADT\n\nThe introduction to Part III of the textbook, page 230, gives this\nspecification:\n\nSEARCH(S; k)\n\n    A query that, given a set S and a key value k, returns a pointer x to an element in S such that x.key = k, or NIL if no such element belongs to S.\n  \nINSERT(S; x)\n\n    A modifying operation that augments the set S with the element pointed to by x. We usually assume that any attributes in element x needed by the set implementation have already been initialized.\n  \nDELETE(S; x)\n\n    A modifying operation that, given a pointer x to an element in the set S, removes x from S. (Note that this operation takes a pointer to an element x, not a key value.)\n  \nMINIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the smallest key.\n  \nMAXIMUM(S)\n\n    A query on a totally ordered set S that returns a pointer to the element of S with the largest key.\n  \nSUCCESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next larger element in S, or NIL if x is the maximum element.\n  \nPREDECESSOR(S; x)\n\n    A query that, given an element x whose key is from a totally ordered set S, returns a pointer to the next smaller element in S, or NIL if x is the minimum element.\n\nThere are some issues with this specification, particularly in the use of x.\n\n  * The specification seems to require that the client know about the the internal implementation of the set (\"We usually assume that any attributes in element x needed by the set implementation have already been initialized\").\n  * Alternatively, if the elements are client objects, the set implementation would have to know how to access these to get the key. \n\nA safer specification would give INSERT and DELETE the key k rather than the\nelement x, hiding implementation details and reducing dependencies between\nclient and ADT. This in turn leads to a performance problem, dicussed below,\nbut it can be resolved.\n\n### Encapsulated Dynamic Set ADT\n\nAn encapsulated version of the ADT is given as a Java interface below. It\ncommunicates with clients primarily through keys and associated elements that\nonly the client need understand.\n\n    \n    \n      public interface **DynamicSet** {\n      // ADT that stores and retrieves Objects according to keys of type KeyType\n     \n         public **DynamicSet**( ); \n         // Creates an instance of ADT DynamicSet and initializes it to the empty set.   \n     \n         public void **insert**(KeyType k; Object e); \n         // Inserts element e in the set under key k.\n     \n         public void **delete**(KeyType k); \n         // Given a key k, removes elements indexed by k from the set.\n     \n         public Object **search**(KeyType k); \n         // Finds an Object with key k and returns a pointer to it,\n         // or null if not found. \n     \n         // The following operations apply when there is a total ordering on KeyType   \n     \n         public Object **minimum**( ); \n         // Finds an Object that has the smallest key, and returns a pointer to it,\n         // or null if the set is empty. \n     \n         public Object **maximum**( ); \n         // Finds an Object that has the largest key, and returns a pointer to it,\n         // or null if the set is empty.\n     \n         public Object **successor**(KeyType k); \n         // Finds an Object that has the next larger key in the set above k, \n         // and returns a pointer to it, or null if k is the maximum element.\n     \n         public Object **predecessor**(KeyType k); \n         // Finds an Object that has the next smaller key in the set below k,\n         // and returns a pointer to it, or null if k is the minimum element.\n     }\n    \n\nAs hinted above, we may pay a cost for proper encapsulation. For example,\nsuppose an application must frequently pair `search` and `delete` operations\nto find elements we want to remove. If `search` cannot communicate the\nlocation found in the underlying datastructure to `delete`, then `delete` will\nhave to search again to find what to operate on.\n\nThis inefficiency could be eliminated by abstracting the concept of a\n**position** in a data structure, and passing around position objects that\nhide implementation details. This solution is not discussed here as it is more\nof a software engineering rather than algorithm design and analysis concern:\nsee Goodrich & Tamassia's Algorithms textbook for one approach.\n\n### Alternative Dynamic Set Implementations\n\nLinked lists can be used to support a viable Dynamic Set implementation for\nsmall sets, for example using `listInsert` and `listSearch` to implement\n`insert` and `search`, respectively.\n\nFuture Topics will present Hash Tables, Binary Search Trees, and Red-Black\nTrees as alternative implementations of DynamicSet. You will use some of these\nin your assignments (and often as a working professional), so need to\nunderstand them well.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:30:23 HST 2014  \nImages are from Cormen et al. Introduction to Algorithms, Third Edition.  \n\n",
 "path"=>"morea//04.adt/reading-notes-4.md"}
</pre>

<h2>/morea/04.adt/reading-screencast-4a.html</h2>

<pre>Hash
{"title"=>"Stacks, queues, and lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-4a",
 "morea_summary"=>"Basic abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=86QY8mBX7Ks",
 "morea_labels"=>["Screencast", "Suthers", "28 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/04.adt/reading-screencast-4a.html",
 "content"=>"",
 "path"=>"morea//04.adt/reading-screencast-4a.md"}
</pre>

<h2>/morea/04.adt/reading-screencast-4b.html</h2>

<pre>Hash
{"title"=>"Trees and dynamic sets",
 "published"=>true,
 "morea_id"=>"reading-screencast-4b",
 "morea_summary"=>"More on abstract data types.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=eECZ_lKXsHs",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/04.adt/reading-screencast-4b.html",
 "content"=>"",
 "path"=>"morea//04.adt/reading-screencast-4b.md"}
</pre>

<h2>/morea/05.probabilistic/experience-indicator-random-variables.html</h2>

<pre>Hash
{"title"=>"Indicator random variables: Homework",
 "published"=>true,
 "morea_id"=>"experience-indicator-random-variables",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about indicator random variables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/05.probabilistic/experience-indicator-random-variables.html",
 "url"=>"/morea/05.probabilistic/experience-indicator-random-variables.html",
 "content"=>
  "# Indicator Random Variable Analysis\n\nUse indicator random variables to compute the expected value of the sum of _n_\nrolls of a fair dice that has _s_ sides. A fair dice can have values from 1 to\n_s_ with equal probability. Do it in these steps, and answer the numbered\nquestions:\n\nDefine the _indicator_ random variable _X__i_ = I {the event of a dice coming\nup with value _i_}, for each _i_ = {1, 2, ... _s_}.\n\n**1.** What is Pr{_X__i_ = 1} for each i? \n\n**2.** What is E[_X__i_]? In other words, the expected value of _X__i_? \n\nDefine the _regular_ random variable _X_ to be the value of a single roll of a\ndice with _s_ sides.\n\n**3.** Write an equation expressing _X_ in terms of _X__i_.   _(Keep in mind that indicator random variables take on values 0 or 1.)_\n\n**4.** Take the expectation of both sides of this equation and solve for E[_X_], the expected value of _X_.   _(Show all steps, like was done in the derivation of the expected number of inversions.)_\n\n**5.** Use the result to write an expression for the expected value of _n_ rolls of an _s_-sided fair dice. \n\n# Additional Activity\n\nIf you finish the above early, this will get you started on future work.\n\nSuppose I assigned the _n_ students in a class randomly to groups, with no\nconstraint on group size, but I decided in advance to have _n_/4 groups.\n\n**6.** Let's pick two students from our class. Call them Michael Jackson and Bruno Mars. What is the probability that Michael and Bruno end up in the same group? Express as a function of _n_. \n\n\n",
 "path"=>"morea//05.probabilistic/experience-indicator-random-variables.md"}
</pre>

<h2>/morea/05.probabilistic/module-probabilistic.html</h2>

<pre>Hash
{"title"=>"Probabilistic Analysis",
 "published"=>true,
 "morea_id"=>"probabilistic",
 "morea_outcomes"=>["outcome-probabilistic"],
 "morea_readings"=>
  ["reading-screencast-5a",
   "reading-screencast-5b",
   "reading-screencast-5c",
   "reading-screencast-5d",
   "reading-screencast-mit-skip-lists",
   "reading-cormen-5",
   "reading-goodrich",
   "reading-notes-5"],
 "morea_experiences"=>["experience-indicator-random-variables"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/05.probabilistic/module-probabilistic.png",
 "morea_sort_order"=>5,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/05.probabilistic/module-probabilistic.html",
 "content"=>
  "Learn to analyze all cases based on a distribution of the probability of each case.\n",
 "path"=>"morea//05.probabilistic/module-probabilistic.md"}
</pre>

<h2>/modules/probabilistic/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-probabilistic.md",
 "title"=>"Probabilistic Analysis",
 "url"=>"/modules/probabilistic/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/probabilistic/index.html"}
</pre>

<h2>/morea/05.probabilistic/outcome-probabilistic.html</h2>

<pre>Hash
{"title"=>"Understand probabilistic analysis.",
 "published"=>true,
 "morea_id"=>"outcome-probabilistic",
 "morea_type"=>"outcome",
 "morea_sort_order"=>6,
 "referencing_modules"=>[#Jekyll:Page @name="module-probabilistic.md"],
 "url"=>"/morea/05.probabilistic/outcome-probabilistic.html",
 "content"=>
  "Understand when and how to analyze an algorithm based on a distribution of the probability of each case.",
 "path"=>"morea//05.probabilistic/outcome-probabilistic.md"}
</pre>

<h2>/morea/05.probabilistic/reading-cormen-5.html</h2>

<pre>Hash
{"title"=>"CLRS 5 - Probabilistic Analysis and Randomized Algorithms",
 "published"=>true,
 "morea_id"=>"reading-cormen-5",
 "morea_summary"=>
  "The hiring problem, indicator random variable, randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "16 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-cormen-5.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-cormen-5.md"}
</pre>

<h2>/morea/05.probabilistic/reading-goodrich.html</h2>

<pre>Hash
{"title"=>"Skip Lists in Java",
 "published"=>true,
 "morea_id"=>"reading-goodrich",
 "morea_summary"=>
  "Skip lists, from Goodrich and Tamassia's Data Structures and Algorithms in Java",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>
  "https://laulima.hawaii.edu/portal/tool/a8c355d6-b3af-4db8-a856-1713858f8720?panel=Main#",
 "morea_labels"=>["Textbook", "10 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-goodrich.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-goodrich.md"}
</pre>

<h2>/morea/05.probabilistic/reading-notes-5.html</h2>

<pre>Hash
{"title"=>"Chapter 5 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-5",
 "morea_summary"=>"Probabilistic analysis and randomized algorithms",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/05.probabilistic/reading-notes-5.html",
 "url"=>"/morea/05.probabilistic/reading-notes-5.html",
 "content"=>
  "## Outline\n\n  1. Probabilistic Analysis\n  2. Randomized Algorithms\n  3. Skip Lists \n\n## Motivations and Preview\n\nInstead of limiting analysis to best case or worst case, analyze all cases\nbased on a distribution of the probability of each case.\n\nWe implicitly used probabilistic analysis when we said that _given random\ninput_ it takes n/2 comparisons _on average_ to find an item in a linked list\nof n items.\n\n### Hiring Problem and Cost\n\nThe book's example is a little strange but illustrates the points well.\nSuppose you are using an employment agency to hire an office assistant.\n\n  * The agency sends you one candidate per day: interview and decide.\n  * Cost to interview is _c__i_ per candidate (fee to agency). \n  * Cost to hire is _c__h_ per candidate (includes firing prior assistant and fee to agency).\n  * _ch_ > _ci_\n  * You always hire the best candidate seen so far.\n    \n    \n      Hire-Assistant(n)\n      1  best = 0                // fictional least qualified candidate\n      2  for i = 1 to n\n      3    interview candidate i // paying cost  _ci_\n      4    if candidate i is better than candidate best\n      5      best = i\n      6      hire candidate i    // paying cost _ch_\n    \n\nWhat is the cost of this strategy?\n\n  * If we interview _n_ candidates and hire _m_ of them, cost is O(_cin_ \\+ _chm_)\n  * We interview all _n_ and _ci_ is small, so we focus on _chm_.\n  * _chm_ varies with each run and depends on interview order\n  * This is a common paradigm: finding the maximum or minimum in a sequence by examining each element, and changing the winner _m_ times.\n\n#### Best Case\n\nIf each candidate is worse than all who came before, we hire one candidate:  \n    O(_cin_ \\+ _ch_) = O(_cin_)\n\n#### Worst Case\n\nIf each candidate is better than all who came before, we hire all _n_ (_m_ =\n_n_):  \n    O(_cin_ \\+ _chn_) = O(_chn_) since _ch_ > _ci_  \nBut this is pessimistic. What happens in the average case?\n\n### Probabilistic Analysis\n\n  * We must know or make assumptions about the distribution of inputs.\n  * The expected cost is over this distribution.\n  * The analysis will give us **_average case_** running time.\n\nWe don't have this information for the Hiring Problem, but suppose we could\nassume that candidates come in random order. Then the analysis can be done by\ncounting permutations:\n\n  * Each ordering of candidates (relative to some reference ordering such as a ranking of the candidates) is equally likely to be any of the n! permutations of the candidates. \n  * In how many do we hire once? twice? three times? ... _n_−1 times? _n_ times?\n  * It depends on how many permutations have zero, one two ... _n_−2 or _n_−1 candidates that come before a better candidate.\n  * This is complicated!\n  * Instead, we can do this analysis with indicator variables (next section)\n\n### Randomized Algorithms\n\nWe might not know the distribution of inputs or be able to model it.\n\nInstead we _randomize_ within the algorithm to _impose_ a distribution on the\ninputs.\n\nAn algorithm is **randomized** if its behavior is determined in parts by\nvalues provided by a random number generator.\n\nThis requires a change in the hiring problem scenario:\n\n  * The employment agency sends us a list of _n_ candidates in advance and lets us choose the interview order.\n  * We choose randomly.\n\nThus we _take control_ of the question of whether the input is randomly\nordered: we _enforce_ random order, so the average case becomes the **_\nexpected value_**.\n\n* * *\n\n## Probabilistic Analysis with Indicator Random Variables\n\nHere we introduce technique for computing the expected value of a random\nvariable, even when there is dependence between variables. Two informal\ndefinitions will get us started:\n\nA **random variable** (e.g., _X_) is a variable that takes on any of a range\nof values according to a probability distribution.\n\nThe **expected value** of a random variable (e.g., E[_X_]) is the average\nvalue we would observe if we sampled the random variable repeatedly.\n\n###  Indicator Random Variables\n\nGiven sample space _S_ and event _A_ in _S_, define the **indicator random\nvariable**\n\n![](fig/indicator-random-variable.jpg)\n\nWe will see that indicator random variables simplify analysis by letting us\nwork with the probability of the values of a random variable separately.\n\n![](fig/lemming.jpg)\n\n#### Lemma 1\n\nFor an event _A_, let _XA_ = I{_A_}. Then the expected value **E[_XA_] =\nPr{_A_}** (the probability of event _A_).\n\n_Proof:_ Let ¬_A_ be the complement of _A_. Then\n\n> E[_XA_] = E[I{_A_}]   (by definition)  \n    = 1*Pr{_A_} + 0*Pr{¬_A_}   (definition of expected value)  \n    = Pr{_A_}. \n\n### Simple Example\n\nWhat is the expected number of heads when flipping a fair coin once?\n\n  * Sample space _S_ is {H, T}\n  * Pr{H} = Pr{T} = 1/2\n  * Define indicator random variable _X_H= I{H}, which counts the number of heads in one flip.\n  * Since Pr{H} = 1/2, Lemma 1 says that E[_X_H] = 1/2. \n\n### Less Simple Example\n\nWhat is the expected number of heads when we flip a fair coin _n_ times?\n\nLet _X_ be a random variable for the number of heads in _n_ flips.\n\nWe could compute E[_X_] = ∑_i_=0,_n__i_ Pr{_X_=_i_} \\-- that is, compute and\nadd the probability of there being 0 heads total, 1 head total, 2 heads total\n... n heads total, as is done in C.37 in the appendix and in my screencast\nlecture [5A](http://youtu.be/MgnvWTZgqcA) \\-- but it's messy!\n\nInstead use indicator random variables to count something we _do_ know the\nprobability for: the probability of getting heads when flipping the coin once:\n\n  * For _i = 1, 2, ... n_ define _Xi_ = I{the _i_th flip results in event H}.\n  * Then _X_ = ∑_i_=1,_n__Xi_.   _ (That is, count the flips individually and add them up.)_\n  * Lemma 1 says that E[_Xi_] = Pr{H} = 1/2 for _i = 1, 2, ... n_.\n  * Expected number of heads is E[_X_] = E[∑_i_=1,_n__Xi_]\n  * _Problem:_ We don't have ∑_i_=1,_n__Xi_; we only have E[_X_1], E[_X_2], ... E[_Xn_].\n  * _Solution:_ **Linearity of expectation** (appendix C): _**expectation of sum equals sum of expectations.**_ Therefore:   \n![](fig/expected-value-n-flips.jpg)\n\nThe key idea: if it's hard to count one way, use indicator random variables to\ncount an easier way!\n\n### Hiring Problem Revisited\n\nAssume that the candidates arrive in random order.\n\nLet _X_ be the random variable for the number of times we hire a new office\nassistant.\n\nDefine indicator random variables _X_1, _X_2, ... _Xn_ where _Xi_ =\nI{candidate _i_ is hired}.\n\nWe will rely on these properties:\n\n  * _X_ = _X_1 \\+ _X_2 \\+ ... + _Xn_   _(The total number of hires is the sum of whether we did each individual hire (1) or not (0).)_\n  * Lemma 1 implies that E[_Xi_] = Pr{candidate _i_ is hired}.\n\nWe need to compute Pr{candidate _i_ is hired}:\n\n  * Candidate _i_ is hired iff candidate _i_ is better than candidates 1, 2, ..., _i_−1\n  * Assumption of random order of arrival means any of the first _i_ candidates are equally likely to be the best one so far. \n  * Thus, Pr{candidate _i_ is the best so far} = 1/i.   \n_(Intuitively, as you add more candidates each candidate is less and less\nlikely to be better than all the ones prior.)_\n\nBy Lemma 1, E[Xi] = _1/i_, a fact that lets us compute E[X]:  \n![](fig/expected-value-hiring-problem.jpg)\n\nThe sum is a harmonic series. From formula A7 in appendix A, the _n_th\n**harmonic number** is:  \n![](fig/A7-harmonic-number.jpg)\n\nThus, the expected hiring cost is O(_ch_ ln _n_), much better than worst case\nO(_chn_)! (ln is the natural log. Formula 3.15 of the text can be used to show\nthat ln _n_ = O(lg _n_.)\n\nWe will see this kind of analysis repeatedly. Its strengths are that it lets\nus count in ways for which we have probabilities (compare to C.37), and that\nit works even when there are dependencies between variables.\n\n### Expected Number of Inversions\n\nThis is Exercise 5.2-5 page 122, for which there is a publicly posted\nsolution. This example shows the great utility of random variables.\n\nLet A[1.. _n_] be an array of _n_ distinct numbers. If _i < j_ and A[_i_] >\nA[_j_], then the pair (_i_, _j_) is called an **inversion** of A (they are\n\"out of order\" with respect to each other). Suppose that the elements of A\nform a uniform random permutation of ⟨1, 2, ... _n_⟩.\n\nWe want to find the expected number of inversions. This has obvious\napplications to analysis of sorting algorithms, as it is a measure of how much\na sequence is \"out of order\". In fact, each iteration of the `while` loop in\ninsertion sort corresponds to the elimination of one inversion (see the posted\nsolution to problem 2-4c).\n\n_If we had to count in terms of whole permutations, figuring out how many\npermutations had 0 inversions, how many had 1, ... etc. (sound familiar? :),\nthat would be a real pain, as there are _n_! permutations of n items. Can\nindicator random variables save us this pain by letting us count something\neasier? _\n\nWe will count the number of inversions directly, without worrying about what\npermutations they occur in:\n\nLet _Xij_, _i < j_, be an indicator random variable for the event where A[_i_] > A[_j_] (they are inverted).\n\nMore precisely, define: X_ij_= I{A[_i_] > A[_j_]} for 1 ≤ _i_ < _j_ ≤ _n_.\n\nPr{X_ij_ = 1} = 1/2 because given two distinct random numbers the probability\nthat the first is bigger than the second is 1/2. _(We don't care where they\nare in a permutation; just that we can easily identify the probabililty that\nthey are out of order. Brilliant in its simplicity!)_\n\nBy Lemma 1, E[X_ij_] = 1/2, and now we are ready to count.\n\nLet X be the random variable denoting the total number of inverted pairs in\nthe array. X is the sum of all X_ij_ that meet the constraint 1 ≤ _i_ < _j_ ≤\n_n_:  \n![](fig/inversions-random-var.jpg)\n\nWe want the expected number of inverted pairs, so take the expectation of both\nsides:  \n![](fig/inversions-expected.jpg)\n\nUsing linearity of expectation, we can simplify this far:  \n![](fig/inversions-solution-a.jpg)\n\nThe fact that our nested summation is choosing 2 things out of _n_ lets us\nwrite this as:  \n![](fig/inversions-solution-b.jpg)\n\nWe can use formula C.2 from the appendix:  \n![](fig/C2-n-choose-k.jpg)\n\nIn screencast [5A](http://youtu.be/MgnvWTZgqcA) I show how to simplify this to\n(_n_(_n_−1))/2, resulting in:\n\n![](fig/inversions-solution-c.jpg)\n\nTherefore the expected number of inverted pairs is _n_(_n_ − 1)/4, or O(_n_2).\n\n* * *\n\n## Randomized Algorithms\n\n![](fig/badguy.jpg)\n\nAbove, we had to _assume_ a distribution of inputs, but we may not have\ncontrol over inputs.\n\nAn \"adversary\" can always mess up our assumptions by giving us worst case\ninputs. (This can be a fictional adversary in making analytic arguments, or it\ncan be a real one ...)\n\nRandomized algorithms foil the adversary by _imposing_ a distribution of\ninputs.\n\nThe modifiation to HIRE-ASSISTANT is trivial: add a line at the beginning that\nrandomizes the list of candidates.\n\n  * The randomization is now in the algorithm, not the input distribution. \n  * Whereas before the algorithm was deterministic, and we could predict the hiring cost for a given input, now we can no longer say what the hiring cost will be.\n  * But our payoff is that no particular input elicits worst-case behavior, even what was worst-case for the deterministic version!\n  * Bad behavior occurs only if we get \"unlucky\" numbers. \n\nHaving done so, the above analysis applies to give us _expected value_ rather\nthan average case.\n\n_Discuss:_ Summarize the difference between probabilistic analysis and\nrandomized algorithms.\n\n####  Randomization Strategies\n\nThere are different ways to randomize algorithms. One way is to randomize the\nordering of the input before we apply the original algorithm (as was suggested\nfor HIRE-ASSISTANT above). A procedure for randomizing an array:\n\n    \n    \n      Randomize-In-Place(A)\n      1  _n_ = A.length\n      2  for _i_ = 1 to _n_\n      3      swap A[_i_] with A[Random(_i_,_n_)]  \n    \n\nThe text offers a proof that this produces a uniform random permutation. It is\nobviously O(_n_).\n\nAnother approach to randomization is to randomize choices made within the\nalgorithm. This is the approach taken by Skip Lists ...\n\n\n\n* * *\n\n## Skip Lists\n\nThis is additional material, not found in your textbook. I introduce Skip\nLists here for three reasons:\n\n  1. They are a natural extension of the linked list implementation of Dynamic Sets, which we covered recently.\n  2. They are a good example of a randomized algorithm, where randomization is used to _improve_ asymptotic behavior from O(_n_) to O(lg _n_).\n  3. They are one candidate implementation to be tested in your homework, the Battle of the Dynamic Sets!\n\nMotivation: Why do we have to search the entire linked list one item at a\ntime? Can't we be more efficient by diving into the middle somewhere?\n\nSkip lists were first described by William Pugh. 1990. Skip lists: a\nprobabilistic alternative to balanced trees. Commun. ACM 33, 6 (June 1990),\n668-676. DOI=10.1145/78973.78977 <http://doi.acm.org/10.1145/78973.78977> or\n<ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf> (actually he had a\nconference paper the year before, but the CACM verion is more accessible).\n\nMy discussion below follows Goodrich & Tamassia (1998), _Data Structures and\nAlgorithms in Java_, first edition, and uses images from their slides. Some\ndetails differ from the edition 4 version of the text.\n\nAn animated applet may be found at\n<http://iamwww.unibe.ch/~wenger/DA/SkipList/>.\n\n### Definition of Skip List\n\nGiven a set _S_ of items with distinct keys, a **skip list** is a series of\nlists _S_0, _S_1, ... _Sh_ (as we shall see, _h_ is the height) such that:\n\n  * Each _S__i_ contains the special keys −∞ and +∞\n  * List _S__h_ contains only −∞ and +∞\n  * List _S_0 contains all of the keys of _S_ in nondecreasing order. \n  * Each list is a subsequence of the previous one: _S_0 ⊇ _S_1 ⊇ ... ⊇ _Sh_. \n![](fig/skip-list.jpg)\n\nWe can implement skip lists with nodes that have `above` and `below` fields as\nwell as the more familiar `prev` and `next`:\n\n![](fig/skip-list-node.jpg)\n\n### Searching a Skip List\n\nAn algorithm for searching for a key _k_ in a skip list as follows:\n\n    \n    \n     SkipSearch(k)\n       Input: search key k\n       Output: Position p in S such that the item at p has the largest key ≤ k.\n       Let p be the topmost-left position of S // _which has at least -∞ and +∞_\n       while below(p) ≠ null do\n           p = below(p)                       // _drop down_\n           while key (next(p)) ≤ k do\n               p = next(p)                    // _scan forward _\n       return p. \n    \n\nExample: Search for 78:\n\n![](fig/skip-list-search.jpg)\n\n### Insertion and Randomization\n\nConstruction of a skip list is randomized:\n\n  * Begin by inserting the new item where it belongs in S0\n  * After inserting an item at level Si, flip a coin to decide whether to also insert it at Si+1.\n  * If Si+1 does not exist, the height of the Skip lists can be increased.   \n_(Alternatively, some policy can be used to limit growth as a function of n,\nbut the probability of a run of \"heads\" diminishes greatly as the number of\nflips increases.)._\n\nThe psuedocode provided by Goodrich & Tamassia uses a helper procedure\n`InsertAfterAbove(p1, p2, k, d)` (left as exercise), which inserts key `k` and\ndata `d` after `p1` and above `p2`. (The following omits code for returning\n\"elements\" not relevant here.)\n\n    \n    \n     SkipInsert(k,d)\n       Input: search key k and data d\n       Instance Variables: s is the start node of the skip list,\n         h is the height of the skip list, and n the number of entries \n       Output: None (list is modified to store d under k)\n       p = SkipSearch(k)\n       q = InsertAfterAbove(p, null, k, d)    // _we are at the bottom level_\n       l = 0                                  // _keeps track of level we are at_ \n       while random(0,1) ≤ 1/2 do\n           l = l + 1\n           if l ≥ h then                      // need to add a level\n               h = h + 1\n               t = next(s)\n               s = insertAfterAbove(null, s, −∞, null)\n               insertAfterAbove(s, t +∞, null) \n           while above(p) == null do\n               p = prev(p)                    // _scan backwards to find tower_\n           p = above(p)                       // _jump higher_\n           q = insertAfterAbove(p, q, k, d)   // _add new item to top of tower_\n       n = n + 1.\n    \n\nFor example, inserting key 15, when the randomization gave two \"heads\",\nforcing growth of _h_ (for simplicity the figure does not include the above\nand below pointers):\n\n![](fig/skip-list-insert.jpg)\n\nDeletion requires finding and removing all occurrences, and removing all but\none empty list if needed. Example for removing key 34:\n\n![](fig/skip-list-delete.jpg)\n\n### Analysis\n\nThe **worst case** performance of skip lists is very bad, but highly unlikely.\nSuppose `random(0,1)` is always less than 1/2. If there were no bound on the\nheight of the data structure, `SkipInsert` would never exit! But this is as\nlikely as an unending sequence of \"heads\" when flipping a fair coin.\n\nIf we do impose a bound _h_ on the height of the list (_h_ can be a function\nof _n_), the worst case is that every item is inserted at every level. Then\nsearching, insertion and deletion is O(_n+h_): you not only have to search a\nlist S0 of _n_ items, as with conventional linked lists; you also have to go\ndown _h_ levels.\n\nBut the probabilistic analysis shows that the expected time is much better.\nThis requires that we find the expected value of the height _h_:\n\n  * Probability that item is stored at level _i_ is the probability of getting _i_ consecutive heads: 1/2_i_.\n  * Probability P_i_ that level _i_ has at least one item: P_i_ ≤ n/2_i_   _(We had n tries at getting i consecutive heads.)_\n  * Probablity that _h_ is larger than _i_ is no more than P_i_.\n  * G&T show that given a constant _c_ > 1, the probability that _h_ is larger than _c_ lg _n_ is at most 1/_n__c_−1 (also worked out in screencast [5A](http://youtu.be/MgnvWTZgqcA)).\n  * For example, for _c_ = 3, the probability that _h_ is larger than 3 lg _n_ is at most 1/_n_2, which gets very small as n grows (e.g., p = .000001 = 1/1000000 for a list of length 1000).\n  * They conclude that the height _h_ is O(lg _n_).\n\nThe search time is proportional to the number of drop-down steps plus the\nnumber of scan-forward steps. The number of drop-down steps is the same as _h_\nor O(lg _n_). So, we need the number of scan-forward steps.\n\nIn their textbook (1998), G&T provide this argument: Let _Xi_ be the number of\nkeys examined scanning forward at level _i_.\n\n![](fig/code-SkipSearch.jpg)\n\n  * After the starting position, each key examined at level _i_ cannot also belong to level _i+1_. _(Why?)_\n  * Thus the probability that any key is counted in _Xi_ is 1/2. _(Why??)_\n  * Therefore the expected value of _Xi_ is the expected number of times we must flip a coin before it comes up heads: 2.\n  * Hence the expected amount of time scanning forward at each level is O(1). _(Wow!)_\n  * Since there are O(lg _n_) levels, the expected search time is O(lg _n_). \n\nIn their slides (2002), they provide this alternative analysis of the number\nof scan-forwards needed. The reasoning is very similar, but based on the odds\nof the list we encounter being constructed:\n\n  * When we scan forward in a list, the destination key does not belong to a higher list.\n  * Therefore, a scan forward is associated with a former coin toss that gave tails (otherwise it would be in the higher list).\n  * The expected number of coin tosses in order to get tails is 2.\n  * Therefore the expected number of scan-forward steps at each level is 2.\n  * Thus the total number of expected scan forward steps (summing across all _h_ or O(lg _n_) levels) is O(lg _n_). \n\nA similar analysis can be applied to insertion and deletion. Thus, skip lists\nare far superior to linked lists in performance.\n\nG&T also show that the expected space requirement is O(n). They leave as an\nexercise the elimination of `above` and `prev` fields: if random(0,1) is\ncalled up to _h_ times in advance of the insertion search, then one can insert\nthe item \"on the way down\" as specified by the results.\n\n* * *\n\nDan Suthers Last modified: Tue Apr 15 16:40:33 HST 2014  \nImages of mathematical expressions are from the instructor's material for\nCormen et al. Introduction to Algorithms, Third Edition. Images of skip lists\nare from lecture slides provided by M. Goodrich & R. Tamassia.  \n\n",
 "path"=>"morea//05.probabilistic/reading-notes-5.md"}
</pre>

<h2>/morea/05.probabilistic/reading-screencast-5a.html</h2>

<pre>Hash
{"title"=>"Indicator random variables",
 "published"=>true,
 "morea_id"=>"reading-screencast-5a",
 "morea_summary"=>"Indicator random variables.",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=MgnvWTZgqcA",
 "morea_labels"=>["Screencast", "Suthers", "18 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-screencast-5a.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-screencast-5a.md"}
</pre>

<h2>/morea/05.probabilistic/reading-screencast-5b.html</h2>

<pre>Hash
{"title"=>"Example analysis: inversions",
 "published"=>true,
 "morea_id"=>"reading-screencast-5b",
 "morea_summary"=>"Inversions",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=k-jusEhrRik",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-screencast-5b.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-screencast-5b.md"}
</pre>

<h2>/morea/05.probabilistic/reading-screencast-5c.html</h2>

<pre>Hash
{"title"=>"Randomized algorithms and skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5c",
 "morea_summary"=>"Randomized algorithms and skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=iaKu6jaKPFw",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-screencast-5c.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-screencast-5c.md"}
</pre>

<h2>/morea/05.probabilistic/reading-screencast-5d.html</h2>

<pre>Hash
{"title"=>"Analysis of skip lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-5d",
 "morea_summary"=>"Analysis of skip lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=oW2VnviRh5M",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-screencast-5d.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-screencast-5d.md"}
</pre>

<h2>/morea/05.probabilistic/reading-screencast-mit-skip-lists.html</h2>

<pre>Hash
{"title"=>"Skip Lists",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-skip-lists",
 "morea_summary"=>"Skip Lists",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec12/",
 "morea_labels"=>["Screencast", "Demaine", "85 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/05.probabilistic/reading-screencast-mit-skip-lists.html",
 "content"=>"",
 "path"=>"morea//05.probabilistic/reading-screencast-mit-skip-lists.md"}
</pre>

<h2>/morea/06.hash-tables/experience-data-structures-homework.html</h2>

<pre>Hash
{"title"=>"Analysis of data structures",
 "published"=>true,
 "morea_id"=>"experience-data-structures-homework",
 "morea_type"=>"experience",
 "morea_summary"=>"Consolidate your understanding of data structures",
 "morea_sort_order"=>3,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/06.hash-tables/experience-data-structures-homework.html",
 "url"=>"/morea/06.hash-tables/experience-data-structures-homework.html",
 "content"=>
  "# Analysis of data structures\n\nThis week's problems focus on ensuring you understand the operations of the\nmain data structures. They are not conceptually difficult but require\ndilligence in execution. Don't be careless or just go on intuition: you should\nactually follow the algorithms and hash functions precisely, or you will go\nwrong.\n\n## Peer Credit Assignment\n\n**(1)** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If three members besides yourself were present at some time, you have a total of 3 points to allocate across all members (_NOT_ 3 points per member!).\n  * If two members besides yourself were present, you have a total of 4 points to allocate across all members.\n  * If only one other member was present, you have a total of 6 points to allocate across all members.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n## Skip Lists\n\n#### 11 points\n\nPlease read carefully; this has multiple parts. Answer the lettered parts in\nboldface.\n\nHere is a skip list, including instance variables **s** (the starting\nposition), **h** (the height: we assume that `h` starts counting from 0), and\n**n** (the number of keys currently stored in the skip list). We won't bother\nto show the data associated with the keys. The double lines in the graphic are\nmeant to remind you that these are doubly linked lists in both the horizontal\nand vertical directions, but you need not draw double lines in your responses.\n\n![](fig/starting-skip-list.jpg)\n\n**(2)** _Trace the path that `SkipSearch(36)` takes, by circling every node that p is assigned to as the `SkipSearch` algorithm executes, starting with s. _\n\nIn the remaining questions, you will how what the skip list shown above looks\nlike after the cumulative operations indicated below, using the pseudocode for\n`SkipInsert` and `SkipSearch` in the lecture notes, and your understanding of\nhow `SkipDelete` works from the class activity.\n\nSince this is a random algorithm and we want everyone to have the same answer\nto facilitate grading, I also give you sequences of random numbers (not all of\nwhich will be used, as I am testing your understanding of when and how the\nrandom numbers are used). Note that the insertion code says ` while\nrandom(0,1) ≤ 1/2 do`...\n\nThe operations are **cumulative:** each step builds on the result of the\nprevious one. Redraw the entire data structure after each operation, and also\nupdate instance variables **s**, **h** and **n** as needed.\n\n**(3)** _Redraw after `SkipInsert(19,data)`_ where `random(0,1)` returns .70, .94, .14, .11, .89, ... \n\n**(4)** _Redraw after `SkipInsert(53,data)`_ where `random(0,1)` returns .14, .51, .22, .68, .45, ... \n\n**(5)** _Redraw after `SkipInsert(32,data)`_ where `random(0,1)` returns .25, .39, .18, .97, .02, ... \n\n**(6)** _ Redraw after `SkipDelete(SkipSearch(15))`. _\n\n_Something to think about (but not graded): What should the list look like if\nwe now deleted 32? There is a choice to be made here that we have not\ndiscussed! _\n\n**(7)** _Now draw what an _empty_ skip list would look like, including s, h and n._\n\n## Hashing\n\n#### 9 points\n\n###  Hashing with Chaining\n\n**(8)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **chaining** and **_h_(_k_) = _k_ mod 11.**_ _Draw this one with a vertical table indexed from 0 to 10, and linked lists going off to the right, as shown._\n\n![](fig/hash-chaining-template.jpg)\n\n###  Open Addressing with Linear Probing\n\n**(9)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size 11 with **linear probing**_ and\n\n> h'(k) = k mod 11  \nh(k,i) = (h'(k) + i) mod 11\n\n_Draw this and the next result as horizontal arrays indexed from 0 to 10 as\nshown below. Show your work to justify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(10)** _How many re-hashes after collision are required for this set of keys?_ _ Show your work here so we can give partial credit or feedback if warranted._\n\n### Open Addressing with Double Hashing\n\n**(11)** _Show the table that results when 20, 51, 10, 19, 32, 1, 66, 40 are cumulatively inserted into an initially empty hash table of size _m_ = 11 with **double hashing**_ and\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _i__h_2(_k_)) mod 11  \n_h_1(_k_) = _k_ mod 11  \n_h_2(_k_) = 1 + (_k_ mod 7)\n\n_Refer to the code in the book for how i is incremented. Show your work to\njustify your answer to the next question!_\n\n![](fig/hash-open-template.jpg)\n\n**(12)** _How many re-hashes after collision are required for this set of keys?_ _Show your work here so we can give partial credit or feedback if warranted._\n\n**(13)** Open addressing insertion is like an unsuccessful search, as you need to find an empty cell, i.e., to _not_ find the key you are looking for! If the open addressing hash functions above were uniform hashing, _ what is the expected number of probes at the time that the last key (40) was inserted?_ _Hints: At that point, 7 keys are in the table. Use the theorem for unsuccessful search in open addressing._ _Answer with a specific number, not O or Theta._\n\n    \n",
 "path"=>"morea//06.hash-tables/experience-data-structures-homework.md"}
</pre>

<h2>/morea/06.hash-tables/experience-deletion.html</h2>

<pre>Hash
{"title"=>"Hash tables: understanding deletion",
 "published"=>true,
 "morea_id"=>"experience-deletion",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about deletion in hash tables.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/06.hash-tables/experience-deletion.html",
 "url"=>"/morea/06.hash-tables/experience-deletion.html",
 "content"=>
  "# Deletion under Open Addressing\n\nFollowing the directions below, write pseudocode for `HASH-DELETE` to delete\nby writing a special `DELETED` value, and modify `HASH-INSERT` to handle the\n`DELETED` value. You will write your response in the exact same form as the\nbook's pseudocode (shown).\n\n**1.** Write `HASH-DELETE` by renaming `HASH-SEARCH` and adding or changing ONE line in the body.\n    \n    \n    Hash-Search (T,k)  // rename to Hash-Delete and remove this line \n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == k\n               return j\n           i = i + 1\n       until T[j] == NIL or i == m\n      return NIL\n    \n\n**2.** Write the new `HASH-INSERT` by changing only ONE line in the following. \n    \n    \n    Hash-Insert (T,k)\n       i = 0\n       repeat\n           j = h(k,i)\n           if T[j] == NIL\n               T[j] = k\n               return j\n           else i = i + 1\n       until i == m\n       error \"hash table overflow\" \n    \n\n**3.** What is the Θ runtime complexity of the worst case for the modified `HASH-INSERT` and `HASH-DELETE` in terms of _n_ (number of elements stored) and _m_ (table size)? (_ Describe the worst possible situation. Express its runtime with Θ. _) \n\n### Deletion from Skip Lists\n\n![](fig/SgkipList-Small.jpg)\n\n`SkipSearch(k)` returns a pointer `p` to the bottom most element of the tower\nyou want to delete. Suppose this were passed to a method `SkipDelete(p)` −\nnotice it takes `p` as argument, not `k`. In this problem you analyze the\ncomplexity of `SkipDelete`. Before you start the analysis, you should discuss\nhow it works! Then, if you have time, you can write pseudocode for it for\nextra credit.\n\n  * Assume that a skip list node has fields `p.next`, `p.prev`, `p.above` and `p.below`.\n  * Assume that `SkipInsert(k,d)` has built the skip list using `random(0,1)` with cutoff of 0.5. \n  * The delete procedure climbs the tower of linked lists above `p`, doing repeated deletion from each doubly linked list that the element occurs in. \n\n**4.** Assuming a uniform distribution of keys stored in random order, what is the Θ expected case performance of `SkipDelete` in terms of _n_, the number of keys stored in the skiplist?\n\n**5.** What is the probability that a given call to `SkipDelete` would have to _delete at least _k_ nodes_? (_Hint: Think of the probability that SkipInsert builds a \"tower\" of _k_ nodes for a given key._) \n\n#### Extra Credit\n\n**6.** If you finish early, write the recursive pseudocode for `SkipDelete(p)`. \n\nHint:\n    \n    SkipDelete(p)\n    if p ≠ null {\n    // splice out of this doubly linked list\n    // recurse to splice out of list above\n    }\n    \n",
 "path"=>"morea//06.hash-tables/experience-deletion.md"}
</pre>

<h2>/morea/06.hash-tables/module-hash-tables.html</h2>

<pre>Hash
{"title"=>"Hash Tables",
 "published"=>true,
 "morea_id"=>"hash-tables",
 "morea_outcomes"=>["outcome-hash-tables"],
 "morea_readings"=>
  ["reading-screencast-6a",
   "reading-screencast-6b",
   "reading-screencast-6c",
   "reading-screencast-6d",
   "reading-cormen-11",
   "reading-notes-6",
   "reading-screencast-mit-hash-tables-1",
   "reading-screencast-mit-hash-tables-2"],
 "morea_experiences"=>
  ["experience-deletion", "experience-data-structures-homework"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/06.hash-tables/module-hash-tables.png",
 "morea_sort_order"=>6,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/06.hash-tables/module-hash-tables.html",
 "content"=>
  "We consider the design and performance characteristics of what might be the best data structure ever invented.\n",
 "path"=>"morea//06.hash-tables/module-hash-tables.md"}
</pre>

<h2>/modules/hash-tables/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-hash-tables.md",
 "title"=>"Hash Tables",
 "url"=>"/modules/hash-tables/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/hash-tables/index.html"}
</pre>

<h2>/morea/06.hash-tables/outcome-hash-tables.html</h2>

<pre>Hash
{"title"=>"Understand hash tables.",
 "published"=>true,
 "morea_id"=>"outcome-hash-tables",
 "morea_type"=>"outcome",
 "morea_sort_order"=>7,
 "referencing_modules"=>[#Jekyll:Page @name="module-hash-tables.md"],
 "url"=>"/morea/06.hash-tables/outcome-hash-tables.html",
 "content"=>
  "Understand the design and run-time characteristics of hash tables and how they compare to related data structures. ",
 "path"=>"morea//06.hash-tables/outcome-hash-tables.md"}
</pre>

<h2>/morea/06.hash-tables/reading-cormen-11.html</h2>

<pre>Hash
{"title"=>"CLRS 11 - Hash tables",
 "published"=>true,
 "morea_id"=>"reading-cormen-11",
 "morea_summary"=>
  "Direct address tables, hash tables, hash functions, and open addressing",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "23 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-cormen-11.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-cormen-11.md"}
</pre>

<h2>/morea/06.hash-tables/reading-notes-6.html</h2>

<pre>Hash
{"title"=>"Chapter 6 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-6",
 "morea_summary"=>"Notes on hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/06.hash-tables/reading-notes-6.html",
 "url"=>"/morea/06.hash-tables/reading-notes-6.html",
 "content"=>
  "## Outline\n\n  1. Motivations and Introduction\n  2. Hash Tables with Chaining \n  3. Hash Functions and Universal Hashing\n  4. Open Addressing Strategies\n\n## Motivations and Introduction\n\nMany applications only need the insert, search and delete operations of a\n[dynamic set](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-04\n.html#dynamicsetadt). Example: symbol table in a compiler.\n\nHash tables are an effective approach. Under reasonable assumptions, they have\nO(1) operations, but they can be Θ(n) worst case\n\n### Direct Addressing\n\nHash tables generalize arrays. Let's look at the idea with arrays first. Given\na key _k_ from a universe _U_ of possible keys, a **direct address table**\nstores and retrieves the element in position _k_ of the array.\n\n![](fig/Fig-11-1-direct-address.jpg)\n\nDirect addressing is applicable when we can allocate an array with one element\nfor every key (i.e., of size |_U_|). It is trivial to implement:\n\n![](fig/pseudocode-direct-address.jpg)\n\nHowever, often the space of possible keys is much larger than the number of\nactual keys we expect, so it would be wasteful of space (and sometimes not\npossible) to allocate an array of size |_U_|.\n\n### Hash Tables and Functions\n\n**Hash tables** are also arrays, but typically of size proportional to the number of keys expected to be stored (rather than to the number of keys). \n\nIf the expected keys K ⊂ U, the Universe of keys, and |K| is substantially\nsmaller than |U|, then hash tables can reduce storage requirements to Θ(|K|).\n\nA **hash function** _h(k)_ maps the larger universe U of external keys to\nindices into the array. Given a table of size _m_ with zero-based indexing (we\nshall see why this is useful):\n\n  * _h_ : U -> {0, 1, ..., _m_-1}.\n  * We say that _k_ **hashes** to slot _h(k)_. \n\n###  Collisions\n\nThe major issue to deal with in designing and implementing hash tables is what\nto do when the hash function maps multiple keys to the same table entry.\n\n![](fig/Fig-11-2-collisions.jpg)\n\nCollisions may or may not happen when |K| ≤ _m_, but definitely happens when\n|K| > _m_. _(Is there any way to avoid this?)_\n\nThere are two major approaches: Chaining (the preferred method) and Open\nAddressing. We'll look at these and also hash function design.\n\n* * *\n\n## Hash Tables with Chaining\n\nA simple resolution: Put elements that hash to the same slot into a linked\nlist. This is called _chaining_ because we chain elements off the slot of the\nhash table.\n\n  * Slot _j_ points to the head of a list of all stored elements that hash to _j_, or to NIL if there are no such elements.\n  * Doubly linked lists may be used when deletions are expected to be frequent.\n  * Sentinels can also be used to simplify the code.\n\n![](fig/Fig-11-3-chaining.jpg)\n\n### Pseudocode for Chaining\n\nImplementation is simple if you already have implemented linked lists:\n\n![](fig/pseudocode-chained-hashing.jpg)\n\n_What are the running times for these algorithms? Which can we state directly,\nand what do we need to know to determine the others?_\n\n### Analysis of Hashing with Chaining\n\nHow long does it take to find an element with a given key, or to determine\nthat there is no such element?\n\n  * Analysis is in terms of the **load factor _α = n/m_**, where \n    * _n_ = number of elements in the table \n    * _m_ = number of slots in the table = number of (possibly empty) linked lists\n  * The load factor α is the average number of elements per linked list. \n  * Can have α < 1; α = 1; or α > 1\\. \n  * Worst case is when all _n_ keys hash to the same slot.   \n_Why? What happens? Θ(_____?)_\n\n  * Average case depends on how well the hash function distributes the keys among the slots. \n\nLet's analyze averge-case performance under the assumption of **simple uniform\nhashing:** any given element is equally likely to hash into any of the _m_\nslots:\n\n  * For _j_ = 0, 1, ..., _m_-1, denote the length of list T[_j_] by _nj_.\n  * Then _n_ = _n0_ \\+ _n1_ \\+ ... + _nm-1_. \n  * Average value of _nj_ is E[_nj_] = α = _n/m_. \n  * Assuming _h(k)_ computed in O(1), so time to search for _k_ depends on length _nh(k)_ of the list T[_h(k)_]. \n\nConsider two cases: Unsuccessful and Successful search. The former analysis is\nsimpler because you always search to the end, but for successful search it\ndepends on where in T[_h(k)_] the element with key _k_ will be found.\n\n#### Unsuccessful Search\n\nSimple uniform hashing means that any key not in the table is equally likely\nto hash to any of the _m_ slots.\n\nWe need to search to end of the list T[_h(k)_]. It has expected length\nE[_nh(k)_] = α = _n/m_.\n\nAdding the time to compute the hash function gives **Θ(1 + α)**. (We leave in\nthe \"1\" term for the initial computation of _h_ since α can be 0, and we don't\nwant to say that the computation takes Θ(0) time).\n\n#### Successful Search\n\nWe assume that the element _x_ being searched for is equally likely to be any\nof the _n_ elements stored in the table.\n\nThe number of elements examined during a successful search for _x_ is 1 more\nthan the number of elements that appear before _x_ in _x_'s list (because we\nhave to search them, and then examine _x_).\n\nThese are the elements inserted _after x_ was inserted (because we insert at\nthe head of the list).\n\nNeed to find on average, over the _n_ elements _x_ in the table, how many\nelements were inserted into _x_'s list after _x_ was inserted. _Lucky we just\nstudied indicator random variables!_\n\nFor _i_ = 1, 2, ..., _n_, let _xi_ be the _i_th element inserted into the\ntable, and let _ki_ = _key_[_xi_].\n\nFor all _i_ and _j_, define the indicator random variable:\n\n> _Xij_ = I{_h(ki)_ = _h(kj)_}.     _(The event that keys _ki_ and _kj_ hash\nto the same slot.)_\n\n![](fig/lemming.jpg)\n\nSimple uniform hashing implies that Pr{_h(ki)_ = _h(kj)_} = 1/_m_ _(Why?)_\n\nTherefore, E[_Xij_] = 1/_m_ by Lemma 1 ([Topic #5](http://www2.hawaii.edu/~sut\nhers/courses/ics311s14/Notes/Topic-05.html#lemma1)).\n\nThe expected number of elements examined in a successful search is those\nelements _j_ that are inserted after the element _i_ of interest _and_ that\nend up in the same linked list (_Xij_):\n\n![](fig/analysis-chaining-1.jpg)\n\n  * The innermost summation is adding up, for all _j_ inserted after _i_ (_j_=_i_+1), those that are in the same hash table (when _Xij_ = 1).\n  * The outermost summation runs this over all _n_ of the keys inserted (indexed by _i_), and finds the average by dividing by _n_.\n\nI fill in some of the implicit steps in the rest of the text's analysis.\nFirst, by linearity of expectation we can move the E in:\n\n![](fig/analysis-chaining-2.jpg)\n\nThat is the crucial move: instead of analyzing the probability of complex\nevents, use indicator random variables to break them down into simple events\nthat we know the probabilities for. In this case we know E[_Xi,j_] (if _you_\ndon't know, ask the lemming above):\n\n![](fig/analysis-chaining-3.jpg)\n\nMultiplying 1/_n_ by the terms inside the summation,\n\n  * For the first term, we get Σ_i_=1,_n_1/_n_, which is just _n_/_n_ or 1\n  * Move 1/_m_ outside the summation of the second term to get 1/_nm_. This leaves Σ_i_=1,_n_(Σ_j_=_i_+1,_n_1), which simplifies as shown below (if you added 1 _n_ times, you would overshoot by _i_).\n![](fig/analysis-chaining-4.jpg)\n\nSplitting the two terms being summed, the first is clearly _n_2, and the\nsecond is the familiar sum of the first _n_ numbers:\n\n![](fig/analysis-chaining-5.jpg)  \n\n![](fig/analysis-chaining-6.jpg)\n\nDistributing the 1/_nm_, we get 1 + (_n_2/_nm_ \\- _n_(_n_+1)/2_nm_   =   1 +\n_n_/_m_ \\- (_n_+1)/2_m_   =   1 + 2_n_/2_m_ \\- (_n_+1)/2_m_, and now we can\ncombine the two fractions:\n\n![](fig/analysis-chaining-7.jpg)\n\nNow we can turn two instances of _n_/_m_ into α with this preparation: 1 +\n(_n_ \\- 1)/2_m_   =   1 + _n_/2_m_ \\- 1/2_m_   =   1 + α/2 - n/2_mn_   =  \n\n![](fig/analysis-chaining-8.jpg)\n\nAdding the time (1) for computing the hash function, the expected total time\nfor a successful search is:\n\n> Θ(2 + α/2 - α/2_n_) = **Θ(1 + α).**\n\nsince the third term vanishes in significance as _n_ grows, and the constants\n2 and 1/2 have Θ(1) growth rate.\n\nThus, **search is an average of Θ(1 + α) in either case.**\n\nIf the number of elements stored _n_ is bounded within a constant factor of\nthe number of slots _m_, i.e., _n_ = O(_m_), then α is a constant, and search\nis O(1) on average.\n\nSince insertion takes O(1) worst case and deletion takes O(1) worst case when\ndoubly linked lists are used, all three operations for hash tables are O(1) on\naverage.\n\n_(I went through that analysis in detail to show again the utility of\nindicator random variables and to demonstrate what is possibly the most\ncrucial fact of this chapter, but we won't do the other analyses in detail.\nWith perserverence you can similarly unpack the other analyses.)_\n\n* * *\n\n## Hash Functions and Universal Hashing\n\nIdeally a hash function satisfies the assumptions of simple uniform hashing.\n\nThis is not possible in practice, since we don't know in advance the\nprobability distribution of the keys, and they may not be drawn independently.\n\nInstead, we use heuristics based on what we know about the domain of the keys\nto create a hash function that performs well.\n\n### Keys as natural numbers\n\nHash functions assume that the keys are natural numbers. When they are not, a\nconversion is needed. Some options:\n\n  * Floating point numbers: If an integer is required, sum the mantissa and exponent, treating them as integers.\n  * Character string: Sum the ASCII or Unicode values of the characters of the string. \n  * Character string: Interpret the string as an integer expressed in some radix notation. (This gives very large integers.) \n\n### Division method\n\nA common hash function: **_h(k)_ = _k_ mod _m_**.  \n_(Why does this potentially produce all legal values, and only legal values?)_\n\n_Advantage:_ Fast, since just one division operation required.\n\n_Disadvantage:_ Need to avoid certain values of _m_, for example:\n\n  * Powers of 2. If _m_ = 2_p_ for integer _p_ then _h(k)_ is the least significant _p_ bits of _k_.   \n(There may be a domain pattern that makes the keys clump together).\n\n  * If character strings are interpreted in radix 2_p_ then _m_ = 2_p_ \\- 1 is a bad choice: permutations of characters hash the same. \n\nA prime number not too close to an exact power of 2 is a good choice for _m_.\n\n### Multiplication method\n\n**_h(k)_ = Floor(_m_(_k_ A mod 1))**, where _k_ A mod 1 = fractional part of _k_A. \n\n  1. Choose a constant A in range 0 < A < 1\\. \n  2. Multiply _k_ by A\n  3. Extract the fractional part of _k_A\n  4. Multiply the fractional part by _m_\n  5. Take the floor of the result. \n\n_Disadvantage:_ Slower than division.\n\n_Advantage:_ The value of _m_ is not critical.\n\nThe book discusses an implementation that we won't get into ...\n\n![](fig/Fig-11-4-multiplication-hashing.jpg)\n\n### Universal Hashing \n\n![](fig/badguy.jpg)\n\nOur malicious adversary is back! He's choosing keys that all hash to the same\nslot, giving worst case behavior and gumming up our servers! What to do?\n\nRandom algorithms to the rescue: randomly choose a different hash function\neach time you construct and use a new hash table.\n\nBut it has to be a good one. Can we define a family of good candidates?\n\nConsider a finite collection _Η_ of hash functions that map universe U of keys\ninto {0, 1, ..., _m_-1}.\n\n_Η_ is **universal** if for each pair of keys _k, l_ ∈ U, where _k ≠ l_, the\nnumber of hash functions _h ∈ Η_ for which _h(k) = h(l)_ is less than or equal\nto _|Η|/m_ (that's the size of _Η_ divided by _m_).\n\nIn other words, with a hash function _h_ chosen randomly from _Η_, the\nprobability of collision between two different keys is no more than _1/m_, the\nchance of a collision when choosing two slots randomly and independently.\n\nUniversal hash functions are good because (proven as Theorem 11.3 in text):\n\n  * If _k_ is not in the table, the expected length E[_nh(k)_] of the list that _k_ hashes to is less than or equal to α. \n  * If _k_ is in the table, the expected length E[_nh(k)_] of the list that holds _k_ is less than or equal to 1 + α. \n\nTherefore, the expected time for search is O(1).\n\nOne candidate for a collection _Η_ of hash functions is:\n\n> _Η_ = {_hab_(_k_) : **_hab_(_k_) = ((_ak + b_) mod _p_) mod _m_)},** where\n_a_ ∈ {1, 2, ..., _p_-1} and _b_ ∈ {0, 1, ..., _p_-1}, where _p_ is prime and\nlarger than the largest key.\n\nDetails in text, including proof that this provides a universal set of hash\nfunctions. Java built in hash functions take care of much of this for you:\nread the Java documentation for details.\n\n* * *\n\n## Open Addressing Strategies\n\nOpen Addressing seeks to avoid the extra storage of linked lists by putting\nall the keys in the hash table itself.\n\nOf course, we need a way to deal with collisions. If a slot is already\noccupied we will apply a systematic strategy for searching for alternative\nslots. This same strategy is used in both insertion and search.\n\n###  Probes and _h_(_k_,_i_)\n\nExamining a slot is called a **probe**. We need to extend the hash function\n_h_ to take the probe number as a second argument, so that _h_ can try\nsomething different on subsequent probes. We count probes from 0 to _m_-1\n(you'll see why later), so the second argument takes on the same values as the\nresult of the function:\n\n> **_h_ : _U_ x {0, 1, ... _m_-1} -> {0, 1, ... _m_-1}**  \n\nWe require that the **probe sequence**\n\n> ⟨ _h_(_k_,0),   _h_(_k_,1)   ...   _h_(_k_,_m_-1) ⟩\n\nbe a permutation of ⟨ 0, 1, ... _m_-1 ⟩. Another way to state this requirement\nis that all the positions are visited.\n\nThere are three possible outcomes to a probe: _k_ is in the slot probed\n(successful search); the slot contains NIL (unsuccessful search); or some\nother key is in the slot (need to continue search).\n\nThe strategy for this continuation is the crux of the problem, but first let's\nlook at the general pseudocode.\n\n### Pseudocode\n\n**Insertion** returns the index of the slot it put the element in _k_, or throws an error if the table is full:\n\n![](fig/pseudocode-open-hash-insert.jpg)\n\n**Search** returns either the index of the slot containing element of key _k_, or NIL if the search is unsuccessful:\n\n![](fig/pseudocode-open-hash-search.jpg)\n\n**Deletion** is a bit complicated. We can't just write NIL into the slot we want to delete. _(Why?)_\n\nInstead, we write a special value DELETED. During search, we treat it as if it\nwere a non-matching key, but insertion treats it as empty and reuses the slot.\n\n_Problem:_ the search time is no longer dependent on α. _(Why?)_\n\nThe ideal is to have **uniform hashing**, where each key is equally likely to\nhave any of the _m_! permutations of ⟨0, 1, ... _m_-1⟩ as its probe sequence.\nBut this is hard to implement: we try to guarantee that the probe sequence is\n_some_ permutation of ⟨0, 1, ... _m_-1⟩.\n\nWe will define the hash functions in terms of ** auxiliary hash functions**\nthat do the initial mapping, and define the primary function in terms of its\n_i_th iterations, where 0 ≤ _i_ < _m_.\n\n### Linear Probing\n\nGiven an **auxiliary hash function _h'_**, the probe sequence starts at\n_h'_(_k_), and continues sequentially through the table:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _i_) mod _m_\n\n_Problem:_ **primary clustering**: sequences of keys with the same _h'_ value\nbuild up long runs of occupied sequences.\n\n### Quadratic Probing\n\nQuadratic probing is attempt to fix this ... instead of reprobing linearly, QP\n\"jumps\" around the table according to a quadratic function of the probe, for\nexample:\n\n> _h_(_k_,_i_) = (_h'_(_k_) + _c_1_i_ \\+ _c_2_i_2) mod _m_,  \nwhere _c_1 and _c_2 are constants.\n\n_Problem:_ **secondary clustering**: although primary clusters across\nsequential runs of table positions don't occur, two keys with the same _h'_\nmay still have the same probe sequence, creating clusters that are broken\nacross the same sequence of \"jumps\".\n\n### Double Hashing\n\nA better approach: use two auxiliary hash functions _h1_ and _h_2, where _h_1\ngives the initial probe and _h_2 gives the remaining probes (here you can see\nthat having _i_=0 initially drops out the second hash until it is needed):\n![](fig/Fig-11-5-double-hashing.jpg)\n\n> _h_(_k_,_i_) = (_h_1(_k_) + _ih_2(_k_)) mod _m_.\n\n_h_2(_k_) must be relatively prime to _m_ (relatively prime means they have no\nfactors in common other than 1) to guarantee that the probe sequence is a full\npermutation of ⟨0, 1, ... _m_-1⟩. Two approaches:\n\n  * Choose _m_ to be a power of 2 and _h_2 to always produce an odd number > 1.\n  * Let _m_ be prime and have 1 < _h_2(_k_) < _m_.   \n(The example figure is _h_1(_k_) = _k_ mod 13, and _h_2(_k_) = 1 + (_k_ mod\n11).)\n\nThere are Θ(_m_2) different probe sequences, since each possible combination\nof _h_1(_k_) and _h_2(_k_) gives a different probe sequence. This is an\nimprovement over linear or quadratic hashing.\n\n### Analysis of Open Addressing\n\nThe textbook develops two theorems you will use to compute the expected number\nof probes for unsuccessful and successful search. (These theorems require α <\n1 because an expression 1/1−α is derived and we don't want to divide by 0.)\n\n> **Theorem 11.6:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in an _**unsuccessful**_ search is\nat most **1/(1 − α)**, assuming uniform hashing.\n\n> **Theorem 11.8:** Given an open-address hash table with load factor α =\n_n_/_m_ < 1, the expected number of probes in a _**successful**_ search is at\nmost **(1/α) ln (1/(1 − α))**, assuming uniform hashing and assuming that each\nkey in the table is equally likely to be searched for.\n\nWe leave the proofs for the textbook, but note particularly the \"intuitive\ninterpretation\" in the proof of 11.6 of the **_expected number of probes_** on\npage 275:\n\n> E[_X_]   =   1/(1-α)   =   1   \\+   α   \\+   α2   \\+   α3   \\+   ...\n\nWe always make the first probe (1). With probability α < 1, the first probe\nfinds an occupied slot, so we need to probe a second time (α). With\nprobability α2, the first two slots are occupied, so we need to make a third\nprobe ...\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:14:59 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//06.hash-tables/reading-notes-6.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-6a.html</h2>

<pre>Hash
{"title"=>"Hash tables: introduction and chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6a",
 "morea_summary"=>"Introduction to hash tables",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"http://www.youtube.com/watch?v=NMm1BKomO_Y",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-6a.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-6a.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-6b.html</h2>

<pre>Hash
{"title"=>"Hash tables: analysis of chaining",
 "published"=>true,
 "morea_id"=>"reading-screencast-6b",
 "morea_summary"=>"Analysis of chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"http://www.youtube.com/watch?v=ei7T9Y97u0M",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-6b.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-6b.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-6c.html</h2>

<pre>Hash
{"title"=>"Hash tables: Hash functions",
 "published"=>true,
 "morea_id"=>"reading-screencast-6c",
 "morea_summary"=>"Examples of hash functions and universal chaining",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"http://www.youtube.com/watch?v=jW4wCfz3DwE",
 "morea_labels"=>["Screencast", "Suthers", "13 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-6c.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-6c.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-6d.html</h2>

<pre>Hash
{"title"=>"Hash table: open addressing",
 "published"=>true,
 "morea_id"=>"reading-screencast-6d",
 "morea_summary"=>
  "Using open addressing to avoid the overhead of linked lists.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"http://www.youtube.com/watch?v=SGGP_HJNUts",
 "morea_labels"=>["Screencast", "Suthers", "16 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-6d.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-6d.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-mit-hash-tables-1.html</h2>

<pre>Hash
{"title"=>"Hash tables I",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-1",
 "morea_summary"=>"Hash tables and the symbol table problem",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec07/",
 "morea_labels"=>["Screencast", "Leiserson", "77 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-mit-hash-tables-1.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-mit-hash-tables-1.md"}
</pre>

<h2>/morea/06.hash-tables/reading-screencast-mit-hash-tables-2.html</h2>

<pre>Hash
{"title"=>"Hash tables II",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-hash-tables-2",
 "morea_summary"=>"Universal hashing",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_url"=>"http://videolectures.net/mit6046jf05_leiserson_lec08/",
 "morea_labels"=>["Screencast", "Leiserson", "79 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/06.hash-tables/reading-screencast-mit-hash-tables-2.html",
 "content"=>"",
 "path"=>"morea//06.hash-tables/reading-screencast-mit-hash-tables-2.md"}
</pre>

<h2>/morea/07.divide-conquer/experience-master-method.html</h2>

<pre>Hash
{"title"=>"Understanding master method and substitution",
 "published"=>true,
 "morea_id"=>"experience-master-method",
 "morea_type"=>"experience",
 "morea_summary"=>
  "Apply your knowledge of the master method and substitution.",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/07.divide-conquer/experience-master-method.html",
 "url"=>"/morea/07.divide-conquer/experience-master-method.html",
 "content"=>
  "### Peer Credit Assignment\n\n**1.** Please list the names of the other members of your peer group for this week and the number of extra credit points you think they deserve for their participation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n### Master Method Practice\n\n**2\\. ** (6 pts) Use the Master Method to give tight Θ bounds for the following recurrence relations. Show _a_, _b_, and _f_(_n_). Then explain why it fits one of the cases, if any. If it fits a case, write and _ simplify _ the final Θ result \n\n**a.**   _T_(_n_) = 2_T_(_n_/4) + √_n_  \n\n**b.**   _T_(_n_) = 2_T_(_n_/4) + _n_  \n\n**c.**   _T_(_n_) = 4_T_(_n_/3) + _n_\n\n\n\n### Substitution Method\n\n**3.** (7 pts) Use substitution _as directed below_ to solve \n\n> _T_(_n_) = 4_T_(_n_/3) + _n_\n\nIt is strongly recommended that you read page 85-86 \"Subtleties\" before trying\nthis!\n\n**a.**   First, use the result from the Master Method in 2c as your \"guess\" and inductive assumption. We will do this without Θ and _c_: just use the algebraic portion. Take the proof up to where it fails and say where and why it fails. (See steps below.) \n\n**b.**   Redo the proof, but subtracting _d__n_ from the guess to construct a new guess. This time it should succeed. \n\nAs a reminder, to do a proof by substitution you:\n\n  1. Write the definition _T_(_n_) = 4_T_(_n_/3) + _n_\n  2. Replace the _T_(_n_/3) with your \"guess\" instantiated for _n_/3 (you can do that by the inductive hypothesis because it's smaller than _n_). \n  3. Operating _only_ on the right hand side of the equation, transform that side into the _exact_ form of your \"guess\".\n  4. Determine any constraints on the constants involved. \n  5. Show the base case holds. \n\n\n",
 "path"=>"morea//07.divide-conquer/experience-master-method.md"}
</pre>

<h2>/morea/07.divide-conquer/experience-substitution.html</h2>

<pre>Hash
{"title"=>"Understanding substitution and induction",
 "published"=>true,
 "morea_id"=>"experience-substitution",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about substitution and induction.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/07.divide-conquer/experience-substitution.html",
 "url"=>"/morea/07.divide-conquer/experience-substitution.html",
 "content"=>
  "## Solving _T_(_n_) = _T_(_n_ − 1) + _n_ with Substitution\n\n### 5 points\n\nUsing substitution and induction, show that the solution of _T_(_n_) = _T_(_n_\n− 1) + _n_ is O(_n_2). In the terminology of CLRS, this is our \"guess\" at the\nsolution to the recurrence relation.\n\n**a.**   Convert the \"guess\" to an equivalent algebraic inequality according to the definition of Big-O (removing the Big-O and adding the implied constant _c_): \n\n> _T_(_n_) =\n\nMake the inductive assumption that what you wrote in (a) holds for all _m_ <\n_n_.\n\nNow you need to use induction and substitution to show that the definition\n_T_(_n_) = _T_(_n_ − 1) + _n_ implies the inequality that you wrote in (a). In\nthe process you will determine the constraints on _c_. We'll do the base case\nlast for your chosen _c_.\n\n**b.**   Write out the definition of T(_n_), and operating _ only_ on the right hand side, substitute in the inductive assumption where appropriate and simplify to isolate the expression (from a) to be proven from the lower order terms:\n\n> _T_(_n_) =\n\n**c.** Use ≤ to get rid of the lower order terms (effectively claiming that what you had above is ≤ _c__n_2), and determine the values of _c_ and _n_ for which the inequality is true:  \n\n> The above is true for all _c_ ≥ ____ and _n_ ≥ ____ because ...\n\n**d.** For the base case, assuming that _T_(0) = 0, show that _T_(1) = _c__n_2 for your choice of _c_:  \n\n> _T_(1) =\n\n### If you finish early:###\n\nTry T(_n_) = 4T(_n_/3) + _n_\n    \n",
 "path"=>"morea//07.divide-conquer/experience-substitution.md"}
</pre>

<h2>/morea/07.divide-conquer/module-divide-conquer.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"divide-conquer",
 "morea_outcomes"=>
  ["outcome-divide-conquer-recognize", "outcome-divide-conquer-apply"],
 "morea_readings"=>
  ["reading-screencast-7a",
   "reading-screencast-7b",
   "reading-screencast-7c",
   "reading-screencast-7d",
   "reading-cormen-4",
   "reading-notes-7",
   "reading-screencast-mit-divide-conquer"],
 "morea_experiences"=>["experience-substitution", "experience-master-method"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/07.divide-conquer/module-divide-conquer.gif",
 "morea_sort_order"=>7,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/07.divide-conquer/module-divide-conquer.html",
 "content"=>
  "Divide the problem into subproblems, conquer by solving them recursively.\n",
 "path"=>"morea//07.divide-conquer/module-divide-conquer.md"}
</pre>

<h2>/modules/divide-conquer/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-divide-conquer.md",
 "title"=>"Divide and conquer",
 "url"=>"/modules/divide-conquer/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/divide-conquer/index.html"}
</pre>

<h2>/morea/07.divide-conquer/outcome-divide-conquer-apply.html</h2>

<pre>Hash
{"title"=>"Design divide and conquer algorithms",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-apply",
 "morea_type"=>"outcome",
 "morea_sort_order"=>9,
 "referencing_modules"=>[#Jekyll:Page @name="module-divide-conquer.md"],
 "url"=>"/morea/07.divide-conquer/outcome-divide-conquer-apply.html",
 "content"=>
  "Successfully design and implement divide and conquer algorithms to solve specific programming problems.\n",
 "path"=>"morea//07.divide-conquer/outcome-divide-conquer-apply.md"}
</pre>

<h2>/morea/07.divide-conquer/outcome-divide-conquer-recognize.html</h2>

<pre>Hash
{"title"=>"Recognize when divide and conquer is appropriate",
 "published"=>true,
 "morea_id"=>"outcome-divide-conquer-recognize",
 "morea_type"=>"outcome",
 "morea_sort_order"=>8,
 "referencing_modules"=>[#Jekyll:Page @name="module-divide-conquer.md"],
 "url"=>"/morea/07.divide-conquer/outcome-divide-conquer-recognize.html",
 "content"=>
  "Be able to recognize when the divide and conquer algorithm is an appropriate algorithm to apply to a programming problem.\n",
 "path"=>"morea//07.divide-conquer/outcome-divide-conquer-recognize.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-cormen-4.html</h2>

<pre>Hash
{"title"=>"CLRS 4 - Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-cormen-4",
 "morea_summary"=>
  "The maximum subarray problem, strassen's algorithm for matrix multiplication, substitution method, recursion tree method, and the master method.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "30 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-cormen-4.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-cormen-4.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-notes-7.html</h2>

<pre>Hash
{"title"=>"Chapter 7 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-7",
 "morea_summary"=>"Notes on divide and conquer",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/07.divide-conquer/reading-notes-7.html",
 "url"=>"/morea/07.divide-conquer/reading-notes-7.html",
 "content"=>
  "# Outline\n\n  1. Divide & Conquer and Recurrences\n  2. Substitution Method\n  3. Recursion Trees\n  4. Master Theorem & Method\n\n## Divide & Conquer Strategy\n\n**Divide**\n    the problem into subproblems that are smaller instances of the same problem. \n**Conquer**\n    the subproblems by solving them recursively. If the subproblems are small enough, solve them trivially or by \"brute force.\"\n**Combine**\n    the subproblem solutions to give a solution to the original problem.\n\n## Recurrences\n\nThe recursive nature of D&C leads to _recurrences_, or functions defined in\nterms of:\n\n  * one or more base cases, and \n  * itself, with smaller arguments.\n\nReviewing from [Topic #2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort), a common (but not the only) form of recurrence\nis as follows. Let _T_(_n_) be the running time on a problem of size _n_.\n\n  * If _n_ is below some constant (often, _n_=1), we can solve the problem directly with brute force or trivially in Θ(1) time.\n  * Otherwise we divide the problem into _a_ subproblems, each 1/_b_ size of the original. \n  * We pay cost _D_(_n_) to divide the problems and _C_(_n_) to combine the solutions. \n  * We also pay cost _aT_(_n_/_b_) solving subproblems. \n\nThen the total time to solve a problem of size _n_ can be expressed as:\n\n![](fig/recurrence-generic.jpg)\n\nSome technical points should be made:\n\n  * Subproblems are not constrained to being a constant fraction of the original problem size, for example, you can have T(_n_) = T(_n-1_) + Θ(1).   _(What's an example algorithm that this describes?)_\n  * There can be other forms, such as multiple ways of dividing the problem. The book gives an example page 91 that divides the problem into 1/3 and 2/3 parts, requiring terms for T(_n/3_) and T(_2n/3_)\n  * Floors and ceilings can easily be removed and don't affect the solution to the recurrence.\n  * Boundary conditions (the smaller order terms that result from base cases) are usually Θ(1) and are omitted from asymptotic analyses, though they do matter for exact solutions.\n  * Recurrences can be inequalities. We use Big-O or Ω as appropriate. \n\nToday we cover three approaches to solving such relations: substitution,\nrecursion tree, and the master method. But first, we look at two examples, one\nof which we have already seen ...\n\n### Merge Sort\n\nSort an array A[_p_ .. _r_] of comparable elements recursivly by divide and\nconquer:\n\n**Divide:**\n    Given A[_p_ .. _r_], split the given array into two subarrays A[_p_ .. _q_] and A[_q_+1 .. _r_] where _q_ is the halfway point of A[_p_ .. _r_].\n**Conquer:**\n    Recursively sort the two subarrays. If they are singletons, we have the base case. \n**Combine:**\n    Merge the two sorted subarrays with a (linear) procedure Merge ... \n![](fig/code-merge-sort.jpg)\n\nWe have seen in [Topic 2](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-02.html#mergesort) that this has the following recurrence (please\nreview Topic 2 if you don't see why):\n\n![](fig/recurrence-merge-subarray.jpg)\n\n### Recursive Solution to Maximum Subarray\n\nSuppose you have an array of numbers and need to find the subarray with the\nmaximum sum of elements in the subarray. (The problem is trival unless there\nare negative numbers involved.)\n\n![](fig/Fig-4-3-Maximum-Subarray.jpg)\n\nThe book provides a not very convincing application: there are applications to\ngraphics (2D version: finding the brightest spot in an image).\n\nThe following algorithm is not the fastest known (a linear solution exists),\nbut it illustrates divide and conquer. The solution strategy, given an array\nA[_low_ .. _high_], is:\n\n**Divide**\n     the subarray into two subarrays of equal size as possible by finding the midpoint _mid_ of the subarrays. \n**Conquer**\n    by finding a maximum subarray of A[_low_ .. _mid_] and A[_mid_+1 .. _high_].\n**Combine**\n    by also finding a maximum subarray that crosses the midpoint, and using the best solution of the three (the subarray crossing the midpoint and the best of the solutions in the Conquer step).\n\nThe strategy works because any subarray must lie in one of these three\npositions:\n\n![](fig/Fig-4-4-a-Subarrays.jpg)\n\n####  Pseudocode\n\nRecursion will handle the lower and upper halves. The algorithm relies on a\nhelper to find the crossing subarray. Any maximum subarray crossing the\nmidpoint must include arrays ending at A[_mid_] and starting at A[_mid_+1]:\n\n![](fig/Fig-4-4-b-Crossing.jpg)\n\nTherefore the pseudocode finds the maximum array on each side and adds them\nup:\n\n![](fig/find-max-crossing-subarray.jpg)\n\nIt should be clear that the above is Θ(n). The recursive solution follows.\n\n![](fig/find-maximum-subarray.jpg)\n\n_Check your understanding: Where is the work done? What adds up the values in\nthe left and right subarrays?_\n\n#### Analysis\n\nThe analysis relies on the simplifying assumption that the problem size is a\npower of 2 (the same assumption for merge sort). Let T(_n_) denote the running\ntime of FIND-MAXIMUM-SUBARRAY on a subarray of _n_ elements.\n\n**Base case:**\n    Occurs when _high_ equals _low_, so that _n=1_: it just returns in Θ(1) time. \n  \n**Recursive Case** (when _n_>1):\n    \n\n  * Dividing takes Θ(1) time. \n  * Conquering solves two subproblems, each on an array of n/2 elements: 2T(_n_/2). \n  * Combining calls FIND-MAX-CROSSING-SUBARRAY, which takes Θ(_n_), and some constant tests: Θ(_n_) + Θ(1). \nT(_n_)   =   Θ(1) + 2T(_n_/2) + Θ(_n_) + Θ(1)   =   2T(_n_/2) + Θ(_n_).\n\nThe resulting recurrence is the same as for merge sort:\n\n![](fig/recurrence-merge-subarray.jpg)\n\nSo how do we solve these? We have three methods: Substitution, Recursion\nTrees, and the Master Method.\n\n* * *\n\n##  Substitution Method\n\nDon't you love it when a \"solution method\" starts with ...\n\n  1. Guess the solution!\n  2. Use induction to find any unspecified constants and show that the solution works.\n\nRecursion trees (next section) are one way to guess solutions. Experience\nhelps too. For example, if a problem is divided in half we may expect to see\nlg _n_ behavior.\n\nAs an example, let's solve the recurrence for merge sort and maximum subarray.\nWe'll start with an exact rather than asymptotic version:\n\n![](fig/recurrence-merge-subarray-exact.jpg)\n\n  1. **Guess:**   T(_n_) = _n_ lg _n_ \\+ _n_.  _(Why this guess?)_\n  \n\n  2. **Induction:**\n\n**_Basis:_**\n    _n_ = 1   ⇒   _n_ lg _n_ \\+ _n_   =   1 lg 1 + 1   =   1   =   T(_n_). \n  \n**_Inductive Step:_**\n    Inductive hypothesis is that T(_k_) = _k_ lg _k_ \\+ _k_ for all _k < n_. We'll use _k = n/2_, and show that this implies that T(_n_) = _n_ lg _n_ \\+ _n_. First we start with the definition of T(_n_); then we substitute ...   \n![](fig/proof-merge-subarray-exact.jpg)\n\nInduction would require that we show our solution holds for the boundary\nconditions. This is discussed in the textbook.\n\nNormally we use asymptotic notation rather than exact forms:\n\n  * writing T(_n_) = 2T(_n/2_) + O(_n_),\n  * assuming T(_n_) = O(1) for sufficiently small _n_,\n  * not worrying about boundary or base cases, and\n  * writing solutions in asymptotic notation, e.g., T(_n_) = O(_n_ lg _n_).\n\nIf we want Θ, sometimes we can prove big-O and Ω separately \"squeezing\" the Θ\nresult.\n\nBut be careful when using asymptotic notation. For example, suppose you have\nthe case where _a_=4 and _b_=4 and want to prove T(_n_) = O(_n_) by guessing\nthat T(_n_) ≤ _cn_ and writing:\n\n![](fig/false-proof.jpg)\n\nOne must prove the _exact form_ of the inductive hypothesis, T(_n_) ≤ _cn_.\n\nSee the text for other strategies and pitfalls.\n\nProblems 4.3-1 and 4.3-2 are good practice problems.\n\n* * *\n\n##  Recursion Trees\n\nAlthough recursion trees can be considered a proof format, they are normally\nused to generate guesses that are verified by substitution.\n\n  * Each node represents the cost of a single subproblem in the set of recursive invocations\n  * Sum the costs with each level of the tree to obtain per-level costs\n  * Sum the costs across levels for the total cost.\n\n### A Familiar Example\n\nWe have already seen recursion trees when analyzing the recurrence relations\nfor Merge Sort:\n\n![](fig/recurrence-mergesort-c.jpg)  \n![](fig/recurrence-tree-mergesort-3.jpg)\n\nThe subproblems are of size _n_/20, _n_/21, _n_/22, .... The tree ends when\n_n_/2_p_ = _n_/_n_ = 1, the trivial subproblem of size 1.\n\nThus the height of the tree is the power _p_ to which we have to raise 2\nbefore it becomes _n_, i.e., _p_ = lg _n_. Since we start at 20 there are lg\n_n_ \\+ 1 levels. Multiplying by the work _cn_ at each level, we get _cn_ lg\n_n_ \\+ _cn_ for the total time.\n\n###  A More Complex Example\n\nA more complex example is developed in the textbook for\n\n> T(_n_) = 3T(_n_/4) + Θ(_n_2)\n\nwhich is rewritten (making the implied constant explicit) as\n\n> T(_n_) = 3T(_n_/4)+ _cn_2\n\n![](fig/Fig-4-5-Recursion-Tree-a.jpg) node, T(_n_) = 3T(_n_/4)\n+_cn_2.\n\nWe can develop the recursion tree in steps, as follows. First, we begin the\ntree with its root ![](fig/Fig-4-5-Recursion-Tree-b.jpg)\n\nNow let's branch the tree for the three recursive terms 3T(_n_/4). There are\nthree children nodes with T(_n_/4) as their cost, and we leave the cost _cn_2\nbehind at the root node.\n\nWe repeat this for the subtrees rooted at each of the nodes for T(_n/4_):\nSince each of these costs 3T((_n_/4)/4) +_c_(_n_/4)2, we make three branches,\neach costing T((_n_/4)/4) = T(_n_/16), and leave the _c_(_n_/4)2 terms behind\nat their roots.\n\n![](fig/Fig-4-5-Recursion-Tree-c.jpg)\n\nContinuing this way until we reach the leaf nodes where the recursion ends at\ntrivial subproblems T(1), the tree looks like this:\n\n![](fig/Fig-4-5-Recursion-Tree-d.jpg)\n\nSubproblem size for a node at depth _i_ is _n_/4_i_, so the subproblem size\nreaches _n_ = 1 when (assuming _n_ a power of 4) _n_/4_i_ = 1, or when _i_ =\nlog4_n_.  \nIncluding _i_ = 0, there are log4_n_ \\+ 1 levels. Each level has 3_i_ nodes.  \nSubstituting _i_ = log4_n_ into 3_i_, there are 3log4_n_ nodes in the bottom\nlevel.  \nUsing alogbc = clogba, there are _n_log43 in the bottom level (_not_ _n_, as\nin the previous problem).\n\nAdding up the levels, we get:  \n![](fig/solution-recursion-tree-1.jpg)\n\nIt is easier to solve this summation if we change the equation to an\ninequality and let the summation go to infinity (the terms are decreasing\ngeometrically), allowing us to apply equation A.6 (∑_k_=0,∞_xk_ = 1/1-_x_):  \n![](fig/gsolution-recursion-tree-2.jpg)\n\nAdditional observation: since the root contributes _cn2_, the root dominates\nthe cost of the tree, and the recurrence must also be Ω(_n_2), so we have\nΘ(_n_2).\n\nPlease see the text for an example involving unequal subtrees. For practice,\nexercises 4.4-6 and 4.4-9 have solutions posted on the book's web site.\n\n* * *\n\n##  Master Theorem & Method\n\nIf we have a divide and conquer recurrence of the form\n\n> T(_n_) = _a_T(_n/b_) + _f(n)_  \n  \nwhere _a ≥ 1_, _b > 1_, and _f(n) > 0_ is asymptotically positive,\n\nthen we can apply the **master method**, which is based on the **master\ntheorem**. We compare _f(n)_ to _nlogba_ under asymptotic (in)equality:\n\n**Case 1: _f(n)_ = O(_nlogba - ε_)** for some constant _ε_ > 0.  \n    (That is, _f(n)_ is polynomially smaller than _nlogba_.)  \n    **_Solution:_** T(_n_) = **Θ(_nlogba_).**   \n    Intuitively: the cost is dominated by the leaves.\n\n**Case 2: _f(n)_ = Θ(_nlogba_)**, or more generally (exercise 4.6-2): _f(n)_ = Θ(_nlogba_lg_k__n_), where _k_ ≥ 0.  \n    (That is, _f(n)_ is within a polylog factor of _nlogba_, but not smaller.)  \n    _**Solution:**_ T(_n_) = **Θ(_nlogba_lg_n_),** or T(_n_) = Θ(_nlogba_lg_k+1__n_) in the more general case.  \n    Intuitively: the cost is _nlogba_lg_k_ at each level and there are Θ(lg_n_) levels.\n\n**Case 3: _f(n)_= Ω(_nlogba + ε_)** for some constant _ε_ > 0, and _f(n)_ satisfies the regularity condition _af(n/b) ≤ cf(n)_ for some constant _c<1_ and all sufficiently large _n_.   \n    (That is, _f(n)_ is polynomially greater than _nlogba_.)  \n    _**Solution:**_ T(_n_) = **Θ(_f(n)_)**,  \n    Intuitively: the cost is dominated by the root.\n\nImportant: there are functions that fall between the cases!\n\n### Examples\n\n**T(_n_) = 5T(_n_/2) + Θ(_n_2)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_2\n  * Compare   _n_2   to   _n_log_b__a_ = _n_log25. \n  * log25 - ε = 2 for some constant ε > 0\\. \n  * Case 1: T(_n_) = Θ(_n_lg 5). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 lg _n_\n  * Compare   _n_3 lg _n_   to   _n_log327 = _n_3\n  * Case 2 with _k_ = 1: T(_n_) = Θ(_n_3 lg2 _n_). \n\n**T(_n_) = 5T(_n_/2) + Θ(_n_3)**\n\n  * _a_ = 5, _b_ = 2, _f_(_n_) = _n_3\n  * Compare   _n_3   to   _n_log25\n  * log25 + ε = 3 for some constant ε > 0\\. \n  * Check regularity condition (not necessary since _f_(_n_) is polynomial:  \n_a__f_(_n_/_b_) = 5(_n_/2)3 = 5_n_3/8 ≤ _cn_3 for _c_ = 5/8 < 1\\.\n\n  * Case 3: T(_n_) = Θ(_n_3). \n\n**T(_n_) = 27T(_n_/3) + Θ(_n_3 / lg _n_)**\n\n  * _a_ = 27, _b_ = 3, _f_(_n_) = _n_3 / lg _n_\n  * Compare   _n_3/lg _n_   to   _n_log327 = _n_3\n  * Cases 1 and 3 won't work as no ε can adjust the exponent of 3 to account for the 1/lg_n_ = lg−1_n_ factor. Only hope is Case 2. \n  * But _n_3/lg _n_ = _n_3 lg−1_n_ ≠ Θ(_n_3 lg_k_ _n_) for any _k_ ≥ 0\\. \n  * Cannot use master method. \n  * Could try substitution, which requires a guess. Drawing the full recursion tree would be tedious, but perhaps visualizing its general form would help with the guess. \n\n* * *\n\n## Next\n\nChapter 12, Binary Search Trees (entire chapter), to which we can apply divide\n& conquer and use recurrence relations.\n\n* * *\n\nDan Suthers Last modified: Sat Feb 8 02:42:12 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//07.divide-conquer/reading-notes-7.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-screencast-7a.html</h2>

<pre>Hash
{"title"=>"Divide and conquer and recurrence relations",
 "published"=>true,
 "morea_id"=>"reading-screencast-7a",
 "morea_summary"=>"Introduction to the divide and conquer algorithm",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=W7rChliGE5M",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-screencast-7a.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-screencast-7a.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-screencast-7b.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: substitution",
 "published"=>true,
 "morea_id"=>"reading-screencast-7b",
 "morea_summary"=>"How to perform substitution.",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=X2D80jsS3sY",
 "morea_labels"=>["Screencast", "Suthers", "8 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-screencast-7b.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-screencast-7b.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-screencast-7c.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: recursion trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-7c",
 "morea_summary"=>
  "How to generate a guess for the form of the solution to the recurrence.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8F2OvQIlGiU",
 "morea_labels"=>["Screencast", "Suthers", "19 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-screencast-7c.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-screencast-7c.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-screencast-7d.html</h2>

<pre>Hash
{"title"=>"Solving recurrence relations: master method",
 "published"=>true,
 "morea_id"=>"reading-screencast-7d",
 "morea_summary"=>
  "Find solutions to recurrence relations of form T(n) = aT(n/b) + h(n), where a and b are constants, a ≥ 1 and b > 1",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=h4Avr0byu1g",
 "morea_labels"=>["Screencast", "Suthers", "17 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-screencast-7d.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-screencast-7d.md"}
</pre>

<h2>/morea/07.divide-conquer/reading-screencast-mit-divide-conquer.html</h2>

<pre>Hash
{"title"=>"Divide and conquer",
 "published"=>true,
 "morea_id"=>"reading-screencast-mit-divide-conquer",
 "morea_summary"=>
  "Divide and conquer: binary search, powering a number, fibonacci numbers, matrix multiplication",
 "morea_type"=>"reading",
 "morea_sort_order"=>7,
 "morea_url"=>"http://videolectures.net/mit6046jf05_demaine_lec03/",
 "morea_labels"=>["Screencast", "Demaine", "68 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/07.divide-conquer/reading-screencast-mit-divide-conquer.html",
 "content"=>"",
 "path"=>"morea//07.divide-conquer/reading-screencast-mit-divide-conquer.md"}
</pre>

<h2>/morea/08.binary-search-trees/experience-binary-search-trees-2.html</h2>

<pre>Hash
{"title"=>"More reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees some more.",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/08.binary-search-trees/experience-binary-search-trees-2.html",
 "url"=>"/morea/08.binary-search-trees/experience-binary-search-trees-2.html",
 "content"=>
  "### Binary Search Trees\n\n**4.** (4 pts) Suppose you have some data keys sorted in an array and you want to construct a _**balanced binary search tree**_ from them. Assume a tree node representation `TreeNode` that includes instance variables `key`, `left`, and `right`.\n\n**a.**  Write pseudocode (or Java if you wish) for an algorithm that constructs the tree and returns the root node. (We won't worry about making the enclosing `BinaryTree` class instance.) You will need to use methods for making a new `TreeNode`, and for setting its left and right children.\n\n_Hints: First, identify the array location of the key that would have to be\nthe root of the balanced BST. Now think about how BinarySearch works on the\narray. Which item does it access first in any given subarray it is called\nwith? Using a similar strategy a simple recursive algorithm is possible._\n\n**b.**   What is the Θ cost to construct the tree? How does the expected runtime of BinarySearch on the array compare to the expected runtime of search in the tree you just constructed? \n\n\n\n**5.** (3 pts) In `Tree-Delete` (page 298 or as shown in the web notes), when node _z_ has two children, we arbitrarily decide to replace it with its successor. We could just as well replace it with its predecessor. \n\n**a.**   Rewrite `Tree-Delete` to use the predecessor rather than the successor. Modify this code just as you need to.\n    \n    \n      TREE-DELETE(T, z)\n      1  if z.left == NIL\n      2      TRANSPLANT(T, z, z.right)\n      3  elseif z.right == NIL\n      4     TRANSPLANT(T, z, z.left)\n      5  else y = TREE-MINIMUM(z.right)  // successor\n      6      if y.p != z\n      7          TRANSPLANT(T, y, y.right) \n      8          y.right = z.right\n      9          y.right.p = y\n      10      TRANSPLANT(T, z, y)\n      11      y.left = z.left\n      12      y.left.p = y\n    \n\n**b.**   Some computer scientists have argued that if equal priority were given to replacing the successor and the predecessor to not skew deletions on one side, better performance might result. How might `Tree-Delete` be modified to implement such a strategy? (_Hint:_ think about last week's topics.)\n",
 "path"=>"morea//08.binary-search-trees/experience-binary-search-trees-2.md"}
</pre>

<h2>/morea/08.binary-search-trees/experience-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Reasoning about binary search trees",
 "published"=>true,
 "morea_id"=>"experience-binary-search-trees",
 "morea_type"=>"experience",
 "morea_summary"=>"Apply your learning about binary trees.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>
  "/ics311s14/morea/08.binary-search-trees/experience-binary-search-trees.html",
 "url"=>"/morea/08.binary-search-trees/experience-binary-search-trees.html",
 "content"=>
  "# Reasoning about binary search trees\n\n**1.** Show that if a node in a binary search tree has two children, then its successor Y has no left child and its predecessor has no right child. (_The proofs are symmetric. Hints: Rule out where the successor cannot be to narrow down to where it must be. Draw Pictures!!!_) \n\n> **(a)** Prove by contradiction that the successor Y cannot be an ancestor of\nX, so Y must be in a subtree.  \n**(b)** Identify and prove the subtree of X that successor Y must be in.   \n**(c)** Show by contradiction that successor Y cannot have a left child.  \n**(d)** Indicate how this proof would be changed for predecessor. \n\n**2\\. ** Delete the nodes with keys 10 and 27 from this Binary Search Tree, indicating for each case what \"if/elseif\" block is executed. (_You will need to apply the cases carefully to get this right: refer to the text or web notes. \"Eyeballing\" it may lead to a legal tree that would not result from the code._) \n\n![](fig/BST-for-Class-Problem-small.jpg)\n\n> **(a)** Lines executed in deletion of 10:  \n**(b)** Lines executed in deletion of 27: \n\n![](fig/pseudocode-tree-delete.jpg)",
 "path"=>"morea//08.binary-search-trees/experience-binary-search-trees.md"}
</pre>

<h2>/morea/08.binary-search-trees/module-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Binary Search Trees",
 "published"=>true,
 "morea_id"=>"binary-search-trees",
 "morea_outcomes"=>["outcome-binary-search-trees"],
 "morea_readings"=>
  ["reading-screencast-8a",
   "reading-screencast-8b",
   "reading-screencast-8c",
   "reading-screencast-8d",
   "reading-cormen-12",
   "reading-notes-8"],
 "morea_experiences"=>
  ["experience-binary-search-trees", "experience-binary-search-trees-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>
  "/morea/08.binary-search-trees/module-binary-search-trees.svg",
 "morea_sort_order"=>8,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/08.binary-search-trees/module-binary-search-trees.html",
 "content"=>
  "Trees in which each vertex is either a leaf or has up to two non-empty descendents.\n",
 "path"=>"morea//08.binary-search-trees/module-binary-search-trees.md"}
</pre>

<h2>/modules/binary-search-trees/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-binary-search-trees.md",
 "title"=>"Binary Search Trees",
 "url"=>"/modules/binary-search-trees/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/binary-search-trees/index.html"}
</pre>

<h2>/morea/08.binary-search-trees/outcome-binary-search-trees.html</h2>

<pre>Hash
{"title"=>"Understand binary search trees",
 "published"=>true,
 "morea_id"=>"outcome-binary-search-trees",
 "morea_type"=>"outcome",
 "morea_sort_order"=>10,
 "referencing_modules"=>[#Jekyll:Page @name="module-binary-search-trees.md"],
 "url"=>"/morea/08.binary-search-trees/outcome-binary-search-trees.html",
 "content"=>
  "Understand the properties of binary search trees and how to apply them. \n",
 "path"=>"morea//08.binary-search-trees/outcome-binary-search-trees.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-cormen-12.html</h2>

<pre>Hash
{"title"=>"CLRS 12 - Binary search trees",
 "published"=>true,
 "morea_id"=>"reading-cormen-12",
 "morea_summary"=>
  "What is a binary search tree; querying, insertion, and deletion.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "13 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/08.binary-search-trees/reading-cormen-12.html",
 "content"=>"",
 "path"=>"morea//08.binary-search-trees/reading-cormen-12.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-notes-8.html</h2>

<pre>Hash
{"title"=>"Chapter 8 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-8",
 "morea_summary"=>"Notes on binary search trees",
 "morea_type"=>"reading",
 "morea_sort_order"=>8,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/08.binary-search-trees/reading-notes-8.html",
 "url"=>"/morea/08.binary-search-trees/reading-notes-8.html",
 "content"=>
  "## Outline\n\n  1. Trees, Binary Trees, Binary Search Trees \n  2. Querying BSTs\n  3. Modifying BSTs (Insertion and Deletion)\n  4. Performance of BSTs \n\n##  Trees, Binary Trees, Binary Search Trees\n\nFirst, a preliminary look at trees. (This should be review. Some of this\nmaterial is taken from Thomas Standish Data Structure Techniques (1980) and\nGoodrich & Tamassia (1998) as well as the Cormen appendix, but is widely\npublished.)\n\n###  Fundamental Theorem of Free Trees\n\nIf _G_=(_V_,_E_) is a finite graph with _v > 1_ vertices, the following\nproperties are equivalent definitions of a generalized or **free tree**:\n\n  1. _G_ is connected and has no simple cycles. \n  2. _G_ has no simple cycles and has _v-1_ edges (|_E_| = |_V_| - 1)\n  3. _G_ is connected and has _v-1_ edges.\n  4. _G_ is acyclic, and if an edge is added that joins two nonadjacent vertices, exactly one cycle is formed.\n  5. _G_ is connected, but if an edge is deleted, _G_ becomes disconnected. \n  6. Every pair of vertices is connected by exactly one path. \n\nAlthough this is a definition, the theorem is that these definitions are\nequivalent. A classic exercise in basic graph theory is to prove each of these\nstatements using the one before it, and #1 from #6.\n\n#### Comments\n\nWhen we use the term \"tree\" without qualification, we will assume that we mean\na free tree unless the context makes it clear otherwise (e.g., when we are\ndiscussing binary trees).\n\nIn some contexts, _G_=({},{}) and _G_=({_v_},{}) are also treated as trees.\nThese are obvious base cases for recursive algorithms.\n\nA **forest** is a (possibly disconnected) graph, each of whose connected\ncomponents is a tree.\n\nAn **oriented tree** is a directed graph having a designated vertex _r_,\ncalled the **root**, and having exacly one oriented path between the root and\nany vertex _v_ distinct from the root, in which _r_ is the origin of the path\nand _v_ the terminus.\n\nIn some fields (such as social network analysis), the word \"node\" is used\ninterchangeably with \"vertex\". I use \"vertex\" in these notes but may slip into\n\"node\" in my recorded lectures or in class.\n\n![](fig/diagram-tree-heights-tall.jpg)\n\n### Binary Trees\n\nA **binary tree** is a finite set of vertices that is either empty or consists\nof a vertex called the root, together with two binary subtrees that are\ndisjoint from each other and from the root and are called the **left** and\n**right subtrees**.\n\nA **full binary tree ** is a binary tree in which each vertex either is a leaf\nor has exactly two nonempty descendants. In a full binary tree of height _h_:\n\n  1. number of leaves = (number internal vertices) + 1.\n  2. number leaves is at least _h_+1 _(first example figure)_ and at most 2_h_ _(second example figure)_.\n\n![](fig/diagram-tree-heights-wide.jpg)\n\n  3. number internal vertices is at least _h_ _(first example)_ and at most 2_h_-1 _(second example)_.\n  4. Total number of vertices (summing the last two results) is at least 2_h_+1 _(first example)_ and at most 2_h+1_-1 _(second example)_.\n  5. Height _h_ is at least lg(_n_+1)-1 _(second example)_ and at most (_n_-1)/2 _(first example)_\n\nA **complete binary tree ** is full binary tree in which all leaves have the\nsame depth and all internal vertices have degree 2 _(e.g., second example\nabove)_.\n\n(_Note:_ some earlier texts allow the last level of a \"complete\" tree to be\nincomplete! They are defined as binary trees with leaves on at most two\nadjacent levels _l-1_ and _l_ and in which the leaves at the bottommost level\n_l_ lie in the leftmost positions of _l_.)\n\n### Binary Search Trees (BSTs)\n\nA **binary search tree** (BST) is a binary tree that satisfies the **binary\nsearch tree property:**\n\n  * if _y_ is in the left subtree of _x_ then _y.key ≤ x.key_. \n  * if _y_ is in the right subtree of _x_ then _y.key ≥ x.key_. \n\nBSTs provide a useful implementation of the Dynamic Set ADT, as they support\nmost of the operations efficiently (as will be seen).\n\nTwo examples on the same data:  \n![](fig/Fig-12-1a-balanced.jpg) ![](fig/Fig-12-1b-\nunbalanced.jpg)\n\n_Could we just just say \"if y is the **left child** of x then y.key ≤ x.key,\netc., and rely on transitivity? What would go wrong?_\n\nImplementations of BSTs include a _root_ instance variable. Implementations of\nBST vertices usually include fields for the _key_, _left_ and _right_\nchildren, and the _parent_.\n\n* * *\n\n## Querying Binary Search Trees\n\nNote that all of the algorithms described here are given a tree vertex as a\nstarting point. Thus, they can be applied to any subtree of the tree as well\nas the full tree.\n\n### Traversing Trees\n\nTraversals of the tree \"visit\" (e.g., print or otherwise operate on) each\nvertex of the tree exactly once, in some systematic order. This order can be\n**Inorder**, **Preorder**, or **Postorder**, according to when a vertex is\nvisited relative to its children. Here is the code for inorder:\n\n![](fig/pseudocode-inorder-tree-walk.jpg)\n\n_Quick exercise: Do INORDER-TREE-WALK on this tree ... in what order are the\nkeys printed?_\n\n![](fig/example-BST-simple.jpg)\n\n_Quick exercise: How would you define Preorder traversal? Postorder\ntraversal?_\n\nTraversals can be done on any tree, not just binary search trees. For example,\ntraversal of an expression tree will produce preorder, inorder or postorder\nversions of the expressions.\n\n#### Time to Traverse a BST\n\n**Time:** Traversals (INORDER-TREE-WALK and its preorder and postorder variations) take _T_(_n_) = Θ(_n_) time for a tree with _n_ vertices, because we visit and print each vertex once, with constant cost associated with moving between vertices and printing them. More formally, we can prove as follows:\n\n_T_(_n_) = Ω(_n_) since these traversals must visit all _n_ vertices of the\ntree.\n\n_T_(_n_) = O(_n_) can be shown by substitution. First the base case of the\nrecurrence relation captures the work done for the test _x_ ≠ NIL:\n\n> _T_(0) = _c_ for some constant c > 0\n\nTo obtain the recurrence relation for _n_ > 0, suppose the traversal is called\non a vertex _x_ with _k_ vertices in the left subtree and _n_−_k_−1 vertices\nin the right subtree, and that it takes constant time _d_ > 0 to execute the\nbody of the traversal exclusive of recursive calls. Then the time is bounded\nby\n\n> _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_.\n\nWe now need to \"guess\" the inductive hypothesis to prove. The \"guess\" that\nCLRS use is _T_(_n_) ≤ (_c_ \\+ _d_)_n_ \\+ _c_, which is clearly O(_n_). It's\nless clear how they got this guess. As discussed in Chapter 4, section 4\n(especially subsection \"Subtleties\" page 85-86), one must prove the exact form\nof the inductive hypothesis, and sometimes you can get a better guess by\nobserving how your original attempt at the proof fails. Perhaps this is what\nthey did. We'll skip the failure part and go directly to proving their\nhypothesis by substitution (showing two steps skipped over in the book):\n\n> **_Inductive hypothesis:_** Suppose that _T_(_m_) ≤ (_c_ \\+ _d_)_m_ \\+ _c_\nfor all _m_ < _n_  \n  \n**_Base Case:_** (_c_ \\+ _d_)0 + _c_ = _c_ = _T_(0) as defined above.  \n  \n**_Inductive Proof:_**  \n   _T_(_n_) ≤ _T_(_k_) + _T_(_n_−_k_−1) + _d_\n_by definition_  \n           = ((_c_ \\+ _d_)_k_ \\+ _c_) + ((_c_ \\+ _d_)(_n_−_k_−1) + _c_) + _d_    _substiting inductive hypothesis for values < n_   \n            = ((_c_ \\+ _d_)(_k_ \\+ _n_ − _k_ − 1) + _c_ \\+ _c_ \\+ _d_             _collecting factors _   \n            = ((_c_ \\+ _d_)(_n_ − 1) + _c_ \\+ _c_ \\+ _d_                         _simplifying _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_ − (_c_ \\+ _d_) + _c_ \\+ _d_                   _multiplying out _n_−1 and rearranging _   \n            = ((_c_ \\+ _d_)_n_ \\+ _c_.                                            _the last terms cancel._\n\n### Searching for an Element in a BST\n\nHere are two implementations of the dynamic set operation `search`:\n\n![](fig/pseudocode-recursive-tree-search.jpg) ![](fig\n/pseudocode-iterative-tree-search.jpg)\n\n_Quick exercise: Do TREE-SEARCH for D and C on this tree ... _\n\n![](fig/example-BST-simple.jpg)\n\nFor now, we will characterize the run time of the remaining algorithms in\nterms of _h_, the height of the tree. Then we will consider what _h_ can be as\na function of _n_, the number of vertices in the tree.\n\n**Time:** Both of the algorithms visit vertices on a downwards path from the root to the vertex sought. In the worst case, the leaf with the longest path from the root is reached, examining _h_+1 vertices (_h_ is the height of the tree, so traversing the longest path must traverse _h_ edges, and _h_ edges connect _h_+1 vertices). Comparisons and movements to the chosen child vertex are O(1), so the algorithm is O(_h_). (_Why don't we say Θ?_) \n\n### Finding the Minimum and Maximum Element\n\nThe BST property guarantees that:\n\n  * The minimum key of a BST is located at the leftmost vertex.\n  * The maximum key of a BST is located at the rightmost vertex.\n\n_(Why?)_ This leads to simple implementations:\n\n![](fig/pseudocode-tree-min-max.jpg) ![](fig/example-\nBST-simple.jpg)\n\n**Time:** Both procedures visit vertices on a path from the root to a leaf. Visits are O(1), so again this algorithm is O(_h_).\n\n###  Finding the Successor or Predecessor of an Element\n\nAssuming that all keys are distinct, the successor of a vertex _x_ is the\nvertex _y_ such that _y.key_ is the smallest _key_ > _x.key_. If _x_ has the\nlargest key in the BST, we define the successor to be NIL.\n\nWe can find _x_'s successor based entirely on the tree structure (no key\ncomparison is needed). There are two cases:\n\n  1. **If vertex _x_ has a non-empty right subtree, then _x_'s successor is the minimum in its right subtree.** _(Why?)_\n  2. **If vertex _x_ has an empty right subtree, then _y_ is the lowest ancestor of _x_ whose left child is also an ancestor of _x_.**   _To see this, consider these facts: _\n    * If _y_ is the successor of _x_ then _x_ is the predecessor of _y_, so _x_ is the maximum in _y_'s left subtree _(flip the reasoning of your answer to the last question)_.\n    * Moving from _x_ to the left up the tree (up through right children) reaches vertices with smaller keys, which must also be in this left subtree. \n![](fig/pseudocode-tree-successor.jpg)\n\n_Exercise: Write the pseudocode for TREE-PREDECESSOR_\n\nLet's trace the min, max, successor (15, 13, 6, 4), and predecessor (6)\noperations:\n\n![](fig/Fig-12-2-example-BST.jpg)\n\n**Time:** The algorithms visit notes on a path down or up the tree, with O(1) operations at each visit and a maximum of _h+1_ visitations. Thus these algorithms are O(_h_). \n\n_Exercise: Show that if a vertex in a BST has two children, then its succesor\nhas no left child and its predecessor has no right child._\n\n* * *\n\n##  Modifying Binary Search Trees\n\nThe key point is that the BST property must be sustained. This is more\nstraightforward with insertion (as we can add a vertex at a leaf position)\nthan with deletion (where an internal vertex may be deleted).\n\n###  Insertion\n\nThe algorithm assumes that the vertex _z_ to be inserted has been initialized\nwith _z.key_ = _v_ and _z.left_ = _z.right_ = NIL.\n\nThe strategy is to conduct a search (as in tree search) with pointer _x_, but\nto sustain a **trailing pointer** _y_ to keep track of the parent of _x_. When\n_x_ drops off the bottom of the tree (becomes NIL), it will be appropriate to\ninsert _z_ as a child of _y_.\n\nComment on variable naming: I would have preferred that they call _x_\nsomething like `leading` and _y_ `trailing`.\n\n![](fig/pseudocode-tree-insert.jpg)\n\nTry `TREE-INSERT(T,C)`:\n\n![](fig/example-BST-simple.jpg)\n\n**Time:** The same as TREE-SEARCH, as there are just a few additional lines of O(1) pointer manipulation at the end.\n\n_Discuss: How would you use TREE-INSERT and INORDER-TREE-WALK to sort a set of\nnumbers?_  \n_Think about at home: How would you prove its time complexity?_\n\n###  Deletion\n\nDeletion is more complex, as the vertex _z_ to be deleted may or may not have\nchildren. We can think of this in terms of three cases:\n\n  1. If _z_ has no children, we can just remove it (by setting _z_'s parent's pointer to NIL). \n  2. If _z_ has just one child _c_, then make _c_ take _z_'s position in the tree, updating _z_'s parent to point to _c_ and \"dragging\" _c_'s subtree along.\n  3. If _z_ has two children, find _z_'s successor _y_ and replace _z_ by _y_ in the tree (noting that _y_ has no left child): \n    * If _y_ is _z_'s right child, then replace _z_ by _y_ (including updating _z_'s parent to point to _y_, and _y_ to point to _z_'s left child) and we are done. \n    * Otherwise _y_ is further down in _z_'s right subtree (and again has no left child): \n      1. Replace _y_ with its own right child. \n      2. The rest of _z_'s right subtree becomes _y_'s new right subtree.\n      3. _z_'s left subtree becomes _y_'s new left subtree.\n      4. Make _z_'s parent point to _y_.\n\nThe code organizes the cases differently to simplify testing and make use of a\ncommon procedure for moving subtrees around. This procedure replaces the\nsubtree rooted at _u_ with the subtree rooted at _v_.\n\n  * It makes _u_'s parent become _v_'s parent (lines 6-7), unless _u_ is the root, in which case it makes _v_ the root (lines 1-2).\n  * _v_ replaces _u_ as _u_'s parent's left or right child (lines 3-5).\n  * It does not update _v.left_ or _v.right_, leaving that up to the caller. \n![](fig/pseudocode-transplant.jpg)\n\n_(If we have time, draw a few examples.)_\n\nHere are the four actual cases used in the main algorithm TREE-DELETE(T,_z_):\n\n![](fig/Fig-12-4-a-no-left-child.jpg)\n\n#### No left child (and possibly no children):\n\nIf _z_ has no left child, replace _z_ by its right child (which may or may not\nbe NIL). This handles case 1 and half of case 2 in the conceptual breakdown\nabove. (Lines 1-2 of final algorithm.)\n\n![](fig/Fig-12-4-b-no-right-child.jpg)\n\n#### No right child (and has left child):\n\nIf _z_ has just one child, and that is its left child, then replace _z_ by its\nleft child. This handles the rest of case 2 in the conceptual breakdown above.\n(Lines 3-4.)\n\nNow we just have to deal with the case where both children are present. Find\n_z_'s successor (line 5), which must lie in _z_'s right subtree and have no\nleft child (_why?_). Handling depends on whether or not the successor is\nimmediately referenced by _z_:\n\n![](fig/Fig-12-4-c-successor-is-child.jpg)\n\n#### Successor is child:\n\nIf successor _y_ is _z_'s right child (line 6), replace _z_ by _y_, \"pulling\nup\" _y_'s right subtree. The left subtree of _y_ is empty so we can make _z_'s\nformer left subtree _l_ be _y_'s new left subtree. (Lines 10-12.)\n\n#### Successor is not child:\n\nOtherwise, _y_ is within _z_'s right subtree rooted at _r _but is not the root\nof this subtree (_y≠r_).\n\n  1. Replace _y_ by its own right child _x_. (Line 7.)\n  2. Set _y_ to be _r_'s parent. (Line 8-9.)\n  3. Then let _y_ take _z_'s place with respect to _z_'s parent __ and left child _l_. (Lines 10-12.)\n![](fig/Fig-12-4-d-successor-not-child.jpg)\n\nNow we are ready for the full algorithm:\n\n![](fig/pseudocode-tree-delete.jpg)\n\nThe last three lines excecute whenever _z_ has two children (the last two\ncases above).\n\nLet's try `TREE-DELETE(T,_x_)` on _x=_ I, G, K, and B:\n\n![](fig/example-BST-to-delete.jpg)\n\n**Time:** Everything is O(1) except for a call to TREE-MINIMUM, which is O(_h_), so TREE-DELETE is O(_h_) on a tree of height _h_. \n\nThe above algorithm fixes a problem with some published algorithms, including\nthe first two editions of the book. Those versions copy data from one vertex\nto another to avoid a tree manipulation. If other program components maintain\npointers to tree vertices (or their positions in Goodrich & Tamassia's\napproach), this could invalidate their pointers. The present version\nguarantees that a call to TREE-DELETE(T, _z_) deletes exactly and only vertex\n_z_.\n\nAn animation is available at\n<http://www.csc.liv.ac.uk/~ullrich/COMP102/applets/bstree/> (The code shown\nprobably has the flaw discussed above.)\n\n* * *\n\n##  Performance of Binary Search Trees\n\nWe have been saying that the asympotic runtime of the various BST operations\n(except traversal) are all O(lg _h_), where _h_ is the height of the tree. But\n_h_ is usually hidden from the user of the ADT implementation and we are more\nconcerned with the runtime as a function of _n_, our input size. So, what is\n_h_ as a function of _n_?\n\nWe know that in the worst case, _h_ = O(_n_) (when the tree degenerates to a\nlinear chain). Is this the expected case? Can we do anything to guarantee\nbetter performance? These two questions are addressed below.\n\n###  Expected height of randomly built binary search trees\n\nThe textbook has a proof in section 12.4 that **the expected height of a\nrandomly build binary search tree on _n_ distinct keys is O(lg _n_).**\n\nWe are not covering the proof (and you are not expected to know it), but I\nrecommend reading it, as the proof elegantly combines many of the ideas we\nhave been developing, including indicator random variables and recurrences.\n(They take a huge step at the end: can you figure out how the log of the last\npolynomial expression simplifies to O(lg _n_)?)\n\nAn alternative proof provided by Knuth (Art of Computer Programming Vol. III,\n1973, p 247), and also summarized by Standish, is based on average path\nlengths in the tree. It shows that about 1.386 lg _n_ comparisons are needed:\n**the average tree is about 38.6% worse than the best possible tree in number\nof comparisons required for average search**.\n\nSurprisingly, _analysts have not yet been able to get clear results when\nrandom deletions are also included_.\n\n### Balanced Trees\n\nGiven the full set of keys in advance, it is possible to build an optimally\nbalanced BST for those keys (guaranteed to be lg _n_ height). See section 15.5\nof the Cormen et al. text.\n\nIf we don't know the keys in advance, many clever methods exist to keep trees\nbalanced, or balanced within a constant factor of optimal, by performing\nmanipulations to re-balance after insertions (AVL trees, Red-Black Trees), or\nafter all operations (in the case of splay trees). We cover Red-Black Trees in\ntwo weeks ([Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html)),\nafter a diversion to heaps (which have tree-like structure) and sorting.\n\n* * *\n\n## Next\n\nIn [Topic\n09](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-09.html) we\nlook at how a special kind of tree, a Heap, can be embedded in an array and\nused to implement a sorting algorithm and priority queues.\n\nAfter a brief diversion to look at other sorting algorithms, we will return to\nother kinds of trees, in particular special kinds of binary search trees that\nare kept balanced to guarantee O(lg _n_) performance, in [Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sun Feb 16 02:15:30 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//08.binary-search-trees/reading-notes-8.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-screencast-8a.html</h2>

<pre>Hash
{"title"=>"Introduction to binary search trees",
 "published"=>true,
 "morea_id"=>"reading-screencast-8a",
 "morea_summary"=>"Basic qualitative facts about BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=bAxzRuu3Uy4",
 "morea_labels"=>["Screencast", "Suthers", "15 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/08.binary-search-trees/reading-screencast-8a.html",
 "content"=>"",
 "path"=>"morea//08.binary-search-trees/reading-screencast-8a.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-screencast-8b.html</h2>

<pre>Hash
{"title"=>"BST Queries",
 "published"=>true,
 "morea_id"=>"reading-screencast-8b",
 "morea_summary"=>"How to perform queries on BSTs",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=LDncFcNOr_I",
 "morea_labels"=>["Screencast", "Suthers", "22 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/08.binary-search-trees/reading-screencast-8b.html",
 "content"=>"",
 "path"=>"morea//08.binary-search-trees/reading-screencast-8b.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-screencast-8c.html</h2>

<pre>Hash
{"title"=>"Modifying BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8c",
 "morea_summary"=>"How to modify binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=qJ7TyaKSf_0",
 "morea_labels"=>["Screencast", "Suthers", "21 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/08.binary-search-trees/reading-screencast-8c.html",
 "content"=>"",
 "path"=>"morea//08.binary-search-trees/reading-screencast-8c.md"}
</pre>

<h2>/morea/08.binary-search-trees/reading-screencast-8d.html</h2>

<pre>Hash
{"title"=>"Analyzing BSTs",
 "published"=>true,
 "morea_id"=>"reading-screencast-8d",
 "morea_summary"=>"Determine the height of binary search trees.",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=vx0uWYHIRes",
 "morea_labels"=>["Screencast", "Suthers", "5 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/08.binary-search-trees/reading-screencast-8d.html",
 "content"=>"",
 "path"=>"morea//08.binary-search-trees/reading-screencast-8d.md"}
</pre>

<h2>/morea/09.heaps/experience-heaps-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps (again)",
 "published"=>true,
 "morea_id"=>"experience-heaps-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps (at home).",
 "morea_sort_order"=>1,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/09.heaps/experience-heaps-2.html",
 "url"=>"/morea/09.heaps/experience-heaps-2.html",
 "content"=>
  "#### 1\\. Peer Credit Assignment\n\nPlease list the names of the other members of your peer group for this week\nand the number of extra credit points you think they deserve for their\nparticipation in group work on Tuesday and Thursday combined.\n\n  * If all three members besides yourself were present at some time, you have a total of 3 points to allocate.\n  * If only two members besides yourself were present, you have a total of 4 points to allocate.\n  * If only one other member was present, you have a total of 6 points to allocate.\n  * You need not allocate all the points available to you. Points allocated to yourself will not be recorded.\n\n* * *\n\n### Heaps\n\n#### 8 points\n\n**2\\. ** Illustrate `Build-Max-Heap` on this data in a 1-based indexing array:\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5],\n    \n\nRewrite the array as it exists after each execution of line 3 (the call to\n`Max-Heapify`). A template is provided below. Please use a plain text editor\nwith fixed width font and replace each underscore character \"_\" with the\ncorrect value.\n\n    \n    \n      index        1  2  3  4  5  6  7  8  9\n      Start:  A = [1, 6, 2, 8, 3, 9, 4, 7, 5]\n    \n      i = 4:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 3:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 2:  A = [_, _, _, _, _, _, _, _, _]\n    \n      i = 1:  A = [_, _, _, _, _, _, _, _, _]\n    \n\nYou may want to draw the heap in tree form and do the operations on the tree.\nYou are encouraged to include these trees in your response to help Robert\n\"debug\" any problems, but grading will initially be done on the above\ntemplate.\n\n**3\\. ** Illustrate `Heap-Extract-Max` on the heap you constructed above. Show the array representation:\n\n**(a)** After line 5 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**(b)** After line 6 has finished executing\n    \n    \n      A = [_, _, _, _, _, _, _, _, _]\n    \n\n**4\\. ** Consider now min-heaps rather than max-heaps. Write pseudocode for `Min-Heapify` and `Heap-Decrease-Key`, by copying the textbook's code for the max versions and changing only what you need to change. To make grading easier, please highlight, boldface or circle your changes.\n    \n    \n      MAX-HEAPIFY (A, i) // Change to **MIN**-HEAPIFY \n      1   l = LEFT(i)\n      2   r = RIGHT(i)\n      3   if l <= A.heap-size and A[l] > A[i]\n      4       largest = l\n      5   else largest = i\n      6   if r <= A.heap-size and A[r] > A[largest]\n      7       largest = r\n      8   if largest != i\n      9       exchange A[i] with A[largest]\n      10      MAX-HEAPIFY (A, largest)\n    \n      HEAP-INCREASE-KEY (A, i, key) // Change to HEAP-**DECREASE**-KEY\n      1  if key < A[i]\n      2      error \"new key is smaller than current key\"\n      3  A[i] = key\n      4  while i > 1 and A[PARENT(i)] < A[i]\n      5      exchange A[i] with A[PARENT(i)]\n      6      i = PARENT(i)\n    \n\n\n",
 "path"=>"morea//09.heaps/experience-heaps-2.md"}
</pre>

<h2>/morea/09.heaps/experience-heaps.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of heaps",
 "published"=>true,
 "morea_id"=>"experience-heaps",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about heaps.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/09.heaps/experience-heaps.html",
 "url"=>"/morea/09.heaps/experience-heaps.html",
 "content"=>
  "### 1\\. Heap-Delete(A, i)\n\n#### 2 points\n\nProcedure `Heap-Delete(_A_, _i_)` deletes the node at index _i_ in heap _A_\n(represented as an array). Give an implementation of `Heap-Delete` that runs\nin O(lg _n_) time for a heap of size _n_ = `A.heapSize`. You may use instance\nvariable `A.heapSize` and any of the other procedures already defined in the\ntext.\n\n_This is very similar to an existing procedure._\n\n>\n\n>     Heap-Delete(A,i)\n\n>  \n\n### 2\\. Heapsort on Sorted Data\n\n#### 3 points\n\n**(a)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_increasing_** order_?\n\n**(b)** What is the asymptotic running time of `Heapsort` on an array _A_ of _n_ elements that is _already sorted in **_decreasing_** order_?\n\n**(c)** For which of these cases would `Heapsort` make _more swaps of elements in the array_, or are they the same?\n\n_Give your reasoning to help grading feedback. You might start working an\nexample for each case, but don't get bogged down in details: return to high\nlevel asymptotic reasoning as soon as you see what is going on. _Refer to line\nnumbers in code _ when discussing your analyses. _\n\n\n",
 "path"=>"morea//09.heaps/experience-heaps.md"}
</pre>

<h2>/morea/09.heaps/module-heaps.html</h2>

<pre>Hash
{"title"=>"Heapsort",
 "published"=>true,
 "morea_id"=>"heaps",
 "morea_outcomes"=>["outcome-heaps"],
 "morea_readings"=>
  ["reading-screencast-9a",
   "reading-screencast-9b",
   "reading-screencast-9c",
   "reading-screencast-9d",
   "reading-cormen-6",
   "reading-notes-9"],
 "morea_experiences"=>["experience-heaps", "experience-heaps-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/09.heaps/module-heaps.jpg",
 "morea_sort_order"=>9,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/09.heaps/module-heaps.html",
 "content"=>
  "Heaps are a useful data structure with applications to sorting and priority queues.\n",
 "path"=>"morea//09.heaps/module-heaps.md"}
</pre>

<h2>/modules/heaps/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-heaps.md",
 "title"=>"Heapsort",
 "url"=>"/modules/heaps/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/heaps/index.html"}
</pre>

<h2>/morea/09.heaps/outcome-heaps.html</h2>

<pre>Hash
{"title"=>"Understand heaps, heapsort, and priority queues",
 "published"=>true,
 "morea_id"=>"outcome-heaps",
 "morea_type"=>"outcome",
 "morea_sort_order"=>11,
 "referencing_modules"=>[#Jekyll:Page @name="module-heaps.md"],
 "url"=>"/morea/09.heaps/outcome-heaps.html",
 "content"=>"Understand how to manipulate heaps and their benefits. \n",
 "path"=>"morea//09.heaps/outcome-heaps.md"}
</pre>

<h2>/morea/09.heaps/reading-cormen-6.html</h2>

<pre>Hash
{"title"=>"CLRS 6 - Heapsort",
 "published"=>true,
 "morea_id"=>"reading-cormen-6",
 "morea_summary"=>
  "Heapsort, heaps, maintaining the heap property, building a heap, priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "19 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/09.heaps/reading-cormen-6.html",
 "content"=>"",
 "path"=>"morea//09.heaps/reading-cormen-6.md"}
</pre>

<h2>/morea/09.heaps/reading-notes-9.html</h2>

<pre>Hash
{"title"=>"Chapter 9 Notes",
 "published"=>true,
 "morea_id"=>"reading-notes-9",
 "morea_summary"=>"Notes on heaps and heapsort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>9,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/09.heaps/reading-notes-9.html",
 "url"=>"/morea/09.heaps/reading-notes-9.html",
 "content"=>
  "# Heaps, Heapsort, and Priority Queues\n\n## Outline\n\n  1. Heaps and their Properties\n  2. Building and Maintaining Heaps\n  3. Application to Sorting\n  4. Application to Priority Queues\n\n##  Heaps and their Properties\n\nHeaps are a useful data structure with applications to sorting and priority\nqueues.\n\nThey are _nearly complete binary trees_ that satisfy a _heap property_ that\norganizes data under a partial ordering of their keys, enabling access to\nelements with maximum (or minimum) keys without having to pay the cost of\nfully sorting the keys.\n\nHeaps are not to be confused with garbage collected storage (a heap of\ngarbage)!\n\n### Heaps as Nearly Complete Binary Trees\n\nConceptually, heaps are **nearly complete binary trees**: they have leaves on\nat most two adjacent levels _l-1_ and _l_ and in which the leaves at the\nbottommost level _l_ lie in the leftmost positions of _l_:\n\n![](fig/Fig-6-1-max-heap-tree.jpg)\n\nThese quantitative properties concerning full and nearly complete binary trees\nwill be useful:\n\n#### Number of elements in nearly complete binary trees of height _h_ (6.1-1)\n\n![](fig/diagram-tree-heights-wide.jpg)\n\nAs discussed in [Topic 8](http://www2.hawaii.edu/~suthers/courses/ics311s14/No\ntes/Topic-08.html#binarytrees), a **complete binary tree** has at most 2_h_+1\n− 1 nodes (vertices). We can see this by adding up the number of elements at\neach level: 20 \\+ 21 \\+ ... + 2h for a complete binary tree of height _h_.\nThen apply formula A.5 with _x_=2 and _n_=_h_:\n\n![](fig/formula-A-5.jpg)\n\nYou get (2_h_+1 − 1) / (2 − 1) = 2_h_+1 − 1\\.\n\nSo, a _nearly_ complete binary tree has _at most_ 2_h_+1 − 1 elements (if it\nis complete, as analyzed above). The _fewest_ number of elements it can have\nat height _h_ is when the last level has just 1 element and the level before\nit is complete. So do the math for a complete binary tree of height _h_−1:\nthere are exactly 2_h_ − 1 elements in levels 1 to _l_−1 and one more element\nin the _l_th level, for a total of 2_h_ elements.\n\n#### Height of an _n_-element nearly complete binary tree (6.1-2)\n\nGiven an _n_-element nearly complete binary tree of height _h_, from 6.1-1:\n\n> 2_h_   ≤   _n_   ≤   2_h+1_ − 1   <   2_h+1_\n\nTaking the log of the first, second and last terms,\n\n> _h_   ≤   lg _n_   <   _h_ \\+ 1\n\nSince _h_ is an integer, _h_ = ⌊lg _n_⌋     _(Notice the \"floor\" notation.)_\n\n#### Number of leaves\n\nAn _n_-element nearly complete binary tree has ⌈n/2⌉ leaves.     _(Notice the\n\"ceiling\" notation. Left as exercise.)_\n\n####  Nodes of height _h_ in a nearly complete binary tree (6.3-3)\n\nThere are at most ⌈n/2h+1⌉ nodes of height _h_ in a nearly complete binary\ntree. (A proof by contradiction is possible.)\n\n### The Heap Property\n\nDepending on whether it is a _max heap_ or a _min heap_, to be a heap the\nbinary tree must also satisfy a heap property:\n\n**Max Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≥ key(_i_).  \n  \nBy induction and transitivity of ≥, the max heap property guarantees that the\nmaximum element of a max-heap is at the root.\n\n  \n**Min Heap Property:**\n  \n    For all nodes _i_, excluding the root, key(parent(_i_)) ≤ key(_i_).  \n  \nBy induction and transitivity of ≤, the min heap property guarantees that the\nminimum element of a min heap is at the root.\n\n### Array Representation\n\nHeaps are usually represented using arrays, following the mapping shown by the\nindices in the tree:\n\n![](fig/Fig-6-1-max-heap-tree-array-indices.jpg)\n![](fig/Fig-6-1-max-heap-array.jpg)  \n\nThe fact that we can see a heap both as a binary tree and as an array is an\nexample of a powerful idea in computer science: mapping between an\nimplementation representation that has efficient computational properties and\na conceptual representation that fits how we think about a problem.\n\n![](fig/code-parent-children.jpg)\n\nIf a heap is stored in array `A`, then movement in the tree is easy:\n\n  * Root of the tree is `A[1]`\n  * Parent of `A[_i_]` is `A[⌊_i_/2⌋]`     _(Notice we are taking the floor of _i_/2)_.\n  * Left Child of `A[_i_]` is `A[_2i_]`\n  * Right Child of `A[_i_]` is `A[_2i+1_]`\n  * Index operations are fast in binary (left and right shifts and setting the low order bit).\n\n#### Indices of leaves (6.1-7)\n\nBy the number of leaves fact, when an _n_-element heap is stored in the array\nrepresentation, the leaves are the nodes indexed by ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2,\n..., _n_. (Left as exercise.)\n\nThis fact will be used in algorithms that only need to process either the\nleaves or the internal nodes of a heap.\n\n* * *\n\n##  Building and Maintaining Heaps\n\n### Maintaining the Heap Property\n\nMAX-HEAPIFY is used to maintain the max-heap property by addressing a possible\nviolation at node `A[_i_]`:\n\n  * MAX-HEAPIFY assumes that the left and right subtrees of _i_ are max-heaps.\n  * When called, `A[_i_]` may (or may not) be smaller than its children, violating the max-heap property if it is.\n  * After MAX-HEAPIFY, the subtree rooted at _i_ will be a heap. \n![](fig/code-max-heapify.jpg)\n\nIt works by comparing `A[_i_]` with its left and right children (lines 3-7),\nand if necessary swapping `A[_i_]` with the larger of the two children to\npreserve the heap property (lines 8-9). _Tail recursion_ after the swap\npropagates this change until the subtree is a heap (line 10).\n\n#### Example\n\nMax-Heapify from the node at index 2 (containing 4):\n\n![](fig/Fig-6-2-max-heapify.jpg)\n\n#### Analysis\n\nIt is easy to see that the body of each call before recursion is O(1), and the\nrecursion repeats this for at most O(lg _n_) nodes on the path from the root\nto the leaves.\n\nMore formally, the worst case is when the bottom level is exactly half full,\nand in this case, the _children's subtrees_ can have at most 2_n_/3 nodes. So,\nadding the cost to recurse on these subtrees plus Θ(1) cost for comparisons at\na given node, we get the recurrence relation:\n\n> _T_(_n_)   ≤   _T_(2_n_/3) + Θ(1).\n\nThis fits case 2 of the Master Theorem (_a_ = 1, _b_ = 3/2 since 1/(3/2) =\n2/3, and _f_(_n_) = 1 = O(_n_log3/21) = O(_n_0)), giving Θ(lg _n_).\n\n### Building a Heap\n\nSuppose we load some keys into an array in arbitrary order from left to right,\ncreating an almost complete binary tree that may not satisfy the heap\nproperty.\n\nEach leaf of the corresponding tree is trivially a heap. If we call MAX-\nHEAPIFY on the parents of the leaves, the assumption that the right and left\nsubtrees are heaps is met. Once MAX-HEAPIFY returns, the parents are roots of\nheaps too, so we call it on _their_ parents.\n\nUsing the previously established result that the leaves begin in the array at\nindex ⌊n/2⌋ \\+ 1, so the last non-leaf node is at ⌊n/2⌋, the implementation is\ntrivial:\n\n![](fig/code-build-max-heap.jpg)\n\n#### Example\n\n![](fig/Fig-6-3-build-max-heap-array.jpg)\n\nLet's trace this on an array of size 10, for _i_ = 5 downto 1:\n\n![](fig/Fig-6-3-build-max-heap-ab.jpg)\n\n(a) The heap rooted at vertex or array index 5 is already a max heap: no\nchange is made.\n\n(b) The heap rooted at index 4 is not a max heap: the value 2 is smaller than\nits children. We restore the max heap property by swapping 2 with the larger\nchild key, 14 (see next figure for result). If we had swapped with 8, it would\nnot be a max heap: this is why we always swap with the larger child.\n\n![](fig/Fig-6-3-build-max-heap-cd.jpg)\n\n(c) Decrementing _i_ to 3, there is another violation of the max heap\nproperty, and we swap value 3 at index 3 with value 10 at index 7 (the larger\nchild).\n\n(d) The heap at index 2 violates the max heap property: we must propagate the\nvalue 1 down by swapping with 16, and then with 7 in a recursive call to Max-\nHeapify (see next figure).\n\n![](fig/Fig-6-3-build-max-heap-ef.jpg)\n\n(e) Finally, checking the value at index 1 (value 4) against its children, we\nfind we need to swap it with value 16 at index 2, and then with value 14 at\nindex 4 and value 8 at index 9 in two recursive calls to Max-Heapify. (f)\nshows the resulting max heap.\n\n#### Correctness\n\n![](fig/code-build-max-heap.jpg)\n\n_**Loop Invariant:**_\n\n    At the start of every iteration of the `for` loop, each node _i_+1, _i_+2, ..., _n_ is a root of a max heap.\n  \n_**Initialization:**_\n\n    By Exercise 6.1-7, each node ⌊n/2⌋ \\+ 1, ⌊n/2⌋ \\+ 2, ..., _n_ is a leaf, which is the root of a trivial max-heap. Since _i_ = ⌊n/2⌋ before the first iteration of the `for` loop, the invariant is initially true. \n  \n_**Maintenance:**_\n\n    Children of node _i_ are indexed higher than _i_, so by the loop invariant, they are both roots of max-heaps. Thus the assumption of MAX-HEAPIFY is met, enabling it to make node _i_ a max-heap root. Decrementing _i_ reestablishes the loop invariant at each iteration.\n  \n_**Termination:**_\n\n    The loop terminates when _i_ = 0. By the loop invariant, each node including the root indexed by 1 is the root of a max-heap.\n\n#### Analysis\n\nSometimes a good approach is to prove an easy bound and then tighten it.\n\nIt is easy to see that there are O(_n_) (about _n_/2) calls to MAX-HEAPIFY,\nand we already know that MAX-HEAPIFY on a tree of height O(lg _n_) is O(lg\n_n_). Therefore an upper bound is O(_n_ lg _n_).\n\nHowever, only the root node and those near it are at height O(lg _n_). Many\nnodes are close to the leaves and we don't even process half of them. So let's\ntry for a tighter bound ...\n\nThere are no more than ⌈n/2h+1⌉ nodes of height _h_ (Exercise 6.3-3), and the\nheap is ⌊lg_n_⌋ high (Exercise 6.1-2). MAX-HEAPIFY called on a node of height\n_h_ is O(_h_), so we need to sum this cost times the number of nodes at each\n_h_ for all relevant _h_:\n\n![](fig/analysis-build-max-heap-1.jpg)\n\nWe can simplify this as follows:\n\n  1. Wrap big-O around the whole thing, leaving h behind.\n  2. Remove the ceiling (which does not affect big-O analysis).\n  3. Rewrite the resulting _nh_/2_h_+1 as (_n_/2)(h/2_h_).\n  4. Move _n_/2 out of the summation, as it does not involve _h_.\n  5. Eliminate the constant 1/2, as we are inside the magical world of big-O!\n\nTricky, huh? Now maybe you can see why the text authors write that as:\n\n![](fig/analysis-build-max-heap-2.jpg)\n\nThe above summation runs up to ⌊lg_n_⌋, but we would like to use a convenient\nformula A-8, shown below, which runs up to ∞:\n\n![](fig/formula-A-8.jpg)\n\nSince big-O implies an inequality (upper bound), we can go ahead and run the\nsummation to ∞ instead of ⌊lg_n_⌋, because all of the additional terms are\npositive (and also very small), so the inequality will be maintained. Then, if\nwe let _x_ = 1/2 (since _h_/2_h_ = _h_(1_h_/2_h_) can be written as h(1/2)h),\nwe get:\n\n![](fig/analysis-build-max-heap-3.jpg)\n\nThus our big-O expression simplifies to O(_n_*2) = O(_n_), which is a tighter\nbound than O(_n_ lg _n_). The same analysis appliles to the min-heap version.\n\n(You might wonder why we can build a heap in O(_n_) time when sorting takes\nO(_n_ lg _n_), as will be proven in [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html).\nThis is because a heap is only a partial order, so less work needs to be done\nto guarantee the heap property.)\n\n* * *\n\n##  Application to Sorting\n\nSuppose `A[1.._n_]` contains keys to be sorted. If we call BUILD-MAX-HEAP on\nthis array, the maximum element will be at `A[1]`. We can swap it with the\nitem at `A[_n_]`, then repeat on `A[1.._n_-1]` (reducing the size of the heap\nby 1 each iteration) until this reaches size 1.\n\n![](fig/code-heapsort.jpg)\n\n#### Analysis:\n\nBUILD-MAX-HEAP is O(_n_) (by analysis above). The `for` loop executes _n_-1\ntimes, with O(1) exchange each iteration and a call to O(lg _n_) MAX-HEAPIFY.\nThus heapsort is O(_n_ lg _n_).\n\n#### Example:\n\nSuppose we have an array A with five integers. First, BUILD-MAX-HEAP is called\non it, resulting in the array A = [7, 4, 3, 1, 2] shown as the tree in (a)\nbelow.\n\n![](fig/Fig-6-4-heapsort-alt-ab.jpg)\n\nThen the loop of HEAPSORT successively takes out the maximum from the first\nindex by swapping it with the last element in the heap, and calls MAX-HEAPIFY.\nSo, 7 is swapped with 2, and then the heap (now one smaller) is reconstructed,\nresulting in the heap shown in (b): A = [4, 2, 3, 1, 7], with the first four\nelements being the heap.\n\n![](fig/Fig-6-4-heapsort-alt-cd.jpg)\n\nThe maximum element 4 (from b) was swapped with the minimum element 1\n(removing 4 from the heap) and the heap restored, resulting in (c) A = [3, 2,\n1, 4, 7] with the first three elements being the heap. Then in (d), the max\nelement 3 was swapped with 1 and the heap restored by percolating 1 down: A =\n[2, 1, 3, 4, 7] with the heap being the first two elements.\n\n![](fig/Fig-6-4-heapsort-alt-e.jpg)\n\n(e) Finally, the maximum element 2 is removed by swapping with the only\nremaining element 1, resulting in the sorted array shown.\n\nHere is a [playing card\ndemonstration](http://www.youtube.com/watch?v=WYII2Oau_VY) of heap sort, in\ncase it helps. This demonstration is using a _min-heap_ to sort the cards with\nthe card of _maximum_ value ending up at the top of the stack of cards.\n\n* * *\n\n##  Application to Priority Queues\n\nAn important application of heaps is implementing **priority queues**. There\nare _min_ and _max_ versions.\n\nA **max-priority queue** is an ADT with the following operations:\n\nINSERT(S,_x_)\n\n    S <- S ∪ {_x_}\n  \nMAXIMUM(S)\n\n    Returns the element of S with the largest key.\n  \nEXTRACT-MAX(S)\n\n    Removes and returns the element of S with the largest key.\n  \nINCREASE-KEY(S,_x_,_k_)\n\n    Increases the value of _x_'s key to the new value _k_ ≥ current key(_x_).\n\nA **min-priority queue** has corresponding operations MINIMUM, EXTRACT-MIN,\nand DECREASE-KEY.\n\nMax-priority queues can be used in job scheduling: the highest priority job is\nalways run next, but job priority can be increased as a job ages in the queue,\nor for other reasons.\n\nMin-priority queues will be very important in graph algorithms we cover later\nin the semester: efficient implementations of EXTRACT-MIN and DECREASE-KEY\nwill be especially important.\n\nMin-priority queues also used in event-driven simulations, where an event may\ngenerate future events, and we need to simulate the events in chronological\norder.\n\n#### Accessing Maximums\n\nIn the array representation, MAXIMUM is trival to implement in O(1) by\nreturning the first element of the array. However, if we EXTRACT-MAX we need\nto restore the heap property afterwards.\n\nHEAP-EXTRACT-MAX takes the root out, replaces it with the last element in the\nheap _(stop and think: why this element?)_, and then uses MAX-HEAPIFY to\npropagate that element (which probably has a small key) down to its proper\nplace:\n\n![](fig/code-heap-extract-max.jpg)\n\nHEAP-EXTRACT-MAX is O(lg _n_) since there is only constant work added to the\nO(lg _n_) MAX-HEAPIFY.\n\n#### Increasing keys\n\nAn increase to the key may require propagating the element _up_ the tree (the\nopposite direction as compared to MAX-HEAPIFY):\n\n![](fig/code-heap-increase-key.jpg)\n\nThis is clearly O(lg _n_) due to following a simple path up the tree. Let's\nwork this example, where the element at _i_ has its key increased from 4 to\n15, and then it is propagated up:\n\n![](fig/Fig-6-5-heap-increase-key.jpg)\n\nThis propagation follows the \"Peter Principle\": the claim that persons in a\nhierarchical organization are promoted through the ranks of management until\nthey reach their level of incompetency!!!\n\n#### Inserting New Elements\n\nWhen inserting, we are going to have to make the heap bigger, so let's add the\nelement at the end and propagate it up to where it belongs.\n\nHEAP-INCREASE-KEY already has the code for this propagation, so if we set the\nkey to the smallest possible value and then try to increase it with HEAP-\nINCREASE-KEY, it will end up in the right place:\n\n![](fig/code-max-heap-insert.jpg)\n\nAgain, this is O(lg _n_).\n\n* * *\n\n## Next\n\nIn [Topic\n10](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10.html) we\nwrap up our examination of sort algorithms with Quicksort, a practical sort\nthat performs well in practice and also illustrates the value of probabilistic\nanalysis and random algorithms.\n\nWe will return to other kinds of trees, in particular special kinds of binary\nsearch trees that are kept balanced to guarantee O(lg _n_) performance, in\n[Topic\n11](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-11.html).\n\n* * *\n\nDan Suthers Last modified: Sat Feb 15 16:37:46 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition.  \n\n",
 "path"=>"morea//09.heaps/reading-notes-9.md"}
</pre>

<h2>/morea/09.heaps/reading-screencast-9a.html</h2>

<pre>Hash
{"title"=>"Introduction to heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9a",
 "morea_summary"=>"Basic ideas about heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=0zh4IiKaVN0",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/09.heaps/reading-screencast-9a.html",
 "content"=>"",
 "path"=>"morea//09.heaps/reading-screencast-9a.md"}
</pre>

<h2>/morea/09.heaps/reading-screencast-9b.html</h2>

<pre>Hash
{"title"=>"Building heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9b",
 "morea_summary"=>"Understanding how to build heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=oAfSx7aRkZM",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/09.heaps/reading-screencast-9b.html",
 "content"=>"",
 "path"=>"morea//09.heaps/reading-screencast-9b.md"}
</pre>

<h2>/morea/09.heaps/reading-screencast-9c.html</h2>

<pre>Hash
{"title"=>"Analyzing heap building",
 "published"=>true,
 "morea_id"=>"reading-screencast-9c",
 "morea_summary"=>"Correctness and run time analysis of heaps",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gMwtzAPDupI",
 "morea_labels"=>["Screencast", "Suthers", "9 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/09.heaps/reading-screencast-9c.html",
 "content"=>"",
 "path"=>"morea//09.heaps/reading-screencast-9c.md"}
</pre>

<h2>/morea/09.heaps/reading-screencast-9d.html</h2>

<pre>Hash
{"title"=>"Applications of heaps",
 "published"=>true,
 "morea_id"=>"reading-screencast-9d",
 "morea_summary"=>"Heapsort and priority queues",
 "morea_type"=>"reading",
 "morea_sort_order"=>4,
 "morea_url"=>"https://www.youtube.com/watch?v=8O5iBigvDIw",
 "morea_labels"=>["Screencast", "Suthers", "14 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/09.heaps/reading-screencast-9d.html",
 "content"=>"",
 "path"=>"morea//09.heaps/reading-screencast-9d.md"}
</pre>

<h2>/morea/10.quicksort/experience-quicksort-2.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort (again)",
 "published"=>true,
 "morea_id"=>"experience-quicksort-2",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort (at home).",
 "morea_sort_order"=>2,
 "morea_labels"=>["Homework"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/10.quicksort/experience-quicksort-2.html",
 "url"=>"/morea/10.quicksort/experience-quicksort-2.html",
 "content"=>
  "### Quicksort\n\n#### 12 points\n\nShow the operation of Partition (not randomized) on this 1-based array:\n\n    \n    \n     A = [1, 6, 2, 8, 3, 9, 4, 7, 5], p=1, q=9 \n    \n\nand the two sub-partitions that result as directed below. In other words, you\nwill trace the three calls to Partition that are highest in the recursion\ntree. (They are _not_ the first three calls: #1 is the first call and #2 is\nthe second call, but the call marked as #3 below takes place after all the\nrecursive calls breaking down #2.)\n\nIn order to make the desired response format clear and to make it easy for the\nTA to grade, I am providing a template for your response. You are to fill in\nwherever the underscore character \"_\" appears. Use a plain text editor with\n_fixed-width font_. Be sure to fill in all fields marked with underscore: use\nsearch to make sure you get them all. I start you off with the first few\nlines: continue in the same pattern.\n\n    \n    \n     \n    **(#1) Call to Partition (A, 1, 9) made in Line 2 of the initial call to Quicksort:**\n    \n      Initially: \n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=0, j=1, pivot = A[r] = A[9] = 5 \n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=1, exchanged A[1] with A[1]\n      A = [1, 6, 2, 8, 3, 9, 4, 7, 5], i=1, j=2, no exchange \n    \n      ... you fill in the rest until the loop exits ... \n    \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=3, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=4, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=5, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=6, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=7, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=8, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does Partition(A, 1, 9) return? __\n    \n    Continuing execution of the top level call to Quicksort, identify the two\n    partitions that will be handled by the recursive calls to Quicksort at\n    this level: \n    (#2) On what subarray will Quicksort in line 3 be called? A[_, _]\n    (#3) On what subarray will Quicksort in line 4 be called? A[_, _]\n    \n    Now trace these two calls in a manner similar to above. \n    \n    **(#2) Call to Partition(A, _, _) handled in the first call to Quicksort line 3: **\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this second call to Partition return? __\n    \n    **(#3) Call to Partition(A, _, _) handled in the first call to Quicksort line 4:**\n    \n      Initially: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, pivot = A[r] = A[_] = _\n    \n      Trace at the conclusion of each pass through the loop lines 3-6\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, ___________\n    \n      After the swap in line 7: \n      A = [_, _, _, _, _, _, _, _, _], i=_, j=_, exchanged A[_] with A[_] \n    \n    What does this third call to Partition return? __\n    \n\nNot graded, but you might think about:\n\n  * What pattern do you see in the second call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What pattern do you see in the third call to Partition? Will this pattern continue in the subsequent calls to Partition in that half of the array? \n  * What do these observations tell us about the runtime of Quicksort on data organized as in these partitions? \n\n* * *\n\nDan Suthers Last modified: Wed Mar 19 23:22:39 HST 2014\n\n",
 "path"=>"morea//10.quicksort/experience-quicksort-2.md"}
</pre>

<h2>/morea/10.quicksort/experience-quicksort.html</h2>

<pre>Hash
{"title"=>"Applying your understanding of quicksort",
 "published"=>true,
 "morea_id"=>"experience-quicksort",
 "morea_type"=>"experience",
 "morea_summary"=>"Learn about quicksort.",
 "morea_sort_order"=>1,
 "morea_labels"=>["In class"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/10.quicksort/experience-quicksort.html",
 "url"=>"/morea/10.quicksort/experience-quicksort.html",
 "content"=>
  "### 1\\. Finding _i_th largest element with Partition\n\n#### 1-2 points TBD\n\nHow would you _use Quicksort's `Partition` procedure_ to write an algorithm\nfor _finding the _i_th smallest element of an unsorted array_? _Hint: what\ndoes the returned value of Partition tell you about the rank ordering of the\npivot?_\n\n  * Describe the strategy in English\n  * Then if you have time after finishing the next question, write pseudocode for an algorithm.\n\n### 2\\. Calls to Partition in Worst and Best Case\n\n#### 3-4 points TBD\n\nWhen we measure runtime efficiency in terms of _number of comparisons_ to be\nmade, Quicksort is Θ(_n_2) in the worst case (when the pivot is always chosen\nto be the smallest or largest element), and Θ(_n_ lg _n_) in the best case\n(when the pivot is always the median key). But we might also try to _measure\nefficiency in terms of number of calls to `Partition`_, since all the work is\ndone in there.\n\n**(a)**   Asymptotically, _how many calls to `Partition` are made in the **worst case runtime**_ as defined above (when the pivot is always chosen to be the smallest or largest element)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\n**(b)**   Asymptotically, _how many calls to `Partition` are made in the **best case runtime**_ as defined above (when the pivot is always the median key)? Answer with Θ(_f_(_n_)), where your job is to identify _f_.\n\nArgue for your conclusions!! _(One approach is to write and solve recurrence\nrelations. Another approach is to notice that the Quicksort recursion trees\nare binary, and use quantitative facts about binary trees. For a big-O rather\nthan Θ reply, a simple counting argument based on the pseudocode is possible,\nbut not as rigorous.)_\n\n![](fig/pseudocode-quicksort.jpg) ![](fig/pseudocode-quicksort-partition.jpg)\n\n\n",
 "path"=>"morea//10.quicksort/experience-quicksort.md"}
</pre>

<h2>/morea/10.quicksort/module-quicksort.html</h2>

<pre>Hash
{"title"=>"Quicksort",
 "published"=>true,
 "morea_id"=>"quicksort",
 "morea_outcomes"=>["outcome-quicksort"],
 "morea_readings"=>
  ["reading-screencast-10a",
   "reading-screencast-10b",
   "reading-screencast-10c",
   "reading-cormen-7",
   "reading-cormen-8",
   "reading-notes-10"],
 "morea_experiences"=>["experience-quicksort", "experience-quicksort-2"],
 "morea_type"=>"module",
 "morea_icon_url"=>"/morea/10.quicksort/module-quicksort.png",
 "morea_sort_order"=>9,
 "referencing_modules"=>[],
 "morea_assessments"=>[],
 "module_page"=>#Jekyll:Page @name="index.html",
 "url"=>"/morea/10.quicksort/module-quicksort.html",
 "content"=>
  "Quicksort, like Mergesort, takes a divide and conquer approach, but on a different basis.\n",
 "path"=>"morea//10.quicksort/module-quicksort.md"}
</pre>

<h2>/modules/quicksort/index.html</h2>

<pre>Hash
{"layout"=>"morea",
 "morea_page"=>#Jekyll:Page @name="module-quicksort.md",
 "title"=>"Quicksort",
 "url"=>"/modules/quicksort/index.html",
 "content"=>
  "<div class=\"container\">\n  <h1><small>Module: </small> {{ page.morea_page.title }} </h1>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n    <h2>Learning outcomes</h2>\n\n    {% if page.morea_page.morea_outcomes.size == 0 %}\n    <p>No outcomes for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_outcomes %}\n      {% assign outcome = site.morea_page_table[page_id] %}\n      <h3>Outcome {{ forloop.index }}: {{ outcome.title }}</h3>\n      {{ outcome.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n  <h2>Readings and other resources</h2>\n\n    {% if page.morea_page.morea_readings.size == 0 %}\n    <p>No readings for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_readings %}\n        {% assign reading = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ reading.morea_url }}\">{{ reading.title }}</a></h4>\n            {{ reading.morea_summary | markdownify }}\n            <p>\n              {% for label in reading.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"light-gray-background\">\n  <div class=\"container\">\n  <h2>Experiential Learning</h2>\n\n    {% if page.morea_page.morea_experiences.size == 0 %}\n    <p>No experiences for this module.</p>\n    {% endif %}\n\n    <div class=\"row\">\n      {% for page_id in page.morea_page.morea_experiences %}\n        {% assign experience = site.morea_page_table[page_id] %}\n        <div class=\"col-sm-3\">\n          <div class=\"thumbnail\">\n            <h4><a href=\"{{ experience.morea_url }}\">{{ experience.title }}</a></h4>\n            {{ experience.morea_summary | markdownify }}\n            <p>\n              {% for label in experience.morea_labels %}\n              <span class=\"badge\">{{ label }}</span>\n              {% endfor %}\n            </p>\n          </div>\n        </div>\n      {% if forloop.index == 4 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 8 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 12 %}\n       </div><div class=\"row\">\n      {% endif %}\n      {% if forloop.index == 16 %}\n      </div><div class=\"row\">\n      {% endif %}\n      {% endfor %}\n\n    </div>\n  </div>\n</div>\n\n<div class=\"white-background\">\n  <div class=\"container\">\n    <h2>Assessment</h2>\n\n    {% if page.morea_page.morea_assessments.size == 0 %}\n    <p>No assessments for this module.</p>\n    {% endif %}\n\n    {% for page_id in page.morea_page.morea_assessments %}\n      {% assign assessment = site.morea_page_table[page_id] %}\n      <h3>Assessment {{ forloop.index }}: {{ assessment.title }}</h3>\n      {{ assessment.content | markdownify }}\n    {% endfor %}\n\n  </div>\n</div>\n\n\n\n",
 "path"=>"modules/quicksort/index.html"}
</pre>

<h2>/morea/10.quicksort/outcome-quicksort.html</h2>

<pre>Hash
{"title"=>"Understand quicksort",
 "published"=>true,
 "morea_id"=>"outcome-quicksort",
 "morea_type"=>"outcome",
 "morea_sort_order"=>12,
 "referencing_modules"=>[#Jekyll:Page @name="module-quicksort.md"],
 "url"=>"/morea/10.quicksort/outcome-quicksort.html",
 "content"=>
  "Understand the quicksort algorithm and how it differs from mergesort.\n",
 "path"=>"morea//10.quicksort/outcome-quicksort.md"}
</pre>

<h2>/morea/10.quicksort/reading-cormen-7.html</h2>

<pre>Hash
{"title"=>"CLRS 7 - Quicksort",
 "published"=>true,
 "morea_id"=>"reading-cormen-7",
 "morea_summary"=>
  "Description and performance of quicksort, a randomized version, and analysis.",
 "morea_type"=>"reading",
 "morea_sort_order"=>5,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "20 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/10.quicksort/reading-cormen-7.html",
 "content"=>"",
 "path"=>"morea//10.quicksort/reading-cormen-7.md"}
</pre>

<h2>/morea/10.quicksort/reading-cormen-8.html</h2>

<pre>Hash
{"title"=>"CLRS 8 - Sorting in linear time",
 "published"=>true,
 "morea_id"=>"reading-cormen-8",
 "morea_summary"=>
  "Lower bounds for sorting, counting sort, radix sort, bucket sort.",
 "morea_type"=>"reading",
 "morea_sort_order"=>6,
 "morea_url"=>"http://mitpress.mit.edu/books/introduction-algorithms",
 "morea_labels"=>["Textbook", "22 pages"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/10.quicksort/reading-cormen-8.html",
 "content"=>"",
 "path"=>"morea//10.quicksort/reading-cormen-8.md"}
</pre>

<h2>/morea/10.quicksort/reading-notes-10.html</h2>

<pre>Hash
{"title"=>"Notes on Quicksort",
 "published"=>true,
 "morea_id"=>"reading-notes-10",
 "morea_summary"=>"Notes on quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>10,
 "morea_labels"=>["Notes"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "morea_url"=>"/ics311s14/morea/10.quicksort/reading-notes-10.html",
 "url"=>"/morea/10.quicksort/reading-notes-10.html",
 "content"=>
  "## Notes on quicksort\n\n  1. Quicksort \n  2. Analysis of Quicksort \n  3. Lower Bound for Comparison Sorts \n  4. O(n) Sorts (briefly)\n\n### Motivations\n\nQuicksort, like Mergesort, takes a divide and conquer approach, but on a\ndifferent basis.\n\nIf we have done two comparisons among three keys and find that _x_ < _p_ and\n_p_ < _y_, do we ever need to compare _x_ to _y_? Where do the three belong\nrelative to each other in the sorted array?\n\nQuicksort uses this idea to partition the set of keys to be sorted into those\nless than the pivot _p_ and those greater than the pivot. (It can be\ngeneralized to allow keys equal to the pivot.) It then recurses on the two\npartitions.\n\n![](fig/quicksort-recursion.jpg)\n\nCompare this to Mergesort.\n\n  * Both take a recursive divide-and-conquer approach.\n  * Mergesort does its work on the way back up the recursion tree (merging), while Quicksort does its work on the way down the recursion tree (partitioning).\n  * Mergesort always partitions in half; for Quicksort the size of the partitions depends on the pivot (this results in Θ(_n_2) worst case behavior, but expected case remains Θ(_n_ lg _n_).\n  * Mergesort requires axillary arrays to copy the data; while as we shall see Quicksort can operate entirely within the given array: it is an **in-place sort**.\n\nQuicksort performs well in practice, and is one of the most widely used sorts\ntoday.\n\n### The Quicksort Algorithm\n\nTo sort any subarray A[_p_ .. _r_],   _p_ < _r_:\n\n**_Divide:_**\n    Partition A[_p_ .. _r_] into two (possibly empty) subarrays \n\n  * A[_p_ .. _q-1_], where every element is ≤ A[_q_]\n  * A[_q + 1_ .. _r_], where A[_q_] ≤ every element\n**_Conquer:_**\n    Sort the two subarrays by recursive calls\n**_Combine:_**\n    No work is needed to combine: all subarrays (including the entire array) are sorted as soon as recursion ends.\n\nAn array is sorted with a call to `QUICKSORT(A, 1, A.length)`:\n\n![](fig/pseudocode-quicksort.jpg)\n\nThe work is done in the PARTITION procedure. A[_r_] will be the pivot. (Note\nthat the _end_ element of the array is taken as the pivot. Given random data,\nthe choice of the position of the pivot is arbitrary; working with an end\nelement simplifies the code):\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nPARTITION maintains four regions.\n\n![](fig/Fig-7-2-partition-regions.jpg)\n\nThree of these are described by the following loop invariants, and the fourth\n(A[_j_ .. _r_-1]) consists of elements that not yet been examined:\n\n> **Loop Invariant:**\n\n>\n\n>   1. All entries in A[_p_ .. _i_] are ≤ pivot.\n\n>   2. All entries in A[_i_+1 .. _j_-1] are > pivot.\n\n>   3. A[_r_] = pivot.\n\n### Example Trace\n\nIt is worth taking some time to trace through and explain each step of this\nexample of the PARTITION procedure, paying particular attention to the\nmovement of the dark lines representing partition boundaries.\n\n![](fig/pseudocode-quicksort-partition.jpg) \n![](fig/quicksort-trace-1.jpg)\n\nContinuing ...\n\n![](fig/quicksort-trace-2.jpg)\n\nHere is the [Hungarian Dance version of\nquicksort](http://www.youtube.com/watch?v=kDgvnbUIqT4), in case that helps to\nmake sense of it!\n\n### Correctness\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nHere use the loop invariant to show correctness:\n\n  1. All entries in A[_p_ .. _i_] are ≤ pivot.\n  2. All entries in A[_i_+1 .. _j_ −1] are > pivot.\n  3. A[_r_] = pivot. \n\n**_Initialization:_**\n    Before the loop starts, _x_ is assigned the pivot A[_r_] (satisfying condition 3), and the subarrays a[_p_ .. _i_] and A[_i_+1 .. _j_−1] are empty (trivially satisfying conditions 1 and 2). \n**_Maintenance:_**\n    While the loop is running, \n\n  * if A[_j_] ≤ pivot, then _i_ is incremented, A[_j_] and A[_i_] are swapped, and _j_ is incremented. Because of the swap, A[_i_] ≤ _x_ for condition 1. The item swapped into A[_j_-1] > _x_ by the loop invariant, for condition 2.\n  * If A[_j_] > pivot, then _j_ is incremented, sustaining condition 2 (the others are unchanged), as the element added was larger\n**_Termination:_**\n    The loop terminates when _j_=_r_, so all elements in A are partitioned into one of three cases: A[_p_ .. _i_] ≤ pivot, A[_i_+1 .. _r_-1] > pivot, and A[_r_] = pivot. The last two lines fix the placement of A[_r_] by moving it between the two subarrays.\n\n* * *\n\n##  Informal Analysis\n\n![](fig/pseudocode-quicksort-partition.jpg)\n\nThe formal analysis will be done on a randomized version of Quicksort. This\ninformal analysis helps to motivate that randomization.\n\nFirst, PARTITION is Θ(_n_): We can easily see that its only component that\ngrows with _n_ is the `for` loop that iterates proportional to the number of\nelements in the subarray).\n\nThe runtime depends on the partitioning of the subarrays:\n\n### Worst Case\n\nThe worst case occurs when the subarrays are completely unbalanced, i.e.,\nthere are 0 elements in one subarray and _n_-1 elements in the other subarray\n(the single pivot is not processed in recursive calls). This gives a familiar\nrecurrence (compare to that for insertion sort):\n\n![](fig/analysis-quicksort-worst-recurrence.jpg)\n\nOne example of data that leads to this behavior is when the data is already\nsorted: the pivot is always the maximum element, so we get partitions of size\n_n_−1 and 0 each time. Thus, _quicksort is O(_n_2) on sorted data_. Insertion\nsort actually does better on a sorted array! (O(_n_))\n\n### Best Case\n\nThe best case occurs when the subarrays are completely balanced (the pivot is\nthe median value): subarrays have about _n_/2 elements. The reucurrence is\nalso familiar (compare to that for merge sort):\n\n![](fig/analysis-quicksort-best-recurrence.jpg)\n\n### Effect of Unbalanced Partitioning\n\nIt turns out that expected behavior is closer to the best case than the worst\ncase. Two examples suggest why expected case won't be that bad.\n\n#### Example: 1-to-9 split\n\nSuppose each call splits the data into 1/10 and 9/10. This is highly\nunbalanced: won't it result in horrible performance?\n\n![](fig/Fig-7-4-quicksort-1-9-recursion-tree.jpg)\n\nWe have log10_n_ full levels and log10/9_n_ levels that are nonempty.\n\nAs long as it's constant, the base of the log does not affect asymptotic\nresults. Any split of constant proportionality will yield a recursion tree of\ndepth Θ(lg _n_). In particular (using ≈ to indicate truncation of low order\ndigits),\n\n> log10/9_n_ = (log2_n_) / (log210/9)     _by formula 3.15_  \n            ≈ (log2_n_) / 0.152   \n            = 1/0.152 (log2_n_)  \n            ≈ 6.5788 (log2_n_)  \n            = Θ(lg _n_), where _c_ = 6.5788. \n\nSo the recurrence and its solution is:\n\n![](fig/analysis-quicksort-9-1-recurrence.jpg)\n\nA general lesson that might be taken from this: sometimes, even very\nunbalanced divide and conquer can be useful.\n\n#### Example: extreme cases cancel out\n\nWith random data there will usually be a mix of good and bad splits throughout\nthe recursion tree.\n\nA mixture of worst case and best case splits is asymptotically the same as\nbest case:\n\n![](fig/Fig-7-5-quicksort-unbalanced-splits.jpg)\n\nBoth these trees have the same two leaves. The extra level on the left hand\nside only increases the height by a factor of 2, and this constant disappears\nin the Θ analysis.\n\nBoth result in O(_n_ lg _n_), though with a larger constant for the left.\n\n* * *\n\n##  Randomized Quicksort\n\n![](fig/no-badguy.jpg)\n\nWe expect good average case behavior if all input permutations are equally\nlikely, but what if it is not?\n\nTo get better performance on sorted or nearly sorted data -- and to foil our\nadversary! -- we can randomize the algorithm to get the same effect as if the\ninput data were random.\n\nInstead of explicitly permuting the input data (which is expensive),\nrandomization can be accomplished trivially by **random sampling** of one of\nthe array elements as the pivot.\n\nIf we swap the selected item with the last element, the existing PARTITION\nprocedure applies:\n\n![](fig/pseudocode-randomized-quicksort.jpg)  \n![](fig/pseudocode-randomized-partition.jpg)\n\nNow, even an already sorted array will give us average behavior.\n\n_Curses! Foiled again!_\n\n* * *\n\n##  Quicksort Analysis\n\nThe analysis assumes that all elements are unique, but with some work can be\ngeneralized to remove this assumption (Problem 7-2 in the text).\n\n### Worst Case\n\nThe previous analysis was pretty convincing, but was based on an assumption\nabout the worst case. This analysis proves that our selection of the worst\ncase was correct, and also shows something interesting: we can solve a\nrecurrence relation with a \"max\" term in it!\n\nPARTITION produces two subproblems, totaling size _n_-1. Suppose the partition\ntakes place at index _q_. The recurrence for the worst case always selects the\nmaximum cost among all possible ways of splitting the array (i.e., it always\npicks the worst possible _q_):\n\n![](fig/analysis-quicksort-worst-1.jpg)\n\nBased on the informal analysis, we guess T(_n_) ≤ _cn_2 for some _c_.\nSubstitute this guess into the recurrence:\n\n![](fig/analysis-quicksort-worst-2.jpg)\n\nThe maximum value of _q_2 \\+ (_n_ \\- _q_ \\- 1)2 occurs when _q_ is either 0 or\n_n_-1 (the second derivative is positive), and has value (_n_ \\- 1)2 in either\ncase:\n\n![](fig/analysis-quicksort-worst-3.jpg)\n\nSubstituting this back into the reucrrence:\n\n![](fig/analysis-quicksort-worst-4.jpg)\n\nWe can pick _c_ so that _c_(2_n_ \\- 1) dominates Θ(_n_). Therefore, the worst\ncase running time is O(_n_2).\n\nOne can also show that the recurrence is Ω(_n_2), so worst case is Θ(_n_2).\n\n### Average (Expected) Case\n\nWith a randomized algorithm, expected case analysis is much more informative\nthan worst-case analysis.\n_[Why?](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-10/why-\nexpected.txt)_\n\nThis analysis nicely demonstrates the use of indicator variables and two\nuseful strategies.\n\n#### Setup\n\nThe dominant cost of the algorithm is partitioning. PARTITION removes the\npivot element from future consideration, so is called at most _n_ times.\n\nQUICKSORT recurses on the partitions. The amount of work in each call is a\nconstant plus the work done in the `for` loop. We can count the number of\nexecutions of the `for` loop by counting the number of comparisons performed\nin the loop.\n\nRather than counting the number of comparisons in each call to QUICKSORT, it\nis easier to derive a bound on the number of comparisons across the entire\nexecution.\n\nThis is an example of a strategy that is often useful: **if it is hard to\ncount one way** (e.g., \"locally\"), **then count another way** (e.g.,\n\"globally\").\n\nLet _X_ be the total number of comparisons in all calls to PARTITION. The\ntotal work done over the entire execution is O(_n_ \\+ _X_), since QUICKSORT\ndoes constant work setting up _n_ calls to PARTITION, and the work in\nPARTITION is proportional to _X_. But what is _X_?\n\n#### Counting comparisons\n\nFor ease of analysis,\n\n  * Call the elements of A _z_1, _z_2, ... _z__n_, with _z__i_ being the _i_th smallest element. \n  * Define the set Z_ij_ = {_z__i_, _z__i_ \\+ 1, ... _z__j_} to be the set of elements between _z__i_ and _z__j_ inclusive. \n\nWe want to count the number of comparisons. Each pair of elements is compared\nat most once, because elements are compared only to the pivot element and then\nthe pivot element is never in any later call to PARTITION.\n\nIndicator variables can be used to count the comparisons. (Recall that we are\ncounting across all calls, not just during one partition.)\n\n> Let _Xij_ = I{ _zi_ is compared to _zj_ }\n\nSince each pair is compared at most once, the total number of comparisons is:\n\n![](fig/analysis-quicksort-expected-1.jpg)\n\nTaking the expectation of both sides, using linearity of expectation, and\napplying Lemma 5.1 (which relates expected values to probabilities):\n\n![](fig/lemming.jpg) ![](fig/analysis-quicksort-expected-2.jpg)\n\n#### Probability of comparisons\n\nWhat's the probability of comparing _z_i to _z_j?\n\nHere we apply another useful strategy: **if it's hard to determine when\nsomething happens, think about when it does _ not_ happen**.\n\nElements (keys) in separate partitions will not be compared. If we have done\ntwo comparisons among three elements and find that _zi_ < _x_ <_zj_, we do not\nneed to compare _zi_ to _zj_ (no further information is gained), and QUICKSORT\nmakes sure we do not by putting _zi_ and _zj_ in different partitions.\n\nOn the other hand, if either _zi_ or _zj_ is chosen as the pivot before any\nother element in Z_ij_, then that element (as the pivot) will be compared to\n_all_ of the elements of Z_ij_ except itself.\n\n  * The probability that _zi_ is compared to _zj_ is the probability that either is the first element chosen.\n  * Since there are _j_ \\- _i_ \\+ 1 elements in Z_ij_, and pivots are chosen randomly and independently, the probability that any one of them is chosen first is 1/(_j_ \\- _i_ \\+ 1). \n\nTherefore (using the fact that these are mutually exclusive events):\n\n![](fig/analysis-quicksort-expected-3.jpg)\n\nWe can now substitute this probability into the analyis of E[_X_] above and\ncontinue it:\n\n![](fig/analysis-quicksort-expected-4.jpg)\n\nThis is solved by applying equation A.7 for harmonic series, which we can\nmatch by substituting _k_ = _j_ \\- _i_ and shifting the summation indices down\n_i_:\n\n![](fig/analysis-quicksort-expected-5.jpg)\n\nWe can get rid of that pesky \"+ 1\" in the denominator by dropping it and\nswitching to inequality (after all, this is an upper bound analysis), and now\nA7 (shown in box) applies:\n\n![](fig/A7-Harmonic-Series.jpg) ![](fig/analysis-quicksort-expected-6.jpg)\n\nAbove we used the fact that logs of different bases (e.g., ln _n_ and lg _n_)\ngrow the same asymptotically.\n\nTo recap, we started by noting that the total cost is O(_n_ \\+ _X_) where _X_\nis the number of comparisons, and we have just shown that _X_ = O(_n_ lg _n_).\n\nTherefore, the _average running time of QUICKSORT on uniformly distributed\npermutations (random data)_ and the _expected running time of randomized\nQUICKSORT_ are both O(_n_ \\+ _n_ lg _n_) = **O(_n_ lg _n_)**.\n\nThis is the same growth rate as merge sort and heap sort. _Empirical studies\nshow quicksort to be a very efficient sort in practice (better than the other\n_n_ lg _n_ sorts) whenever data is not already ordered._ (When it is nearly\nordered, such as only one item being out of order, insertion sort is a good\nchoice.)\n\n* * *\n\n##  Lower Bound for Comparison Sorts\n\nWe have been studying sorts in which the only operation that is used to gain\ninformation is pairwise comparisons between elements. So far, we have not\nfound a sort faster than O(_n_ lg _n_).\n\nIt turns out it is not possible to give a better guarantee than O(_n_ lg _n_)\nin a comparison sort.\n\nThe proof is an example of a different level of analysis: of all _possible_\nalgorithms of a given type for a problem, rather than particular algorithms\n... pretty powerful.\n\n### Decision Tree Model\n\nA decision tree abstracts the structure of a comparison sort. A given tree\nrepresents the comparisons made by a specific sorting algorithm on inputs of a\ngiven size. Everything else is abstracted, and we count only comparisons.\n\n#### Example Decision Tree\n\nFor example, here is a decision tree for insertion sort on 3 elements.\n\n![](fig/decision-tree-insertion-sort.jpg)\n\nEach internal node represents a branch in the algorithm based on the\ninformation it determines by comparing between elements indexed by their\noriginal positions. For example, at the nodes labeled \"2:3\" we are comparing\nthe item that was originally at position 2 with the item originally at\nposition 3, although they may now be in different positions.\n\nLeaves represent permutations that result. For example, \"⟨2,3,1⟩\" is the\npermutation where the first element in the input was the largest and the third\nelement was the second largest.\n\nThis is just an example of one tree for one sort algorithm on 3 elements. Any\ngiven comparison sort has one tree for each _n_. The tree models all possible\nexecution traces for that algorithm on that input size: a path from the root\nto a leaf is one computation.\n\n#### Reasoning over All Possible Decision Trees\n\nWe don't have to know the specific structure of the trees to do the following\nproof. We don't even have to specify the algorithm(s): the proof works for any\nalgorithm that sorts by comparing pairs of keys. We don't need to know what\nthese comparisons are. Here is why:\n\n  * The root of the tree represents the unpermuted input data.\n  * The leaves of the tree represent the possible permuted (sorted) results.\n  * The branch at each internal node of the tree represents the outcome of a comparision that changes the state of the computation. \n  * The paths from the root to the leaves represent possible courses that the computation can take: to get from the unsorted data at the root to the sorted result at a leaf, the algorithm must traverse a path from the root to the correct leaf by making a series of comparisons (and permuting the elements as needed) \n  * The length of this path is the runtime of the algorithm on the given data.\n  * Therefore, if we can derive a lower bound on the height of _any_ such tree, we have a lower bound on the running time _any_ comparison sort algorithm. \n\n### Proof of Lower Bound\n\nWe get our result by showing that the number of leaves for a tree of input\nsize _n_ implies that the tree must have minimum height O(_n_ lg _n_). This\nwill be a lower bound on the running time of _any_ comparison sort algorithm.\n\n  * There are at least _n_! leaves because every permutation appears at least once (the algorithm must correctly sort every possible permutation): _l_ ≥ _n_! \n  * Any binary tree of height _h_ has _l_ ≤ 2_h_ leaves ([Notes #8](http://www2.hawaii.edu/~suthers/courses/ics311s14/Notes/Topic-08.html))\n  * Putting these facts together:   _n_! ≤ _l_ ≤ 2_h_   or   2_h_ ≥ _n_!\n  * Taking logs:   _h_ ≥ lg(_n_!) \n  * Using Sterling's approximation (formula 3.17):   _n_! > (_n_/_e_)_n_\n  * Substituting into the inequality: \n\n> _h_   ≥   lg(_n_/_e_)_n_  \n    =   _n_ lg(_n_/_e_)  \n    =   _n_ lg _n_ \\- _n_ lg _e_   \n    =   Ω (_n_ lg _n_). \n\nThus, the height of a decision tree that permutes _n_ elements to all possible\npermutations cannot be less than _n_ lg _n_.\n\nA path from the leaf to the root in the decision tree corresponds to a\nsequence of comparisons, so there will always be some input that requires at\nleast O(_n_ lg _n_) comparisions in _any_ comparision based sort.\n\nThere may be some specific paths from the root to a leaf that are shorter. For\nexample, when insertion sort is given sorted data it follows an O(_n_) path.\nBut to give an o(_n_ lg _n_) guarantee (i.e, strictly better than O(_n_ lg\n_n_)), one must show that _ all_ paths are shorter than O(_n_ lg _n_), or that\nthe tree height is o(_n_ lg _n_) and we have just shown that this is\nimpossible since it is Ω(_n_ lg _n_).\n\n* * *\n\n##  O(n) Sorts\n\nUnder some conditions it is possible to sort data without comparing two\nelements to each other. If we know something about the structure of the data\nwe can sometimes achieve O(n) sorting. Typically these algorithms work by\nusing information about the keys themselves to put them \"in their place\"\nwithout comparisons. We only introduce these algorithms very briefly so you\nare aware that they exist.\n\n### Counting Sort\n\nAssumes (requires) that keys to be sorted are integers in {0, 1, ... _k_}.\n\nFor each element in the input, determines how many elements are less than that\ninput.\n\nThen we can place the element directly in a position that leaves room for the\nelements below it.\n\n![](fig/pseudocode-counting-sort.jpg)\n\nAn example ...\n\n![](fig/Fig-8-2-counting-sort-trace.jpg)\n\nCounting sort is a **stable sort**, meaning that two elements that are equal\nunder their key will stay in the same order as they were in the original\nsequence. This is a useful property ...\n\nCounting sort requires Θ(_n_ \\+ _k_). Since _k_ is constant in practice, this\nis Θ(_n_).\n\n### Radix Sort\n\n![](fig/320px-Punch_card_sorter.JPG)\n\nUsing a stable sort like counting sort, we can sort from least to most\nsignificant digit:\n\n![](fig/Fig-8-3-radix-sort-trace.jpg)\n\nThis is how punched card sorters used to work. _ (When I was an undergraduate\nstudent my University still had punched cards, and we had to do an assignment\nusing them mainly so that we would appreciate not having to use them!)_\n\nThe code is trivial, but requires a stable sort and only works on _n_ _d_-\ndigit numbers in which each digit can take up to _k_ possible values:\n\n![](fig/pseudocode-radix-sort.jpg)\n\nIf the stable sort used is Θ(_n_ \\+ _k_) time (like counting sort) then RADIX-\nSORT is Θ(_d_(_n_ \\+ _k_)) time.\n\n### Bucket Sort\n\nThis one is reminiscent of hashing with chaining.\n\nIt maps the keys to the interval [0, 1), placing each of the _n_ input\nelements into one of _n_-1 buckets. If there are collisions, chaining (linked\nlists) are used.\n\nThen it sorts the chains before concatenating them.\n\nIt assumes that the input is from a random distribution, so that the chains\nare expected to be short (bounded by constant length).\n\n![](fig/pseudocode-bucket-sort.jpg)\n\n#### Example:\n\nThe numbers in the input array A are thrown into the buckets in B according to\ntheir magnitude. For example, 0.78 is put into bucket 7, which is for keys 0.7\n≤ _k_ < 0.8. Later on, 0.72 maps to the same bucket: like chaining in hash\ntables, we \"push\" it onto the beginning of the linked list.\n\n![](fig/Fig-8-4-bucket-sort-trace.jpg)\n\nAt the end, we sort the lists (B shows the lists after they are sorted;\notherwise we would have 0.23, 0.21, 0.26) and then copy the values from the\nlists back into an array.\n\nBut sorting linked lists is awkward, and I am not sure why CLRS's pseudocode\nand figure imply that one does this. In an alternate implementation, steps 7-9\ncan be done simultaneously: scan each linked list in order, inserting the\nvalues into the array and keeping track of the next free position. Insert the\nnext value at this position and then scan back to find where it belongs,\nswapping if needed as in insertion sort.\n\nSince the values are already partially sorted, an insertion procedure won't\nhave to scan back very far. For example, suppose 0.78 had been inserted after\n0.72. The insertion would only have to scan over one item to put 0.78 in its\nplace, as all values in lists 0..6 are smaller.\n\n* * *\n\n## Comparing the Sorts\n\n![](fig/comparing-sorts.jpg)\n\nYou can also compare some of the sorts with these animations (set to 50\nelements): <http://www.sorting-algorithms.com/>. Do the algorithms make more\nsense now?\n\n* * *\n\n## Next\n\nWe return to the study of trees, with balanced trees.\n\n* * *\n\nDan Suthers Last modified: Wed Feb 19 02:14:38 HST 2014  \nImages are from the instructor's material for Cormen et al. Introduction to\nAlgorithms, Third Edition, and from Wikipedia commons.  \n\n",
 "path"=>"morea//10.quicksort/reading-notes-10.md"}
</pre>

<h2>/morea/10.quicksort/reading-screencast-10a.html</h2>

<pre>Hash
{"title"=>"Introduction to quicksort",
 "published"=>true,
 "morea_id"=>"reading-screencast-10a",
 "morea_summary"=>"Basic ideas about quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>1,
 "morea_url"=>"https://www.youtube.com/watch?v=v1ghdc_hwMI",
 "morea_labels"=>["Screencast", "Suthers", "24 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/10.quicksort/reading-screencast-10a.html",
 "content"=>"",
 "path"=>"morea//10.quicksort/reading-screencast-10a.md"}
</pre>

<h2>/morea/10.quicksort/reading-screencast-10b.html</h2>

<pre>Hash
{"title"=>"Quicksort: Randomization and analysis",
 "published"=>true,
 "morea_id"=>"reading-screencast-10b",
 "morea_summary"=>"Randomizing and analyzing quicksort",
 "morea_type"=>"reading",
 "morea_sort_order"=>2,
 "morea_url"=>"https://www.youtube.com/watch?v=qS9oMz4_kTU",
 "morea_labels"=>["Screencast", "Suthers", "23 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/10.quicksort/reading-screencast-10b.html",
 "content"=>"",
 "path"=>"morea//10.quicksort/reading-screencast-10b.md"}
</pre>

<h2>/morea/10.quicksort/reading-screencast-10c.html</h2>

<pre>Hash
{"title"=>"Bounds on sorting",
 "published"=>true,
 "morea_id"=>"reading-screencast-10c",
 "morea_summary"=>"Lower bounds on comparison sorts, and O(n) sorts",
 "morea_type"=>"reading",
 "morea_sort_order"=>3,
 "morea_url"=>"https://www.youtube.com/watch?v=gZmEYyqHefk",
 "morea_labels"=>["Screencast", "Suthers", "20 min"],
 "referencing_modules"=>[],
 "layout"=>"morea",
 "topdiv"=>"container",
 "url"=>"/morea/10.quicksort/reading-screencast-10c.html",
 "content"=>"",
 "path"=>"morea//10.quicksort/reading-screencast-10c.md"}
</pre>

<h2>/morea//footer.html</h2>

<pre>Hash
{"title"=>"Footer",
 "morea_id"=>"footer",
 "morea_type"=>"footer",
 "referencing_modules"=>[],
 "url"=>"/morea//footer.html",
 "content"=>
  "Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>\nsuthers@hawaii.edu\n\n",
 "path"=>"morea//footer.md"}
</pre>

<h2>/morea//home.html</h2>

<pre>Hash
{"title"=>"Home",
 "morea_id"=>"home",
 "morea_type"=>"home",
 "referencing_modules"=>[],
 "url"=>"/morea//home.html",
 "content"=>
  "## Welcome to ICS 311, Spring 2014\n\n**If you are an ICS 311, Spring 2014 student, do not use this site.**  Instead, refer to the [real site](http://www2.hawaii.edu/~suthers/courses/ics311s14/index.html).\n\nThis is a subset of the course material for ICS 311 Algorithms, Spring 2014, created to illustrate the use of the \nMorea Framework.\n\nGenerally the course is run within\n[Laulima](https://laulima.hawaii.edu/portal/site/MAN.82792.201430). Notes\nand other material are posted on this site.  However, course\nparticipants will need to log into Laulima regularly to watch\nscreencasts, take quizzes, submit assignments, and to use other\nfacilities such as discussions and the mail tool as needed. \n\nClasses are in [Webster 101](http://www2.hawaii.edu/~suthers/courses/ics311s14/map.jpg).",
 "path"=>"morea//home.md"}
</pre>

</div>



<div class="dark-blue-background">
<footer>
  <div class="container page-footer">
    
      <p>Daniel Suthers | Information and Computer Sciences | University of Hawaii <br>
suthers@hawaii.edu</p>

    
    <p style="margin: 0">Powered by the <a style="color: white" href="http://morea-framework.github.io/">Morea Framework</a><br>
       Last update on: <span>2014-04-15 17:25:58 -1000</span></p>
  </div>
</footer>
</div>
</body>
</html>
